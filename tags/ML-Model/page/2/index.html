<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: ML Model - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="follow_it-verification-code" content="SVBypAPPHxjjr7Y4hHfn"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">ML Model</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">9 minutes read (About 1389 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/long_short_term_memory.html">Long Short Term Memory</a></h1><div class="content"><h2 id="Long-Short-Term-Memory"><a href="#Long-Short-Term-Memory" class="headerlink" title="Long Short Term Memory"></a>Long Short Term Memory</h2><p><em>LSTM</em>：通过刻意设计、默认可以学习长期依赖信息的RNN网络</p>
<p><img src="/imgs/lstm_flow_along_time.png" alt="lstm_flow_along_time">
<img src="/imgs/lstm_flow_notations.png" alt="lstm_flow_notions"></p>
<ul>
<li><p>LSTM中每个重复的模块（层）称为细胞</p>
<ul>
<li>细胞结构经过特殊设计，相较于准RNN简单细胞结构能较好
保留长时信息</li>
</ul>
</li>
<li><p>多个LSTM细胞可以组成<em>block</em>，其中细胞<strong>门权值共享</strong></p>
<ul>
<li>block中各个细胞状态不同</li>
<li>这个是同时刻、不同层的真权值共享，类似CNN中的卷积核</li>
<li>减少参数个数，效率更高</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>long term memory</em>：长期记忆，参数</li>
<li><em>short term memory</em>：短期记忆，数据流</li>
<li><em>long short term memory</em>：长[的]短期记忆，细胞状态</li>
</ul>
</blockquote>
<h3 id="LSTM标准细胞结构"><a href="#LSTM标准细胞结构" class="headerlink" title="LSTM标准细胞结构"></a>LSTM标准细胞结构</h3><p><img src="/imgs/lstm_cell_structure.png" alt="lstm_cell_structure"></p>
<script type="math/tex; mode=display">\begin{align*}
i^{(t)} & = \sigma(W_i[x^{(t)}, h^{(t-1)}], b_i), & input gate \\
f^{(t)} & = \sigma(W_f[x^{(t)}, h^{(t-1)}], b_f), & forget gate \\
o^{(t)} & = \sigma(W_o[x^{(t)}, h^{(t-1)}], b_o), & output gate \\
\tilde C^{(t)} & = tanh((W_c[x^{(t)}, h^{(t)}])), & memory alternates \\
C^{(t)} & = f^{(t)} \odot c^{(t-1)} + i^{(t)} \odot c^{(t)}, & new memory \\
h^{(t)} & = o^{(t)} \odot tanh(c^{(t)}), & output
\end{align*}</script><blockquote>
<ul>
<li>$W_i, b_i, W_f, b_f, W_o, b_o$：输入门、遗忘门、输出门
  参数</li>
<li>$\odot$：逐项乘积</li>
<li>$x_t$：第$t$期输入</li>
<li>$i^{(t)}$：输出门权重，决定需要更新的信息</li>
<li>$f^{(t)}$：遗忘门权重，决定需要遗忘的信息</li>
<li>$o^{(t)}$：输出门权重，决定需要输出的信息</li>
<li>$h^{(t-1)}$：第$t-1$期细胞状态输出</li>
<li>$\tilde C_t$：第$t$期更新备选内容</li>
<li>$C^{(t)}$：第$t$期更新完成后细胞状态</li>
</ul>
</blockquote>
<ul>
<li><p>输入、遗忘、输出门特点</p>
<ul>
<li>当期输入$x^{(t)}$、上期输出$h^{(t-1)}$作为输入</li>
<li>sigmoid作为激活函数，得到$[0,1]$间<strong>控制、权重</strong>向量<ul>
<li><em>1</em>：完全保留</li>
<li><em>0</em>：完全舍弃</li>
</ul>
</li>
</ul>
</li>
<li><p>细胞状态、输出特点</p>
<ul>
<li>tanh作激活函数，得到$[-1,1]$间<strong>信息</strong>向量<ul>
<li>$h^{(t-1)}, x^t$：备选更新信息输入</li>
<li>$C^{(t-1)}$：输出信息输入</li>
</ul>
</li>
<li>与门限权重逐项乘积确定最终遗忘、输入、输出</li>
</ul>
<blockquote>
<ul>
<li>细胞状态选择（候选、输出）都是使用双曲正切激活，应该
 是为了有正由负</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="Gates"><a href="#Gates" class="headerlink" title="Gates"></a>Gates</h4><ul>
<li><p><em>Forget Gate</em>：遗忘门，决定要从细胞状态中舍弃的信息</p>
<p><img src="/imgs/lstm_forget_gate.png" alt="lstm_forget_gate"></p>
</li>
<li><p><em>Input Gate</em>：输入门，决定向细胞状态中保留的信息</p>
<p><img src="/imgs/lstm_input_gate.png" alt="lstm_input_gate"></p>
</li>
<li><p><em>Ouput Gate</em>：输出门，决定从细胞状态中输出的信息</p>
<p><img src="/imgs/lstm_output_gate.png" alt="lstm_output_gate"></p>
</li>
</ul>
<h4 id="Cell-State"><a href="#Cell-State" class="headerlink" title="Cell State"></a>Cell State</h4><p>细胞状态：LSTM中最重要的核心思想</p>
<p><img src="/imgs/lstm_cell_status.png" alt="lstm_cell_status"></p>
<ul>
<li><p>随着时间流动，承载之前所有状态信息，代表长期记忆</p>
<ul>
<li>类似于传送带，直接在整个链上运行，只有少量<strong>线性交互</strong></li>
<li>信息其上流派很容易保持不变</li>
<li>通过“三个门”保护、控制</li>
</ul>
</li>
<li><p>LSTM可以保证长短时记忆可以理解为</p>
<ul>
<li>$C_t$中历史信息比重由$f^{(t)}$确定</li>
<li>$f^{(t)}$趋近于1时历史信息能较好的保留</li>
</ul>
</li>
</ul>
<h3 id="Gated-Recurrent-Unit"><a href="#Gated-Recurrent-Unit" class="headerlink" title="Gated Recurrent Unit"></a>Gated Recurrent Unit</h3><p><img src="/imgs/lstm_gru.png" alt="lstm_gru"></p>
<script type="math/tex; mode=display">\begin{align*}
r^{(t)} & = \sigma(W_r [h^{(t-1)}, x^{(t)}] + b_r), & reset gate \\
z^{(t)} & = \sigma(W_z [h^{(t-1)}, x^{(t)}] + b_z), & update gate \\
\tilde h^{(t)} &= tanh(W_h [r^{(t)} h^{(t-1)}, x^{(t)}]),
    & memory alternates\\
h^{(t)} & = (1 - z^{(t)}) \odot h^{(t-1)} + z^{(t)} \odot \tilde h^{(t)},
    & new memory
\end{align*}</script><blockquote>
<ul>
<li>$W_r, b_r, W_z, b_z$：重置门、更新门参数</li>
<li>$h^{(t)}$：原细胞状态、隐层输出合并</li>
<li>$\tilde{h}_t$：第$t$期更新备选信息</li>
<li>$r^{(t)}$：重置门权重输出，重置上期状态$h_{t-1}$再作为更新
  门输入</li>
<li>$z^{(t)]$：更新门权重输出，当期状态$h<em>t$中$h</em>{t-1}$、
  $\tilde{h}_t$占比（遗忘、更新的结合）</li>
</ul>
</blockquote>
<ul>
<li>合并细胞状态、隐层输出</li>
<li>合并遗忘门、输出门为更新门</li>
</ul>
<h3 id="其他变体结构"><a href="#其他变体结构" class="headerlink" title="其他变体结构"></a>其他变体结构</h3><h4 id="Vanilla-LSTM"><a href="#Vanilla-LSTM" class="headerlink" title="Vanilla LSTM"></a><em>Vanilla LSTM</em></h4><p><img src="/imgs/lstm_peephole_connection.png" alt="lstm_peephole_connection"></p>
<blockquote>
<ul>
<li><em>Peephole Connection</em>：细胞状态也作为3个门中sigmoid的
  输入，影响控制向量的生成</li>
</ul>
</blockquote>
<h4 id="Coupled-Input-and-Forget-Gate"><a href="#Coupled-Input-and-Forget-Gate" class="headerlink" title="Coupled Input and Forget Gate"></a><em>Coupled Input and Forget Gate</em></h4><p><img src="/imgs/lstm_cifg.png" alt="lstm_cifg"></p>
<blockquote>
<ul>
<li>$1-f_i$代替$i_t$，结合遗忘门、输入门</li>
</ul>
</blockquote>
<h4 id="结构比较"><a href="#结构比较" class="headerlink" title="结构比较"></a>结构比较</h4><p>在Vanilla LSTM基础上的8个变体在TIMIT语音识别、手写字符识别、
复调音乐建模三个应用中比较</p>
<blockquote>
<ul>
<li><em>No Input Gate</em>：NIG，没有输入门</li>
<li><em>No Forget Gate</em>：NFG，没有遗忘门</li>
<li><em>No Output Gate</em>：NOG，没有输出门</li>
<li><em>No Input Acitivation Function</em>：NIAF，输入门没有tanh
  激活</li>
<li><em>No Output Activation Function</em>：NOAF，输出门没有tanh
  激活</li>
<li><em>No Peepholes</em>：NP，普通LSTM</li>
<li><em>Coupled Input and Forget Gate</em>：CIFG，遗忘、输出门结合</li>
<li><em>Full Gate Recurrence</em>：FGR，所有门之间有回路</li>
</ul>
</blockquote>
<ul>
<li><p>Vanilla LSTM效果均良好，其他变体没有性能提升</p>
</li>
<li><p>细胞结构</p>
<ul>
<li>遗忘门、输入门是最重要的部分<ul>
<li>遗忘门对LSTM性能影响十分关键</li>
<li>输出门对限制无约束细胞状态输出必要</li>
</ul>
</li>
<li>CIFG、NP简化结构，单对结果没有太大影响</li>
</ul>
</li>
<li><p>超参</p>
<ul>
<li>学习率、隐层数量是LSTM主要调节参数<ul>
<li>两者之间没有相互影响，可以独立调参</li>
<li>学习率可以可以使用小网络结构独立校准</li>
</ul>
</li>
<li>动量因子影响不大</li>
<li>高斯噪声的引入有损性能、增加训练时间</li>
</ul>
</li>
</ul>
<p>?时间</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:10:26.000Z" title="7/16/2021, 3:10:26 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">4 minutes read (About 647 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/sys2sys.html">Seq2Seq</a></h1><div class="content"><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p><em>Seq2Seq</em>/<em>Encoder-Decoder</em>：允许任意长度序列输入、输出学习</p>
<p><img src="/imgs/seq2seq_structure.png" alt="seq2seq_structure"></p>
<script type="math/tex; mode=display">\begin{align*}
p(y_1, \cdots, y_{T^{'}} | x_1, \cdots, x_T) & = \prod_{t=1}^T
    p(y_t|c, y_1, \cdots, y_{t-1}) \\

c & = q(\{h_1, \cdots, h_T\}) \\

h_t & = f(x_t, h_{t-1})
\end{align*}</script><blockquote>
<ul>
<li>$T^{‘} \neq T$：输出序列长度、输入序列长度</li>
<li>$p(y_t|\cdots)$：一般为softmax函数计算字典中各词概率</li>
<li>$c$：定长向量</li>
<li>$h_t$：隐状态</li>
<li>$q$：将隐状态映射为定长向量存储信息，如：
  $q(\cdots) = h_T$</li>
<li>$f$：根据输入映射为隐状态，如：RNN、LSTM</li>
</ul>
</blockquote>
<h3 id="实现策略"><a href="#实现策略" class="headerlink" title="实现策略"></a>实现策略</h3><blockquote>
<ul>
<li><em>encoder</em>：将输入序列映射为定长向量</li>
<li><em>decoder</em>：将该定长向量映射为目标输出
  （通过将联合概率有序分解来定义翻译概率）</li>
</ul>
</blockquote>
<ul>
<li><p>RNN：以<strong>内部隐状态作为定长向量</strong>存储输入信息</p>
<ul>
<li>理论上可实现在定长向量中存储相关信息、再解码</li>
<li>但由于长时梯度消失，实际难以训练</li>
</ul>
</li>
<li><p>LSTM：类似RNN在内部隐状态中存储信息</p>
<ul>
<li>学习长时依赖更有效、容易训练</li>
</ul>
</li>
</ul>
<h2 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h2><ul>
<li><p>采用LSTM、RNN结构的Seq2Seq结构很难将输入序列转化为定长
向量而保存所有有效信息</p>
<ul>
<li>序列末尾对定长向量影响更大，难以学习长距离依赖</li>
<li>随着输入序列长度增加，预测效果显著下降</li>
</ul>
</li>
<li><p>使用Attention机制的Seq2Seq无需多期迭代传递信息，不存在
长距离依赖</p>
</li>
</ul>
<h3 id="BiRNN2RNN-with-Attention"><a href="#BiRNN2RNN-with-Attention" class="headerlink" title="BiRNN2RNN with Attention"></a>BiRNN2RNN with Attention</h3><p><img src="/imgs/seq2seq_birnn2rnn_with_attention.png" alt="seq2seq_birnn2rnn_with_attention"></p>
<script type="math/tex; mode=display">\begin{align*}
p(y_t | y_1, \cdots, y_{t-1}, x) & = g(y_{t-1}, s_t, c_t) \\

s_t & = f(s_{t-1}, y_{t-1}, c_t) \\

c_t & = \sum_{j=t}^T \alpha_{j=1}^T \alpha_{t,j} h_j \\

\alpha_{t,j} & = softmax(e_{t,j}) \\
& = \frac {exp(e_{t,j})} {\sum_{k=1}^T exp(e_{t,k})} \\

e_{t,j} & = a(s_{t-1}, h_j)
\end{align*}</script><blockquote>
<ul>
<li>$y_t$：当前$t$时刻输出</li>
<li>$p(y_t|\cdots)$：$t$时刻输出条件概率</li>
<li>$s_t$：解码器$t$时刻隐状态</li>
<li>$h_j$：编码器$j$时刻隐状态</li>
<li>$c_t$：<em>expected annotation</em>，对输出$t$的上下文向量</li>
<li>$T$：输入序列长度</li>
<li>$e<em>{t,j}, \alpha</em>{t,j}$：输入$j$对输出$t$重要性，反映
  模型注意力分布</li>
<li>$a$：<em>alignment model</em>，输入输出相关性模型，同整个系统
  联合训练的前向神经网络，attention机制核心</li>
</ul>
</blockquote>
<ul>
<li>编码器：<em>Bi-RNN</em></li>
<li>解码器：attention机制加权的RNN</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:50:26.000Z" title="7/16/2021, 2:50:26 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">8 minutes read (About 1192 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/factorization_machine.html">Factorization Machine</a></h1><div class="content"><h2 id="因子分解机"><a href="#因子分解机" class="headerlink" title="因子分解机"></a>因子分解机</h2><p>因子分解机：将变量交互影响因子化
（每个变量用隐向量代表、衡量其交叉影响）</p>
<script type="math/tex; mode=display">
\hat y(x) := w_0 + \sum_{i=1}^m w_i x_i + \sum_{i=1}^m
    \sum_{j=i+1}^m <v_i, v_j> x_i x_j</script><blockquote>
<ul>
<li>$w_0$：全局偏置</li>
<li>$w_i$：变量$i$权重</li>
<li>$w_{i,j} := <v_i, v_j>$：变量$i$、$j$之间交互项权重</li>
<li>$v_i$：$k$维向量，变量交叉影响因子</li>
</ul>
</blockquote>
<ul>
<li><p>FM通过<strong>因子化交互影响解耦交互项参数</strong></p>
<ul>
<li><p>即使没有足够数据也能较好估计高维稀疏特征交互影响参数</p>
<ul>
<li>无需大量有交互影响（交互特征取值同时非0）样本</li>
<li>包含某交互影响数据也能帮助估计相关的交互影响</li>
<li><strong>可以学习数据不存在的模式</strong></li>
</ul>
</li>
<li><p>可以视为embedding，特征之间关联性用embedding向量
（隐向量）內积表示</p>
</li>
</ul>
</li>
<li><p>参数数量、模型复杂度均为线性</p>
<ul>
<li>可以方便使用SGD等算法对各种损失函数进行优化</li>
<li>无需像SVM需要支持向量，可以扩展到大量数据集</li>
</ul>
</li>
<li><p>适合任何实值特征向量，对某些输入特征向量即类似
<em>biased MF</em>、<em>SVD++</em>、<em>PITF</em>、<em>FPMC</em></p>
</li>
</ul>
<blockquote>
<ul>
<li>另外还有d-way因子分解机，交互作用以PARAFAC模型因子化<script type="math/tex; mode=display">
  \hat y(x) := w_0 + \sum_{i=1}^n w_i x_i + \sum_{l=2}^d \sum_{i_1=1}
      \cdots \sum_{i_l=i_{l-1}+1}(\prod_{j=1}^l x_{i_j})
      (\sum_{f=1} \prod_{j=1}^l v_{i_j,f}^{(l)}) \\</script><blockquote>
<ul>
<li>$V^{(l)} \in R^{n * k_l}, k_l \in N_0^{+}$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h3 id="模型表达能力"><a href="#模型表达能力" class="headerlink" title="模型表达能力"></a>模型表达能力</h3><ul>
<li><p>考虑任何正定矩阵$W$总可以被分解为$W=V V^T$，则$k$足够大
时，FM总可以表达（还原）交叉项权重矩阵$W$</p>
<ul>
<li>FM是MF降维的推广，在用户-物品评分矩阵基础上集成其他
特征</li>
<li>特征组合发生所有变量之间</li>
</ul>
</li>
<li><p>实际应该选取较小的$k$</p>
<ul>
<li>对较大$k$，稀疏特征没有足够数据估计复杂交叉项权重
矩阵$W$</li>
<li>限制FM的表达能力，模型有更好的泛化能力、交互权重矩阵</li>
</ul>
</li>
</ul>
<h3 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h3><script type="math/tex; mode=display">\begin{align*}
\sum_{i=1}^m \sum_{j=i+1}^m <v_i, v_j> x_i x_j & = 
    \frac 1 2 \sum_{i=1}^m \sum_{j=i}^m <v_i, v_j> x_i x_j -
    \frac 1 2 \sum_{i=1}^m <v_i, v_i> x_i^2 \\
& = \frac 1 2 (x^T V^T V x - x^T diag(V^T V) x) \\
& = \frac 1 2 (\|Vx\|_2^2 - x^T diag(V^T V) x) \\
& = \frac 1 2 \sum_{f=1}^k ((\sum_{i=1}^m v_{i,f} x_i)^ 2
    - \sum_{i=1}^m v_{i,f}^2 x_i^2) \\
\end{align*}</script><blockquote>
<ul>
<li>$V = (v_1, v_2, \cdots, v_m)$</li>
<li>$x = (x_1, x_2, \cdots, x_m)^T$</li>
</ul>
</blockquote>
<ul>
<li><p>模型计算复杂度为线性$\in O(kn)$</p>
</li>
<li><p>模型可以使用梯度下降类方法高效学习</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial \hat y(x)} {\partial \theta} & = \left \{
   \begin{array}{l}
       1, & \theta := w_0 \\
       x_i, & \theta := w_i \\
       x_i Vx - v_i x_i^2& \theta := v_i
   \end{array} \right. \\
& = \left \{ \begin{array}{l}
       1, & \theta := w_0 \\
       x_i, & \theta := w_i \\
       x_i \sum_{j=1}^m v_{j,f} x_j - v_{i,f} x_i^2,
           & \theta := v_{i,f}
   \end{array} \right.
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li>考虑到稀疏特征，內积只需计算非零值</li>
</ul>
</blockquote>
<h3 id="模型适用"><a href="#模型适用" class="headerlink" title="模型适用"></a>模型适用</h3><ul>
<li>回归：直接用$\hat y(x)$作为回归预测值</li>
<li>二分类：结合logit损失、hinge损失优化</li>
<li>ranking：$\hat y(x)$作为得分排序，使用成对分类损失优化</li>
</ul>
<h2 id="Field-aware-Factorization-Machines"><a href="#Field-aware-Factorization-Machines" class="headerlink" title="Field-aware Factorization Machines"></a>Field-aware Factorization Machines</h2><p>域感知因子分解机：在FM基础上考虑对特征分类，特征对其他类别
特征训练分别训练隐向量</p>
<script type="math/tex; mode=display">\begin{align*}
\hat y(x) & = w_0 + \sum_{i=0}^m w_i x_i + \sum_{a=1}^m
    \sum_{b=a+1}^m <V_{a, f_b}, V_{b, f_a}> x_a x_b \\
& = w_0 + \sum_{i=1}^M \sum_{j=1}^{M_i} w_{i,j} x_{i,j} +
    \sum_{i=1}^M \sum_{j=1}^{M_i} \sum_{a=i}^M \sum_{b=1}^{M_i}
    <V_{i,j,a}, V_{a,b,i}> x_{i,j} x_{a,b}
\end{align*}</script><blockquote>
<ul>
<li>$m$：特征数量</li>
<li>$M, M_i$：特征域数量、各特征域中特征数量</li>
<li>$V_{i,j,a}$：特征域$i$中$j$特征对特征与$a$的隐向量</li>
<li>$V_{a, f_b}$：特征$x_a$对特征$b$所属域$f_b$的隐向量</li>
</ul>
</blockquote>
<ul>
<li><p>FFM中特征都属于特定域，相同特征域中特征性质应该相同，
一般的</p>
<ul>
<li>连续特征自己单独成域</li>
<li>离散0/1特征按照性质划分，归于不同特征域</li>
</ul>
</li>
<li><p>特征对其他域分别有隐向量表示<strong>和其他域的隐含关系</strong></p>
<ul>
<li>考虑交互作用时，对不同域使用不同隐向量计算交互作用</li>
<li>FFM中隐变量维度也远远小于FM中隐向量维度</li>
</ul>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/imgs/ffm_steps.png" alt="ffm_steps"></p>
<h3 id="模型特点"><a href="#模型特点" class="headerlink" title="模型特点"></a>模型特点</h3><ul>
<li>模型总体类似FM，仅通过多样化隐向量实现细化因子分解</li>
<li>模型总体较FM复杂度大、参数数量多<ul>
<li>无法抽取公因子化简为线性</li>
<li>数据量较小时可能无法有效训练隐向量</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T09:19:32.000Z" title="8/4/2021, 5:19:32 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Technique/">ML Technique</a><span> / </span><a class="link-muted" href="/categories/ML-Technique/Neural-Network/">Neural Network</a></span><span class="level-item">12 minutes read (About 1782 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Technique/Neural-Network/batch_normalization.html">Batch Normalization</a></h1><div class="content"><h2 id="Internal-Covariate-Shift"><a href="#Internal-Covariate-Shift" class="headerlink" title="Internal Covariate Shift"></a><em>Internal Covariate Shift</em></h2><p><em>ICS</em>：由于网络参数变化，引起内部节点（输入）数据分布发生变化的过程</p>
<ul>
<li><p>网络中层与层之间高度耦合，具有强关联性</p>
<ul>
<li>网络中任意层都可以视为单独网络</li>
<li>上层输入可视为作为当前层外部输入</li>
</ul>
</li>
<li><p>随训练进行，网络中参数不断发生改变</p>
<ul>
<li>任意层中参数变化会导致之后层输入发生改变</li>
<li>高层需要不断适应输入分布的改变，即其输入分布性质影响
该层训练</li>
<li>由此导致模型训练困难</li>
</ul>
</li>
</ul>
<h3 id="负面影响"><a href="#负面影响" class="headerlink" title="负面影响"></a>负面影响</h3><ul>
<li><p>上层网络需要不断调整输入适应数据分布变换，降低网络学习
效率</p>
</li>
<li><p>输入数据量级不稳定、各维度数据量级差距不稳定</p>
<ul>
<li>降低学习效率<ul>
<li>小量级维度参数要求更小的学习率</li>
<li>否则参数可能在最优解附近反复波动</li>
</ul>
</li>
<li>容易出现梯度消失，难以训练饱和非线性模型<ul>
<li>大量级维度训练过程中容易陷入梯度饱和区，参数更新
速度慢，减缓网络收敛速度</li>
<li>训练过程中参数更新更有可能使得输入移向激活函数
饱和区</li>
<li>且该效应随着网络深度加深被进一步放大</li>
</ul>
</li>
<li>参数初始化需要更复杂考虑</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>还可以使用非饱和激活函数ReLU等避免陷入梯度饱和区</li>
</ul>
</blockquote>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p><em>Batch Normalization</em>：规范化batch数据，使样本<strong>各维度</strong>
标准化，即均值为0、方差为1</p>
<script type="math/tex; mode=display">\begin{align*}
\y & = BN_{\gamma, \beta}(z) = \gamma \odot \hat z + \beta \\
\hat z & = \frac {z - E(z)} {\sqrt {Var(z) + \epsilon}}
\end{align*}</script><blockquote>
<ul>
<li>$B$：mini-batch</li>
<li>$z, y$：<strong>某层</strong>输入向量、规范化后输入向量
  （即以个神经元中激活前标量值$z=Wx+b$为一维）</li>
<li>$\odot$：逐元素乘积</li>
<li>$E(x)$：均值使用移动平均均值</li>
<li>$Var(x)$：方差使用移动平均无偏估计</li>
<li>$\gamma, \beta$：待学习向量，用于<strong>恢复网络的表示能力</strong></li>
<li>$\epsilon$：为数值计算稳定性添加</li>
</ul>
</blockquote>
<ul>
<li><p>BN可以视为<em>whitening</em>的简化</p>
<ul>
<li>简化计算过程：避免过高的运算代价、时间</li>
<li>保留数据信息：未改变网络每层各特征之间相关性</li>
</ul>
</li>
<li><p>BN层引入可学习参数$\gamma, \beta$以恢复数据表达能力</p>
<ul>
<li>Normalization操作缓解了ICS问题，使得每层输入稳定
，也导致数据表达能力的缺失</li>
<li>输入分布均值为0、方差为1时，经过sigmoid、tanh激活
函数时，容易陷入其线性区域</li>
<li>$\gamma = \sqrt {Var(z)}, \beta = E(z)$时为等价变换
，并保留原始输入特征分布信息</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>Whitening</em>：白化，对输入数据变换使得各特征同均值、
  同方向、不相关，可以分为PCA白化、ZCA白化</li>
</ul>
</blockquote>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ul>
<li><p>规范化在每个神经元内部非线性激活前$z=Wu$进行，而不是
[也]在上一层输出$u$上进行，即包含BN最终为</p>
<script type="math/tex; mode=display">
z = act(BN(Wu))</script><blockquote>
<ul>
<li>$act$：激活函数</li>
<li>偏置$b$：可以被省略，BN中减去均值</li>
</ul>
</blockquote>
<ul>
<li>$u$的分布形状可以在训练过程中改变</li>
<li>而$u$两次正则化无必要</li>
<li>$z=Wu$分布更可能对称、稠密、类似高斯分布</li>
</ul>
</li>
<li><p>以batch统计量作为整体训练样本均值、方差估计</p>
<ul>
<li>每层均需存储均值、方差的移动平均统计量用于测试时
归一化测试数据</li>
</ul>
</li>
<li><p>对卷积操作，考虑卷积特性，不是只为激活函数（即卷积核）
学习$\gamma, \beta$，而是为每个<em>feature map</em>学习
（即每个卷积核、对每个特征图层分别学习）</p>
</li>
</ul>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><ul>
<li><p>预测过程中各参数（包括均值、方差）为定值，BN仅仅对数据
做了线性变换</p>
<ul>
<li><p>使用训练总体的无偏统计量对测试数据归一化
（训练时存储）</p>
<script type="math/tex; mode=display">\begin{align*}
\mu_{test} & = E(\mu_{batch}) \\
\sigma^2_{test} = \frac m {m-1} E(\sigma^2_{batch})
\end{align*}</script></li>
<li><p>还可以使用样本指数加权平均统计量</p>
</li>
</ul>
</li>
</ul>
<h3 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h3><blockquote>
<ul>
<li>BN通过规范化输入数据各维度分布减少<em>ICS</em>，使得网络中每层
  输入数据分布相对稳定</li>
</ul>
</blockquote>
<ul>
<li><p>实现网络层与层之间的解耦</p>
<ul>
<li>方便迁移学习</li>
<li>加速模型学习速度：后层网络无需不断适应输入分布变化，
利于提高神经网络学习速度</li>
</ul>
</li>
<li><p>降低模型对网络超参数、初始值敏感度，使得网络学习更加稳定</p>
<ul>
<li>简化调参过程</li>
<li>允许使用更大的学习率提高学习效率</li>
</ul>
<script type="math/tex; mode=display">\begin{align*}
BN(Wu) & = BN((aW)u) \\
\frac {\partial BN(aWu)} {\partial u} & = \frac
   {\partial BN(Wu)} {\partial u} \\
\frac {BN(aWu)} {\partial aW} & = \frac 1 a \frac
   {\partial BN(Wu)} {\partial W}
\end{align*}</script><blockquote>
<ul>
<li>$a$：假设某层权重参数变动$a$倍</li>
</ul>
</blockquote>
<ul>
<li>激活函数函数输入不受权重$W$放缩影响</li>
<li>梯度反向传播更稳定，权重$W$的Jacobian矩阵将包含接近
1的奇异值，保持梯度稳定反向传播</li>
</ul>
</li>
<li><p>允许网络使用饱和激活函数（sigmoid、tanh等），而不至于
停滞在饱和处，缓解梯度消失问题</p>
<ul>
<li>深度网络的复杂性容易使得网络变化积累到上层网络中，
导致模型容易进入激活函数梯度饱和区</li>
</ul>
</li>
<li><p>有正则化作用，提高模型泛化性能，减少对Dropout的需求</p>
<ul>
<li>不同batch均值、方差有所不同，为网络学习过程增加随机
噪声</li>
<li>与Dropout关闭神经元给网络带来噪声类似，一定程度上
有正则化效果</li>
</ul>
</li>
</ul>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>层归一化：假设非线性激活前的输入随机变量分布接近，可以直接
基于每层所有非线性激活前输入估计均值、方差</p>
<script type="math/tex; mode=display">\begin{align*}
\mu^l & = \frac 1 H \sum_{i=1}^H h_i^l \\
\sigma^l &= \sqrt {\frac 1 H \sum_{i=1}^H (h_i^l - \mu^l)^2} \\
h^l & = W^l x^{l-1} + b^l \\
LN(h^l) & = \frac {g^l} {\sigma^l} \odot (h^l - \mu^l) + b^l \\
x^l & = g(LN(h^l))
\end{align*}</script><blockquote>
<ul>
<li>$h^l$：第$l$隐层激活前值</li>
<li>$\mu^l, \sigma^l$：第$l$隐层对应LN均值、方差
  （标量，是同层神经元激活前值统计量）</li>
</ul>
</blockquote>
<ul>
<li><p>相对于BN，其适应范围更广</p>
<ul>
<li>循环神经网络中，BN无法处理长于训练序列的测试序列</li>
<li>BN无法应用到在线学习、超大分布式模型任务，此时训练
batch较小，计算的均值、方差无法有效代表训练总体</li>
</ul>
</li>
<li><p>LN假设非线性激活前输入随机变量分布接近，而CNN网络中图像
边缘对应kernel大量隐藏单元未被激活，假设不成立，所以
CNN网络中LN效果没有BN效果好</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T11:35:20.000Z" title="8/4/2021, 7:35:20 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Technique/">ML Technique</a><span> / </span><a class="link-muted" href="/categories/ML-Technique/Neural-Network/">Neural Network</a></span><span class="level-item">5 minutes read (About 812 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Technique/Neural-Network/dropout.html">Dropout</a></h1><div class="content"><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p><em>Dropout</em>：<strong>训练时</strong>根据随机隐藏部分神经元、对应连接边避免
过拟合</p>
<h3 id="固定概率丢弃"><a href="#固定概率丢弃" class="headerlink" title="固定概率丢弃"></a>固定概率丢弃</h3><p>Dropout最简单方法：设置固定概率p，对每个神经元以概率p判定
是否需要保留</p>
<script type="math/tex; mode=display">\begin{align*}
y & = f(W * d(x) + b) \\
d(x) & = \left \{ \begin{array}{l}
    m \odot x, & 训练阶段
    px, & 测试阶段
\end{array} \right.
\end{align*}</script><blockquote>
<ul>
<li>$d(x)$：丢弃函数</li>
<li>$m \in {0, 1}^d$：丢弃掩码，通过概率为p的伯努利
  分布随机生成</li>
</ul>
</blockquote>
<ul>
<li><p>$p$可以设置为0.5，对大部分网络、任务比较有效</p>
<ul>
<li>此时随机生成多的网络最具多样性</li>
</ul>
</li>
<li><p>训练时</p>
<ul>
<li>激活神经元数量为原来的p倍</li>
<li>每个batch分别进行drop，相当于对每个batch都有独特网络</li>
</ul>
</li>
<li><p>测试时</p>
<ul>
<li>所有神经元都被激活，造成训练、测试时网络输出不一致，
需将每个神经元输出乘p避免</li>
<li>也相当于把不同网络做平均</li>
</ul>
</li>
<li><p>在预测时，类似bagging技术将多个模型组合</p>
<ul>
<li>只是类似，各个drop后的子网并不独立，在不同子网中相同
神经元的权重相同</li>
<li>多个模型组合组合可以一定程度上抵消过拟合</li>
<li>因为在训练时子网中部分神经元被drop，剩余部分权重相较
完全网络有$\frac 1 {1-p}$，所以在完整网络中，各部分
权重需要$ * (1-p)$</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>讲道理应该是隐藏部分神经元而不是连接，否则会使神经元偏向
  某些输入，还不如隐藏部分神经元，这样可以让神经元随机降低
  样本权重，理论上能减弱过拟合</li>
</ul>
</blockquote>
<h3 id="丢弃方法"><a href="#丢弃方法" class="headerlink" title="丢弃方法"></a>丢弃方法</h3><ul>
<li><p>输入层神经元丢弃率更接近1，使得输入变化不会太大</p>
<ul>
<li>输入层神经元丢失时，相当于给数据增加噪声，提高网络
稳健性</li>
</ul>
</li>
<li><p>循环神经网络丢弃</p>
<ul>
<li>不能直接丢弃隐状态，会损害循环网络在时间维度上的记忆
能力</li>
<li>简单方法：可以考虑对非循环连接进行随机丢弃</li>
<li>变分丢弃法：根据贝叶斯对丢弃法是对参数的采样解释，
采样参数需要每个时刻保持不变<ul>
<li>需要对参数矩阵的每个元素随机丢弃</li>
<li>所有时刻使用相同的丢弃掩码</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h3><ul>
<li><p>集成学习解释</p>
<ul>
<li>每次丢弃，相当于从原网络采样得到子网络</li>
<li>每次迭代，相当于训练不同的子网络，共享原始网络参数</li>
<li>最终网络可以近似看作是集成了指数个不同网络的组合模型</li>
</ul>
</li>
<li><p>贝叶斯学习解释</p>
<ul>
<li>对需要学习的网络$y = f(x, \theta)$，贝叶斯学习假设
参数$\theta$为随机向量</li>
<li><p>设先验分布为$q(\theta)$，贝叶斯方法预测为</p>
<script type="math/tex; mode=display">\begin{align*}
E_{q(\theta)}[y] &  = \int_q f(x, \theta) q(\theta)
  d\theta \\
& \approx \frac 1 M \sum_{m=1}^M f(x, \theta_m)
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li>$f(x, \theta_m)$：第$m$次应用丢弃方法的网络</li>
<li>$\theta_m$：对全部参数的采样</li>
</ul>
</blockquote>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-23T00:39:04.000Z" title="7/23/2019, 8:39:04 AM">2019-07-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-23T00:39:04.000Z" title="7/23/2019, 8:39:04 AM">2019-07-23</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">6 minutes read (About 890 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/stacking.html">Stacked Generalization</a></h1><div class="content"><h2 id="Stacked-Generalization"><a href="#Stacked-Generalization" class="headerlink" title="Stacked Generalization"></a>Stacked Generalization</h2><p>堆栈泛化：使用<strong>多种模型</strong>分别训练训练，将其结果叠加作为下层
模型的输入，最终得到预测输出</p>
<p><img src="/imgs/stacking.png" alt="stacking"></p>
<ul>
<li><p>属于异源集成模型，可以视为</p>
<ul>
<li><p>复合函数</p>
<p><img src="/imgs/stacking_workflow_2.png" alt="stacing_workflow_2"></p>
</li>
<li><p>短路网络</p>
<p><img src="/imgs/stacking_workflow_1.png" alt="stacing_workflow_1"></p>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>从某种意义上，复杂模型都是stacking</li>
</ul>
</blockquote>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>不同模型侧重于获取数据不同方面的特征</p>
<ul>
<li>使用基学习器抽取数据特征进行表示学习，提取不同角度的
数据高维特征</li>
<li>考虑到使用全量训练数据训练、预测作为下层模型输入会
导致过拟合，可使用K折交叉验证避免过拟合</li>
<li>有些基学习器只使用适合其部分特征训练<ul>
<li>GBDT、DNN适合低维稠密特征</li>
</ul>
</li>
</ul>
</li>
<li><p>元学习器组合多个基学习器的输出</p>
<ul>
<li>从数据高维特征学习数据模式，具有更好的泛化能力，避免
过拟合</li>
</ul>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>输入：模型$M<em>1, M_2, \cdots, M_d$、训练特征：$X</em>{n*m}$、
  训练标签$Y_{n}$、测试特征$X^{‘}$</li>
<li>输出：stacking模型、预测标签</li>
</ul>
</blockquote>
<ul>
<li><p>将训练数据K折划分，对第$i$轮划分</p>
<ul>
<li>使用模型$M<em>1, M_2, \cdots, M_d$分别在相应训练集
$[X[:n_i,:], X[n</em>{i+1}:,:]]$、
$[Y[:n<em>i], Y[n</em>{i+1}:]]$上训练</li>
<li>在相应验证集$X[n<em>i:n</em>{i+1}, :]$上验证、并记录验证
结果</li>
<li>将验证集验证结果叠加得到部分样本新特征
$N[n<em>i: n</em>{i+1}, d]$</li>
</ul>
</li>
<li><p>将K轮划分得到的部分新特征拼接得到训练集的完整新特征
$N_{n * d}$，将新特征作为输入，训练下层模型，得到最终
stacking模型</p>
</li>
<li><p>将测试特征如上作为输入经过两层模型预测，得到最终预测结果</p>
</li>
</ul>
<blockquote>
<ul>
<li>以上以2层stacking为例，有深层stacking</li>
</ul>
</blockquote>
<h2 id="常用模型"><a href="#常用模型" class="headerlink" title="常用模型"></a>常用模型</h2><h3 id="基学习器"><a href="#基学习器" class="headerlink" title="基学习器"></a>基学习器</h3><ul>
<li>交叉项、原始特征本身也可以视为线性基学习器学习到的特征</li>
</ul>
<blockquote>
<ul>
<li>具体模型参见
  <em>ml_specification/rec_system/ctr_stacking_models</em></li>
</ul>
</blockquote>
<h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p><img src="img/gbdt_in_stacking.png" alt="gbdt_in_stacking"></p>
<blockquote>
<ul>
<li>各树中各节点对应元学习器一维输入特征</li>
</ul>
</blockquote>
<ul>
<li><p>适合低维稠密通用特征，对输入特征分布没有要求</p>
</li>
<li><p>GBDT树根据熵增益（Gini系数增益）划分节点，每条路径
都代表一定区分能力</p>
<ul>
<li>以叶子节点（路径）作为特征，相当于自动进行特征
转换、组合、选择、离散化，得到<strong>高维组合特征</strong></li>
</ul>
</li>
<li><p>GDBT相较于单棵树、或RF更适合stacking</p>
<ul>
<li>单棵树表达能力弱，无法表达多个有区分性特征组合，
集成模型可将样本映射为多个特征</li>
<li>GBDT拟合残差意味着各树对样本区分度不同，对各特征
区别对待更合理</li>
</ul>
</li>
</ul>
<h4 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h4><ul>
<li>适合普通稠密特征、embedding特征</li>
<li>模型表达能力强，能抽取有良好分布数据的深层次特征，提高
模型准确性、泛化能力</li>
<li>容易扩充其他类别特征，如：图片、文字</li>
</ul>
<h3 id="元学习器"><a href="#元学习器" class="headerlink" title="元学习器"></a>元学习器</h3><ul>
<li><p>LR</p>
<ul>
<li>适合低维稀疏特征，可对所有特征离散化以引入非线性</li>
</ul>
</li>
<li><p>FM</p>
<ul>
<li>适合低维稀疏特征</li>
<li>LR基础上自动组合二阶交叉项</li>
</ul>
</li>
<li><p>Linear：训练模型、对训练结果线性加权</p>
</li>
</ul>
<p>?</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T08:06:22.000Z" title="7/16/2021, 4:06:22 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Unsupervised-Model/">Unsupervised Model</a></span><span class="level-item">9 minutes read (About 1297 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Unsupervised-Model/associating.html">频繁项集/序列</a></h1><div class="content"><h2 id="频繁项集"><a href="#频繁项集" class="headerlink" title="频繁项集"></a>频繁项集</h2><blockquote>
<ul>
<li>频繁项集：频繁出现项集合（无序）</li>
<li>频繁项序列：频繁出现项序列（有序）</li>
</ul>
</blockquote>
<ul>
<li>相关关联规则算法：数据量大时，无法直接发现频繁项集</li>
<li>频繁项集评估标准</li>
</ul>
<h3 id="评估标准"><a href="#评估标准" class="headerlink" title="评估标准"></a>评估标准</h3><ul>
<li><p>支持度：数据关联出现概率，关联数据在数据集中出现次数占
总数据集比重</p>
<script type="math/tex; mode=display">
Support(X, Y) = P(XY) = \frac {num(XY)} {num(All)}</script><ul>
<li>支持度高数据不一定构成频繁项集，但支持度数据肯定不能
不构成频繁项集</li>
</ul>
</li>
<li><p>置信度：数据出现条件概率，某个数据出现、另一数据出现概率</p>
<script type="math/tex; mode=display">
Confidence(X \Leftarrow Y) = P(X|Y) = \frac {P(XY)} {P(Y)}</script></li>
<li><p>提升度：数据之间关联关系，某数据出现、另一数据出现概率同
其总体出现概率之比</p>
<script type="math/tex; mode=display">\begin{align*}
Lift(X \Leftarrow Y) & = \frac {P(X|Y)} {P(X)} \\
& = \frac {Confidence(X \Leftarrow)}{P(X)} \\
& = \frac {P(XY)} {P(X)P(Y)}
\end{align*}</script><ul>
<li>提升度大于1则为有效的强关联规则，否则为无效的强关联
规则</li>
<li>若X、Y不相关，则提升度为1</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>选择频繁数据集，一般需要自定义评估标准：自定义支持度、
  自定义支持度和置信度组合</li>
</ul>
</blockquote>
<h2 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h2><p>Apriori算法</p>
<ul>
<li>以支持度作为评估标准，找出数据集中<strong>最大的</strong>频繁$k$项集<ul>
<li>找到符合支持度标准的频繁$k$项集</li>
<li>迭代直到无法找到项数更大的频繁项集</li>
</ul>
</li>
</ul>
<p><img src="/imgs/apriori_example.png" alt="apriori_example"></p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>输入：数据集合D、支持度阈值$\alpha$</li>
<li>输出：最大的频繁K项集</li>
</ul>
</blockquote>
<ul>
<li>置$k=1$，扫描整个数据集，以所有出现过数据作为候选1项集</li>
<li>挖掘候选$k$项集<ul>
<li>扫描数据、计算候选$k$项集支持度</li>
<li>去除支持度低于阈值$\alpha$的项集得到频繁$k$项集<ul>
<li>若频繁$k$项集只包含1项，直接返回</li>
<li>若频繁$k$项集为空，返回频繁$k-1$项集</li>
</ul>
</li>
<li>基于频繁$k$项集连接、生成候选$k+1$项集</li>
<li>置$k=k+1$</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>需要频繁扫描数据、效率低</li>
<li>频繁项集的子项集肯定也是频繁项集</li>
</ul>
</blockquote>
<h2 id="FPTree-FPGrowth"><a href="#FPTree-FPGrowth" class="headerlink" title="FPTree/FPGrowth"></a>FPTree/FPGrowth</h2><p>FPTree：对Apriori算法改进，不在需要多次扫描数据</p>
<ul>
<li><p>FPTree引入部分数据结构以临时存储数据</p>
<p><img src="/imgs/fptree_data_structure.png" alt="fptree_data_structure"></p>
<ul>
<li>项头表：按频繁1项集出现频数降序排列的表</li>
<li>FP Tree：包含原始数据、频数的多叉树</li>
<li>节点链表：链接项头表中频繁1项集、FPTree中相应节点
的链表</li>
</ul>
</li>
<li><p>特点：效率高</p>
<ul>
<li>只需要扫描两次数据</li>
<li>使用多叉树存储临时数据，利用高频频繁项集</li>
</ul>
</li>
</ul>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul>
<li><p>建立项头表</p>
<ul>
<li>扫描数据，得到所有1项集频数、剔除支持度低于阈值者，
并按支持度（频数）降序排列</li>
<li>第二次扫描数据，剔除每条数据中非频繁1项集、
<strong>在每条数据内部</strong>按支持度降序排列</li>
</ul>
<p><img src="/imgs/fptree_item_head_table.png" alt="fptree_item_head_table"></p>
</li>
<li><p>建立FPTree：逐条读取处理后排序后数据，依次插入树中</p>
<ul>
<li>每条数据中排序靠前者为祖先节点</li>
<li>若有<strong>直系公共祖先</strong>则公共祖先节点计数+1</li>
<li>新节点通过链表和项头表链接</li>
</ul>
<p><img src="/imgs/fptree_build_fptree.png" alt="fptree_item_head_table"></p>
</li>
<li><p>挖掘FPTree：对项表头中每项，找到其条件模式基</p>
<ul>
<li>将子树中每个节点计数置为叶子节点计数和，则子树中节点
取值即为其与当前项组合出现频数/支持度</li>
<li>删除（当前子树内）支持度/频数低于支持度阈值$\alpha$
节点</li>
<li>剩余节点项、当前项组合即为相应频繁$k$项集</li>
</ul>
<p><img src="/imgs/fptree_mine_item_set.png" alt="fptree_mine_item_set"></p>
<blockquote>
<ul>
<li>条件模式基：节点<strong>作为叶子节点</strong>所对应的FP子树</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="Prefix-Projected-Pattern-Growth"><a href="#Prefix-Projected-Pattern-Growth" class="headerlink" title="Prefix-Projected Pattern Growth"></a>Prefix-Projected Pattern Growth</h2><p><em>PrefixSpan</em>：前缀投影模式挖掘</p>
<ul>
<li>以支持度为标准，挖掘数据集中<strong>频繁序列</strong><ul>
<li>每条数据为若干项集组成的序列，<strong>序列内项集间有序</strong></li>
<li>为方便，每条数据序列中项集中的项已排序</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>可以将每条数据序列整体视为串</li>
<li>频繁序列：频繁出现<strong>子序列</strong></li>
</ul>
</blockquote>
<h3 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>输入：序列数据集$S$、支持度$\alpha$</li>
<li>所有满足阈值要求的频繁序列</li>
</ul>
</blockquote>
<ul>
<li><p>找出所有长度1前缀（即所有项）、对应投影</p>
<ul>
<li>计数、剔除持度小于阈值$\alpha$者，得到频繁1项序列</li>
<li>置$k=1$</li>
</ul>
</li>
<li><p>对每个长度为$k$前缀递归挖掘</p>
<ul>
<li>若前缀对应投影为空，返回</li>
<li>若前缀对应投影中所有项支持度均小于阈值$\alpha$，返回</li>
<li>同满足阈值要求阈值$\alpha$要求项合并，得到新前缀</li>
<li>置$k=k+1$</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>prefix</em>：前缀，正在处理的子序列</li>
<li><em>projected</em>：投影，各数据序列中位于前缀之后子串
?串</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T08:08:56.000Z" title="7/16/2021, 4:08:56 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Unsupervised-Model/">Unsupervised Model</a></span><span class="level-item">21 minutes read (About 3169 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Unsupervised-Model/clustering.html">聚类</a></h1><div class="content"><h2 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h2><p>聚类：按照某特定标准（如距离准则）把数据集分割成不同类、簇，
簇内数据相似性尽可能大、不同簇间数据对象差异性仅可能大</p>
<ul>
<li><p>属于无监督学习，目标是把相似的样本聚集在一起</p>
<ul>
<li>通常只需要给定相似度的计算即可</li>
<li>无需使用训练数据学习</li>
</ul>
</li>
<li><p>聚类算法分类</p>
<ul>
<li>基于划分</li>
<li>Hierarchical Methods：基于层次</li>
<li>基于密度</li>
<li>基于网络</li>
<li>基于模型</li>
<li>模糊聚类</li>
<li>基于约束</li>
<li>基于粒度</li>
<li>谱聚类</li>
<li>核聚类</li>
<li>量子聚类</li>
</ul>
</li>
</ul>
<h3 id="衡量聚类算法优劣"><a href="#衡量聚类算法优劣" class="headerlink" title="衡量聚类算法优劣"></a>衡量聚类算法优劣</h3><p><img src="/imgs/clustering_comparision.png" alt="clustering_comparision"></p>
<ul>
<li><p>算法的处理能力</p>
<ul>
<li>处理大数据的能力</li>
<li>处理噪声数据能力</li>
<li>处理任意形状数据的能力，如：有间隙的嵌套数据</li>
</ul>
</li>
<li><p>算法是否需要预测条件</p>
<ul>
<li>聚类数目</li>
<li>相关领域知识</li>
</ul>
</li>
<li><p>输入数据关联性</p>
<ul>
<li>结果是否和数据输入顺序相关</li>
<li>对数据维度敏感性（是否能处理高维数据）</li>
<li>对数据类型要求</li>
</ul>
</li>
</ul>
<h2 id="Hierarchical-Methods"><a href="#Hierarchical-Methods" class="headerlink" title="Hierarchical Methods"></a>Hierarchical Methods</h2><p>层次聚类</p>
<ul>
<li><p>自底向上合并的层次聚类</p>
<ul>
<li>最底层开始，通过合并最相似类簇形成上层类簇</li>
<li>全部数据点合并到同一类簇、或达到终止条件时结束</li>
</ul>
</li>
<li><p>自顶向下分裂的层次聚类</p>
<ul>
<li>从包含全部数据点的类簇开始，递归分裂出最相异的下层
类簇</li>
<li>每个类簇仅包含单个数据点时结束</li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>可解释性好：如需要创建分类方法时</li>
<li>研究表明能产生高质量聚类，可以应用在较大K的K-means
后的合并阶段</li>
<li>可以解决非球形类簇</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>时间复杂度高$O(N^2 log N)$（$N$为数据点数目）</li>
<li>贪心算法无法取得最优解</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>距离选择参见<em>ml_tech/#todo</em></li>
</ul>
</blockquote>
<h3 id="AGENS"><a href="#AGENS" class="headerlink" title="AGENS"></a>AGENS</h3><p>AGENS：自下向上层次聚类</p>
<ul>
<li>组连接：组与组之间距离<ul>
<li>single linkage</li>
<li>average linkage</li>
<li>complete linkage</li>
</ul>
</li>
<li>算法复杂度：$n^2logn$</li>
</ul>
<h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><ul>
<li>每个数据点视为一类，计算两两直接最小距离</li>
<li>合并距离最小两个两类别为新类</li>
<li>重新计算新类、所有类之间距离</li>
<li>重复以上，直至所有类合并为一类</li>
</ul>
<h3 id="Divisive-Analysis"><a href="#Divisive-Analysis" class="headerlink" title="Divisive Analysis"></a>Divisive Analysis</h3><p><em>DIANA</em>：自定向下层次聚类</p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ul>
<li>所有数据归为一组$C_1=(p_1, p_2, dots, p_n)$</li>
<li>计算所有点之间的距离矩阵，选择到其他点平均距离最大的点，
记为$q$，取该点作为新组起始点</li>
<li>$\forall p, p \notin C_1$，计算
$d_arg(p, C_1) - d_arg(p, C_2)$，
若小于零则属于$C_1$，否则属于$C_2$</li>
</ul>
<h3 id="Balanced-Itertive-Reducing-and-Clustering-Using-Hierarchies"><a href="#Balanced-Itertive-Reducing-and-Clustering-Using-Hierarchies" class="headerlink" title="Balanced Itertive Reducing and Clustering Using Hierarchies"></a>Balanced Itertive Reducing and Clustering Using Hierarchies</h3><p><em>BIRCH</em>：利用层次方法的平衡迭代规约和聚类，利用层次方法聚类
、规约数据</p>
<ul>
<li>特点<ul>
<li>利用CF树结构快速聚类</li>
<li>只需要单遍扫描数据</li>
<li>适合在数据类型为数值型、数据量大时使用</li>
</ul>
</li>
</ul>
<h3 id="常见算法、改进"><a href="#常见算法、改进" class="headerlink" title="常见算法、改进"></a>常见算法、改进</h3><ul>
<li>A Hierarchical Clustering Algorithm Using Dynamic
Modeling：使用KNN算法计算作为linkage、构建图<ul>
<li>较BIRCH好，但算法复杂度依然为$O(n^2)$</li>
<li>可以处理比较复杂形状</li>
</ul>
</li>
</ul>
<h2 id="Partition-Based-Methods"><a href="#Partition-Based-Methods" class="headerlink" title="Partition-Based Methods"></a>Partition-Based Methods</h2><p>基于划分的方法</p>
<ul>
<li><p>基本流程</p>
<ul>
<li>确定需要聚类的数目，挑选相应数量点作为初始中心点</li>
<li>再根据预定的启发式算法队数据点做迭代</li>
<li>直到达到类簇内点足够近、类簇间点足够远</li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>对大型数据集同样简单高效、时空复杂度低</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>数据集越大，结果容易越容易陷入局部最优</li>
<li>需要预先设置k值，对初始k中心点选取敏感</li>
<li>对噪声、离群点敏感</li>
<li>只适合数值性</li>
<li>不适合非凸形状</li>
</ul>
</li>
<li><p>影响结果因素</p>
<ul>
<li>原始问题是否可分</li>
<li>分类数目K</li>
<li>初始点选择</li>
</ul>
</li>
</ul>
<h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><ul>
<li><p>数据：$\Omega={X_1, X_2, \dots, X_N}$，分k个组</p>
<script type="math/tex; mode=display">
C_1, C_2, \dots, C_k \\
C_1 \cup C_2 \cup \dots \cup C_k = \Omega \\</script><p>每个样本点包含p个特征：$X_i = (x_1, x_2, \dots, x_p)$</p>
</li>
<li><p>目标：极小化每个样本点到聚类中心距离之和</p>
<script type="math/tex; mode=display">
\arg_{C_1, C_2, \dots, C_K} \min \sum_{i=1}^K
   \sum_{x_j in \C_i} d(x_j, C_i)</script><ul>
<li>若定义距离为平方欧式距离，则根据组间+组内=全，
极小化目标就是中心点距离极大化</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>优化问题是NP-hard问题，需要采用近似方法</li>
</ul>
</blockquote>
<h4 id="K值选择"><a href="#K值选择" class="headerlink" title="K值选择"></a>K值选择</h4><ul>
<li>经验选择</li>
<li>特殊方法：Elbow Method，肘部法则，画出距离和K的点图，
选择剧烈变化的点的K值</li>
</ul>
<h4 id="Lloyd’s-Algorithm"><a href="#Lloyd’s-Algorithm" class="headerlink" title="Lloyd’s Algorithm"></a>Lloyd’s Algorithm</h4><ul>
<li>随机选择K对象，每个对象初始地代表类簇中心</li>
<li>对剩余对象，计算与各簇中心距离，归于距离最近地类簇</li>
<li>重新计算各类簇平均值作为新簇中心</li>
<li>不断重复直至准则函数收敛</li>
</ul>
<blockquote>
<ul>
<li>算法时间效率：$\in O(K * N^{\pi})$</li>
</ul>
</blockquote>
<h3 id="常见算法、改进-1"><a href="#常见算法、改进-1" class="headerlink" title="常见算法、改进"></a>常见算法、改进</h3><ul>
<li>K-means++、Intelligent K-means、Genetic K-means：改进
K-means对初值敏感</li>
<li>K-medoids、K-medians：改进K-means对噪声、离群点敏感</li>
<li>K-modes：适用于分类型数据</li>
<li>Kernel-Kmeans：可以解决非凸问题</li>
</ul>
<h2 id="Density-Based-Methods"><a href="#Density-Based-Methods" class="headerlink" title="Density-Based Methods"></a>Density-Based Methods</h2><p>基于密度的方法</p>
<ul>
<li>优点<ul>
<li>对噪声不敏感</li>
<li>能发现任意形状聚类</li>
</ul>
</li>
<li>缺点<ul>
<li>聚类结果和参数关系很大</li>
</ul>
</li>
</ul>
<h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><ul>
<li><p>核心点：半径eps的邻域内点数量不少于阈值MinPts的点</p>
</li>
<li><p>直接可达：核心点半径eps的领域内被称为直接可达</p>
<ul>
<li><strong>没有任何点是由非核心点直接可达的</strong></li>
</ul>
</li>
<li><p>可达：若存在$p_1, \cdots, p_n$点列中相邻点直接可达，
则$p_1, p_n$可达</p>
<ul>
<li>非对称关系，因为核心点没有直接可达点</li>
</ul>
</li>
<li><p>连接性：若存在点$o$可达$p,q$，则$p,q$称为[密度]连接</p>
<ul>
<li>对称关系</li>
<li>聚类内点都是相连接的</li>
<li>若p由q可达，则p在q所属聚类中</li>
</ul>
</li>
<li><p>局外点：不由任何点可达的点</p>
</li>
</ul>
<h3 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h3><blockquote>
<ul>
<li>Density-Based Spatial Clustering of Applications with Noise</li>
</ul>
</blockquote>
<h4 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h4><ul>
<li>从任意对象点p开始</li>
<li>寻找合并核心点p对象直接密度可达对象<ul>
<li>若p是核心点，则找到聚类</li>
<li>若p是边界，则寻找下个对象点</li>
</ul>
</li>
<li>重复直到所有点被处理</li>
</ul>
<h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><ul>
<li><p>DBSCAN用固定参数识别聚类，类簇稀疏程度不同时，相同判断
标准会破坏类自然结构</p>
<ul>
<li>较稀疏类簇会被划分为多个</li>
<li>密度大距离近多个类被合并</li>
</ul>
</li>
<li><p>参数影响</p>
<ul>
<li>eps过大大多数点聚为同一簇中、过小则会导致簇分裂</li>
<li>MinPts值过大则同簇中点被标记为噪声点、过小则有大量
核心点</li>
</ul>
</li>
<li><p>超参半径eps、最小点数量MinPts经验选取</p>
<ul>
<li>计算所有点k距离</li>
<li>对各点k距离排序、绘制折线图</li>
<li>观察折线图，以发现极具变化的位置对应k距离作为半径</li>
<li>k即作为最小点数量</li>
</ul>
<blockquote>
<ul>
<li>k距离：距离点第k近点距离</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="常见算法、改进-2"><a href="#常见算法、改进-2" class="headerlink" title="常见算法、改进"></a>常见算法、改进</h3><ul>
<li>Ordering Points to Indentify Clustering Structure：优先
搜索高密度，然后根据高密度特点设置参数，改善DBSCAN</li>
</ul>
<h2 id="Grid-Based-Methods"><a href="#Grid-Based-Methods" class="headerlink" title="Grid-Based Methods"></a>Grid-Based Methods</h2><p>基于网络的方法</p>
<ul>
<li><p>优点</p>
<ul>
<li>速度快，速度与数据对象个数无关，只依赖数据空间中每维
上单元数目</li>
<li>可以和基于密度算法共同使用</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>对参数敏感</li>
<li>无法处理不规则分布的数据</li>
<li>维数灾难</li>
<li>聚类结果精确性低：算法效率提高的代价</li>
</ul>
</li>
</ul>
<h3 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h3><ul>
<li>将数据空间划分为网格单元：不同算法主要区别</li>
<li>将数据对象集映射到网格单元中，计算各单元密度</li>
<li>根据预设的阈值判断每个网格单元是否为高密度单元</li>
<li>将相连的高度密度网格单元识别为类簇</li>
</ul>
<h3 id="常见算法、改进-3"><a href="#常见算法、改进-3" class="headerlink" title="常见算法、改进"></a>常见算法、改进</h3><ul>
<li>statistical information grid</li>
<li>wave-cluster</li>
<li>clustering-quest</li>
</ul>
<h2 id="Model-Based-Methods"><a href="#Model-Based-Methods" class="headerlink" title="Model-Based Methods"></a>Model-Based Methods</h2><p>基于模型的方法：为每个类簇假定模型，寻找对给定模型的最佳拟合</p>
<ul>
<li>优点<ul>
<li>对类划分以概率形式表示</li>
<li>每类特征可以用概率表达</li>
</ul>
</li>
<li>缺点<ul>
<li>执行效率不高，尤其是分布数量多、数据量少时</li>
</ul>
</li>
</ul>
<h3 id="SOM"><a href="#SOM" class="headerlink" title="SOM"></a>SOM</h3><p>SOM：假设输入对象中存在一些拓扑结构、顺序，可以实现从输入
空间到输入平面的降维映射，且映射具有拓扑特征保持性质</p>
<ul>
<li><p>网络结构</p>
<ul>
<li>输入层：高维输入向量</li>
<li>输入层：2维网络上的有序节点</li>
</ul>
</li>
<li><p>学习过程</p>
<ul>
<li>找到、更新与输入节点距离最短的输出层单元，即获胜单元</li>
<li>更新邻近区域权值，保持输出节点具有输入向量拓扑特征</li>
</ul>
</li>
</ul>
<h4 id="SOM算法流程"><a href="#SOM算法流程" class="headerlink" title="SOM算法流程"></a>SOM算法流程</h4><ul>
<li>网络初始化：初始化输出层节点权重</li>
<li>随机选取输入样本作为输入向量，找到与输入向量距离最小的
权重向量</li>
<li>定义获胜单元，调整获胜单元邻近区域权重、向输入向量靠拢</li>
<li>收缩邻域半径、减小学习率、重复，直到小于允许值，输出聚类
结果</li>
</ul>
<h3 id="常见算法"><a href="#常见算法" class="headerlink" title="常见算法"></a>常见算法</h3><ul>
<li>概率生成模型：假设数据是根据潜在概率分布生成<ul>
<li>Gaussian Mixture Model</li>
</ul>
</li>
<li>基于神经网络模型的方法<ul>
<li>Self Organized Maps</li>
</ul>
</li>
</ul>
<h2 id="模糊聚类"><a href="#模糊聚类" class="headerlink" title="模糊聚类"></a>模糊聚类</h2><p>模糊聚类：样本以一定概率属于某个类</p>
<ul>
<li>优点<ul>
<li>对正态分布的数据聚类效果较好</li>
<li>算法对孤立点敏感</li>
</ul>
</li>
</ul>
<h2 id="Fuzzy-C-means-FCM"><a href="#Fuzzy-C-means-FCM" class="headerlink" title="Fuzzy C-means(FCM)"></a>Fuzzy C-means(FCM)</h2><p><em>FCM</em>：对K-means的推广软聚类</p>
<ul>
<li><p>算法最终输出$C$个聚类中心向量、$C*N$模糊划分矩阵</p>
<ul>
<li>表示每个样本点对每个类的隶属度</li>
<li>根据划分矩阵、按照最大隶属原则确定样本点归属</li>
<li>聚类中心表示类平均特征，可以作为类代表</li>
</ul>
</li>
<li><p>特点</p>
<ul>
<li>算法性能依赖初始聚类中心，需要依赖其他算法快速确定
初始聚类中心、或多次执行算法</li>
<li>不能确保收敛于最优解</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>soft cluster</em>：点可以属于多个类</li>
</ul>
</blockquote>
<h4 id="参数选择"><a href="#参数选择" class="headerlink" title="参数选择"></a>参数选择</h4><ul>
<li>聚类数目$C$：$C$远远小于聚类样本总数目，且大于1</li>
<li>柔性参数$m$<ul>
<li>$m$过大：聚类效果差</li>
<li>$m$过小：算法接近HCM聚类算法</li>
</ul>
</li>
</ul>
<h4 id="算法流程-2"><a href="#算法流程-2" class="headerlink" title="算法流程"></a>算法流程</h4><ul>
<li>标准化数据矩阵</li>
<li>建立模糊相似矩阵，初始化隶属矩阵</li>
<li><p>迭代，直到目标函数收敛到极小值</p>
<script type="math/tex; mode=display">
w_k(x_i) = \frac 1 \sum_{i=1}^k 
   (\frac {d(x_i, \mu_k)} {d(x_i, \mu_i} )^{1/(m-2)})</script></li>
<li><p>根据迭代结果，由最终隶属矩阵确定数据所属类，得到聚类结果</p>
</li>
</ul>
<h3 id="常见算法、改进-4"><a href="#常见算法、改进-4" class="headerlink" title="常见算法、改进"></a>常见算法、改进</h3><ul>
<li>HCM算法</li>
</ul>
<h2 id="基于约束的算法"><a href="#基于约束的算法" class="headerlink" title="基于约束的算法"></a>基于约束的算法</h2><p>基于约束的算法：考虑聚类问题中的约束条件，利用约束知识进行
推理</p>
<ul>
<li><p>约束</p>
<ul>
<li>对聚类参数的约束</li>
<li>对数据点的约束</li>
</ul>
</li>
<li><p>典型算法</p>
<ul>
<li>Clustering with Obstructed Distance：用两点之间障碍
距离取代一般的欧式距离计算最小距离</li>
</ul>
</li>
</ul>
<h2 id="量子聚类"><a href="#量子聚类" class="headerlink" title="量子聚类"></a>量子聚类</h2><p>量子聚类：用量子理论解决聚类过程中初值依赖、确定类别数目的
问题</p>
<ul>
<li>典型算法<ul>
<li>基于相关点的Pott自旋、统计机理提供的量子聚类模型：
将聚类问题视为物理系统</li>
</ul>
</li>
</ul>
<h2 id="核聚类"><a href="#核聚类" class="headerlink" title="核聚类"></a>核聚类</h2><p>核聚类：增加对样本特征的优化过程，利用Mercer核把输入空间映射
至高维特征空间，在特征空间中进行聚类</p>
<ul>
<li><p>特点</p>
<ul>
<li>方法普适</li>
<li>性能上优于经典聚类算算法</li>
<li>可以通过非线性映射较好分辨、提取、放大有用特征</li>
<li>收敛速度快</li>
</ul>
</li>
<li><p>典型算法</p>
<ul>
<li>SVDD算法</li>
<li>SVC算法</li>
</ul>
</li>
</ul>
<h2 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h2><p>谱聚类：建立在图论中谱图理论基础上，本质是将聚类问题转换为
图的最优划分问题</p>
<h3 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h3><ul>
<li>根据样本数据集定义描述成对数据点的相似度亲和矩阵</li>
<li>计算矩阵特征值、特征向量</li>
<li>选择合适的特征向量聚类不同点</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">20 minutes read (About 2987 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/model_enhancement.html">Model Enhancement</a></h1><div class="content"><h2 id="Emsemble-Learning"><a href="#Emsemble-Learning" class="headerlink" title="Emsemble Learning"></a>Emsemble Learning</h2><blockquote>
<ul>
<li>集成学习：训练多个基模型，并将其组合起来，以达到更好的
  预测能力、泛化能力、稳健性</li>
<li><em>base learner</em>：基模型，基于<strong>独立样本</strong>建立的、一组
  <strong>具有相同形式</strong>的模型中的一个</li>
<li>组合预测模型：由基模型组合，即集成学习最终习得模型</li>
</ul>
</blockquote>
<ul>
<li><p>源于样本均值抽样分布思路</p>
<ul>
<li>$var(\bar{X}) = \sigma^2 / n$</li>
<li>基于独立样本，建立一组具有相同形式的基模型</li>
<li>预测由这组模型共同参与</li>
<li>组合预测模型稳健性更高，类似于样本均值抽样分布方差
更小</li>
</ul>
</li>
<li><p>关键在于</p>
<ul>
<li>获得多个独立样本的方法</li>
<li>组合多个模型的方法</li>
</ul>
</li>
</ul>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul>
<li><p><em>homogenous ensemble</em>：同源集成，基学习器属于同一类型</p>
<ul>
<li><em>bagging</em></li>
<li><em>boosting</em></li>
</ul>
</li>
<li><p><em>heterogenous ensemble</em>：异源集成，基学习器不一定属于同
一类型</p>
<ul>
<li><em>[genralization] stacking</em></li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Target</th>
<th>Data</th>
<th>parallel</th>
<th>Classifier</th>
<th>Aggregation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bagging</td>
<td>减少方差</td>
<td>基于boostrap随机抽样，抗异常值、噪声</td>
<td>模型间并行</td>
<td>同源不相关基学习器，一般是树</td>
<td>分类：投票、回归：平均</td>
</tr>
<tr>
<td>Boosting</td>
<td>减少偏差</td>
<td>基于误分分步</td>
<td>模型间串行</td>
<td>同源若学习器</td>
<td>加权投票</td>
</tr>
<tr>
<td>Stacking</td>
<td>减少方差、偏差</td>
<td>K折交叉验证数据、基学习器输出</td>
<td>层内模型并行、层间串行</td>
<td>异质强学习器</td>
<td>元学习器</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<ul>
<li>以上都是指原始版本、主要用途</li>
</ul>
</blockquote>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>提升方法：将弱可学习算法<strong>提升</strong>为强可学习算法的组合元算法</p>
<ul>
<li>属于加法模型：即基函数的线性组合</li>
<li>各模型之间存在依赖关系</li>
</ul>
<p><img src="/imgs/boosting.png" alt="boosting"></p>
<h4 id="分类Boosting"><a href="#分类Boosting" class="headerlink" title="分类Boosting"></a>分类Boosting</h4><blockquote>
<ul>
<li><strong>依次</strong>学习多个基分类器</li>
<li>每个基分类器<strong>依之前分类结果调整权重</strong></li>
<li><strong>堆叠</strong>多个分类器提高分类准确率</li>
</ul>
</blockquote>
<ul>
<li><p>boosting通过组合多个误分率略好于随机猜测的分类器得到
误分率较小的分类器，因此boosting适合这两类问题</p>
<ul>
<li>个体之间难度有很大不同，boosting能够更加关注较难的
个体</li>
<li>学习器对训练集敏感，boosting驱使学习器在趋同的、
“较难”的分布上学习，此时boosting就和bagging一样能够
使得模型更加稳健（但原理不同）</li>
</ul>
</li>
<li><p>boosting能减小预测方差、偏差、过拟合</p>
<ul>
<li><p>直觉上，使用在不同的样本上训练的基学习器加权组合，
本身就能减小学习器的随机变动</p>
</li>
<li><p>基于同样的理由，boosting同时也能减小偏差</p>
</li>
<li><p>过拟合对集成学习有些时候有正面效果，其带来多样性，
使模型泛化能力更好，前提是样本两足够大，否则小样本
仍然无法提供多样性</p>
</li>
</ul>
</li>
</ul>
<h4 id="回归Boosting"><a href="#回归Boosting" class="headerlink" title="回归Boosting"></a>回归Boosting</h4><blockquote>
<ul>
<li><strong>依次</strong>训练多个基学习器</li>
<li>每个基学习器以<strong>之前学习器拟合残差</strong>为目标</li>
<li><strong>堆叠</strong>多个学习器减少整体损失</li>
</ul>
</blockquote>
<ul>
<li><p>boosting组合模型整体损失（结构化风险）</p>
<script type="math/tex; mode=display">
R_{srm} = \sum_{i=1}^N l(y_i, \hat y_i) +
   \sum_{t=1}^M \Omega(f_t)</script><blockquote>
<ul>
<li>$l$：损失函数</li>
<li>$f_t$：基学习器</li>
<li>$\Omega(f_t)$：单个基学习器的复杂度罚</li>
<li>$N, M$：样本数目、学习器数目</li>
</ul>
</blockquote>
</li>
<li><p>基学习器损失</p>
<script type="math/tex; mode=display">
obj^{(t)} = \sum_{i=1}^N l(y_i, \hat y_i^{(t)}) +
   \Omega(f_t)</script></li>
</ul>
<h4 id="最速下降法"><a href="#最速下降法" class="headerlink" title="最速下降法"></a>最速下降法</h4><p>使用线性函数拟合$l(y_i, \hat y_i^{(t)})$</p>
<script type="math/tex; mode=display">\begin{align*}
obj^{(t)} & = \sum_i^N l(y_i, \hat y_i^{(t-1)} + f_t(x_i)) +
    \Omega(f_t) \\
& \approx \sum_{i=1}^N [l(y_i, \hat y^{(t-1)}) + g_i f_t(x_i)]
    + \Omega(f_t)
\end{align*}</script><blockquote>
<ul>
<li>$g<em>i = \partial</em>{\hat y} l(y_i, \hat y^{t-1})$</li>
</ul>
</blockquote>
<ul>
<li>一次函数没有极值</li>
<li>将所有样本损失视为向量（学习器权重整体施加），则负梯度
方向损失下降最快，考虑使用负梯度作为伪残差</li>
</ul>
<h4 id="Newton法"><a href="#Newton法" class="headerlink" title="Newton法"></a>Newton法</h4><p>使用二次函数拟合$l(y_i, \hat y_i^{(t)}$</p>
<script type="math/tex; mode=display">\begin{align*}
obj^{(t)} & = \sum_i^N l(y_i, \hat y_i^{(t-1)} + f_t(x_i)) +
    \Omega(f_t) \\
& \approx \sum_{i=1}^N [l(y_i, \hat y^{(t-1)}) + g_i f_t(x_i)
    + \frac 1 2 h_i f_t^2(x_i)] + \Omega(f_t) \\
\end{align*}</script><blockquote>
<ul>
<li>$h<em>i = \partial^2</em>{\hat y} l(y_i, \hat y^{t-1})$</li>
</ul>
</blockquote>
<ul>
<li>二次函数本身有极值</li>
<li>可以结合复杂度罚综合考虑，使得每个基学习器损失达到最小</li>
</ul>
<h3 id="Boosting-amp-Bagging"><a href="#Boosting-amp-Bagging" class="headerlink" title="Boosting&amp;Bagging"></a>Boosting&amp;Bagging</h3><ul>
<li><p>基分类器足够简单时，boosting表现均显著好于bagging</p>
<ul>
<li>仅靠单次决策（单个属性、属性组合）分类</li>
</ul>
</li>
<li><p>使用C4.5树作为基分类器时，boosting仍然具有优势，但是不够
有说服力</p>
</li>
</ul>
<blockquote>
<ul>
<li>结论来自于<em>Experiments with a New Boosting Algorithm</em></li>
</ul>
</blockquote>
<h4 id="Boosting-amp-Bagging-1"><a href="#Boosting-amp-Bagging-1" class="headerlink" title="Boosting&amp;Bagging"></a>Boosting&amp;Bagging</h4><ul>
<li><p>基分类器足够简单时，boosting表现均显著好于bagging</p>
<ul>
<li>仅靠单次决策（单个属性、属性组合）分类</li>
</ul>
</li>
<li><p>使用C4.5树作为基分类器时，boosting仍然具有优势，但是不够
有说服力</p>
</li>
</ul>
<blockquote>
<ul>
<li>结论来自于<em>Experiments with a New Boosting Algorithm</em></li>
</ul>
</blockquote>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><em>probably approximately correct</em>：概率近似正确，在概率近似
正确学习的框架中</p>
<ul>
<li><p><em>strongly learnable</em>：强可学习，一个概念（类），如果存在
一个多项式的学习算法能够学习它，并且<strong>正确率很高</strong>，那么
就称为这个概念是强可学习的</p>
</li>
<li><p><em>weakly learnable</em>：弱可学习，一个概念（类），如果存在
一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测
略好，称此概念为弱可学习的</p>
</li>
<li><p><em>Schapire</em>证明：在PAC框架下强可学习和弱可学习是等价的</p>
</li>
</ul>
<h3 id="具体措施"><a href="#具体措施" class="headerlink" title="具体措施"></a>具体措施</h3><blockquote>
<ul>
<li>弱学习算法要比强学习算法更容易寻找，所以具体实施提升就是
  需要解决的问题</li>
</ul>
</blockquote>
<ul>
<li><p><strong>改变训练数据权值、概率分布的方法</strong></p>
<ul>
<li>提高分类错误样本权值、降低分类正确样本权值</li>
</ul>
</li>
<li><p><strong>将弱学习器组合成强学习器的方法</strong></p>
<ul>
<li><em>competeing</em></li>
<li><em>simple majority voting</em></li>
<li><em>weighted majority voting</em></li>
<li><em>confidence-based weighting</em></li>
</ul>
</li>
</ul>
<h3 id="学习器组合方式"><a href="#学习器组合方式" class="headerlink" title="学习器组合方式"></a>学习器组合方式</h3><blockquote>
<ul>
<li>很多模型无法直接组合，只能组合预测结果</li>
</ul>
</blockquote>
<ul>
<li><p><em>simple majority voting</em>/<em>simple average</em>：简单平均</p>
<script type="math/tex; mode=display">
h = \frac 1 K \sum_{k=1}_K h_k</script><blockquote>
<ul>
<li>$h_k$：第k个预测</li>
</ul>
</blockquote>
</li>
<li><p><em>weighted majority voting</em>/<em>weighted average</em>：加权平均</p>
<script type="math/tex; mode=display">
h = \frac {\sum_{k=1}^K w_k h_k} {\sum_{k=1}^K w_k}</script><blockquote>
<ul>
<li>$w_k$：第k个预测权重，对分类器可以是准确率</li>
</ul>
</blockquote>
</li>
<li><p><em>competing voting</em>/<em>largest</em>：使用效果最优者</p>
</li>
<li><p><em>confidence based weighted</em>：基于置信度加权</p>
<script type="math/tex; mode=display">\begin{align*}
h = \arg\max_{y \in Y} \sum_{k=1}^K ln(\frac {1 - e_k}
   {e_k}) h_k
\end{align*}</script><blockquote>
<ul>
<li>$e_k$：第k个模型损失</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="Meta-Learning"><a href="#Meta-Learning" class="headerlink" title="Meta Learning"></a>Meta Learning</h2><p>元学习：自动学习关于关于机器学习的元数据的机器学习子领域</p>
<ul>
<li><p>元学习主要目标：使用学习到元数据解释，自动学习如何
<em>flexible</em>的解决学习问题，借此提升现有学习算法性能、
学习新的学习算法，即学习学习</p>
</li>
<li><p>学习算法灵活性即可迁移性，非常重要</p>
<ul>
<li>学习算法往往基于某个具体、假象的数据集，有偏</li>
<li>学习问题、学习算法有效性之间的关系没有完全明白，对
学习算法的应用有极大限制</li>
</ul>
</li>
</ul>
<h3 id="要素"><a href="#要素" class="headerlink" title="要素"></a>要素</h3><ul>
<li>元学习系统必须包含子学习系统</li>
<li>学习经验通过提取元知识获得经验，元知识可以在先前单个
数据集，或不同的领域中获得</li>
<li>学习<em>bias</em>（影响用于模型选择的前提）必须动态选择<ul>
<li><em>declarative bias</em>：声明性偏见，确定假设空间的形式
，影响搜索空间的大小<ul>
<li>如：只允许线性模型</li>
</ul>
</li>
<li><em>procedural bias</em>：过程性偏见，确定模型的优先级<ul>
<li>如：简单模型更好</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Recurrent-Neural-networks"><a href="#Recurrent-Neural-networks" class="headerlink" title="Recurrent Neural networks"></a><em>Recurrent Neural networks</em></h3><p>RNN：<em>self-referential</em> RNN理论上可以通过反向传播学习到，
和反向传播完全不同的权值调整算法</p>
<h3 id="Meta-Reinforcement-Learning"><a href="#Meta-Reinforcement-Learning" class="headerlink" title="Meta Reinforcement Learning"></a><em>Meta Reinforcement Learning</em></h3><p>MetaRL：RL智能体目标是最大化奖励，其通过不断提升自己的学习
算法来加速获取奖励，这也涉及到自我指涉</p>
<h2 id="Additional-Model"><a href="#Additional-Model" class="headerlink" title="Additional Model"></a>Additional Model</h2><p>加法模型：将模型<strong>视为</strong>多个基模型加和而来</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M \beta_m b(x;\theta_m)</script><blockquote>
<ul>
<li>$b(x;\theta_m)$：基函数</li>
<li>$\theta_m$：基函数的参数</li>
<li>$\beta_m$：基函数的系数</li>
</ul>
</blockquote>
<ul>
<li><p>则相应风险极小化策略</p>
<script type="math/tex; mode=display">
\arg\min_{\beta_m, \theta_m} \sum_{i=1}^N
   L(y_i, \sum_{m=1}^M \beta_m b(x_i;\theta_m))</script><blockquote>
<ul>
<li>$L(y, f(x))$：损失函数</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Forward-Stagewise-Algorithm"><a href="#Forward-Stagewise-Algorithm" class="headerlink" title="Forward Stagewise Algorithm"></a>Forward Stagewise Algorithm</h3><p>前向分步算法：从前往后，每步只学习<strong>加法模型</strong>中一个基函数
及其系数，逐步逼近优化目标函数，简化优化复杂度</p>
<ul>
<li><p>即每步只求解优化</p>
<script type="math/tex; mode=display">
\arg\min_{\beta, \theta} \sum_{i=1}^N
   L(y_i, \hat f_m(x_i) + \beta b(x_i;\theta))</script><blockquote>
<ul>
<li>$\hat f_m$：前m轮基函数预测值加和</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><blockquote>
<ul>
<li>输入：训练数据集$T={(x_1,y_1), \cdots, (x_N,y_N)}$，损失
  函数$L(y,f(x))$，基函数集${b(x;\theta)}$</li>
<li>输出：加法模型$f(x)$</li>
</ul>
</blockquote>
<ul>
<li><p>初始化$f_0(x)=0$</p>
</li>
<li><p>对$m=1,2,\cdots,M$，加法模型中M个基函数</p>
<ul>
<li><p>极小化损失函数得到参数$\beta_m, \theta_m$</p>
<script type="math/tex; mode=display">
(\beta_m, \theta_m) = \arg\min_{\beta, \theta}
  \sum_{i=1}^N L(y_i, f_{m-1}(x_1) +
  \beta b(x_i; \theta))</script></li>
<li><p>更新</p>
<script type="math/tex; mode=display">
f_m(x) = f_{m-1}(x) + \beta_m b(x;y_M)</script></li>
</ul>
</li>
<li><p>得到加法模型</p>
<script type="math/tex; mode=display">
f(x) = f_M(x) = \sum_{i=1}^M \beta_m b(x;\theta_m)</script></li>
</ul>
<h3 id="AdaBoost-amp-前向分步算法"><a href="#AdaBoost-amp-前向分步算法" class="headerlink" title="AdaBoost&amp;前向分步算法"></a>AdaBoost&amp;前向分步算法</h3><p>AdaBoost（基分类器loss使用分类误差率）是前向分步算法的特例，
是由基本分类器组成的加法模型，损失函数是指数函数</p>
<ul>
<li><p>基函数为基本分类器时加法模型等价于AdaBoost的最终分类器
$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$</p>
</li>
<li><p>前向分步算法的损失函数为指数函数$L(y,f(x))=exp(-yf(x))$
时，学习的具体操作等价于AdaBoost算法具体操作</p>
<ul>
<li><p>假设经过m-1轮迭代，前向分步算法已经得到</p>
<script type="math/tex; mode=display">\begin{align*}
f_{m-1}(x) & = f_{m-2}(x) + \alpha_{m-1}G_{m-1}(x) \\
  & = \alpha_1G_1(x) + \cdots +
  \alpha_{m-1}G_{m-1}(x)
\end{align*}</script></li>
<li><p>经过第m迭代得到$\alpha_m, G_m(x), f_m(x)$，其中</p>
<script type="math/tex; mode=display">\begin{align*}
(\alpha_m, G_m(x)) & = \arg\min_{\alpha, G}
      \sum_{i=1}^N exp(-y_i(f_{m-1}(x_i) +
      \alpha G(x_i))) \\
  & = \arg\min_{\alpha, G} \sum_{i=1}^N \bar w_{m,i}
      exp(-y_i \alpha G(x_i))
\end{align*}</script><blockquote>
<ul>
<li>$\bar w<em>{m,i}=exp(-y_i f</em>{m-1}(x_i))$：不依赖
$\alpha, G$</li>
</ul>
</blockquote>
</li>
<li><p>$\forall \alpha &gt; 0$，使得损失最小应该有
（提出$\alpha$）</p>
<script type="math/tex; mode=display">\begin{align*}
G_m^{*}(x) & = \arg\min_G \sum_{i=1}^N \bar w_{m,i}
      exp(-y_i f_{m-1}(x_i)) \\
  & = \arg\min_G \sum_{i=1}^N \bar w_{m,i}
      I(y_i \neq G(x_i))
\end{align*}</script><p>此分类器$G_m^{*}$即为使得第m轮加权训练误差最小分类器
，即AdaBoost算法的基本分类器</p>
</li>
<li><p>又根据</p>
<script type="math/tex; mode=display">\begin{align*}
\sum_{i=1}^N \bar w_{m,i} exp(-y_i \alpha G(x_i)) & =
  \sum_{y_i = G_m(x_i)} \bar w_{m,i} e^{-\alpha} +
  \sum_{y_i \neq G_m(x_i)} \bar w_{m,i} e^\alpha \\
& = (e^\alpha - e^{-\alpha}) \sum_{i=1}^N (\bar w_{m,i}
  I(y_i \neq G(x_i))) + e^{-\alpha}
  \sum_{i=1}^N \bar w_{m,i}
\end{align*}</script><p>带入$G_m^{*}$，对$\alpha$求导置0，求得极小值为</p>
<script type="math/tex; mode=display">\begin{align*}
\alpha_m^{*} & = \frac 1 2 log \frac {1-e_m} {e_m} \\
e_m & = \frac {\sum_{i=1}^N (\bar w_{m,i}
      I(y_i \neq G_m(x_i)))}
  {\sum_{i=1}^N \bar w_{m,i}} \\
& = \frac {\sum_{i=1}^N (\bar w_{m,i}
      I(y_i \neq G_m(x_i)))} {Z_m} \\
& = \sum_{i=1}^N w_{m,i} I(y_i \neq G_m(x_i))
\end{align*}</script><blockquote>
<ul>
<li>$w_{m,i}, Z_M$同AdaBoost中</li>
</ul>
</blockquote>
<p>即为AdaBoost中$\alpha_m$</p>
</li>
<li><p>对权值更新有</p>
<script type="math/tex; mode=display">
\bar w_{m+1,i} = \bar w_{m,i} exp(-y_i \alpha_m G_m(x))</script><p>与AdaBoost权值更新只相差规范化因子$Z_M$</p>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:11:28.000Z" title="7/16/2021, 3:11:28 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">15 minutes read (About 2228 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/adaboost.html">AdaBoost</a></h1><div class="content"><h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><p>通过改变训练样本权重，学习多个分类器，并将分类器进行线性
组合，提高分类性能</p>
<ul>
<li>对离群点、奇异点敏感</li>
<li>对过拟合不敏感</li>
</ul>
<h3 id="Boosting实现"><a href="#Boosting实现" class="headerlink" title="Boosting实现"></a>Boosting实现</h3><blockquote>
<ul>
<li><p>改变训练数据权值或概率分布：提高分类错误样本权值、降低
  分类正确样本权值</p>
</li>
<li><p>弱分类器组合：加权多数表决，即加大分类误差率小的弱分类器
  权值，使其在表决中起更大作用；减小分类误差率大的弱分类器
  权值，使其在表决中起更小作用</p>
</li>
</ul>
</blockquote>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p><img src="/imgs/adaboost_steps.png" alt="adaboost_steps"></p>
<blockquote>
<ul>
<li>输入：训练数据集$T={(x_1, y_1), \cdots, (x_N, y_N)}$，
  弱分类器算法$G(x)$<blockquote>
<ul>
<li>$x_i \in \mathcal{X \subset R^n}$</li>
<li>$y_i \in \mathcal{Y} = {-1, +1 }$</li>
</ul>
</blockquote>
</li>
<li>输出：最终分类器$G(x)$</li>
</ul>
</blockquote>
<ul>
<li><p>初始化训练数据权值分布：
$D<em>1=(w</em>{11}, \cdots, w<em>{1N}), w</em>{1i}=\frac 1 N$</p>
</li>
<li><p>对$m=1,2,\cdots,M$（即训练M个弱分类器）</p>
<ul>
<li><p>使用具有<strong>权值分布</strong>$D_m$的训练数据学习，得到基本
分类器</p>
<script type="math/tex; mode=display">
G_m(x):\mathcal{X} \rightarrow \{-1, +1\}</script></li>
<li><p>计算$G_m(x)$在训练数据集上的<strong>分类误差率</strong></p>
<script type="math/tex; mode=display">\begin{align*}
e_m & = P(G_m(x_i)) \neq y_i) \\
  & = \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i) \\
  & = \sum_{G_m(x_i) \neq y_i} w_{mi}
\end{align*}</script></li>
<li><p>计算$G_m(x)$组合为最终分类器时权重</p>
<script type="math/tex; mode=display">
\alpha = \frac 1 2 log \frac {1-e_m} {e_m}</script><blockquote>
<ul>
<li>$\alpha_m$表示就简单分类器$G_m(x)$在最终分类器中
的重要性，随$e_m$减小而增加
（弱分类器保证$e_m \leq 1/2$）</li>
</ul>
</blockquote>
</li>
<li><p>更新训练集权值分布</p>
<script type="math/tex; mode=display">\begin{align*}
D_{m+1} & = (w_{m+1,1}, \cdots, w_{m+1,N}) \\
w_{m+1,i} & = \frac {w_{mi}} {Z_m}
  exp(-\alpha y_i G_m(x_i)) = \left \{
  \begin{array}{l}
      \frac {w_mi} {Z_m} e^{-\alpha_m},
          & G_m(x_i) = y_i \\
      \frac {w_mi} {Z_m} e^{\alpha_m},
          & G_m(x_i) \neq y_i \\
  \end{array} \right. \\
Z_m & = \sum_{i=1}^N w_{mi} exp(-\alpha_m y_i G_m(x_i))
\end{align*}</script><blockquote>
<ul>
<li>$Z<em>m$：规范化因子，是第m轮调整后的权值之和，其
使得$D</em>{m+1}$成为概率分布</li>
<li>误分类样本权值相当于被放大
$e^{2\alpha_m} = \frac {e_m} {1 - e_m}$倍</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>构建基本分类器线性组合</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M \alpha_m G_m(x)</script><p>得到最终分类器</p>
<script type="math/tex; mode=display">
G(x) = sign(f(x)) = sign(\sum_{m=1}^M \alpha_m G_m(x))</script><blockquote>
<ul>
<li>这里$\alpha_m$没有规范化，和不为1，规范化没有必要</li>
<li>$f(x)$符号决定分类预测结果，绝对值大小表示分类确信度</li>
</ul>
</blockquote>
</li>
</ul>
<blockquote>
<ul>
<li>AdaBoost中分类器学习和之后的分类误差率“无关”，基分类器
  学习算法中的loss不是分类误差率，可以是其他loss，只是需要
  考虑训练数据的权值分布<blockquote>
<ul>
<li>好像基学习器的loss就要是和集成部分调权的loss一致<h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1></li>
<li><strong>按权值分布有放回的抽样</strong>，在抽样集上进行训练</li>
<li>各样本loss按权重加权，类似分类误差率中加权</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h3 id="训练误差边界"><a href="#训练误差边界" class="headerlink" title="训练误差边界"></a>训练误差边界</h3><p>AdaBoost算法最终分类器的训练误差边界为</p>
<script type="math/tex; mode=display">
\frac 1 N \sum_{i=1}^N I(G(x_i) \neq y_i) \leq
    \frac 1 N \sum_i exp(-y_if(x_i)) = \prod_m Z_m</script><ul>
<li><p>$G(x_i) \neq y_i$时，$y_if(x_i)&lt;0$，所以
$exp(-y_i f(x_i)) \geq 1$，则不等式部分可证</p>
</li>
<li><script type="math/tex; mode=display">\begin{align*}
\frac 1 N \sum_i exp(-y_i f(x_i))
   & = \frac 1 N \sum_i exp(-\sum_{m=1}^M
       \alpha_m y_i G_m(x_i)) \\
   & = \sum_i (w_{1,i} \prod_{m=1}^M
       exp(-\alpha_m y_i G_m(x_i))) \\
   & = \sum_i (Z_1 w_{2,i} \prod_{m=2}^M
       exp(-\alpha_m y_i G_m(x_i))) \\
   & = \prod_{m=1}^M Z_i \sum_i w_{M+1,i} \\
   & = \prod_{m=1}^M Z_i
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li>AdaBoost训练误差边界性质的关键：权重调整与基本分类器权重
  调整<strong>共系数</strong>（形式不完全一样）</li>
<li>这也是AdaBoost权重调整设计的依据，方便给出误差上界</li>
</ul>
</blockquote>
<h4 id="二分类训练误差边界"><a href="#二分类训练误差边界" class="headerlink" title="二分类训练误差边界"></a>二分类训练误差边界</h4><script type="math/tex; mode=display">
\prod_{m=1}^M Z_m = \prod_{m=1}^M (2\sqrt{e_m(1-e_m)})
    = \prod_{m=1}^M \sqrt{(1-4\gamma_m^2)}
    \leq exp(-2\sum_{m=1}^M \gamma_m^2)</script><blockquote>
<ul>
<li>$\gamma_m = \frac 1 2 - e_m$</li>
</ul>
</blockquote>
<ul>
<li><script type="math/tex; mode=display">\begin{align*}
Z_m & = \sum_{i=1}^N w_{m,i} exp(-\alpha y_i G_m(x_i)) \\
   & = \sum_{y_i = G_m(x_i)} w_{m,i}e^{-\alpha_m} +
       \sum_{y_i \neq G_m(x_i)} w_{m,i}e^{\alpha_m} \\
   & = (1-e_m)e^{-\alpha_m} + e_m e^{\alpha_m} \\
   & = 2\sqrt{e_m(1-e_m)} \\
   & = \sqrt{1-4\gamma^2}
\end{align*}</script></li>
<li><p>由$\forall x \in [0, 0.5], e^{-x} &gt; \sqrt{1-2x}$可得，
$\sqrt{1-4\gamma_m^2} \leq exp(-2\gamma_m^2)$</p>
</li>
</ul>
<blockquote>
<ul>
<li>二分类AdaBoost误差边界性质的关键：$\alpha$的取值，也是
  前向分步算法（损失函数）要求</li>
<li>若存$\gamma &gt; 0$，对所有m有$\gamma_m \geq \gamma$，则<script type="math/tex; mode=display">
  \frac 1 N \sum_{i=1}^N I(G(x_i) \neq y_i) \neq
      exp(-2M\gamma^2)</script>  即AdaBoost的训练误差是<strong>指数下降</strong>的</li>
<li>分类器下界$\gamma$可以未知，AdaBoost能适应弱分类器各自
  训练误差率，所以称为<em>adptive</em></li>
</ul>
</blockquote>
<h2 id="Adaboost-M1"><a href="#Adaboost-M1" class="headerlink" title="Adaboost.M1"></a><em>Adaboost.M1</em></h2><p>Adaboost.M1是原版AdaBoost的多分类升级版，基本思想同Adaboost</p>
<h3 id="Boosting实现-1"><a href="#Boosting实现-1" class="headerlink" title="Boosting实现"></a>Boosting实现</h3><ul>
<li><p>基分类器组合方式</p>
<ul>
<li>仍然是加权投票，且投票权重同Adaboost</li>
<li>出于多分类考虑，没有使用<code>sign</code>符号函数</li>
</ul>
</li>
<li><p>改变训练数据权值或概率分布：和Adaboost形式稍有不同，但
相对的错误分类样本提升比率完全相同</p>
<ul>
<li>被上个分类器错误分类样本，权值保持不变</li>
<li>被上个分类器正确分类样本，权值缩小比例是Adaboost平方</li>
</ul>
</li>
</ul>
<h3 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h3><ul>
<li><p>输入</p>
<ul>
<li>训练集：$T={x_i, y_i}, i=1,\cdots,N; y_i \in C, C={c_1, \cdots, c_m}$</li>
<li>训练轮数：T</li>
<li>弱学习器：I</li>
</ul>
</li>
<li><p>输出：提升分类器</p>
<script type="math/tex; mode=display">
H(x) = \arg\max_{y \in C} \sum_{m=1}^M
   ln(\frac 1 {\beta_m}) [h_m(x) = y]</script><blockquote>
<ul>
<li>$h_t, h_t(x) \in C$：分类器</li>
<li>$\beta_t$：分类器权重</li>
</ul>
</blockquote>
</li>
</ul>
<p><img src="/imgs/adaboostm1_steps.png" alt="adaboostm1_steps"></p>
<h3 id="误分率上界"><a href="#误分率上界" class="headerlink" title="误分率上界"></a>误分率上界</h3><blockquote>
<ul>
<li>对弱学习算法产生的伪损失$\epsilon<em>1,\cdots,\epsilon_t$，
  记$\gamma_t = 1/2 \epsilon_t$，最终分类器$h</em>{fin}$误分率
  上界有<script type="math/tex; mode=display">
  \frac 1 N |\{i: h_{fin}(x_i) \neq y_i \}| \leq
      \prod_{t-1}^T \sqrt {1-4\gamma^2} \leq
      exp(-2 \sum_{t-1}^T \gamma^2)</script></li>
</ul>
</blockquote>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>Adaboost.M1和Adaboost基本上没有区别</p>
<ul>
<li>类别数目为2的Adaboost.M1就是Adaboost</li>
<li>同样无法处理对误分率高于0.5的情况，甚至在多分类场合，
误分率小于0.5更加难以满足</li>
<li>理论误分率上界和Adaboost相同</li>
</ul>
<h2 id="Adaboost-M2"><a href="#Adaboost-M2" class="headerlink" title="Adaboost.M2"></a><em>Adaboost.M2</em></h2><p>AdaboostM2是AdaboostM1的进阶版，更多的利用了基分类器信息</p>
<ul>
<li>要求基学习器能够输出更多信息：输出对样本分别属于各类别
的置信度向量，而不仅仅是最终标签</li>
<li>要求基分类器更加精细衡量错误：使用伪损失代替误分率
作为损失函数</li>
</ul>
<h3 id="Psuedo-Loss"><a href="#Psuedo-Loss" class="headerlink" title="Psuedo-Loss"></a><em>Psuedo-Loss</em></h3><script type="math/tex; mode=display">\begin{align*}
L & = \frac 1 2 \sum_{(i,y) \in B} D_{i,y}
    (1 - h(x_i, y_i) + h(x_i, y)) \\
& = \frac 1 2 \sum_{i=1}^N D_i (1 - h(x_i, y_i) +
    \sum_{y \neq y_i} (w_{i,y} h(x_i, y)))
\end{align*}</script><blockquote>
<ul>
<li>$D$：权重分布（行和为1，但不满足列和为1）<blockquote>
<ul>
<li>$D_{i,y}$：个体$x_i$中错误标签$y$的权重，代表从个体
 $x_i$中识别出错误标签$y$的重要性</li>
</ul>
</blockquote>
</li>
<li>$B = {(i, y)|y \neq y_i, i=1,2,\cdots,N }$</li>
<li>$w$：个体各错误标签权重边际分布</li>
<li>$h(x, y)$：模型$h$预测样本$x$为$y$的置信度<blockquote>
<ul>
<li>$h(x_i,y_i)$：预测正确的置信度</li>
<li>$h(x_i,y), y \neq y_i$：预测$x_i$为错误分类$y$置信度</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li>伪损失函数同时考虑了样本和<strong>标签</strong>的权重分布</li>
<li>通过改变此分布，能够更明确的关注难以预测的个体标签，
而不仅仅个体</li>
</ul>
<h3 id="Boosting实现-2"><a href="#Boosting实现-2" class="headerlink" title="Boosting实现"></a>Boosting实现</h3><ul>
<li><p>改变数据权值或者概率分布</p>
<ul>
<li>使用<em>psuedo-loss</em>替代误分率，以此为导向改变权值</li>
<li>对多分类每个错误分类概率分别计算错误占比，在此基础上
分别计算</li>
</ul>
</li>
<li><p>基分类器组合方式：同Adaboost.M1</p>
</li>
</ul>
<h3 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a>步骤</h3><p><img src="/imgs/adaboostm2_steps.png" alt="adaboostm2_steps"></p>
<h3 id="训练误差上界"><a href="#训练误差上界" class="headerlink" title="训练误差上界"></a>训练误差上界</h3><blockquote>
<ul>
<li>对弱学习算法产生的伪损失$\epsilon<em>1,\cdots,\epsilon_t$，
  记$\gamma_t = 1/2 \epsilon_t$，最终分类器$h</em>{fin}$误分率
  上界有</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\frac 1 N |\{i: h_{fn}(x_i) \neq y_i \}| \leq
    (M-1) \prod_{t-1}^T \sqrt {1-4\gamma^2} \leq
    (M-1) exp(-2 \sum_{t-1}^T \gamma^2)</script><h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>基于伪损失的Adaboost.M2能够提升稍微好于随机预测的分类器</p>
</li>
<li><p>Adaboosting.M2能够较好的解决基分类器对噪声的敏感性，但是
仍然距离理论最优<em>Bayes Error</em>有较大差距，额外误差主要
来自于</p>
<ul>
<li>训练数据</li>
<li>过拟合</li>
<li>泛化能力</li>
</ul>
</li>
<li><p>控制权值可以有效的提升算法，减小最小训练误差、过拟合
、泛化能力</p>
<ul>
<li>如对权值使用原始样本比例作为先验加权</li>
</ul>
</li>
<li><p>其分类结果不差于AdaBoost.M1（在某些基分类器、数据集下）</p>
</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/tags/ML-Model/">Previous</a></div><div class="pagination-next"><a href="/tags/ML-Model/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/tags/ML-Model/">1</a></li><li><a class="pagination-link is-current" href="/tags/ML-Model/page/2/">2</a></li><li><a class="pagination-link" href="/tags/ML-Model/page/3/">3</a></li><li><a class="pagination-link" href="/tags/ML-Model/page/4/">4</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/WWxwMVBsOUtoNTdMSlJ4Z1lWVnRISERsd2t6ek9MeVpEUWs0YldlZGxUdXlKdDNmMEZVV1hWaFZFYWFSNmFKL25penZodWx3UzRiaVkxcnREWCtOYUJhZWhNbWpzaUdyc1hPangycUh5RTVjRXFnZnFGdVdSTzZvVzJBcTJHKzl8aXpDK1ROWWl4N080YkFEK3QvbEVWNEJuQjFqdWdxODZQcGNoM1NqbERXST0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>