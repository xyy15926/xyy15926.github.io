<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: Math Mixin - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">Math Mixin</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-07-12T02:06:33.000Z" title="7/12/2021, 10:06:33 AM">2021-07-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-19T11:00:51.000Z" title="7/19/2021, 7:00:51 PM">2021-07-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a><span> / </span><a class="link-muted" href="/categories/Math-Mixin/Statistics/">Statistics</a></span><span class="level-item">a few seconds read (About 111 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/Statistics/stat_stats.html">统计量</a></h1><div class="content"><h2 id="统计量"><a href="#统计量" class="headerlink" title="统计量"></a>统计量</h2><p>统计量：统计理论中对数据进行分析、检验的变量</p>
<ul>
<li><p>传统的统计量具有显式解析表达式</p>
<ul>
<li>均值：数据之和除数量</li>
<li>中位数：数据中间者</li>
</ul>
</li>
<li><p>统计量同样可以理解为和数据相关<strong>优化问题的解</strong></p>
<ul>
<li>均值：离差平方和最小</li>
<li>中位数：划分均匀</li>
</ul>
<blockquote>
<ul>
<li>优化问题目标本身也是统计量</li>
</ul>
</blockquote>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-07-12T02:03:58.000Z" title="7/12/2021, 10:03:58 AM">2021-07-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-12T02:03:58.000Z" title="7/12/2021, 10:03:58 AM">2021-07-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a><span> / </span><a class="link-muted" href="/categories/Math-Mixin/Statistics/">Statistics</a></span><span class="level-item">7 minutes read (About 1069 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/Statistics/stat_derived.html">统计量 - 衍生特征</a></h1><div class="content"><h2 id="Odds-Odds-Ratio"><a href="#Odds-Odds-Ratio" class="headerlink" title="Odds/Odds Ratio"></a><em>Odds/Odds Ratio</em></h2><ul>
<li><p><em>Odds</em>：几率/优势，事件发生与不发生的概率比值</p>
<script type="math/tex; mode=display">
odds = \frac p {1-p}</script><blockquote>
<ul>
<li>$p$：事件发生概率</li>
</ul>
</blockquote>
</li>
<li><p><em>Odds Ratio</em>：优势比，两组事件 <em>odds</em> 的比值</p>
<script type="math/tex; mode=display">
OR = \frac {odds_1} {odds_0}</script></li>
</ul>
<h2 id="WOE-值"><a href="#WOE-值" class="headerlink" title="WOE 值"></a><em>WOE</em> 值</h2><p><em>WOE</em> 值：将预测变量（二分类场景中）集中度作为分类变量编码的数值</p>
<script type="math/tex; mode=display">\begin{align*}
WOE_i & = log(\frac {\%B_i} {\%G_i}) \\
& = log(\frac {\#B_i / \#B_T} {\#G_i / \#G_T}) \\
& = log(\frac {\#B_i / \#G_i} {\#B_T / \#G_T}) \\
& = log(\frac {\#B_i} {\#G_i}) - log(\frac {\#B_T} {\#G_T}) \\
& = log(\frac {\#B_i / ({\#B_i + \#G_i})}
    {\#G_i / (\#B_i + \#G_i)}) -
    log(\frac {\#B_T} {\#G_T}) \\
& = log(odds_i) - log(odds_T)
\end{align*}</script><blockquote>
<ul>
<li>$\%B_i, \%G_i$：分类变量取第 $i$ 值时，预测变量为 <em>B</em> 类、<em>G</em> 类占所有 <em>B</em> 类、<em>G</em> 类比例</li>
<li>$#B_i, #B_T$：分类变量取第 $i$ 值时预测变量为 <em>B</em> 类数量，所有 <em>B</em> 类总数量</li>
<li>$#G_i, #G_T$：分类变量取第 $i$ 值时预测变量为 <em>G</em> 类数量，所有 <em>G</em> 类样本总数量</li>
<li>$odds_i$：分类变量取第 $i$ 值时，预测变量取 <em>B</em> 类优势</li>
<li>$odds_T$：所有样本中，预测变量取 <em>B</em> 类优势</li>
<li>其中 $log$ 一般取自然对数</li>
</ul>
</blockquote>
<ul>
<li><p><em>WOE</em> 编码是有监督的编码方式，可以衡量分类变量各取值中</p>
<ul>
<li><em>B</em> 类占所有 <em>B</em> 类样本比例、<em>G</em> 类占所有 <em>G</em> 类样本比例的差异</li>
<li><em>B</em> 类、<em>G</em> 类比例，与所有样本中 <em>B</em> 类、<em>G</em> 类比例的差异</li>
</ul>
</li>
<li><p><em>WOE</em> 编码值能体现分类变量取值的预测能力，变量各取值 <em>WOE</em> 值方差越大，变量预测能力越强</p>
<ul>
<li><em>WOE</em> 越大，表明该取值对应的取 <em>B</em> 类可能性越大</li>
<li><em>WOE</em> 越小，表明该取值对应的取 <em>G</em> 类可能性越大</li>
<li><em>WOE</em> 接近 0，表明该取值预测能力弱，对应取 <em>B</em> 类、<em>G</em> 类可能性相近</li>
</ul>
</li>
</ul>
<h3 id="OR与WOE线性性"><a href="#OR与WOE线性性" class="headerlink" title="OR与WOE线性性"></a>OR与WOE线性性</h3><script type="math/tex; mode=display">\begin{align*}
log(OR_{j,i}) &= log(odds_i) - log(odds_j) \\
&= WOE_i - WOE_j
\end{align*}</script><ul>
<li><p>即：预测变量对数优势值与 <em>WOE</em> 值呈线性函数关系</p>
<ul>
<li>预测变量在取 $i,j$ 值情况下，预测变量优势之差为取 $i,j$ 值的 <em>WOE</em> 值之差</li>
<li><em>WOE</em> 值编码时，分类变量在不同取值间跳转时类似于线性回归中数值型变量</li>
</ul>
<p><img src="/imgs/woe_encoding_linear_sketch.png" alt="woe_encoding_linear_sketch"></p>
</li>
<li><p>考虑到对数优势的数学形式，单变量 <em>LR</em> 模型中分类型变量 <em>WOE</em> 值可以类似数值型变量直接入模</p>
<ul>
<li>当然，<em>WOE</em> 值编码在多元 <em>LR</em> 中无法保证单变量分类情况下的线性</li>
<li>或者说多变量 <em>LR</em> 中个变量系数值不一定为 1</li>
<li>在基于单变量预测能力优秀在多变量场合也优秀的假设下，<em>WOE</em> 值编码（<em>IV</em> 值）等单变量分析依然有价值</li>
</ul>
</li>
</ul>
<h3 id="Bayes-Factor、WOE-编码、多元-LR"><a href="#Bayes-Factor、WOE-编码、多元-LR" class="headerlink" title="Bayes Factor、WOE 编码、多元 LR"></a><em>Bayes Factor</em>、<em>WOE</em> 编码、多元 <em>LR</em></h3><script type="math/tex; mode=display">\begin{align*}
ln(\frac {P(Y=1|x_1,x_2,\cdots,x_D)}
    {P(Y=0|x_1,x_2,\cdots,x_D)})
    &= ln(\frac {P(Y=1)} {P(Y=0)}) \\
    & \overset {conditionally independent} {=}
        ln (\frac {P(Y=1)} {P(Y=0)}) + 
        \sum_{i=1}^D ln(\frac {P(x_i|Y=1)} {P(x_i|Y=0)}) \\
ln(\frac {P(Y=1|x_1,x_2,\cdots,x_D)} 
    {P(Y=0|x_1,x_2,\cdots,x_D)})
    & \overset {semi} {=} ln (\frac {P(Y=1)} {P(Y=0)}) +
        \sum_{i=1}^D \beta_i ln(\frac {P(x_i|Y=1)}
        {P(x_i|Y=0)})
\end{align*}</script><blockquote>
<ul>
<li>$\frac {P(x_i|Y=1)} {P(x_i|Y=0)}$：贝叶斯因子，常用于贝叶斯假设检验</li>
</ul>
</blockquote>
<ul>
<li><p><em>Naive Bayes</em> 中满足各特征 $X$ 关于 $Y$ 条件独立的强假设下，第二个等式成立</p>
</li>
<li><p><em>Semi-Naive Bayes</em> 中放宽各特征关于 $Y$ 条件独立假设，使用权重体现变量相关性，此时则可以得到多元 <em>LR</em> 的预测变量取值对数 <em>OR</em> 形式</p>
<ul>
<li>则多元 <em>LR</em> 场景中，<em>WOE</em> 值可以从非完全条件独立的贝叶斯因子角度理解</li>
</ul>
</li>
</ul>
<h3 id="IV-值"><a href="#IV-值" class="headerlink" title="IV 值"></a><em>IV</em> 值</h3><script type="math/tex; mode=display">\begin{align*}
IV_i &= (\frac {\#B_i} {\#B_T} - \frac {\#G_i} {\#G_T}) * 
    WOE_i \\
&= (\frac {\#B_i} {\#B_T} - \frac {\#G_i} {\#G_T}) *
    log(\frac {\#B_i / \#B_T} {\#G_i / \#G_T}) \\
IV &= \sum IV_i
\end{align*}</script><blockquote>
<ul>
<li>$IV_i$：特征 $i$ 取值 <em>IV</em> 值</li>
<li>$IV$：特征总体 <em>IV</em> 值</li>
</ul>
</blockquote>
<ul>
<li>特征总体的 <em>IV</em> 值实际上是其各个取值 <em>IV</em> 值的加权和<ul>
<li>类似交叉熵为各取值概率的加权和</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-07-12T01:55:27.000Z" title="7/12/2021, 9:55:27 AM">2021-07-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-12T01:55:27.000Z" title="7/12/2021, 9:55:27 AM">2021-07-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a><span> / </span><a class="link-muted" href="/categories/Math-Mixin/Statistics/">Statistics</a></span><span class="level-item">16 minutes read (About 2441 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/Statistics/stat_entropy.html">统计量 - 熵</a></h1><div class="content"><h2 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a><em>Entropy</em></h2><blockquote>
<ul>
<li>（信息）熵：在概率分布上对复杂程度/多样性/不确定性/混乱程度的度量</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\begin{align*}
HOD(X) & = -E_P log P(x) \\
& = \sum_d^D P(x_d) log \frac 1 {P(x_d)} \\
& = - \sum_d^D p_d log p_d \\
\end{align*}</script><blockquote>
<ul>
<li>$p_d$：随机变量各取值对应概率</li>
<li>事件 $i$ 发生概率 $p_d=0$：约定 $p_d log(p_d)$ 为 0</li>
<li>其中 $log$ 以 2 为底，单位为 <em>bit</em>，以 $e$ 为底，单位为 <em>nat</em></li>
</ul>
</blockquote>
<ul>
<li><p>信息论中，熵越高能传输越多信息</p>
<ul>
<li>可携带的信息量 = 单位消息熵 * 消息长度</li>
<li>熵衡量系统复杂程度，提高系统确定性即削弱系统多样性，降低熵</li>
</ul>
</li>
<li><p>概率分布包含的信息即其复杂程度（可能取值数量）</p>
<ul>
<li>考虑按照 $(p_1,\cdots,p_D)$ 分布、长度为 $N$ 的随机变量序列，其可能排列数为 $\frac {N!} {\prod_d^D (p_d N)!}$</li>
<li><p>则根据 <em>Stirling</em> 公式有</p>
<script type="math/tex; mode=display">\begin{align*}
log (\frac {N!} {\prod_d^D (p_d N)!}) & = log(N!)
  - \sum_d^D log((p_d N)!) \\
& \overset {\lim_{N \rightarrow \infty}} = log(\sqrt {2\pi N}
  ({\frac N e})^N) + \sum_d^D log(\sqrt {2\pi p_dN}
  ({\frac {p_dN} e})^{p_dN}) \\
& = log(\sqrt {2\pi N}) + N(logN-1) - \sum_d^D log(\sqrt {2\pi p_dN})
  - \sum_d^D p_dN (log(p_dN) - 1) \\
& = log(\sqrt {2\pi N} + \sum_d^D log(\sqrt {2\pi p_dN}))
  + N \sum_d^D p_d log p_d \\
& \approx N \sum_d^D p_d log p_d
\end{align*}</script></li>
<li><p>则长度为 $N$ 的随机变量串的多样性、信息量为 $H * N$，其中 $H=\sum_d^D p_d log p_d$ 概率分布的信息熵</p>
</li>
</ul>
</li>
<li><p>某个事件包含的信息可以用编码长度理解</p>
<ul>
<li>对概率 $p$ 事件，编码 $1/p$ 个需编码（2进制编码）长度 $log_2 \frac 1 p$</li>
<li>则概率 $p$ 事件包含信息量可以定义为 $log \frac 1 p$，即事件包含的信息量可用表示事件需要编码的长度表示
（底数则取决于编码元，只影响系数）</li>
<li>则整个随机变量的信息为各事件信息量加权和</li>
</ul>
</li>
<li><p>熵可以视为变量取值概率的加权和</p>
<ul>
<li>只依赖随机变量 $X$ 的分布，与其取值无关，可将其记为 $H(P)$</li>
<li>由定义 $0 \leq H(P) \leq log_2 k$<ul>
<li>$H(p) = 0$：$\exists j, p_j=1$，随机变量只能取一个值，无不确定性</li>
<li>$H(p) = log k$：$\forall j, p_j=1/k$，随机变量在任意取值概率相等，不确定性最大</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>empirical entropy</em>：经验熵，熵中的概率由数据估计时（尤极大似然估计）</li>
<li>参考链接<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)">https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27876027">https://zhuanlan.zhihu.com/p/27876027</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73710585">https://zhuanlan.zhihu.com/p/73710585</a></li>
</ul>
</blockquote>
</li>
<li><em>Stirling</em> 公式即用积分近似计算 $\sum logn$：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143992660">https://zhuanlan.zhihu.com/p/143992660</a></li>
</ul>
</blockquote>
<h3 id="熵的性质"><a href="#熵的性质" class="headerlink" title="熵的性质"></a>熵的性质</h3><ul>
<li><p>对称性：事件取值不影响熵</p>
</li>
<li><p>极值性</p>
<ul>
<li><p>所有符号有同等机会出现的情况下，熵达到极大（琴生不等式）</p>
<script type="math/tex; mode=display">\begin{align*}
H(X) & = E[log(\frac 1 {P(X)})] \leq log(E[\frac 1 {P(x)}])
  & = log(n)
\end{align*}</script></li>
<li><p>仅有一个符号确定出现的情况下，熵达到极小 0</p>
</li>
</ul>
</li>
<li><p><em>Continuity</em>连续性：度量连续，概率微小变化只能引起熵微小变化</p>
</li>
<li><p><em>Normalization</em>规范化：$H_2(\frac 1 2, \frac 1 2) = 1$</p>
</li>
<li><p><em>Grouping</em>组合法则/可加和性：熵与过程如何划分无关
（此即要求熵形式为对数）</p>
<ul>
<li><p>若子系统间相互作用已知，则可以通过子系统熵值计算系统整体熵</p>
<script type="math/tex; mode=display">
H(X) = H(X_1,\cdots,X_K) + \sum_{k=1}^K
  \frac {|X_k|} {|X|} H(X_k)</script><blockquote>
<ul>
<li>$X_1,\cdots,X_K$：$K$ 个子系统，可以理解为将随机变量 $X$ 划分为 $K$ 种情况</li>
<li>$H(X_1,\cdots,X_K)$：子系统相互作用熵</li>
</ul>
</blockquote>
<ul>
<li>子系统相互作用熵可以认为是，通过已知信息消除的多样性（即信息增益）</li>
<li>子系统熵之和则是利用已知信息消除多样性之后，系统剩余混乱程度</li>
</ul>
</li>
<li><p>一般的，两个事件 $X,Y$ 熵满足以下计算关系</p>
<script type="math/tex; mode=display">\begin{align*}
H(X, Y) & = H(X) + H(Y|X) \\
& = H(Y) + H(X|Y) \\
& \leqslant H(X) + H(Y) \\
H(X|Y) & \leqslant H(X) \\
\end{align*}</script></li>
<li><p>特别的，若事件 $X, Y$ 相互独立</p>
<script type="math/tex; mode=display">\begin{align*}
H(X|Y) &= H(X) \\
H(X, Y) &= H(X) + H(Y)
\end{align*}</script></li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>满足以上特性的熵定义必然为如下形式</li>
</ul>
</blockquote>
<pre><code>$$
-K \sum P(x)log(P(x))
$$
</code></pre><blockquote>
<ul>
<li>在热力学、信息论等领域，熵有多种不同定义，满足熵性质的测度泛函，只能具有（<em>Shannon</em> 熵和 <em>Hartley</em> 熵）或（<em>von Neumann</em> 熵和 <em>Shannon</em> 熵）线性组合的函数形式，若不要求满足组合法则，还有 <em>Tsallis</em> 熵等</li>
</ul>
</blockquote>
<h3 id="Conditinal-Entropy"><a href="#Conditinal-Entropy" class="headerlink" title="Conditinal Entropy"></a><em>Conditinal Entropy</em></h3><p>条件熵：随机变量 $X$ 给定条件下，随机变量 $Y$ 的<strong>条件概率分布的熵</strong>对 $X$ 的数学期望</p>
<script type="math/tex; mode=display">\begin{align*}
H(Y|X) & = \sum_{i=1}^N p_i H(Y|X=x_i) \\
H(Y|x=x_i) & = - \sum_j P(y_j|x_i) log P(y_j|x_i)
\end{align*}</script><blockquote>
<ul>
<li>$P(X=x<em>i, Y=y_j)=p</em>{i,j}$：随机变量 $(X,Y)$ 联合概率分布</li>
<li>$p_i=P(X=x_i)$</li>
<li>$H(Y|X=x_i)$：后验熵</li>
</ul>
</blockquote>
<ul>
<li><p>特别的，考虑数据集 $D$ 被分为 $D_1,\cdots,D_m$，条件经验熵可计算如下</p>
<script type="math/tex; mode=display">\begin{align*}
H(D|A) & = \sum_{m=1}^M \frac {|D_m|} {|D|} H(D_m) \\
& = -\sum_{m=1}^M \frac {|D_m|} {|D|}
   \sum_{k=1}^K \frac {|D_{m,k}|} {|D_m|}
   log_2 \frac {|D_{m,k}|} {|D_m|}
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li><em>postorior entropy</em>：后验熵，随机变量 $X$ 给定条件下，随机变量 $Y$ 的<strong>条件概率分布的熵</strong></li>
<li><em>empirical conditional entropy</em>：经验条件熵，概率由数据估计</li>
</ul>
</blockquote>
<h3 id="Infomation-Gain-Mutual-Infomation"><a href="#Infomation-Gain-Mutual-Infomation" class="headerlink" title="Infomation Gain/Mutual Infomation"></a><em>Infomation Gain</em>/<em>Mutual Infomation</em></h3><p>互信息/信息增益：（经验）熵与（经验）条件熵之差</p>
<script type="math/tex; mode=display">\begin{align*}
g(Y|X) & = H(Y) - H(Y|X) \\
& = \sum_{x \in X} \sum_{y \in Y} P(x,y) log
    \frac {P(x,y)} {P(x)P(y)}
\end{align*}</script><ul>
<li><p>与数据集具体分布有关、与具体取值无关</p>
<ul>
<li>绝对大小同易受熵影响，（经验）熵较大时，互信息也相对较大</li>
<li>由于误差存在，分类取值数目较多者信息增益较大</li>
</ul>
</li>
<li><p>可衡量变量 $X$ 对 $Y$ 预测能力、减少不确定性的能力</p>
<ul>
<li>信息增益越大，变量之间相关性越强，自变量预测因变量能力越强</li>
<li>只能考察特征对整个系统的贡献，无法具体到特征某个取值</li>
<li>只适合作全局特征选择，即所有类使用相同的特征集合</li>
</ul>
</li>
</ul>
<h3 id="Infomation-Gain-Ratio"><a href="#Infomation-Gain-Ratio" class="headerlink" title="Infomation Gain Ratio"></a><em>Infomation Gain Ratio</em></h3><p>信息增益比：信息增益对原始信息熵的比值</p>
<script type="math/tex; mode=display">\begin{align*}
g_R(Y|X) & = \frac {g(Y|X)} {H(X)}
\end{align*}</script><ul>
<li>考虑熵大小，减弱熵绝对大小的影响</li>
</ul>
<h3 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a><em>Cross Entropy</em></h3><blockquote>
<ul>
<li>信息论：基于相同事件测度的两个概率分布 $P, Q$，基于非自然（相较于真实分布 $P$）概率分布 $Q$ 进行编码，在事件集合中唯一标识事件所需 <em>bit</em></li>
<li>概率论：概率分布 $P, Q$ 之间差异</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">\begin{align*}
H(P, Q) & = E_P[-log Q] = \left \{ \begin{array}{l}
    -\sum_{X} P(x) logQ(x), & 离散分布 \\
    -\int_X P(x) log(Q(x)) d(r(x)), & 连续分布
\end{array} \right. \\
& = H(P) + D_{KL}(P||Q)
\end{align*}</script><blockquote>
<ul>
<li>$P(x), Q(x)$：概率分布（密度）函数</li>
<li>$r(x)$：测度，通常是 $Borel \sigma$ 代数上的勒贝格测度</li>
<li>$D_{KL}(P||Q)$：$P$ 到 $Q$ 的 <em>KL</em> 散度（$P$ 相对于 $Q$ 的相对熵）</li>
</ul>
</blockquote>
<ul>
<li>信息论中，交叉熵可以看作是信息片段在错误分布 $Q$ 分布下的期望编码长度<ul>
<li>信息实际分布实际为 $P$，所以期望基于 $P$</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>交叉熵是常用的损失函数：效果等价于 <em>KL</em> 散度，但计算方便</li>
<li><em>sigmoid</em> 激活函数时：相较于二次损失，收敛速度更快</li>
</ul>
</blockquote>
<h2 id="Entropy-衍生指标"><a href="#Entropy-衍生指标" class="headerlink" title="Entropy 衍生指标"></a><em>Entropy</em> 衍生指标</h2><h3 id="Kullback-Leibler-Divergence"><a href="#Kullback-Leibler-Divergence" class="headerlink" title="Kullback-Leibler Divergence"></a><em>Kullback-Leibler Divergence</em></h3><p><em>KL</em> 散度/相对熵：衡量概率分布 $P, Q$ 之间差异的量化指标</p>
<script type="math/tex; mode=display">\begin{align*}
D_{KL}(P||Q) & = E_P[(-log Q(x)) - (-log P(x))] \\
& = E_P[log P(x) - log Q(x)] \\
& = \sum_{d=1}^D P(x_d) (log P(x_d) - log Q(x_d)) \\
& = \sum_{d=1} P(x_d) log \frac {P(x_d)} {Q(x_d)}
\end{align*}</script><ul>
<li><p><em>KL</em> 散度含义</p>
<ul>
<li>原始分布 $P$、近似分布 $Q$ 之间对数差值期望</li>
<li>若使用观察分布 $Q$ 描述真实分布 $P$，还需的额外信息量</li>
</ul>
</li>
<li><p><em>KL</em> 散度不对称，分布 $P$ 度量 $Q$、$Q$ 度量 $P$ 损失信息不同</p>
<ul>
<li>从计算公式也可以看出</li>
<li>KL散度不能作为不同分布之间距离的度量</li>
</ul>
</li>
</ul>
<h3 id="Population-Stability-Index"><a href="#Population-Stability-Index" class="headerlink" title="Population Stability Index"></a><em>Population Stability Index</em></h3><p><em>PSI</em>：衡量分布 $P, Q$ 之间的差异程度</p>
<script type="math/tex; mode=display">\begin{align*}
PSI &= \sum_d^D (P_d - Q_d) * log \frac {P_d} {Q_d} \\
&= \sum_d^D P_d log \frac {P_d} {Q_d} +
    \sum_d^D Q_d log \frac {Q_d} {P_d} \\
&= D_{KL}(P||Q) + D_{KL}(Q||P)
\end{align*}</script><ul>
<li>是 <em>KL</em> 散度的对称操作<ul>
<li>更全面的描述两个分布的差异</li>
</ul>
</li>
</ul>
<h2 id="Gini-指数"><a href="#Gini-指数" class="headerlink" title="Gini 指数"></a><em>Gini</em> 指数</h2><p>基尼指数：可视为信息熵的近似替代</p>
<script type="math/tex; mode=display">\begin{align*}
Gini(p) & = \sum_{k=1}^K p_k(1-p_k) \\
    & = 1 - \sum_{k=1}^K p_k^2
\end{align*}</script><blockquote>
<ul>
<li>$p$：概率分布</li>
<li>异质性最小：<em>Gini</em> 系数为 0</li>
<li>异质性最大：<em>Gini</em> 系数为 $1 - \frac 1 k$</li>
</ul>
</blockquote>
<ul>
<li><em>Gini</em> 指数度量分布的不纯度<ul>
<li>包含类别越多，<em>Gini</em> 指数越大</li>
<li>分布越均匀，<em>Gini</em> 指数越大</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>熵较 <em>Gini</em> 指数对不纯度判罚更重</li>
</ul>
</blockquote>
<p><img src="/imgs/gini_entropy_error_rate_in_binary_classification.png" alt="gini_entropy_error_rate_in_binary_classification"></p>
<blockquote>
<ul>
<li>经济学领域的 <em>Gini</em> 系数更类似 <em>AUC</em> 值</li>
</ul>
</blockquote>
<h3 id="与-Entropy-关系"><a href="#与-Entropy-关系" class="headerlink" title="与 Entropy 关系"></a>与 <em>Entropy</em> 关系</h3><script type="math/tex; mode=display">\begin{align*}
H(X) & = -E_P log P(x) \\
& = - \sum_i^N p_i log p_i \\
& = - \sum_i^N p_i (log (1 + (p_i-1))) \\
& = - \sum_i^N p_i (p_i - 1 + \xi(p_i^{'}-1)) \\
& \approx 1 - \sum_i^N p_i^2
\end{align*}</script><ul>
<li><em>Gini</em> 指数可以视为是熵在 1 附近的一阶泰勒展开近似</li>
</ul>
<h3 id="条件-Gini-指数"><a href="#条件-Gini-指数" class="headerlink" title="条件 Gini 指数"></a>条件 <em>Gini</em> 指数</h3><script type="math/tex; mode=display">
Gini(Y|X) = \sum_{k=1}^K P(X=x_k)Gini(Y|X=x_k)</script><blockquote>
<ul>
<li>性质类似信息增益</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-07-12T01:53:01.000Z" title="7/12/2021, 9:53:01 AM">2021-07-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-12T01:53:01.000Z" title="7/12/2021, 9:53:01 AM">2021-07-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a><span> / </span><a class="link-muted" href="/categories/Math-Mixin/Statistics/">Statistics</a></span><span class="level-item">9 minutes read (About 1370 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/Statistics/stat_corrs.html">统计量 - 相关</a></h1><div class="content"><h2 id="Pearson-积矩相关系数"><a href="#Pearson-积矩相关系数" class="headerlink" title="Pearson 积矩相关系数"></a><em>Pearson</em> 积矩相关系数</h2><script type="math/tex; mode=display">
\rho_{X,Y} = \frac {cov(X, Y)} {\sigma_X \sigma_Y}</script><blockquote>
<ul>
<li>$cov(X, Y)$：变量 $X, Y$ 协方差</li>
<li>$\sigma_X, \sigma_Y$：变量 $X, Y$ 方差</li>
</ul>
</blockquote>
<ul>
<li><em>Pearson</em> 积矩相关系数取值范围为 $[-1, 1]$<ul>
<li>$1, -1$ 分别表示变量成正线性、负线性函数关系</li>
</ul>
</li>
</ul>
<h3 id="显著性检验"><a href="#显著性检验" class="headerlink" title="显著性检验"></a>显著性检验</h3><h4 id="Fisher-变换"><a href="#Fisher-变换" class="headerlink" title="Fisher 变换"></a><em>Fisher</em> 变换</h4><script type="math/tex; mode=display">
z = \frac 1 2 ln(\frac {1+r} {1-r}) = arctanh(r)</script><blockquote>
<ul>
<li>$z$：<em>Pearson</em> 积矩相关系数的 <em>Fisher</em> 变换</li>
<li>$r$：样本的 <em>Pearson</em> 积矩相关系数值</li>
</ul>
</blockquote>
<ul>
<li>当 $(X, Y)$ 为二元正态分布时，$z$ 近似正态分布<ul>
<li>均值：$\frac 1 2 ln(\frac {1+\rho} {1-\rho})$</li>
<li>标准差：$\frac 1 {\sqrt {N - 3}}$</li>
</ul>
</li>
</ul>
<h4 id="基于数学的近似方法"><a href="#基于数学的近似方法" class="headerlink" title="基于数学的近似方法"></a>基于数学的近似方法</h4><script type="math/tex; mode=display">
t = r \sqrt{\frac {N - 2} {1 - r^2}}</script><ul>
<li>当 $(X, Y)$ 为二元正态分布且不相关时，$t$ 服从自由度为 $n-2$的 <em>t-分布</em></li>
</ul>
<h2 id="Spearman-秩相关系数"><a href="#Spearman-秩相关系数" class="headerlink" title="Spearman 秩相关系数"></a><em>Spearman</em> 秩相关系数</h2><script type="math/tex; mode=display">\begin{align*}
\rho_{X, Y} & = \frac {cov(Rank(X) - Rank(Y))}
    {\sigma_{Rank(X)} \sigma_{Rank(Y)}} \\
& = 1 - \frac {6 \sum_i^N d_i^2} {N(N^2-1)} \\
\end{align*}</script><blockquote>
<ul>
<li>$Rank(X), Rank(Y)$：变量 $X, Y$ 的秩（应同序）（相同值秩取均值）</li>
<li>$d_i$：变量对 $X, Y$ 中，二者秩差值</li>
</ul>
</blockquote>
<ul>
<li><em>Spearman</em> 秩相关系数被定义为变量秩的 <em>Pearson</em> 相关系数</li>
</ul>
<blockquote>
<ul>
<li><em>Spearman</em> 秩相关系数也可以使用 <em>Fisher</em> 变换检验显著性</li>
</ul>
</blockquote>
<h2 id="Kendell-秩相关系数"><a href="#Kendell-秩相关系数" class="headerlink" title="Kendell 秩相关系数"></a><em>Kendell</em> 秩相关系数</h2><script type="math/tex; mode=display">\begin{align*}
\tau_a &= \frac {N_c - N_d} {N_0} \\
\tau_b &= \frac {N_c - N_d} {\sqrt{(N_0 - N_X)(N_0 - N_Y)}} \\
\tau_c &= \frac {2(N_c - N_d)} {N^2 \frac {M-1} M}
\end{align*}</script><blockquote>
<ul>
<li>$N_0 = \frac {N(N-1)} 2$：变量对数量</li>
<li>$N_c, N_d$：变量对 $X, Y$ 中有序对数量、无序对数量</li>
<li>$N_X, N_Y$：变量对 $X, Y$ 中 $X$ 取值、$Y$ 取值相同对数量</li>
<li>$M$：变量 $X, Y$ 中较小取值数量者取值数量</li>
</ul>
</blockquote>
<ul>
<li><p><em>Kendell</em> 秩相关系数取值范围同样为 $[-1, 1]$</p>
<ul>
<li>-1 仅在变量 $X, Y$ 取值完全反向取到</li>
</ul>
</li>
<li><p>$\tau_a$ 是 $\tau_b$ 在变量不存在取值相同时的特例</p>
</li>
<li><p>$\tau_c$ 适合“层级”数据，即两个变量取值类似划分、内部细分</p>
<p>||A|B|C|
|——-|——-|——-|——-|
|I-1|30|0|0|
|I-2|30|0|0|
|II-1|0|30|0|
|II-1|0|30|0|
|III-2|0|0|30|
|III-2|0|0|30|</p>
<ul>
<li>对以上数据，$\tau_b$ 取值在 0.9 附近，而 $\tau_c$ 取 1</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>有序对：对 $(X_i, Y_i), (X_j, Y_j)$，满足 $X_i &lt; X_j, Y_i &lt; Y_j$ 或 $X_i &gt; X_j,Y_i &gt; Y_j$ 则为有序对</li>
<li>无序对：对$(X_i, Y_i), (X_j, Y_j)$，满足 $X_i &lt; X_j, Y_i &gt; Y_j$ 或 $X_i &gt; X_j, Y_i &lt; Y_j$ 则为无序对</li>
</ul>
</blockquote>
<h2 id="卡方统计量"><a href="#卡方统计量" class="headerlink" title="卡方统计量"></a>卡方统计量</h2><p>卡方统计量：通过观察实际与理论值的偏差确定理论正确与否</p>
<script type="math/tex; mode=display">
\chi^2 = \sum \frac {(A - E)^2} E</script><blockquote>
<ul>
<li>$A$：自变量、因变量组合对应频数观察值</li>
<li>$E$：自变量、因变量组合对应频数期望值</li>
</ul>
</blockquote>
<ul>
<li><p>将模型预测结果视为实际分布、先验分布（均匀分布）视为理论分布</p>
</li>
<li><p>卡方检验：检验定性变量之间相关性，假设两个变量确实独立，观察实际值、理论值偏差程度判断变量之间相关性</p>
<ul>
<li>若偏差足够小，认为误差是自然的样本误差，两者确实独立</li>
<li>若偏差大到一定程度，误差不可能由偶然、测量精度导致，
认为两者相关</li>
</ul>
</li>
<li><p>若模型预测结果同先验分布差别很大，说明模型有效，且卡方统计量值越大表示预测把握越大</p>
</li>
</ul>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>由于随机误差存在，卡方统计量容易<ul>
<li>夸大频数较小的特征影响</li>
<li>相应的，取值数较少（各取值频数相对而言可能较大）特征影响容易被低估</li>
</ul>
</li>
</ul>
<h3 id="分布证明"><a href="#分布证明" class="headerlink" title="分布证明"></a>分布证明</h3><ul>
<li><p>考虑随机变量 $X=(x_1,\cdots,x_D)$ 服从 <em>Multinomial</em> 分布，分布参数为 $n, p=(p_1,\cdots,p_D)$</p>
</li>
<li><p>考虑服从理论分布的随机变量 $X$ 协方差矩阵</p>
<script type="math/tex; mode=display">\begin{align*}
\Sigma = Cov(X) &= \begin{bmatrix}
   np_1(1-p_1) & -np_1p_2 & \cdots & -np_1p_D \\
   np_2p_1 & -np_2(1-p_2) & \cdots & -np_2p_D \\
   \vdots & \vdots & \ddots & \vdots \\
   -np_Dp_1 & -np_Dp_2 & \cdots & np_D(1-p_D)
\end{bmatrix} \\
&= n\begin{bmatrix}
   p_1 & 0 & \cdots & 0 \\
   0 & p_2 & \cdots & 0 \\
   \vdots & \vdots & \ddots & \vdots \\
   0 & 0 & \cdots & p_D
\end{bmatrix} - npp^T \\
\end{align*}</script></li>
<li><p>则由中心极限定理有，如下依分布收敛的结论</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {(X - np)} {\sqrt n} & \overset {D} {\rightarrow} N(0,\Sigma) \\
\end{align*}</script></li>
<li><p>考虑服从理论分布的随机变量 $X$ 的 $\chi^2$ 参数</p>
<script type="math/tex; mode=display">\begin{align*}
\chi^2 &= \frac 1 n (X-np)^T D^2 (X-np) \\
D &= \begin{bmatrix}
   \frac 1 {\sqrt {p_1}} & 0 & \cdots & 0 \\
   0 & \frac 1 {\sqrt {p_2}} & \cdots & 0 \\
   \vdots & \vdots & \ddots & \vdots \\
   0 & 0 & \cdots & \frac 1 {\sqrt {p_D}}
\end{bmatrix}
\end{align*}</script></li>
<li><p>并由连续映射定理可以得到 $D\frac {x-np} {\sqrt n}$ 分布，且其协方差矩阵 $\Sigma_0$ 满足</p>
<script type="math/tex; mode=display">\begin{align*}
D\frac {x-np} {\sqrt n} & \overset {D} {\rightarrow} N(0, D \Sigma D^T) \\
\Sigma_0 &= D \Sigma D^T \\
\Sigma_0^2 &= (E - \sqrt p {\sqrt p}^T)(E - \sqrt p {\sqrt p}^T) = \Sigma_0 \\
\end{align*}</script></li>
<li><p>由以上，$\Sigma_0$ 仅有特征值 0，1</p>
<ul>
<li>特征值 0 对应特征向量有且仅有 $\sqrt p$</li>
<li>特征值 1 对应特征向量有 $D-1$ 个</li>
</ul>
<script type="math/tex; mode=display">\begin{align*}
\Sigma_0 \sqrt p - 0 \sqrt p &= \sqrt p - \sqrt p = 0 \\
\Sigma_0 \lambda - 1 \lambda &= \lambda - \sqrt p {\sqrt p}^T \lambda
   = \sqrt p {\sqrt p}^T \lambda = 0
\end{align*}</script></li>
<li><p>则 $\chi^2$ 统计量依分布收敛于自由度为 $D-1$ 的卡方分布</p>
<script type="math/tex; mode=display">\begin{align*}
\chi^2 &= \sum_{d=1}^D \frac {(x_d - np_d)^2} {np_d}
   \overset {D} {\rightarrow} \chi_{D-1}
\end{align*}</script></li>
<li><p>可据此构造统计量进行卡方检验，检验实际值实际分布频率 $(a_1,\cdots,a_D)$ 是否符合该分布</p>
<ul>
<li>构造卡方统计量 $\chi^2 = \sum_{d=1}^D \frac {(x_d - na_d)^2} {na_d}$</li>
<li>则卡方统计量在随机变量满足多项分布情况下依分布收敛于自由度为 $D-1$ 的卡方分布</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/309694332/answer/952401910">https://www.zhihu.com/question/309694332/answer/952401910</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/198864907">https://zhuanlan.zhihu.com/p/198864907</a></li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-06-09T09:46:15.000Z" title="6/9/2021, 5:46:15 PM">2021-06-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-09T09:46:15.000Z" title="6/9/2021, 5:46:15 PM">2021-06-09</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a></span><span class="level-item">a few seconds read (About 5 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/inequality.html">常用不等式</a></h1><div class="content"><h2 id="Cauthy-Schwarz-不等式"><a href="#Cauthy-Schwarz-不等式" class="headerlink" title="Cauthy-Schwarz 不等式"></a><em>Cauthy-Schwarz</em> 不等式</h2><blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/22004031">https://zhuanlan.zhihu.com/p/22004031</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/129033407">https://zhuanlan.zhihu.com/p/129033407</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70315155">https://zhuanlan.zhihu.com/p/70315155</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/85283405">https://zhuanlan.zhihu.com/p/85283405</a></li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-05-13T09:16:42.000Z" title="5/13/2021, 5:16:42 PM">2021-05-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:35:24.000Z" title="8/4/2021, 11:35:24 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a></span><span class="level-item">a few seconds read (About 40 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/equality.html">常用等式</a></h1><div class="content"><h2 id="常用定理"><a href="#常用定理" class="headerlink" title="常用定理"></a>常用定理</h2><h3 id="Lucas-定理"><a href="#Lucas-定理" class="headerlink" title="Lucas 定理"></a><em>Lucas</em> 定理</h3><script type="math/tex; mode=display">
C(n, m) \% p = (C(n//p, m//p) * C(n\%p, m\%p)) \% p</script><blockquote>
<ul>
<li>$p &lt; 10^5$：必须为素数</li>
</ul>
</blockquote>
<h3 id="Holder-定理"><a href="#Holder-定理" class="headerlink" title="Holder 定理"></a><em>Holder</em> 定理</h3><p>$|x|^{*}_p = |x|_q$</p>
<blockquote>
<ul>
<li>$\frac 1 p + \frac 1 q = 1$</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-04-26T11:01:08.000Z" title="4/26/2021, 7:01:08 PM">2021-04-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-02T10:44:10.000Z" title="7/2/2021, 6:44:10 PM">2021-07-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a></span><span class="level-item">2 minutes read (About 252 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/uncharted_concepts.html">未归类概念</a></h1><div class="content"><h3 id="Radial-Basis-Function"><a href="#Radial-Basis-Function" class="headerlink" title="Radial Basis Function"></a><em>Radial Basis Function</em></h3><ul>
<li><p><em>RBF</em> 径向基函数：取值仅依赖到原点距离的实值函数，即 $\phi(x) = \phi(|x|)$</p>
<ul>
<li>也可以按照距离某中心点 $c$ 的距离定义，即 $\phi(x) = \phi(|x-c|)$</li>
<li>其中距离一般为使用 $L_2$ 范数，即欧式距离</li>
<li>函数 $\phi$ 一般与 $|x|$ 负相关</li>
</ul>
</li>
<li><p>径向基函数最初用于解决多变量插值问题</p>
<ul>
<li>即以各样本为中心创建多个径向基函数</li>
<li>多个径向基函数加权加和即得到拟合的函数曲线，可用于函数插值</li>
</ul>
<p><img src="/imgs/rbf_for_interpolation.png" alt="rbf_for_interpolation"></p>
</li>
</ul>
<h4 id="常见径向基函数"><a href="#常见径向基函数" class="headerlink" title="常见径向基函数"></a>常见径向基函数</h4><blockquote>
<ul>
<li>定义 $r=|x-x_i|$</li>
</ul>
</blockquote>
<ul>
<li><p>高斯函数</p>
<script type="math/tex; mode=display">\phi(r) = e^{-(\epsilon r)^2}</script></li>
<li><p><em>Multiquadric</em> 多二次函数：</p>
<script type="math/tex; mode=display">\phi(r) = \sqrt {1 + (\epsilon r)^2}</script></li>
<li><p><em>Inverse Quadric</em> 逆二次函数：</p>
<script type="math/tex; mode=display">\phi(r) = \frac 1 {1 + (\epsilon r)^2}</script></li>
<li><p><em>Polyharmonic Spline</em> 多重调和样条：</p>
<script type="math/tex; mode=display">\begin{align*}
\phi(r) &= r^k, & k=1,3,5,\cdots \\
\phi(r) &= r^k (ln(r))^{}, & k=2,4,6,\cdots  \\
\end{align*}</script></li>
<li><p><em>Thin Plate Spline</em> 薄板样条（多重调和样条特例）：</p>
<script type="math/tex; mode=display">\phi(r) = r^2 ln(r)</script></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a></span><span class="level-item">9 minutes read (About 1338 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/func_distance.html">距离函数</a></h1><div class="content"><h2 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h2><ul>
<li>距离可认为是两个对象 $x,y$ 之间的 <strong>相似程度</strong><ul>
<li>距离和相似度是互补的</li>
<li>可以根据处理问题的情况，自定义距离</li>
</ul>
</li>
</ul>
<h3 id="Bregman-Divergence"><a href="#Bregman-Divergence" class="headerlink" title="Bregman Divergence"></a><em>Bregman Divergence</em></h3><script type="math/tex; mode=display">
D(x, y) = \Phi(x) - \Phi(y) - <\nabla \Phi(y), (x - y)></script><blockquote>
<ul>
<li>$Phi(x)$：凸函数</li>
</ul>
</blockquote>
<ul>
<li><p>布雷格曼散度：穷尽所有关于“正常距离”的定义</p>
<ul>
<li>给定 $R^n * R^n \rightarrow R$ 上的正常距离 $D(x,y)$，一定可以表示成布雷格曼散度形式</li>
<li>直观上：$x$处函数、函数过$y$点切线（线性近似）之差<ul>
<li>可以视为是损失、失真函数：$x$由$y$失真、近似、添加噪声得到</li>
</ul>
</li>
</ul>
</li>
<li><p>特点</p>
<ul>
<li>非对称：$D(x, y) = D(y, x)$</li>
<li>不满足三角不等式：$D(x, z) \leq D(x, y) + D(y, z)$</li>
<li>对凸集作 <em>Bregman Projection</em> 唯一<ul>
<li>即寻找凸集中与给定点Bregman散度最小点</li>
<li>一般的投影指欧式距离最小</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Domain</th>
<th>$\Phi(x)$</th>
<th>$D_{\Phi}(x,y)$</th>
<th>Divergence</th>
</tr>
</thead>
<tbody>
<tr>
<td>$R$</td>
<td>$x^2$</td>
<td>$(x-y)^2$</td>
<td>Squared Loss</td>
</tr>
<tr>
<td>$R_{+}$</td>
<td>$xlogx$</td>
<td>$xlog(\frac x y) - (x-y)$</td>
<td></td>
</tr>
<tr>
<td>$[0,1]$</td>
<td>$xlogx + (1-x)log(1-x)$</td>
<td>$xlog(\frac x y) + (1-x)log(\frac {1-x} {1-y})$</td>
<td>Logistic Loss</td>
</tr>
<tr>
<td>$R_{++}$</td>
<td>$-logx$</td>
<td>$\frac x y - log(\frac x y) - 1$</td>
<td>Itakura-Saito Distance</td>
</tr>
<tr>
<td>$R$</td>
<td>$e^x$</td>
<td>$e^x - e^y - (x-y)e^y$</td>
<td></td>
</tr>
<tr>
<td>$R^d$</td>
<td>$\</td>
<td>x\</td>
<td>$</td>
<td>$\</td>
<td>x-y\</td>
<td>$</td>
<td>Squared Euclidean Distance</td>
</tr>
<tr>
<td>$R^d$</td>
<td>$x^TAx$</td>
<td>$(x-y)^T A (x-y)$</td>
<td>Mahalanobis Distance</td>
</tr>
<tr>
<td>d-Simplex</td>
<td>$\sum_{j=1}^d x_j log_2 x_j$</td>
<td>$\sum_{j=1}^d x_j log_2 log(\frac {x_j} {y_j})$</td>
<td>KL-divergence</td>
</tr>
<tr>
<td>$R_{+}^d$</td>
<td>$\sum_{j=1}^d x_j log x_j$</td>
<td>$\sum<em>{j=1}^d x_j log(\frac {x_j} {y_j}) - \sum</em>{j=1}^d (x_j - y_j)$</td>
<td>Genelized I-divergence</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<ul>
<li><em>正常距离</em>：对满足任意概率分布的点，点平均值点（期望点）应该是空间中距离所有点平均距离最小的点</li>
<li>布雷格曼散度对一般概率分布均成立，而其本身限定由凸函数生成<blockquote>
<ul>
<li>和 <em>Jensen</em> 不等式有关？凸函数隐含部分对期望的度量</li>
</ul>
</blockquote>
</li>
<li><a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume6/banerjee05b/banerjee05b.pdf">http://www.jmlr.org/papers/volume6/banerjee05b/banerjee05b.pdf</a></li>
</ul>
</blockquote>
<h2 id="单点距离"><a href="#单点距离" class="headerlink" title="单点距离"></a>单点距离</h2><h3 id="Minkowski-Distance"><a href="#Minkowski-Distance" class="headerlink" title="Minkowski Distance"></a><em>Minkowski Distance</em></h3><p>闵科夫斯基距离：向量空间 $\mathcal{L_p}$ 范数</p>
<script type="math/tex; mode=display">
d_{12} = \sqrt [1/p] {\sum_{k=1}^n |x_{1,k} - x_{2,k}|^p}</script><ul>
<li><p>表示一组距离族</p>
<ul>
<li>$p=1$：<em>Manhattan Distance</em>，曼哈顿距离</li>
<li>$p=2$：<em>Euclidean Distance</em>，欧式距离</li>
<li>$p \rightarrow \infty$：<em>Chebychev Distance</em>，切比雪夫距离</li>
</ul>
</li>
<li><p>闵氏距离缺陷</p>
<ul>
<li>将各个分量量纲视作相同</li>
<li>未考虑各个分量的分布</li>
</ul>
</li>
</ul>
<h3 id="Mahalanobis-Distance"><a href="#Mahalanobis-Distance" class="headerlink" title="Mahalanobis Distance"></a><em>Mahalanobis Distance</em></h3><p>马氏距离：表示数据的协方差距离</p>
<script type="math/tex; mode=display">
d_{12} = \sqrt {({x_1-\mu}^T) \Sigma^{-1} (x_2-\mu)}</script><blockquote>
<ul>
<li>$\Sigma$：总体协方差矩阵</li>
</ul>
</blockquote>
<ul>
<li>优点<ul>
<li>马氏距离和原始数据量纲无关</li>
<li>考虑变量相关性</li>
</ul>
</li>
<li>缺点<ul>
<li>需要知道总体协方差矩阵，使用样本估计效果不好</li>
</ul>
</li>
</ul>
<h3 id="LW-Distance"><a href="#LW-Distance" class="headerlink" title="LW Distance"></a><em>LW Distance</em></h3><p>兰氏距离：<em>Lance and Williams Distance</em>，堪培拉距离</p>
<script type="math/tex; mode=display">
d_{12} = \sum^{n}_{k=1} \frac {|x_{1,k} - x_{2,k}|} {|x_{1,k} + x_{2,k}|}</script><ul>
<li>特点<ul>
<li>对接近0的值非常敏感</li>
<li>对量纲不敏感</li>
<li>未考虑变量直接相关性，认为变量之间相互独立</li>
</ul>
</li>
</ul>
<h3 id="Hamming-Distance"><a href="#Hamming-Distance" class="headerlink" title="Hamming Distance"></a><em>Hamming Distance</em></h3><p>汉明距离：差别</p>
<script type="math/tex; mode=display">
diff = \frac 1 p \sum_{i=1}^p  (v^{(1)}_i - v^{(2)}_i)^k</script><blockquote>
<ul>
<li>$v_i \in {0, 1}$：虚拟变量</li>
<li>$p$：虚拟变量数量</li>
</ul>
</blockquote>
<ul>
<li>可以衡量定性变量之间的距离</li>
</ul>
<h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a><em>Embedding</em></h4><ul>
<li>找到所有点、所有维度坐标值中最大值 $C$</li>
<li>对每个点 $P=(x_1, x_2, \cdots, x_d)$<ul>
<li>将每维 $x_i$ 转换为长度为 $C$ 的 0、1 序列</li>
<li>其中前 $x_i$ 个值为 1，之后为 0</li>
</ul>
</li>
<li>将 $d$ 个长度为 $C$ 的序列连接，形成长度为 $d * C$ 的序列</li>
</ul>
<blockquote>
<ul>
<li>以上汉明距离空间嵌入对曼哈顿距离是保距的</li>
</ul>
</blockquote>
<h3 id="Jaccard-系数"><a href="#Jaccard-系数" class="headerlink" title="Jaccard 系数"></a><em>Jaccard</em> 系数</h3><p><em>Jaccard</em> 系数：度量两个集合的相似度，值越大相似度越高</p>
<script type="math/tex; mode=display">
sim = \frac {\|S_1 \hat S_2\|} {\|S_1 \cup S_2\|}</script><blockquote>
<ul>
<li>$S_1, S_2$：待度量相似度的两个集合</li>
</ul>
</blockquote>
<h3 id="Consine-Similarity"><a href="#Consine-Similarity" class="headerlink" title="Consine Similarity"></a><em>Consine Similarity</em></h3><p>余弦相似度</p>
<script type="math/tex; mode=display">
similarity = cos(\theta) = \frac {x_1 x_2} {\|x_1\|\|x_2\|}</script><blockquote>
<ul>
<li>$x_1, x_2$：向量</li>
</ul>
</blockquote>
<h3 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h3><h4 id="点到平面"><a href="#点到平面" class="headerlink" title="点到平面"></a>点到平面</h4><blockquote>
<ul>
<li>$T={(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)}$：样本点集</li>
<li>$wx + b = 0$：超平面</li>
</ul>
</blockquote>
<h5 id="Functional-Margin-函数间隔"><a href="#Functional-Margin-函数间隔" class="headerlink" title="Functional Margin 函数间隔"></a><em>Functional Margin</em> 函数间隔</h5><script type="math/tex; mode=display">
\hat{\gamma_i} = y_i(wx_i + b)</script><ul>
<li><p>函数间隔可以表示分类的正确性、确信度</p>
<ul>
<li>正值表示正确</li>
<li>间隔越大确信度越高</li>
</ul>
</li>
<li><p>点集与超平面的函数间隔取点间隔最小值 $\hat{T} = \min_{i=1,2,\cdots,n} \hat{\gamma_i}$</p>
</li>
<li><p>超平面参数 $w, b$ 成比例改变时，平面未变化，但是函数间隔成比例变化</p>
</li>
</ul>
<h5 id="Geometric-Margin-几何间隔"><a href="#Geometric-Margin-几何间隔" class="headerlink" title="Geometric Margin 几何间隔"></a><em>Geometric Margin</em> 几何间隔</h5><script type="math/tex; mode=display">\begin{align*}
\gamma_i & = \frac {y_i} {\|w\|} (wx_i + b) \\
    & = \frac {\hat \gamma_i} {\|w\|}
\end{align*}</script><ul>
<li><p>几何间隔一般是样本点到超平面的 <em>signed distance</em></p>
<ul>
<li>点正确分类时，几何间隔就是点到直线的距离</li>
</ul>
</li>
<li><p>几何间隔相当于使用 $|w|$ 对函数间隔作规范化</p>
<ul>
<li>$|w|=1$ 时，两者相等</li>
<li>几何间隔对确定超平面、样本点是确定的，不会因为超平面表示形式改变而改变</li>
</ul>
</li>
<li><p>点集与超平面的几何间隔取点间隔最小值 $\hat{T} = \min_{i=1,2,\cdots,n} \hat{\gamma_i}$</p>
</li>
</ul>
<h3 id="Levenshtein-Edit-Distance"><a href="#Levenshtein-Edit-Distance" class="headerlink" title="Levenshtein/Edit Distance"></a><em>Levenshtein/Edit Distance</em></h3><p>（字符串）编辑距离：两个字符串转换需要进行插入、删除、替换操作的次数</p>
<script type="math/tex; mode=display">
lev_{A,B}(i, j) = \left \{ \begin{array}{l}
    i, & j = 0 \\
    j, & i = 0 \\
    min \left \{ \begin{array}{l}
        lev_{A,B}(i,j-1) + 1 \\
        lev_{A,B}(i-1,j) + 1 \\
        lev_{A,B}(i-1, j-1) + 1
    \end{array} \right. & A[i] != B[j] \\
    min \left \{ \begin{array}{l}
        lev_{A,B}(i,j-1) + 1 \\
        lev_{A,B}(i-1,j) + 1 \\
        lev_{A,B}(i-1, j-1)
    \end{array} \right. & A[i] = B[j] \\
\end{array} \right.</script><h2 id="组间距离"><a href="#组间距离" class="headerlink" title="组间距离"></a>组间距离</h2><h3 id="Single-Linkage"><a href="#Single-Linkage" class="headerlink" title="Single Linkage"></a><em>Single Linkage</em></h3><h3 id="Average-Linkage"><a href="#Average-Linkage" class="headerlink" title="Average Linkage"></a><em>Average Linkage</em></h3><h3 id="Complete-Linkage"><a href="#Complete-Linkage" class="headerlink" title="Complete Linkage"></a><em>Complete Linkage</em></h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a></span><span class="level-item">27 minutes read (About 4010 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/func_hash.html">Hashing</a></h1><div class="content"><h2 id="Hash-Function"><a href="#Hash-Function" class="headerlink" title="Hash Function"></a><em>Hash Function</em></h2><blockquote>
<ul>
<li><em>hash</em>：散列/哈希，将任意类型值转换为关键码值</li>
<li><em>hash function</em>：哈希/散列函数，从任何数据中创建小的数字“指纹”的方法</li>
<li><em>hash value</em>：哈希值，哈希函数产生关键码值</li>
<li><em>collision</em>：冲突，不同两个数据得到相同哈希值</li>
</ul>
</blockquote>
<ul>
<li>哈希函数应该尽可能使得哈希值均匀分布在目标空间中<ul>
<li>降维：将高维数据映射到低维空间</li>
<li>数据应该低维空间中尽量均匀分布</li>
</ul>
</li>
</ul>
<h3 id="数据相关性"><a href="#数据相关性" class="headerlink" title="数据相关性"></a>数据相关性</h3><ul>
<li><p><em>Data Independent Hashing</em>：数据无关哈希，无监督，哈希函数基于某种概率理论</p>
<ul>
<li>对原始的特征空间作均匀划分</li>
<li>对分布不均、有趋向性的数据集时，可能会导致高密度区域哈希桶臃肿，降低索引效率</li>
</ul>
</li>
<li><p><em>Data Dependent Hashing</em>：数据依赖哈希，有监督，通过学习数据集的分布从而给出较好划分的哈希函数</p>
<ul>
<li>得到针对数据密度动态划分的哈希索引</li>
<li>破坏了传统哈希函数的数据无关性，索引不具备普适性</li>
</ul>
</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ul>
<li>查找数据结构：<em>cs_algorithm/data_structure/hash_table</em><ul>
<li>哈希表</li>
</ul>
</li>
<li>信息安全方向：<em>cs_algorithm/specification/info_security</em><ul>
<li>文件检验</li>
<li>数字签名</li>
<li>鉴权协议</li>
</ul>
</li>
</ul>
<h2 id="哈希函数"><a href="#哈希函数" class="headerlink" title="哈希函数"></a>哈希函数</h2><ul>
<li><p>简单哈希函数主要用于提升查找效率（构建哈希表）</p>
<ul>
<li>要求哈希函数的降维、缩小查找空间性质</li>
<li>计算简单、效率高</li>
</ul>
</li>
<li><p>复杂哈希函数主要用于信息提取</p>
<ul>
<li>要求哈希函数的信息提取不可逆、非单调映射</li>
<li>查表哈希<ul>
<li><em>CRC</em> 系列算法：本身不是查表，但查表是其最快实现</li>
<li><em>Zobrist Hashing</em></li>
</ul>
</li>
<li>混合哈希：利用以上各种方式<ul>
<li><em>MD5</em></li>
<li><em>Tiger</em></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="单值输入"><a href="#单值输入" class="headerlink" title="单值输入"></a>单值输入</h3><ul>
<li><p>直接寻址法：取关键字、或其某个线性函数值 $hash(key) = (a * key + b) \% prime$</p>
<ul>
<li>$prime$：一般为质数，以使哈希值尽量均匀分布，常用的如：$2^{32}-5$</li>
</ul>
</li>
<li><p>数字分析法：寻找、利用数据规律构造冲突几率较小者</p>
<ul>
<li>如：生日信息前 2、3 位大体相同，冲突概率较大，优先舍去</li>
</ul>
</li>
<li><p>平方取中法：取关键字平方后中间几位</p>
</li>
<li><p>折叠法：将关键字分割为位数相同部分，取其叠加和</p>
</li>
<li><p>随机数法：以关键字作为随机数种子生成随机值</p>
<ul>
<li>适合关键字长度不同场合</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>常用于之前哈希结果再次映射为更小范围的最终哈希值</li>
</ul>
</blockquote>
<h3 id="序列输入"><a href="#序列输入" class="headerlink" title="序列输入"></a>序列输入</h3><h4 id="加法哈希"><a href="#加法哈希" class="headerlink" title="加法哈希"></a>加法哈希</h4><p>加法哈希：将输入元素相加得到哈希值</p>
<ul>
<li><p>标准加法哈希</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">AddingHash(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> += ele</span><br><span class="line">	<span class="comment"># prime 为任意质数，常用 2^32 - 5</span></span><br><span class="line">	<span class="built_in">hash</span> = <span class="built_in">hash</span>  % prime</span><br></pre></td></tr></table></figure>
<ul>
<li>最终哈希结果 $\in [0, prime-1]$</li>
</ul>
</li>
</ul>
<h4 id="位运算哈希"><a href="#位运算哈希" class="headerlink" title="位运算哈希"></a>位运算哈希</h4><p>位运算哈希：利用位运算（移位、异或等）充分混合输入元素</p>
<ul>
<li><p>标准旋转哈希</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">RotationHash(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = (<span class="built_in">hash</span> &lt;&lt; <span class="number">4</span>) ^ (<span class="built_in">hash</span> &gt;&gt; <span class="number">28</span>) ^ ele</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">hash</span> % prime</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形 1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> = (<span class="built_in">hash</span>&lt;&lt; <span class="number">5</span>) ^ (<span class="built_in">hash</span> &gt;&gt; <span class="number">27</span>) ^ ele</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> += ele</span><br><span class="line"><span class="built_in">hash</span> ^= (<span class="built_in">hash</span> &lt;&lt; <span class="number">10</span>)</span><br><span class="line"><span class="built_in">hash</span> ^= (<span class="built_in">hash</span> &gt;&gt; <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形3</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (ele &amp; <span class="number">1</span>) == <span class="number">0</span>:</span><br><span class="line">	<span class="built_in">hash</span> ^= (<span class="built_in">hash</span> &lt;&lt; <span class="number">7</span>) ^ ele ^ (<span class="built_in">hash</span> &gt;&gt; <span class="number">3</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	<span class="built_in">hash</span> ^= ~((<span class="built_in">hash</span> &lt;&lt; <span class="number">11</span>) ^ ele ^ (<span class="built_in">hash</span> &gt;&gt; <span class="number">5</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形4</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> += (<span class="built_in">hash</span> &lt;&lt; <span class="number">5</span>) + ele</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形5</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> = ele + (<span class="built_in">hash</span> &lt;&lt; <span class="number">6</span>) + (<span class="built_in">hash</span> &gt;&gt; <span class="number">16</span>) - <span class="built_in">hash</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>变形6</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> ^= (<span class="built_in">hash</span> &lt;&lt; <span class="number">5</span>) + ele + (<span class="built_in">hash</span> &gt;&gt; <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="乘法哈希"><a href="#乘法哈希" class="headerlink" title="乘法哈希"></a>乘法哈希</h4><p>乘法哈希：利用乘法的不相关性</p>
<ul>
<li><p>平方取头尾随机数生成法：效果不好</p>
</li>
<li><p><em>Bernstein</em> 算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Bernstein(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = <span class="number">33</span> * <span class="built_in">hash</span> + ele</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">hash</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>其他常用乘数：31、131、1313、13131、131313</li>
</ul>
</blockquote>
</li>
<li><p>32位 <em>FNV</em> 算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">M_SHIFT =</span><br><span class="line">M_MASK =</span><br><span class="line">FNVHash(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">2166136261</span>;</span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = (<span class="built_in">hash</span> * <span class="number">16777619</span>) ^ ele</span><br><span class="line">	<span class="keyword">return</span> (<span class="built_in">hash</span> ^ (<span class="built_in">hash</span> &gt;&gt; M_SHIFT)) &amp; M_MASK</span><br></pre></td></tr></table></figure>
</li>
<li><p>改进的 <em>FNV</em> 算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FNVHash_2(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">2166136261</span>;</span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = (<span class="built_in">hash</span> ^ ele) * <span class="number">16777619</span></span><br><span class="line">	<span class="built_in">hash</span> += <span class="built_in">hash</span> &lt;&lt; <span class="number">13</span></span><br><span class="line">	<span class="built_in">hash</span> ^= <span class="built_in">hash</span> &gt;&gt; <span class="number">7</span></span><br><span class="line">	<span class="built_in">hash</span> += <span class="built_in">hash</span> &lt;&lt; <span class="number">3</span></span><br><span class="line">	<span class="built_in">hash</span> ^= <span class="built_in">hash</span> &gt;&gt; <span class="number">17</span></span><br><span class="line">	<span class="built_in">hash</span> += <span class="built_in">hash</span> &lt;&lt; <span class="number">5</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">hash</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>乘数不固定</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RSHash(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	a, b = <span class="number">378551</span>, <span class="number">63689</span></span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = <span class="built_in">hash</span> * a + ele</span><br><span class="line">		a *= b</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">hash</span> &amp; <span class="number">0x7FFFFFFF</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<ul>
<li>除法也类似乘法具有不相关性，但太慢</li>
</ul>
</blockquote>
<h3 id="定长序列"><a href="#定长序列" class="headerlink" title="定长序列"></a>定长序列</h3><ul>
<li><p>两步随机数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">main_rand_seq = randint(k)</span><br><span class="line">TwoHashing(<span class="built_in">input</span>[<span class="number">0</span>,...,k]):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">from</span> i=<span class="number">0</span> to k:</span><br><span class="line">		<span class="built_in">hash</span> += <span class="built_in">input</span>[i] * main_rand_seq[i]</span><br><span class="line">	<span class="built_in">hash</span> = <span class="built_in">hash</span> mod prime</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Universal-Hashing"><a href="#Universal-Hashing" class="headerlink" title="Universal Hashing"></a><em>Universal Hashing</em></h2><blockquote>
<ul>
<li>全域哈希：键集合 $U$ 包含 $n$ 个键、哈希函数族 $H$ 中哈希函数 $h_i: U \rightarrow 0..m$，若 $H$ 满足以下则为全域哈希 $$<pre><code>  \forall x \neq y \in U, | \&#123;h|h \in H, h(x) = h(y) \&#125; | = \frac &#123;|H|&#125; m
</code></pre>  $$<blockquote>
<ul>
<li>$|H|$：哈希函数集合 $H$ 中函数数量</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li>独立与键值随机从中选择哈希函数，避免发生最差情况</li>
<li>可利用全域哈希构建完美哈希</li>
</ul>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ul>
<li><p>全域哈希 $H$ 中任选哈希函数 $h_i$，对任意键 $x \neq y \in U$ 冲突概率小于 $\frac 1 m$</p>
<ul>
<li>由全域哈希函数定义，显然</li>
</ul>
</li>
<li><p>全域哈希 $H$ 中任选哈希函数 $h<em>i$，对任意键 $x \in U$，与其冲突键数目期望为 $\frac n m$，即 $E</em>{[collision_x]}=\frac n m$</p>
<script type="math/tex; mode=display">\begin{align*}
E(C_x) &= E[\sum_{y \in U - \{x\}} C_{xy}] \\
   &= \sum_{y \in U - \{x\}} E[C_{xy}] \\
   &= \sum_{y \in U - \{x\}} \frac 1 m \\
   &= \frac {n-1} m
\end{align*}</script><blockquote>
<ul>
<li>$C_x$：任选哈希函数，与 $x$ 冲突的键数量</li>
<li>$C_{xy} = \left { \begin{matrix} 1, &amp; h_i(x) = h_i(y) \ 0, &amp; otherwise \end{matrix} \right.$：指示 $x,y$ 是否冲突的指示变量</li>
</ul>
</blockquote>
<ul>
<li>$m = n^2$ 时，冲突期望小于 0.5<ul>
<li>$n$ 个键两两组合数目为 $C_n^2$</li>
<li>则 $E_{total} &lt; C_n^2 \frac 1 n &lt; 0.5$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="例"><a href="#例" class="headerlink" title="例"></a>例</h3><blockquote>
<ul>
<li>以下构造 $[0,p-1] \rightarrow [0,m-1]$ 全域哈希</li>
</ul>
</blockquote>
<ul>
<li><p>$p$ 为足够大素数使得所有键值 $\in [0,p-1]$</p>
<ul>
<li>记 $Z_p = { 0,1,\cdots,p-1 }$</li>
<li>记 $Z_p^{*}={ 1,2,\cdots,p-1 }$</li>
<li>且哈希函数映射上限（哈希表长度） $m &lt; max(U) &lt; p$</li>
</ul>
</li>
<li><p>记哈希函数</p>
<script type="math/tex; mode=display">
\forall a \in Z_p^{*}, b \in Z_p, h_{a, b}(k) = ((a k + b) \% p) \% m</script></li>
<li><p>则以下哈希函数族即为全域哈希</p>
<script type="math/tex; mode=display">
H_{p,m} = {h_{a,b}|a \in Z_p^{*}, b \in Z_p}</script></li>
</ul>
<h2 id="Locality-Sensitive-Hashing"><a href="#Locality-Sensitive-Hashing" class="headerlink" title="Locality Sensitive Hashing"></a><em>Locality Sensitive Hashing</em></h2><p><em>LSH</em>：局部敏感哈希</p>
<blockquote>
<ul>
<li>$(r_1,r_2,P_1,P_2)-sensitive$ 哈希函数族 $H$ 需满足如下条件 $$
  \begin{align*}<pre><code>  Pr_&#123;H&#125;[h(v) = h(q)] \geq P_1, &amp; \forall q \in B(v, r_1) \\
  Pr_&#123;H&#125;[h(v) = h(q)] \geq P_2, &amp; \forall q \notin B(v, r_2) \\
</code></pre>  \end{align*}$$<blockquote>
<ul>
<li>$h \in H$</li>
<li>$r_1 &lt; r_2, P_1 &gt; P_2$：函数族有效的条件</li>
<li>$B(v, r)$：点 $v$ 的 $r$ 邻域</li>
<li>$r_1, r_2$：距离，强调比例时会表示为 $r_1 = R, r_2 = cR$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li>此时 <strong>相似目标（距离小）有更大概率发生冲突</strong></li>
</ul>
<h3 id="LSH查找"><a href="#LSH查找" class="headerlink" title="LSH查找"></a>LSH查找</h3><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p><img src="/imgs/general_lsh_comparsion.png" alt="general_lsh_comparsion"></p>
<ul>
<li><p>相似目标更有可能映射到相同哈希桶中</p>
<ul>
<li>则只需要在目标所属的哈希桶中进行比较、查找即可</li>
<li>无需和全集数据比较，大大缩小查找空间</li>
</ul>
</li>
<li><p>可视为降维查找方法</p>
<ul>
<li>将高维空间数据映射到 1 维空间，寻找可能近邻的数据点</li>
<li>缩小范围后再进行精确比较</li>
</ul>
</li>
</ul>
<h4 id="概率放大"><a href="#概率放大" class="headerlink" title="概率放大"></a>概率放大</h4><blockquote>
<ul>
<li>期望放大局部敏感哈希函数族 $Pr_1, Pr_2$ 之间差距</li>
</ul>
</blockquote>
<ul>
<li><p>增加哈希值长度（级联哈希函数中基本哈希函数数量） $k$</p>
<ul>
<li>每个哈希函数独立选择，则对每个级联哈希函数 $g_i$ 有 $Pr[g_i(v) = g_i(q)] \geq P_1^k$</li>
<li>虽然增加哈希键位长会减小目标和近邻碰撞的概率，但同时也更大程度上减少了和非近邻碰撞的概率、减少搜索空间</li>
</ul>
<blockquote>
<ul>
<li>级联哈希函数返回向量，需要对其再做哈希映射为标量，方便查找</li>
</ul>
</blockquote>
</li>
<li><p>增加级联哈希函数数量（哈希表数量） $L$</p>
<ul>
<li>$L$个哈希表中候选项包含真实近邻概率 <strong>至少</strong> 为 $1 - (1 - P_1^k)^L$</li>
<li>增加哈希表数量能有效增加候选集包含近邻可能性</li>
<li>但同时也会增大搜索空间</li>
</ul>
</li>
</ul>
<h4 id="搜索近似最近邻"><a href="#搜索近似最近邻" class="headerlink" title="搜索近似最近邻"></a>搜索近似最近邻</h4><ul>
<li>使用 $L$ 个级联哈希函数分别处理待搜索目标</li>
<li>在 $L$ 个哈希表分别寻找落入相同哈希桶个体作为候选项</li>
<li>在所有候选项中线性搜索近邻</li>
</ul>
<h3 id="基于汉明距离的-LSH"><a href="#基于汉明距离的-LSH" class="headerlink" title="基于汉明距离的 LSH"></a>基于汉明距离的 <em>LSH</em></h3><ul>
<li>在汉明距离空间中搜索近邻<ul>
<li>要求数据为二进制表示</li>
<li>其他距离需要嵌入汉明距离空间才能使用<ul>
<li>欧几里得距离没有直接嵌入汉明空间的方法<ul>
<li>一般假设欧几里得距离和曼哈顿距离差别不大</li>
<li>直接使用对曼哈顿距离保距嵌入方式</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="设计哈希函数族"><a href="#设计哈希函数族" class="headerlink" title="设计哈希函数族"></a>设计哈希函数族</h4><ul>
<li><p>考虑哈希函数族 $H = { h_1, h_2, \cdots, h_m }$</p>
<ul>
<li>其中函数 $h_i$ 为 ${0, 1}^d$ 到 ${0, 1}$ 的映射：随机返回特定比特位上的值</li>
</ul>
</li>
<li><p>从 $H$ 中随机的选择哈希函数 $h_i$</p>
<ul>
<li>则 $Pr[h_i(v) = h_i(q)]$ 等于 $v, q$ 相同比特数比例，则<ul>
<li>$Pr_1 = 1 - \frac R d$</li>
<li>$Pr_2 = 1 - \frac {cR} d$</li>
</ul>
</li>
<li>考虑到 $Pr_1 &gt; Pr_2$，即此哈希函数族是局部敏感的</li>
</ul>
</li>
</ul>
<h3 id="基于-Jaccard-系数的-LSH"><a href="#基于-Jaccard-系数的-LSH" class="headerlink" title="基于 Jaccard 系数的 LSH"></a>基于 <em>Jaccard</em> 系数的 <em>LSH</em></h3><ul>
<li><p>考虑 $M * N$ 矩阵 $A$，元素为 0、1</p>
<ul>
<li>其中<ul>
<li>$M$：集合元素数量</li>
<li>$N$：需要比较的集合数量</li>
</ul>
</li>
<li>目标：寻找相似集合，即矩阵中相似列</li>
</ul>
</li>
<li><p>用 <em>Jaccard</em> 系数代表集合间相似距离，用于搜索近邻</p>
<ul>
<li>要求各数据向量元素仅包含 0、1：表示集合是否包含该元素</li>
</ul>
</li>
</ul>
<h4 id="定义-Min-hashing-函数族"><a href="#定义-Min-hashing-函数族" class="headerlink" title="定义 Min-hashing 函数族"></a>定义 <em>Min-hashing</em> 函数族</h4><ul>
<li><p>对矩阵 $A$ 进行 <strong>行随机重排</strong> $\pi$，定义 <em>Min-hashing</em> 如下</p>
<script type="math/tex; mode=display">h_{\pi}(C) = \min \pi(C)</script><blockquote>
<ul>
<li>$C$：列，表示带比较集合</li>
<li>$\min \pi(C)$：$\pi$ 重排矩阵中 $C$ 列中首个 1 所在行数</li>
</ul>
</blockquote>
</li>
<li><p>则不同列（集合） <em>Min-hashing</em> 相等概率等于二者 <em>Jaccard</em> 系数</p>
<script type="math/tex; mode=display">\begin{align*}
Pr(h_{\pi}(C_1)  = h_{\pi}(C_2)) & = \frac a {a + b} \\
& = Jaccard_d(C_1, C_2)
\end{align*}</script><blockquote>
<ul>
<li>$a$：列 $C_1, C_2$ 取值均为 1 的行数</li>
<li>$b$：列 $C_1, C_2$ 中仅有一者取值为 1 的行数</li>
<li>根据 <em>Min-hashing</em> 定义，不同列均取 0 行被忽略</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="Min-hashing-实现"><a href="#Min-hashing-实现" class="headerlink" title="Min-hashing 实现"></a><em>Min-hashing</em> 实现</h4><ul>
<li><p>数据量过大时，对行随机重排仍然非常耗时，考虑使用哈希函数模拟行随机重排</p>
<ul>
<li>每个哈希函数对应一次随机重排<ul>
<li>哈希函数视为线性变换</li>
<li>然后用哈希函数结果对总行数取模</li>
</ul>
</li>
<li>原行号经过哈希函数映射即为新行号</li>
</ul>
</li>
<li><p>为减少遍历数据次数，考虑使用迭代方法求解</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i from <span class="number">0</span> to N<span class="number">-1</span>:</span><br><span class="line">	<span class="keyword">for</span> j from <span class="number">0</span> to M<span class="number">-1</span>:</span><br><span class="line">		<span class="keyword">if</span> D[i][j] == <span class="number">1</span>:</span><br><span class="line">			<span class="keyword">for</span> k from <span class="number">1</span> to K:</span><br><span class="line">				# 更新随机重拍后，第 `j` 列首个 <span class="number">1</span> 位置</span><br><span class="line">				DD[k][j] = min(h_k(i), DD[k][j])</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>$D$：原始数据特征矩阵</li>
<li>$DD$：$Min-hashing* 签名矩阵</li>
<li>$N$：特征数量，原始特征矩阵行数</li>
<li>$M$：集合数量，原始特征矩阵列数</li>
<li>$K$：模拟的随机重排次数，<em>Min-hashing</em> 签名矩阵行数</li>
<li>$h_k,k=1,…,K$：$K$ 个模拟随机重排的哈希函数，如 $h(x) = (2x + 7) mod N$</li>
</ul>
</blockquote>
<ul>
<li>初始化 <em>Min-hashing</em> 签名矩阵所有值为 $\infty$</li>
<li>遍历 $N$ 个特征、$M$ 个集合<ul>
<li>查看每个对应元素是否为 1</li>
<li>若元素为 1，则分别使用 $K$ 个哈希函数计算模拟重排后对应的行数</li>
<li>若计算出行数小于当前 *Min-hash$ 签名矩阵相应哈希函数、集合对应行数，更新</li>
</ul>
</li>
<li>遍历一遍原始数据之后即得到所有模拟重排的签名矩阵</li>
</ul>
</li>
</ul>
<h3 id="Exact-Euclidean-LSH"><a href="#Exact-Euclidean-LSH" class="headerlink" title="Exact Euclidean LSH"></a><em>Exact Euclidean LSH</em></h3><ul>
<li><p>$E^2LSH$：欧式局部LSH，<em>LSH Based-on P-stable Distribution</em></p>
<ul>
<li>使用内积将向量随机映射到哈希值</li>
<li><em>p-stable</em> 分布性质将欧式距离同哈希值相联系，实现局部敏感</li>
</ul>
</li>
<li><p>$E^2LSH$ 特点</p>
<ul>
<li>基于概率模型生成索引编码结果不稳定</li>
<li>随编码位数 $k$ 增加的，准确率提升缓慢</li>
<li>级联哈希函数数量 $L$ 较多时，需要大量存储空间，不适合大规模数据索引</li>
</ul>
</li>
</ul>
<h4 id="p-stable-哈希函数族"><a href="#p-stable-哈希函数族" class="headerlink" title="p-stable 哈希函数族"></a><em>p-stable</em> 哈希函数族</h4><script type="math/tex; mode=display">
h_{a, b}(v) = \lfloor \frac {av + b} r \rfloor</script><blockquote>
<ul>
<li>$v$：$n$ 维特征向量</li>
<li>$a = (X_1,X_2,\cdots,X_n)$：其中分量为独立同 <em>p-stable</em> 分布的随机变量</li>
<li>$b \in [0, r]$：均匀分布随机变量</li>
</ul>
</blockquote>
<h4 id="p-stable-哈希函数碰撞概率"><a href="#p-stable-哈希函数碰撞概率" class="headerlink" title="p-stable 哈希函数碰撞概率"></a><em>p-stable</em> 哈希函数碰撞概率</h4><blockquote>
<ul>
<li>考虑$|v_1 - v_2|_p = c$的两个样本碰撞概率</li>
</ul>
</blockquote>
<ul>
<li><p>显然，仅在 $|av<em>1 - av_2| \leq r$ 时，才存在合适的 $b$ 使得 $h</em>{a,b}(v<em>1) = h</em>{a,b}(v_2)$</p>
<ul>
<li>即两个样本碰撞，不失一般性可设 $av_1 \leq av_2$</li>
<li>此 $r$ 即代表局部敏感的 <strong>局部范围</strong></li>
</ul>
</li>
<li><p>若 $(k-1)r \leq av_1 \leq av_2 &lt; kr$，即两个样本与 $a$ 内积在同一分段内</p>
<ul>
<li>易得满足条件的 $b \in [0,kr-av_2) \cup [kr-av_1, r]$</li>
<li>即随机变量 $b$ 取值合适的概率为 $1 - \frac {av_2 - av_1} r$</li>
</ul>
</li>
<li><p>若 $(k-1)r \leq av_1 \leq kr \leq av_2$，即两个样本 $a$ 在相邻分段内</p>
<ul>
<li>易得满足条件的 $b \in [kr-av_1, (k+1)r-av_2)$</li>
<li>即随机变量 $b$ 取值合适的概率同样为 $1 - \frac {av_2 - av_1} r$</li>
</ul>
</li>
<li><p>考虑 $av_2 - av_1$ 分布为 $cX$，则两样本碰撞概率为</p>
<script type="math/tex; mode=display">\begin{align*}
p(c)  & = Pr_{a,b}(h_{a,b}(v_1) = h_{a,b}(v_2)) \\
& = \int_0^r \frac 1 c f_p(\frac t c)(1 - \frac t r)dt
\end{align*}</script><blockquote>
<ul>
<li>$c = |v_1 - v_2|_p$：特征向量之间$L_p$范数距离</li>
<li>$t = a(v_1 - v_2)$</li>
<li>$f$：p稳定分布的概率密度函数</li>
</ul>
</blockquote>
<ul>
<li><p>$p=1$ 柯西分布</p>
<script type="math/tex; mode=display">
p(c) = 2 \frac {tan^{-1}(r/c)} \pi - \frac 1 {\pi(r/c)} ln(1 + (r/c)^2)</script></li>
<li><p>$p=2$ 正态分布</p>
<script type="math/tex; mode=display">
p(c) = 1 - 2norm(-r/c) - \frac 2 {\sqrt{2\pi} r/c} (1 - e^{-(r^2/2c^2)})</script></li>
</ul>
</li>
</ul>
<h4 id="性质、实现"><a href="#性质、实现" class="headerlink" title="性质、实现"></a>性质、实现</h4><h5 id="限制近邻碰撞概率"><a href="#限制近邻碰撞概率" class="headerlink" title="限制近邻碰撞概率"></a>限制近邻碰撞概率</h5><ul>
<li><p>$r$ 最优值取决于数据集、查询点</p>
<ul>
<li>根据文献，建议$r = 4$</li>
</ul>
</li>
<li><p>若要求近邻 $v \in B(q,R)$以不小于$1-\sigma$ 概率碰撞，则有</p>
<script type="math/tex; mode=display">\begin{align*}
1 - (1 - p(R)^k)^L & \geq 1 - \sigma \\
\Rightarrow L & \geq \frac {log \sigma} {log(1 - p(R)^k)}
\end{align*}</script><p>则可取</p>
<script type="math/tex; mode=display">
L = \lceil \frac {log \sigma} {log(1-p(R)^k)} \rceil</script></li>
<li><p>$k$ 最优值是使得 $T_g + T_c$ 最小者</p>
<ul>
<li>$T_g = O(dkL)$：建表时间复杂度</li>
<li>$T_c = O(d |collisions|)$：精确搜索时间复杂度</li>
<li>$T_g$、$T_c$ 随着 $k$ 增大而增大、减小</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>具体实现参考<a target="_blank" rel="noopener" href="https://www.mit.edu/~andoni/LSH/manual.pdf">https://www.mit.edu/~andoni/LSH/manual.pdf</a></li>
</ul>
</blockquote>
<h5 id="限制搜索空间"><a href="#限制搜索空间" class="headerlink" title="限制搜索空间"></a>限制搜索空间</h5><ul>
<li><p>哈希表数量 $L$ 较多时，所有碰撞样本数量可能非常大，考虑只选择 $3L$ 个样本点</p>
</li>
<li><p>此时每个哈希键位长 $k$、哈希表数量 $L$ 保证以下条件，则算法正确</p>
<ul>
<li>若存在 $v^{ <em> }$ 距离待检索点 $q$ 距离小于 $r_1$，则存在 $g_j(v^{ </em> }) = g_j(q)$</li>
<li><p>与 $q$ 距离大于 $r_2$、可能和 $q$ 碰撞的点的数量小于 $3L$</p>
<script type="math/tex; mode=display">
\sum_{j=1}^L |(P-B(q,r_2)) \cap g_j^{-1}(g_j(q))|
  < 3L</script></li>
</ul>
</li>
<li><p>可以证明，$k, L$ 取以下值时，以上两个条件以常数概率成立
（此性质是局部敏感函数性质，不要求是 $E^2LSH$）</p>
<script type="math/tex; mode=display">\begin{align*}
k & = log_{1/p_2} n\\
L & = n^{\rho} \\
\rho & = \frac {ln 1/p_1} {ln 1/p_2}
\end{align*}</script></li>
<li><p>$\rho$ 对算法效率起决定性作用，且有以下定理</p>
<blockquote>
<ul>
<li>距离尺度 $D$ 下，若 $H$ 为 $(R,cR,p<em>1,p_2)$-敏感哈希函数族，则存在适合 <em>(R,c)-NN</em> 的算法，其空间复杂度为 $O(dn + n^{1+\rho})$、查询时间为 $O(n^{\rho})$ 倍距离计算、哈希函数计算为 $O(n^{\rho} log</em>{1/p_2}n)$， 其中 $\rho = \frac {ln 1/p_1} {ln 1/p_2}$</li>
</ul>
</blockquote>
<ul>
<li>$r$ 足够大、充分远离 0 时，$\rho$ 对其不是很敏感</li>
<li>$p<em>1, p_2$ 随 $r$ 增大而增大，而 $k = log</em>{1/p_2} n$ 也随之增大，所以 $r$ 不能取过大值</li>
</ul>
</li>
</ul>
<h4 id="Scalable-LSH"><a href="#Scalable-LSH" class="headerlink" title="Scalable LSH"></a><em>Scalable LSH</em></h4><p><em>Scalable LSH</em>：可扩展的 <em>LSH</em></p>
<ul>
<li><p>对动态变化的数据集，固定哈希编码的局部敏感哈希方法对数据 <strong>动态支持性有限</strong>，无法很好的适应数据集动态变化</p>
<ul>
<li>受限于初始数据集分布特性，无法持续保证有效性</li>
<li>虽然在原理上支持数据集动态变化，但若数据集大小发生较大变化，则其相应哈希参数（如哈希编码长度）等需要随之调整，需要从新索引整个数据库</li>
</ul>
</li>
<li><p>在 $E^2LSH$ 基础上通过 <strong>动态增强哈希键长</strong>，增强哈希函数区分能力，实现可扩展 <em>LSH</em></p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-14T12:04:44.000Z" title="7/14/2019, 8:04:44 PM">2019-07-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:32:19.000Z" title="8/4/2021, 11:32:19 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a><span> / </span><a class="link-muted" href="/categories/Math-Mixin/Statistics/">Statistics</a></span><span class="level-item">13 minutes read (About 2022 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/Statistics/stat_evaluation.html">常用统计量</a></h1><div class="content"><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><ul>
<li>对比实际类别值、预测类别值，编制混淆矩阵</li>
<li>基于混淆矩阵，计算各类错判率、总错判率（总错判率会受到数据不平衡性的影响）</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>真实情况\预测结果</th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td>正例</td>
<td><em>TP</em>（真正例）</td>
<td><em>FN</em>（假反例）</td>
</tr>
<tr>
<td>反例</td>
<td><em>FP</em>（假正例）</td>
<td><em>TN</em>（真反例）</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/imgs/confusion_matrix.png" alt="confusion_matrix"></p>
<h3 id="F-Measure"><a href="#F-Measure" class="headerlink" title="F-Measure"></a><em>F-Measure</em></h3><p><em>F-测度</em>：准率率和召回率综合值，越大越好</p>
<script type="math/tex; mode=display">
F-measure = \frac {(\beta^2 + 1) * P * R} {\beta^2 * P + R}</script><blockquote>
<ul>
<li>$P = \frac {TP} {TP+FP}$：查准率、精确率</li>
<li>$R = \frac {TP} {TP+FN}$：查全率、召回率、覆盖率</li>
</ul>
</blockquote>
<h4 id="F1-值"><a href="#F1-值" class="headerlink" title="F1 值"></a><em>F1</em> 值</h4><p><em>F1值</em>：$\beta=1$ 时的 <em>F测度</em></p>
<script type="math/tex; mode=display">\begin{align*}
\frac {1} {F_{1}} &= \frac {1} {2} \left( \frac {1} {P} + \frac {1} {R} \right) \\
\Rightarrow F_{1} &= \frac {2 * P * R} {P + R} = \frac {2 * TP} {样例总数 + TP - TN}
\end{align*}</script><h3 id="TPR、FPR"><a href="#TPR、FPR" class="headerlink" title="TPR、FPR"></a><em>TPR</em>、<em>FPR</em></h3><ul>
<li><p><em>TPR</em>、<em>FPR</em> 可视为对 <em>TP</em>、<em>FP</em> 用样本数量归一化的结果</p>
<ul>
<li>样本全体中正、负样本数量往往差距很大，直接比较 <em>TP</em>、<em>FP</em> 不合理</li>
<li>考虑使用样本正、负数量归一化，即计算正比例 <em>TPR</em>、负比例 <em>FPR</em></li>
</ul>
</li>
<li><p><em>TPR</em> 越高越好，<em>FPR</em> 越低越好，但是这两个指标相互制约，两者同时增加、减小</p>
<ul>
<li>模型倾向于将样本 <strong>判定为</strong> 为正例，则 <em>TP</em>、<em>FP</em> 同时增加，<em>TPR</em>、<em>FPR</em> 同时变大</li>
<li>即模型取不同阈值，会产生正相关的 <em>TPR</em>、<em>FPR</em> 的点列</li>
</ul>
</li>
</ul>
<h3 id="Recevier-Operating-Characteristic-Curve"><a href="#Recevier-Operating-Characteristic-Curve" class="headerlink" title="Recevier Operating Characteristic Curve"></a><em>Recevier Operating Characteristic Curve</em></h3><p><em>ROC</em> 曲线：不同 <strong>正样本概率</strong> 划分阈值下 <em>TPR</em>、<em>FPR</em> 绘制的折线/曲线</p>
<script type="math/tex; mode=display">
TPR = \frac {TP} {TP+FN} \\
FPR = \frac {FP} {FP+TN}</script><ul>
<li><p><em>ROC</em> 曲线即以 <em>FPR</em> 为横坐标、<em>TPR</em> 为正坐标绘制曲线</p>
<ul>
<li><em>FPR</em> 接近 1 时，<em>TPR</em> 也接近 1，这是不可避免的</li>
<li>而 <em>FPR</em> 接近 0 时，<em>TPR</em> 越大越好</li>
<li>所以模型 <em>ROC</em> 曲线下方面积越大，模型判断正确效果越好</li>
</ul>
</li>
<li><p>理解</p>
<ul>
<li>将正负样本的正样本概率值分别绘制在 <code>x=1</code>、<code>x=-1</code> 两条直线上</li>
<li>阈值即为 <code>y=threshold</code> 直线</li>
<li><em>TPR</em>、<em>FPR</em> 则为 <code>x=1</code>、<code>x=-1</code> 两条直线在阈值直线上方点数量，与各直线上所有点数量比值</li>
</ul>
</li>
</ul>
<h3 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a><em>Accuracy</em></h3><p>准确率、误分率：评价分类器性能一般指标</p>
<script type="math/tex; mode=display">\begin{align*}
acc & = \frac 1 N sign(y_i = \hat y_i) \\
& = \frac {TP+TN} N \\
mis & = 1 - acc
\end{align*}</script><blockquote>
<ul>
<li>$y_i$：第 $i$ 样本实际类别</li>
<li>$\hat y_i$：第 $i$ 样本预测类别</li>
<li>$N$：样本数量</li>
</ul>
</blockquote>
<ul>
<li>对给定测试集，分类器正确分类样本数与总样本数比值</li>
<li>即 <em>0-1</em> 损失函数时经验风险</li>
</ul>
<h3 id="Top-PR"><a href="#Top-PR" class="headerlink" title="Top PR"></a><em>Top PR</em></h3><p>头部准召：评估模型头部性能</p>
<script type="math/tex; mode=display">
pr_{top} = \frac {TP_{top}} {TOP}</script><blockquote>
<ul>
<li>$TOP$：指定的头部数量</li>
<li>$TP_{top}$：头部中正例数量（正例指已知原 $TOP$ 样本）</li>
</ul>
</blockquote>
<h2 id="Area-Under-Curve"><a href="#Area-Under-Curve" class="headerlink" title="Area Under Curve"></a><em>Area Under Curve</em></h2><p><em>AUC</em> 值：<em>ROC</em> 曲线下方面积，越大越好</p>
<ul>
<li><p><em>AUC</em> 值实际含义：随机抽取一对正、负样本，对其中正样本的正样本预测概率值、大于负样本的正样本预测概率值的概率</p>
<ul>
<li>$=1$：完美预测，存在一个阈值可以让模型 <em>TPR</em> 为 1，<em>FPR</em> 为 0</li>
<li>$(0.5, 1)$ ：优于随机预测，至少存在某个阈值，模型 $TPR &gt; FPR$</li>
<li>$=0.5$：同随机预测，无价值</li>
<li>$[0, 0.5)$：差于随机预测，但是可以反向取预测值</li>
</ul>
</li>
</ul>
<h3 id="AUC-计算"><a href="#AUC-计算" class="headerlink" title="AUC 计算"></a><em>AUC</em> 计算</h3><ul>
<li><p>绘制 <em>ROC</em> 曲线，计算曲线下面积</p>
<ul>
<li>给定一系列阈值（最精确时为样本数量），分别计算 <em>TPR</em>、<em>FPR</em></li>
<li>根据 <em>TPR</em>、<em>FPR</em> 计算 <em>AUC</em></li>
</ul>
</li>
<li><p>正负样本分别配对，计算正样本预测概率大于负样本比例</p>
<script type="math/tex; mode=display">\begin{align*}
auc & = \frac {\sum I(P_P > P_N)} {M * N} \\
I(P_P, P_N) & = \left \{ \begin{array}{l}
   1, & P_P > P_N, \\
   0.5, & P_P = P_N, \\
   0, & P_P < P_N
\end{array} \right.
\end{align*}</script><blockquote>
<ul>
<li>$M, N$：正、负样本数量</li>
</ul>
</blockquote>
</li>
<li><p><em>Mann-Witney U</em> 检验：即正、负样本分别配对的简化公式</p>
<script type="math/tex; mode=display">
auc = \frac {\sum_{i \in Pos} rank(i) - \frac {M * (M+1)} 2} {M * N}</script><blockquote>
<ul>
<li>$Pos$：正样本集合</li>
<li>$rank(i)$：样本 $i$ 的按正样本概率排序的秩（对正样本概率值相同样本，应将秩加和求平均保证其秩相等）</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Weighted-AUC"><a href="#Weighted-AUC" class="headerlink" title="Weighted-AUC"></a><em>Weighted-AUC</em></h3><p><em>WAUC</em>：给 <strong>每个样本</strong> 赋权，计算统计量时考虑样本权重</p>
<ul>
<li><p><em>FPR</em>、<em>TPR</em> 绘图</p>
<script type="math/tex; mode=display">\begin{align*}
WTPR & = \frac {\sum_{i \in Pos} w_i I(\hat y_i=1)}
   {\sum_{i \in Pos} w_i} \\
WFPR & = \frac {\sum_{j \in Neg} w_j I(\hat y_j=1)}
   {\sum_{j \in Neg} w_j}
\end{align*}</script><blockquote>
<ul>
<li>$WTPR, WFPR$：加权 <em>TPR</em>、加权 <em>FPR</em></li>
<li>$\hat y_i$：样本预测类别</li>
<li>$w_i$：样本权重</li>
</ul>
</blockquote>
</li>
<li><p><em>Mann-Witney U</em> 检验：考虑其意义，带入权重即可得</p>
<script type="math/tex; mode=display">\begin{align*}
auc = \frac {\sum_{i \in Pos} w_i * rank(i) -
   \sum_{i \in Pos} w_i * rank_{pos}(i)}
   {\sum_{i \in Pos} w_i * \sum_{j \in Neg} w_j}
\end{align*}</script><blockquote>
<ul>
<li>$rank_{pos}(i)$：正样本内部排序，样本$i$秩</li>
<li>$Neg$：负样本集合</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="多分类-AUC"><a href="#多分类-AUC" class="headerlink" title="多分类 AUC"></a>多分类 <em>AUC</em></h3><ul>
<li><p><em>Micro-AUC</em>：将每个类别视为样本标签，计算全体样本的正标签、负标签的 <em>AUC</em></p>
<ul>
<li>$n$ 个样本的 $m$ 维标签展平， 则其中有 $n$ 个正样本、$n * (m-1)$ 个负样本</li>
<li>$n$ 个样本的 $m$ 个分类器共 $n * m$ 个得分展平</li>
<li>使用以上预测得分、标签计算 <em>AUC</em></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># one-vs-rest分类器得分</span></span><br><span class="line">y_score = classifer.transform(X_test)</span><br><span class="line"><span class="comment"># 展平后计算fpr、tpr</span></span><br><span class="line">fpr_micro, tpr_micro, threshhold_micro = \</span><br><span class="line">	skilearn.metrics.roc_curve(y_test.ravel(), y_score.ravel())</span><br><span class="line"><span class="comment"># 利用fpr、tpr计算auc</span></span><br><span class="line">auc_micro = skilearn.metrics.auc(fpr_micro, tpr_micro)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于直接调用</span></span><br><span class="line">auc_micro = skilearn.metrics.roc_auc_score(y_test, y_score, average=<span class="string">&quot;micro&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><em>Macro-AUC</em>：对各类别，分别以计算 <em>ROC</em> 曲线（即 <em>TPR</em>、<em>FPR</em>），计算平均 <em>ROC</em> 曲线得到 <em>AUC</em></p>
<ul>
<li>对各类别分别计算 <em>TPR</em>、<em>FPR</em>，共 $m$ 组 <em>TPR</em>、<em>FPR</em></li>
<li><p>平均合并 <em>TPR</em>、<em>FPR</em>，计算 <em>AUC</em></p>
<ul>
<li><p>方法1：合并 <em>FPR</em>、去除重复值，使用 $m$ 组 <em>TPR</em>、<em>FPR</em> 分别求合并后 <em>FPR</em> 插值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分别计算各类别fpr、tpr</span></span><br><span class="line">fprs, tprs = [<span class="number">0</span>] * n_classes, [<span class="number">0</span>] * n_classes</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">	fprs[idx], tprs[idx], _ = sklearn.metrics.ruc_curve(</span><br><span class="line">		y_test[:, i], y_score[:, i])</span><br><span class="line"><span class="comment"># 合并fpr</span></span><br><span class="line">all_fpr = np.unique(np.concatenate(fprs))</span><br><span class="line">mean_tpr = np.zeros_like(all_fpr)</span><br><span class="line"><span class="comment"># 计算合并后fpr插值</span></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(n_classes):</span><br><span class="line">	mean_tpr += scipy.interp(all_fpr, fpr[idx], tpr[idx])</span><br><span class="line">mean_tpr /= n_classes</span><br><span class="line">auc_macro = sklearn.metrics.auc(all_fpr, mean_tpr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是和以下结果不同</span></span><br><span class="line">auc_macro = sklearn.metrics.roc_auc_score(fprs)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>以上分类器均为 <em>one-vs-rest</em> 分类器，$m$ 个类别则 $m$ 个分类器、每个样本 $m$ 个得分</li>
</ul>
</blockquote>
<h3 id="Kolmogorov-Smirnov-统计量"><a href="#Kolmogorov-Smirnov-统计量" class="headerlink" title="Kolmogorov-Smirnov 统计量"></a><em>Kolmogorov-Smirnov</em> 统计量</h3><p><em>KS</em> 值：刻画区分正负样本能力</p>
<script type="math/tex; mode=display">
KS = max \{|TPR - FPR|\}</script><ul>
<li><em>KS</em> 值体现 <strong>最理想情况</strong> 下，对正负样本区分能力<ul>
<li>即 <em>ROC</em> 曲线与 $TPR = FPR$ 直线的最远距离</li>
</ul>
</li>
</ul>
<h2 id="Squared-Error"><a href="#Squared-Error" class="headerlink" title="Squared Error"></a><em>Squared Error</em></h2><h3 id="Mean-Squared-Error"><a href="#Mean-Squared-Error" class="headerlink" title="Mean Squared Error"></a><em>Mean Squared Error</em></h3><p><em>MSE</em>：均方误差（偏差）</p>
<script type="math/tex; mode=display">
MSE = \frac 1 n \sum_{i=1}^{n} (y_{i} - \hat{y_{i}})^{2}</script><h4 id="Mean-Absolute-Error"><a href="#Mean-Absolute-Error" class="headerlink" title="Mean Absolute Error"></a><em>Mean Absolute Error</em></h4><p><em>MAE</em>：平均绝对误差</p>
<script type="math/tex; mode=display">
MAE = \frac 1 n \sum_{i=1}^n |y_i - \hat {y_i}|</script><h4 id="Mean-Absolute-Percentage-Error"><a href="#Mean-Absolute-Percentage-Error" class="headerlink" title="Mean Absolute Percentage Error"></a><em>Mean Absolute Percentage Error</em></h4><p><em>MAPE</em>：平均绝对百分比误差</p>
<script type="math/tex; mode=display">
MAPE = \frac 1 n \sum_{i=1}^n |\frac {y_i - \hat {y_i}} {y_i}|</script><h4 id="Symmetric-Mean-Absolute-Percentage-Error"><a href="#Symmetric-Mean-Absolute-Percentage-Error" class="headerlink" title="Symmetric Mean Absolute Percentage Error"></a><em>Symmetric Mean Absolute Percentage Error</em></h4><p><em>SMAPE</em>：对称平均绝对百分比误差</p>
<script type="math/tex; mode=display">
MAPE = \frac 1 n \sum_{i=1}^n |\frac {y_i - \hat {y_i}} {(|y_i| + |\hat {y_i}|) / 2}|</script><h3 id="R-2"><a href="#R-2" class="headerlink" title="$R^2$"></a>$R^2$</h3><script type="math/tex; mode=display">\begin{align*}
R^2 & = 1 - \frac {SSE} {SST} = \frac {SSR} {SST} \\
R^2_{adj} & = 1 - \frac {1 - R^2} {n - p - 1}
\end{align*}</script><blockquote>
<ul>
<li>$n, p$：样本量、特征数量</li>
<li>$SSE$：残差平方和</li>
<li>$SSR$：回归平方和、组内平方和</li>
<li>$SST$：离差平方和</li>
<li>$R^2_{adj}$：调整的$R^2$</li>
</ul>
</blockquote>
<h3 id="Akaike-Information-Criterion"><a href="#Akaike-Information-Criterion" class="headerlink" title="Akaike Information Criterion"></a><em>Akaike Information Criterion</em></h3><p><em>AIC</em> ：赤池信息准则</p>
<script type="math/tex; mode=display">\begin{align*}
AIC & = -2log(L(\hat \theta, x)) + 2p \\
& = nln(SSE/n) + 2p
\end{align*}</script><blockquote>
<ul>
<li>$n, p$：样本量、特征数量</li>
<li>$\theta$：带估参数</li>
<li>$L(\theta, x)$：似然函数</li>
<li>$SSE$：残差平方和</li>
</ul>
</blockquote>
<h3 id="Bayesian-Information-Criterion"><a href="#Bayesian-Information-Criterion" class="headerlink" title="Bayesian Information Criterion"></a><em>Bayesian Information Criterion</em></h3><p><em>BIC</em>：贝叶斯信息准则</p>
<script type="math/tex; mode=display">\begin{align*}
BIC & = -2log(L(\hat \theta, x)) + ln(n)p \\
& = nln(SSE/n) + ln(n)p
\end{align*}</script><h3 id="C-p"><a href="#C-p" class="headerlink" title="$C_p$"></a>$C_p$</h3><script type="math/tex; mode=display">\begin{align*}
C_p & = \frac {SSE} {\hat {\sigma^2}} - n + 2p \\
& = (n - m - 1) \frac {SSE_p} {SSE_m} - n + 2p
\end{align*}</script><blockquote>
<ul>
<li>$p$：选模型特征子集中特征数量</li>
<li>$m$：所有特征数量</li>
<li>$SSE_p$：选模型中残差平方和</li>
<li>$SSE_m$：全模型中残差平方和</li>
</ul>
</blockquote>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/Math-Mixin/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/Math-Mixin/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/Math-Mixin/">1</a></li><li><a class="pagination-link" href="/categories/Math-Mixin/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>