<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: Python - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="follow_it-verification-code" content="SVBypAPPHxjjr7Y4hHfn"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Python</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:15.000Z" title="3/21/2019, 5:27:15 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-28T07:12:54.000Z" title="2/28/2019, 3:12:54 PM">2019-02-28</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a></span><span class="level-item">22 minutes read (About 3353 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/config.html">Python安装配置</a></h1><div class="content"><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><h3 id="Python3包管理安装"><a href="#Python3包管理安装" class="headerlink" title="Python3包管理安装"></a>Python3包管理安装</h3><ul>
<li>CentOS7依赖缺失<ul>
<li><code>zlib-devel</code></li>
<li><code>bzip2-devel</code></li>
<li><code>readline-devel</code></li>
<li><code>openssl-devel</code></li>
<li><code>sqlite(3)-devel</code></li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>根据包用途、名称可以确认对应的应用，缺少就是相应
  <code>-devel(-dev)</code></li>
</ul>
</blockquote>
<h3 id="Python-Implementation"><a href="#Python-Implementation" class="headerlink" title="Python Implementation"></a>Python Implementation</h3><p>名称中flag体现该发行版中python特性</p>
<ul>
<li><p><code>-d</code>：with pydebug</p>
</li>
<li><p><code>-m</code>：with pymalloc</p>
</li>
<li><p><code>-u</code>：with wide unicode</p>
</li>
</ul>
<blockquote>
<ul>
<li>pymalloc是specialized object allocator<blockquote>
<ul>
<li>比系统自带的allocator快，且对python项目典型内存
 分配模式有更少的开销</li>
<li>使用c的<code>malloc</code>函数获得更大的内存池</li>
</ul>
</blockquote>
</li>
<li><p>原文：Pymalloc, a specialized object allocator written
  by Vladimir Marangozov, was a feature added to Python2.1.
  Pymalloc is intended to be faster than the system
  malloc() and to have less memory overhead for allocation
  patterns typical of Python programs. The allocator uses
  C’s malloc() function to get large pools of memory and
  then fulfills smaller memory requests from these pools.J</p>
<p> 注意：有时也有可能只是hard link</p>
</li>
</ul>
</blockquote>
<h2 id="Python配置"><a href="#Python配置" class="headerlink" title="Python配置"></a>Python配置</h2><h3 id="Python相关环境变量"><a href="#Python相关环境变量" class="headerlink" title="Python相关环境变量"></a>Python相关环境变量</h3><ul>
<li><code>PYTHONPATH</code>：python库查找路径</li>
<li><code>PYTHONSTARTUP</code>：python自动执行脚本</li>
</ul>
<h3 id="自动补全"><a href="#自动补全" class="headerlink" title="自动补全"></a>自动补全</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> readline</span><br><span class="line"><span class="keyword">import</span> rlcompleter</span><br><span class="line">	<span class="comment"># 为自动补全`rlcompleter`不能省略</span></span><br><span class="line"><span class="keyword">import</span> atexit</span><br><span class="line">readline.parse_and_bind(<span class="string">&quot;tab:complete&quot;</span>)</span><br><span class="line">	<span class="comment"># 绑定`&lt;tab&gt;`为自动补全</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">	readline.read_history(<span class="string">&quot;/path/to/python_history&quot;</span>)</span><br><span class="line">		<span class="comment"># 读取上次存储的python历史</span></span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line">atexit.register(</span><br><span class="line">	readline.write_history_file,</span><br><span class="line">	<span class="string">&quot;/path/to/python_history&quot;</span></span><br><span class="line">)</span><br><span class="line">	<span class="comment"># 将函数注册为推出python环境时执行</span></span><br><span class="line">	<span class="comment"># 将python历史输入存储在的自定以文件中</span></span><br><span class="line">	<span class="comment"># 这部分存储、读取历史其实不必要</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> readline, rlcompleter</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>每次</strong>在python解释器中<strong>执行</strong>生效</p>
</li>
<li><p>保存为文件<code>python_startup.py</code>，将添加到环境变量
<code>PYTHONSTARTUP</code>中，每次开启python自动执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> .bashrc</span></span><br><span class="line">export PYTHONSTARTUP=pythonstartup.py</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这个不能像*PATH一样添加多个文件，只能由一个文件</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Pip"><a href="#Pip" class="headerlink" title="Pip"></a>Pip</h2><p>python包、依赖管理工具</p>
<ul>
<li><p>pip包都是源码包 </p>
<ul>
<li>需要在安装时编译，因此可能在安装时因为系统原因出错</li>
<li>现在有了<code>wheels</code>也可以安装二进制包</li>
</ul>
</li>
</ul>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><ul>
<li>编译安装python一般包括<code>pip</code>、<code>setuptools</code></li>
<li>系统自带python无<code>pip</code>时，可用<code>apt</code>、<code>yum</code>等工具可以直接安装</li>
<li>虚拟python环境，无般法使用系统包管理工具安装pip，则只能
下载<code>pip</code>包使用<code>setuptools</code>安装</li>
</ul>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>配置文件：<code>~/.config/pip/pip.conf</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https:?//pypi.tuna.tsinghua.edu.cn/simple/</span><br><span class="line">	# pypi源地址</span><br><span class="line">format = columns</span><br><span class="line">	# pip list输出格式（legacy，columns）</span><br></pre></td></tr></table></figure>
<h3 id="依赖管理"><a href="#依赖管理" class="headerlink" title="依赖管理"></a>依赖管理</h3><p>pip通过纯文本文件（一般命名为<code>requirements.txt</code>）来记录、
管理python项目依赖</p>
<ul>
<li><code>$ pip freeze</code>：按照<code>package_name=version</code>的格式输出
已安装包
<code>$ pip install -r</code>：可按照指定文件（默认<code>requirements.txt</code>）
安装依赖</li>
</ul>
<h2 id="Virtualenv-Venv"><a href="#Virtualenv-Venv" class="headerlink" title="Virtualenv/Venv"></a>Virtualenv/Venv</h2><p>虚拟python环境管理器，使用<code>pip</code>直接安装</p>
<ul>
<li>将多个项目的python依赖隔离开，避免多个项目的包依赖、
python版本冲突</li>
<li>包依赖可以安装在项目处，避免需要全局安装python包的权限
要求、影响</li>
</ul>
<h3 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h3><p><code>$ virtualenv venv-dir</code>复制python至创建虚拟环境的文件夹中，
<code>$ source venv-dir/bin/activate</code>即激活虚拟环境，修改系统环境
变量，把python、相关的python包指向当前虚拟环境夹</p>
<h3 id="Virtualenv使用"><a href="#Virtualenv使用" class="headerlink" title="Virtualenv使用"></a>Virtualenv使用</h3><h2 id="Pyenv"><a href="#Pyenv" class="headerlink" title="Pyenv"></a>Pyenv</h2><p>python版本管理器，包括各种python发行版</p>
<h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>不需要事先安装python</p>
<ul>
<li><p>从github获取pyenv：<a href="git://github.com/yyuu/pyenv.git">git://github.com/yyuu/pyenv.git</a></p>
</li>
<li><p>将以下配置写入用户配置文件（建议是<code>.bashrc</code>)，也可以在
shell里面直接执行以暂时使用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PYENV_ROOT=&quot;$HOME/pyenv路径&quot;</span><br><span class="line">export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;</span><br><span class="line">eval &quot;$(pyenv init -)&quot;</span><br></pre></td></tr></table></figure>
<p>以上配置可参见<code>home_config/bashrc_addon</code>，以项目详情为准</p>
</li>
</ul>
<h3 id="Pyenv安装Python发行版问题"><a href="#Pyenv安装Python发行版问题" class="headerlink" title="Pyenv安装Python发行版问题"></a>Pyenv安装Python发行版问题</h3><p>使用pyenv安装python时一般是从<code>PYTHON_BUILD_MIRROR_URL</code>表示
的地址下载安装文件（或者说整个系统都是从这个地址下载），缺省
是<a target="_blank" rel="noopener" href="http://pypi.python.org">http://pypi.python.org</a>，但国内很慢</p>
<pre><code>#todo
</code></pre><ul>
<li>设置这个环境变量为国内的镜像站，如
<a target="_blank" rel="noopener" href="http://mirrors.sohu.com/python">http://mirrors.sohu.com/python</a>，但这个好像没用</li>
<li>在镜像站点下载安装包，放在<code>pyenv/cache</code>文件夹下（没有就
新建）</li>
</ul>
<p>pyenv安装python时和使用一般安装应用一样，只是安装prefix不是
<code>/usr/bin/</code>之类的地方，而是pyenv安装目录，因此pyenv编译安装
python也需要先安装依赖</p>
<h3 id="实现原理-1"><a href="#实现原理-1" class="headerlink" title="实现原理"></a>实现原理</h3><h4 id="修改-PATH环境变量"><a href="#修改-PATH环境变量" class="headerlink" title="修改$PATH环境变量"></a>修改<code>$PATH</code>环境变量</h4><ul>
<li>用户配置文件将<code>PYENV_ROOT/bin</code>放至<code>$PATH</code>首位</li>
<li>初始化pyenv时会将<code>PYENV_ROOT/shims</code>放至$PATH首位</li>
</ul>
<p><code>shims</code>、<code>bin</code>放在最前，优先使用pyenv中安装的命令</p>
<ul>
<li><p><code>bin</code>中包含的是pyenv自身命令（还有其他可执行文件，但是
无法直接执行?）</p>
</li>
<li><p><code>shims</code>则包含的是<strong>所有</strong>已经安装python组件</p>
<ul>
<li><p>包括python、可以执行python包、无法直接执行的python包</p>
</li>
<li><p>这些组件是内容相同的脚本文件，仅名称是pyenv所有安装
的python包</p>
<ul>
<li>用于截取python相关的命令</li>
<li>并根据设置python发行版做出相应的反应</li>
<li>因此命令行调用安装过的python包，pyenv会给提示
即使不是安装在当前python环境中</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>因此一般将命令放在<code>.profile</code>文件中，这样每次登陆都会设置好
pyenv放在<code>.bashrc</code>中会设置两次（没有太大关系）</p>
<h4 id="使用指定Python发行版"><a href="#使用指定Python发行版" class="headerlink" title="使用指定Python发行版"></a>使用指定Python发行版</h4><ul>
<li><p><code>$ pyenv local py-version</code>指定是在文件夹下生成
<code>.python-version</code>文件，写明python版本</p>
</li>
<li><p>所有的python相关的命令都被shims中的脚本文件截取</p>
</li>
</ul>
<p>pyenv应该是逐层向上查找<code>.python-version</code>文件，查找到文件则
按照相应的python发行版进行执行，否则按global版本</p>
<h2 id="Conda"><a href="#Conda" class="headerlink" title="Conda"></a>Conda</h2><p>通用包管理器</p>
<ul>
<li><p>管理任何语言、类型的软件</p>
<ul>
<li><p>conda默认可以从<a target="_blank" rel="noopener" href="http://repo.continuum.io">http://repo.continuum.io</a>安装<strong>已经
编译好</strong>二进制包</p>
</li>
<li><p>conda包和pip包只是<strong>部分</strong>重合，有些已安装conda包
甚至无法被pip侦测到（非python脚本包）</p>
</li>
<li><p>python本身也作为conda包被管理</p>
</li>
</ul>
</li>
<li><p>创建、管理虚拟python环境（包括python版本）</p>
</li>
</ul>
<h3 id="安装-2"><a href="#安装-2" class="headerlink" title="安装"></a>安装</h3><ul>
<li><p>conda在Miniconda，Anaconda发行版中默认安装</p>
<ul>
<li><p>Miniconda是只包括conda的发行版，没有Anaconda中默认
包含的包丰富</p>
</li>
<li><p>在其他的发行版中可以直接使用pip安装，但是这样安装的
conda功能不全，可能无法管理包</p>
</li>
</ul>
</li>
<li><p>Miniconda、Anaconda安装可以自行设置安装位置，无需介怀</p>
</li>
</ul>
<h3 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h3><p>conda配置文件为<code>$HOME/.condarc</code>，其中可以设置包括源在内
    配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br></pre></td></tr></table></figure>
<h4 id="添加国内源"><a href="#添加国内源" class="headerlink" title="添加国内源"></a>添加国内源</h4><p>conda源和pypi源不同（以下为清华源配置，当然可以直接修改
配置文件）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda config --<span class="built_in">set</span> show_channel_urls yes</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>conda源不是pypi源，不能混用</li>
</ul>
</blockquote>
<h4 id="Win平台设置"><a href="#Win平台设置" class="headerlink" title="Win平台设置"></a>Win平台设置</h4><ul>
<li><p>添加菜单项</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 可用于恢复菜单项</span></span><br><span class="line">$ <span class="built_in">cd</span> /path/to/conda_root</span><br><span class="line">$ python .\Lib\_nsis.py mkmenus</span><br></pre></td></tr></table></figure>
</li>
<li><p>VSCode是通过查找、执行<code>activate.bat</code>激活虚拟环境</p>
<ul>
<li>所以若虚拟环境中未安装<code>conda</code>（无<code>activate.bat</code>）
则虚拟环境无法自动激活</li>
</ul>
</li>
</ul>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda create [--<span class="built_in">clone</span> ori_env] -n env [packages[packages]]</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 创建虚拟环境</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> python也是conda包，可指定`python=x.x`</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda remove -n env --all</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 删除虚拟环境</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda info -e</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda env list</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 列出虚拟环境</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda info</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 列出conda配置</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda activate env</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 激活env环境</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda deactivate</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 退出当前环境</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda list -n env</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 列出env环境/当前环境安装conda包</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda search package</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 搜索包</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda install [-n env] packages</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> env环境/当前环境安装conda包</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda update [-n env] packages</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> env环境/当前环境升级conda包</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda remove [-n env] packages</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> env环境/当前环境移除包</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>使用conda而不是pip安装包更合适，方便管理</li>
</ul>
</blockquote>
<ul>
<li>创建新环境时，默认不安装任何包，包括<code>pip</code>，此时切换到
虚拟环境后，<code>pip</code>等命令都是默认环境的命令</li>
</ul>
<h2 id="Pipenv"><a href="#Pipenv" class="headerlink" title="Pipenv"></a>Pipenv</h2><p>pip、virtualenv、Pipfile（版本控制）功能的综合，事实上就
依赖于pip、virtualenv（功能封装）</p>
<ul>
<li><code>$ pipenv sync/install</code>替代<code>$ pip install</code></li>
<li><code>$ pipenv shell</code>替代<code>$ activate</code></li>
<li><code>$ pipenv run</code>甚至可以不用激活虚拟环境运行某些命令</li>
<li><code>Pipfile</code>控制dev、release包管理，<code>Pipfile.lock</code>锁定包依赖</li>
</ul>
<h3 id="安装-3"><a href="#安装-3" class="headerlink" title="安装"></a>安装</h3><ul>
<li>使用pip直接安装</li>
<li>系统安装源里有pipenv，也可以用系统包管理工具安装</li>
</ul>
<h3 id="实现原理-2"><a href="#实现原理-2" class="headerlink" title="实现原理"></a>实现原理</h3><h4 id="Python版本"><a href="#Python版本" class="headerlink" title="Python版本"></a>Python版本</h4><p>pipenv和virtualenv一样指定python版本也需要已经安装该python
版本</p>
<ul>
<li><p><code>$PATH</code>中的路径无法寻找到相应的python版本就需要手动
指定</p>
</li>
<li><p>不是有版本转换，将当前已安装版本通过类似2to3的“中
间层”转换为目标版本</p>
</li>
</ul>
<h4 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h4><p>pipenv会在<code>~/.local/share/virtualenv</code>文件夹下为所有的虚拟
python环境生成文件夹</p>
<ul>
<li><p>文件夹名字应该是“虚拟环境文件夹名称-文件夹全路径hash”</p>
</li>
<li><p>包括已安装的python包和python解释器</p>
</li>
<li><p>结构和virtualenv的内容类似，但virtualenv是放在项目目录下</p>
</li>
<li><p><code>$ python shell</code>启动虚拟环境就是以上文件夹路径放在
<code>$PATH</code>最前</p>
</li>
</ul>
<h4 id="依赖管理-1"><a href="#依赖管理-1" class="headerlink" title="依赖管理"></a>依赖管理</h4><p>pipenv通过Pipfile管理依赖（环境）</p>
<ul>
<li><p>默认安装：<code>$ pipenv install pkg</code></p>
<ul>
<li><p>作为默认包依赖安装pkg，并记录于<code>Pipfile</code>文件
<code>[packages]</code>条目下</p>
</li>
<li><p>相应的<code>$ pipenv install</code>则会根据<code>Pipfile</code>文件
<code>[packages]</code>条目安装默认环境包依赖</p>
</li>
</ul>
</li>
<li><p>开发环境安装：<code>$ pipenv install --dev pkg</code></p>
<ul>
<li><p>作为开发环境包依赖安装pkg，并记录于<code>Pipfile</code>
文件<code>[dev-packages]</code>条目下</p>
</li>
<li><p>相应的<code>$ pipenv intall --dev</code>则会根据<code>Pipfile</code>
文件中<code>[dev-packages]</code>安装开发环境包依赖</p>
</li>
</ul>
</li>
</ul>
<h4 id="Pipfile和Pipfile-lock"><a href="#Pipfile和Pipfile-lock" class="headerlink" title="Pipfile和Pipfile.lock"></a>Pipfile和Pipfile.lock</h4><ul>
<li><code>Pipfile</code>中是包依赖<strong>可用（install时用户指定）</strong>版本</li>
<li><code>Pipfile.lock</code>则是包依赖<strong>具体</strong>版本<ul>
<li>是pipenv安装包依赖时具体安装的版本，由安装时包源的
决定</li>
<li><code>Pipfile.lock</code>中甚至还有存储包的hash值保证版本一致</li>
<li><code>Pipfile</code>是用户要求，<code>Pipfile.lock</code>是实际情况</li>
</ul>
</li>
</ul>
<p>因此</p>
<ul>
<li><p><code>$ pipenv install/sync</code>优先依照<code>Pipfile.lock</code>安装具体
版本包，即使有更新版本的包也满足<code>Pipfile</code>的要求</p>
</li>
<li><p><code>Pipfile</code>和<code>Pipfile.lock</code>是同时更新、内容“相同”，
而不是手动锁定且手动更新<code>Pipfile</code>，再安装包时会默认更新
<code>Pipfile.lock</code></p>
</li>
</ul>
<h3 id="Pipenv用法"><a href="#Pipenv用法" class="headerlink" title="Pipenv用法"></a>Pipenv用法</h3><p>详情<a target="_blank" rel="noopener" href="https://docs.pipenv.org">https://docs.pipenv.org</a></p>
<h4 id="创建新环境"><a href="#创建新环境" class="headerlink" title="创建新环境"></a>创建新环境</h4><p>具体查看<code>$pipenv --help</code>，只是记住<code>$pipenv --site-packages</code>
表示虚拟环境可以共享系统python包</p>
<h4 id="默认环境和开发环境切换"><a href="#默认环境和开发环境切换" class="headerlink" title="默认环境和开发环境切换"></a>默认环境和开发环境切换</h4><p>pipenv没有像git那样的切换功能</p>
<ul>
<li>默认环境“切换”为dev环境：<code>$ pipenv install --dev</code></li>
<li>dev环境“切换”为默认环境：<code>$ pipenv uninstall --all-dev</code></li>
</ul>
<h4 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h4><p><code>$ pipenv sync</code></p>
<p>官方是说从<code>Pipfile.lock</code>读取包依赖安装，但是手动修改<code>Pipfile</code>
后<code>$ pipenv sync</code>也会先更新<code>Pipfile.lock</code>，然后安装包依赖，
感觉上和<code>$ pipenv install</code>差不多</p>
<h3 id="Pipenv特性"><a href="#Pipenv特性" class="headerlink" title="Pipenv特性"></a>Pipenv特性</h3><h4 id="和Pyenv的配合"><a href="#和Pyenv的配合" class="headerlink" title="和Pyenv的配合"></a>和Pyenv的配合</h4><p>pipenv可以找到pyenv已安装的python发行版，且不是通过<code>$PATH</code>
中<code>shims</code>获得实际路径</p>
<ul>
<li><p>pipenv能够找到pyenv实际安装python发行版的路径versions，
而不是脚本目录<code>shims</code></p>
</li>
<li><p>pipenv能自行找到pyenv安装的python发行版，即使其当时没有
被设置为local或global</p>
<ul>
<li><p>pyenv已安装Anaconda3和3.6并指定local为3.6的情况下
<code>$ pipenv --three</code>生成的虚拟python使用Anaconda3</p>
</li>
<li><p>后系统全局安装python34，无local下<code>pipenv --three</code>
仍然是使用Aanconda3</p>
</li>
<li><p>后注释pyenv的初始化命令重新登陆，<code>pipenv --three</code>就
使用python34</p>
</li>
</ul>
</li>
</ul>
<p>目前还是无法确定pipenv如何选择python解释器，但是根据以上测试
和github上的feature介绍可以确定的是和pyenv命令有关</p>
<p>pipenv和pyenv一起使用可以出现一些很蠢的用法，比如：pyenv指定
的local发行版中安装pipenv，然后用这pipenv可以将目录设置为
另外版本虚拟python环境（已经系统安装或者是pyenv安装）</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>除了以上的包管理、配置工具，系统包管理工具也可以看作是python
的包管理工具</p>
<ul>
<li>事实上conda就可以看作是pip和系统包管理工具的交集</li>
<li>系统python初始没有pip一般通过系统包管理工具安装</li>
</ul>
<h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>优先级：pip &gt; conda &gt; 系统包管理工具</p>
<ul>
<li>纯python库优先使用pip安装，需要额外编译的库使用conda</li>
<li>conda源和系统源都有的二进制包，优先conda，版本比较新</li>
</ul>
<h4 id="2018-04-06经验"><a href="#2018-04-06经验" class="headerlink" title="2018/04/06经验"></a>2018/04/06经验</h4><p>最后最合适的多版本管理是安装pipenv</p>
<ul>
<li><p>系统一般自带python2.7，所以用系统包管理工具安装一个
python3</p>
</li>
<li><p>使用新安装的python3安装pipenv，因为系统自带的python2.7
安装的很多包版过低</p>
</li>
<li><p>最后如果对python版本要求非常严格</p>
<ul>
<li>还可以再使用pyenv安装其他版本</li>
<li>然后仅手动启用pyenv用于指示pipenv使用目标python版本</li>
</ul>
</li>
</ul>
<h4 id="2019-02-20经验"><a href="#2019-02-20经验" class="headerlink" title="2019/02/20经验"></a>2019/02/20经验</h4><p>直接全局（如<code>/opt/miniconda</code>）安装Miniconda也是很好的选择</p>
<ul>
<li>由conda管理虚拟环境，虚拟环境创建在用户目录下，登陆时
激活</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:15.000Z" title="3/21/2019, 5:27:15 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-02T08:02:58.000Z" title="8/2/2021, 4:02:58 PM">2021-08-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Twists/">Twists</a></span><span class="level-item">2 minutes read (About 256 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Twists/twists.html">Python注意事项</a></h1><div class="content"><h2 id="Python原生数据结构"><a href="#Python原生数据结构" class="headerlink" title="Python原生数据结构"></a>Python原生数据结构</h2><h3 id="list"><a href="#list" class="headerlink" title="list"></a><code>list</code></h3><ul>
<li><p>方法</p>
<ul>
<li><code>==</code>：<code>list</code> 的 <code>==</code> 是逐值比较</li>
<li><code>__contains__</code>：方法中使用 <code>==</code> 比较元素<ul>
<li><code>in</code> 判断列表包含时也是逐值比较</li>
</ul>
</li>
</ul>
</li>
<li><p>迭代技巧</p>
<ul>
<li>需要修改列表元素时尽量不直接迭代列表，考虑<ul>
<li>新建列表存储元素值</li>
<li>迭代列表下标</li>
</ul>
</li>
<li>迭代过程会更改列表元素数量时<ul>
<li>使用 <code>.pop</code> 方法</li>
<li>确定迭代数量</li>
</ul>
</li>
</ul>
</li>
<li><p>运算注意</p>
<ul>
<li><code>.append</code>：直接修改原列表，不返回</li>
<li><code>.extend</code>：直接修改原列表，不返回</li>
<li><code>__add__</code>：返回新列表</li>
</ul>
</li>
</ul>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul>
<li>勿使用列表、字典等指针类型作为默认参数，否则函数重入结果很可能出现问题<ul>
<li>原因：函数体中任何对参数的修改都会被保留</li>
<li>替代方式：<code>None</code> + 函数体内判断</li>
</ul>
</li>
</ul>
<h3 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h3><ul>
<li>需要多次迭代时，应该将迭代器转换为可重复迭代数据结构，如：列表<ul>
<li>迭代器值会被消耗</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">14 minutes read (About 2173 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/convolutional_layers.html">卷积层</a></h1><div class="content"><h2 id="Conv1D"><a href="#Conv1D" class="headerlink" title="Conv1D"></a>Conv1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Conv1D(</span><br><span class="line">	filters(<span class="built_in">int</span>),</span><br><span class="line">	kernel_size(<span class="built_in">int</span>),</span><br><span class="line">	strides=<span class="number">1</span>,</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	dilation_rate=<span class="number">1</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>一维卷积层（即时域卷积）</p>
<ul>
<li><p>说明</p>
<ul>
<li>用以在一维输入信号上进行邻域滤波</li>
<li>作为首层时，需要提供关键字参数<code>input_shape</code></li>
<li>该层生成将输入信号与卷积核按照单一的空域（或时域）
方向进行卷积</li>
<li>可以将Convolution1D看作Convolution2D的快捷版</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>filters</code>：卷积核的数目（即输出的维度）</p>
</li>
<li><p><code>kernel_size</code>：整数或由单个整数构成的list/tuple，
卷积核的空域或时域窗长度</p>
</li>
<li><p><code>strides</code>：整数或由单个整数构成的list/tuple，为卷积
步长</p>
<ul>
<li>任何不为1的strides均与任何不为1的dilation_rate
均不兼容</li>
</ul>
</li>
<li><p><code>padding</code>：补0策略</p>
</li>
<li><p><code>activation</code>：激活函数</p>
</li>
<li><p><code>dilation_rate</code>：整数或由单个整数构成的list/tuple，
指定dilated convolution中的膨胀比例</p>
<ul>
<li>任何不为1的dilation_rate均与任何不为1的strides
均不兼容</li>
</ul>
</li>
<li><p><code>use_bias</code>：布尔值，是否使用偏置项</p>
</li>
<li><p><code>kernel_initializer</code>：权值初始化方法</p>
<ul>
<li>预定义初始化方法名的字符串</li>
<li>用于初始化权重的初始化器（参考initializers）</li>
</ul>
</li>
<li><p><code>bias_initializer</code>：偏置初始化方法</p>
<ul>
<li>为预定义初始化方法名的字符串</li>
<li>用于初始化偏置的初始化器</li>
</ul>
</li>
<li><p><code>kernel_regularizer</code>：施加在权重上的正则项，为
Regularizer对象</p>
</li>
<li><p><code>bias_regularizer</code>：施加在偏置向量上的正则项</p>
</li>
<li><p><code>activity_regularizer</code>：施加在输出上的正则项</p>
</li>
<li><p><code>kernel_constraints</code>：施加在权重上的约束项</p>
</li>
<li><p><code>bias_constraints</code>：施加在偏置上的约束项</p>
</li>
</ul>
</li>
<li><p>输入：形如<code>(batch, steps, input_dim)</code>的3D张量</p>
</li>
<li><p>输出：形如<code>(batch, new_steps, filters)</code>的3D张量</p>
<ul>
<li>因为有向量填充的原因，<code>steps</code>的值会改变</li>
</ul>
</li>
</ul>
<h2 id="Conv2D"><a href="#Conv2D" class="headerlink" title="Conv2D"></a>Conv2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Conv2D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>二维卷积层，即对图像的空域卷积</p>
<ul>
<li><p>说明</p>
<ul>
<li>该层对二维输入进行滑动窗卷积</li>
<li>当使用该层作为第一层时，应提供</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>filters</code>：卷积核的数目（即输出的维度）</p>
</li>
<li><p><code>kernel_size</code>：单个整数或由两个整数构成的list/tuple，
卷积核的宽度和长度</p>
<ul>
<li>如为单个整数，则表示在各个空间维度的相同长度</li>
</ul>
</li>
<li><p><code>strides</code>：单个整数或由两个整数构成的list/tuple，
卷积的步长</p>
<ul>
<li>如为单个整数，则表示在各个空间维度的相同步长</li>
<li>任何不为1的strides均与任何不为1的dilation_rate
均不兼容</li>
</ul>
</li>
<li><p><code>padding</code>：补0策略</p>
</li>
<li><p><code>activation</code>：激活函数</p>
</li>
<li><p><code>dilation_rate</code>：单个或两个整数构成的list/tuple，
指定dilated convolution中的膨胀比例</p>
<ul>
<li>任何不为1的dilation_rate均与任何不为1的strides
均不兼容</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch, channels, rows, cols)</code>
（”channels_first”）4D张量</p>
</li>
<li><p>输出：<code>(batch, filters, new_rows, new_cols)</code>
（”channels_first”）4D张量</p>
<ul>
<li>输出的行列数可能会因为填充方法而改变</li>
</ul>
</li>
</ul>
<h2 id="SeparableConv2D"><a href="#SeparableConv2D" class="headerlink" title="SeparableConv2D"></a>SeparableConv2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.SeparableConv2D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	depth_multiplier=<span class="number">1</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	depthwise_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	pointwise_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	depthwise_regularizer=<span class="literal">None</span>,</span><br><span class="line">	pointwise_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	depthwise_constraint=<span class="literal">None</span>,</span><br><span class="line">	pointwise_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>该层是在深度方向上的可分离卷积。</p>
<ul>
<li><p>说明</p>
<ul>
<li>首先按深度方向进行卷积（对每个输入通道分别卷积）</li>
<li>然后逐点卷积，将上步卷积结果混合到输出通道中</li>
<li>直观来说，可分离卷积可以看做讲一个卷积核分解为两个小
卷积核，或看作Inception模块的一种极端情况</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>depth_multiplier</code>：按深度卷积的步骤中，每个输入通道
使用（产生）多少个输出通道</p>
</li>
<li><p><code>depthwise_regularizer</code>：按深度卷积的权重上的正则项</p>
</li>
<li><p><code>pointwise_regularizer</code>：按点卷积的权重上的正则项</p>
</li>
<li><p><code>depthwise_constraint</code>：按深度卷积权重上的约束项</p>
</li>
<li><p><code>pointwise_constraint</code>：在按点卷积权重的约束项</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>输入：<code>(batch, channels, rows, cols)</code>4DT
（”channels_first”)</p>
</li>
<li><p>输出：<code>(batch, filters, new_rows, new_cols)</code>4DTK
（”channels_first”）</p>
<ul>
<li>输出的行列数可能会因为填充方法而改变</li>
</ul>
</li>
</ul>
<h2 id="Conv2DTranspose"><a href="#Conv2DTranspose" class="headerlink" title="Conv2DTranspose"></a>Conv2DTranspose</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Conv2DTranspose(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	output_padding=<span class="literal">None</span>/<span class="built_in">int</span>/<span class="built_in">tuple</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>该层是反卷积操作（转置卷积）</p>
<ul>
<li><p>说明</p>
<ul>
<li>通常发生在用户想要对普通卷积的结果做反方向的变换</li>
<li>参考文献<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.07285">A guide to convolution arithmetic for deep learning</a></li>
<li><a target="_blank" rel="noopener" href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic">Transposed convolution arithmetic</a></li>
<li><a target="_blank" rel="noopener" href="http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf">Deconvolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>output_padding</code>：指定输出的长、宽padding<ul>
<li>必须小于相应的<code>stride</code></li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch, rows, cols, channels)</code>4DT
（”channels_last”)</p>
</li>
<li><p>输出：<code>(batch, new_rows, new_cols, filters)</code>4DT
（”channels_last”）</p>
<ul>
<li>输出的行列数可能会因为填充方法而改变</li>
</ul>
</li>
</ul>
<h2 id="Conv3D"><a href="#Conv3D" class="headerlink" title="Conv3D"></a>Conv3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Conv3D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	dilation_rate=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>三维卷积对三维的输入（视频）进行滑动窗卷积</p>
<ul>
<li>输入：<code>(batch, channels, conv_dim1, conv_dim2, conv_dim3)</code>
5D张量（”channnels_first”）</li>
</ul>
<h2 id="Cropping1D"><a href="#Cropping1D" class="headerlink" title="Cropping1D"></a>Cropping1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Cropping1D(</span><br><span class="line">	cropping=(<span class="number">1</span>, <span class="number">1</span>)/<span class="built_in">tuple</span>/<span class="built_in">int</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在时间轴上对1D输入（即时间序列）进行裁剪</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>cropping</code>：指定在序列的首尾要裁剪掉多少个元素<ul>
<li>单值表示首尾裁剪相同</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch, axis_to_crop, features)</code>的3DT</p>
</li>
<li><p>输出：<code>(batch, cropped_axis, features)</code>的3DT</p>
</li>
</ul>
<h2 id="Cropping2D"><a href="#Cropping2D" class="headerlink" title="Cropping2D"></a>Cropping2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Cropping2D(</span><br><span class="line">	cropping=((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>)),</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对2D输入（图像）进行裁剪</p>
<ul>
<li><p>说明</p>
<ul>
<li>将在空域维度，即宽和高的方向上裁剪</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>cropping</code>：长为2的整数tuple，分别为宽和高方向上头部
与尾部需要裁剪掉的元素数<ul>
<li>单值表示宽高、首尾相同</li>
<li>单元组类似</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch, rows, cols, channels)</code>4DT（”channels_last”）</p>
</li>
<li><p>输出：<code>(batch, cropped_rows, cropped_cols, channels)</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">	<span class="comment"># Crop the input 2D images or feature maps</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Cropping2D(cropping=((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                     input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">3</span>)))</span><br><span class="line">	<span class="comment"># now model.output_shape == (None, 24, 20, 3)</span></span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">model.add(Cropping2D(cropping=((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>))))</span><br><span class="line">	<span class="comment"># now model.output_shape == (None, 20, 16, 64)</span></span><br></pre></td></tr></table></figure>
<h2 id="Cropping3D"><a href="#Cropping3D" class="headerlink" title="Cropping3D"></a>Cropping3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Cropping3D(</span><br><span class="line">	cropping=((<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对3D输入（空间、时空）进行裁剪</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>cropping</code>：长为3的整数tuple，分别为三个方向上头部
与尾部需要裁剪掉的元素数</li>
</ul>
</li>
<li><p>输入：<code>(batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)</code>
（”channels_first”）</p>
</li>
<li><p>输出：<code>(batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)</code></p>
</li>
</ul>
<h2 id="UpSampling1D"><a href="#UpSampling1D" class="headerlink" title="UpSampling1D"></a>UpSampling1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.UpSampling1D(</span><br><span class="line">	size=<span class="number">2</span>/integer</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在时间轴上，将每个时间步重复<code>size</code>次</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>size</code>：轴上采样因子</li>
</ul>
</li>
<li><p>输入：<code>(batch, steps, features)</code>的3D张量</p>
</li>
<li><p>输出：<code>(batch, upsampled_steps, feature)</code>的3D张量</p>
</li>
</ul>
<h2 id="UpSampling2D"><a href="#UpSampling2D" class="headerlink" title="UpSampling2D"></a>UpSampling2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.UpSampling2D(</span><br><span class="line">	size=(<span class="number">2</span>, <span class="number">2</span>)/<span class="built_in">tuple</span>/<span class="built_in">int</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将数据的行和列分别重复<code>size[0]</code>和<code>size[1]</code>次</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>size</code>：分别为行和列上采样因子</li>
</ul>
</li>
<li><p>输入：<code>(batch, channels, rows, cols)</code>的4D张量
（”channels_first”）</p>
</li>
<li><p>输出：<code>(batch, channels, upsampled_rows, upsampled_cols)</code></p>
</li>
</ul>
<h2 id="UpSampling3D"><a href="#UpSampling3D" class="headerlink" title="UpSampling3D"></a>UpSampling3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.UpSampling3D(</span><br><span class="line">	size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)/<span class="built_in">tuple</span>/<span class="built_in">int</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将数据的三个维度上分别重复<code>size</code>次</p>
<ul>
<li><p>说明</p>
<ul>
<li>本层目前只能在使用Theano为后端时可用</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>size</code>：代表在三个维度上的上采样因子</li>
</ul>
</li>
</ul>
<ul>
<li><p>输入：<code>(batch, dim1, dim2, dim3, channels)</code>5DT
（”channels_last”）</p>
</li>
<li><p>输出：<code>(batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)</code></p>
</li>
</ul>
<h2 id="ZeroPadding1D"><a href="#ZeroPadding1D" class="headerlink" title="ZeroPadding1D"></a>ZeroPadding1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.ZeroPadding1D(</span><br><span class="line">	padding=<span class="number">1</span>/<span class="built_in">int</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对1D输入的首尾端（如时域序列）填充0</p>
<ul>
<li><p>说明</p>
<ul>
<li>以控制卷积以后向量的长度</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>padding</code>：整数，在axis 1起始和结束处填充0数目</li>
</ul>
</li>
<li><p>输入：<code>(batch, axis_to_pad, features)</code>3DT</p>
</li>
<li><p>输出：<code>(batch, paded_axis, features)</code>3DT</p>
</li>
</ul>
<h2 id="ZeroPadding2D"><a href="#ZeroPadding2D" class="headerlink" title="ZeroPadding2D"></a>ZeroPadding2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.ZeroPadding2D(</span><br><span class="line">	padding=(<span class="number">1</span>, <span class="number">1</span>)/<span class="built_in">tuple</span>/<span class="built_in">int</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对2D输入（如图片）的边界填充0</p>
<ul>
<li><p>说明</p>
<ul>
<li>以控制卷积以后特征图的大小</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>padding</code>：在要填充的轴的起始和结束处填充0的数目</li>
</ul>
</li>
</ul>
<h2 id="ZeroPadding3D"><a href="#ZeroPadding3D" class="headerlink" title="ZeroPadding3D"></a>ZeroPadding3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.ZeroPadding3D(</span><br><span class="line">	padding=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将数据的三个维度上填充0</p>
<ul>
<li>说明<ul>
<li>本层目前只能在使用Theano为后端时可用</li>
</ul>
</li>
</ul>
<p>结束处填充0的数目</p>
<h2 id="ZeroPadding3D-1"><a href="#ZeroPadding3D-1" class="headerlink" title="ZeroPadding3D"></a>ZeroPadding3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.ZeroPadding3D(</span><br><span class="line">	padding=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将数据的三个维度上填充0</p>
<ul>
<li>说明<ul>
<li>本层目前只能在使用Theano为后端时可用</li>
</ul>
</li>
</ul>
<p>?时可用</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:42:51.000Z" title="8/4/2021, 11:42:51 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">4 minutes read (About 578 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/about_layers.html">Layers 总述</a></h1><div class="content"><h3 id="Layer方法"><a href="#Layer方法" class="headerlink" title="Layer方法"></a>Layer方法</h3><p>所有的Keras层对象都有如下方法：</p>
<ul>
<li><p><code>layer.get_weights()</code>：返回层的权重NDA</p>
</li>
<li><p><code>layer.set_weights(weights)</code>：从NDA中将权重加载到该层中
，要求NDA的形状与<code>layer.get_weights()</code>的形状相同</p>
</li>
<li><p><code>layer.get_config()</code>：返回当前层配置信息的字典，层也可以
借由配置信息重构</p>
</li>
<li><p><code>layer.from_config(config)</code>：根据<code>config</code>配置信息重构层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layer = Dense(<span class="number">32</span>)</span><br><span class="line">config = layer.get_config()</span><br><span class="line">reconstructed_layer = Dense.from_config(config)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">config = layer.get_config()</span><br><span class="line">layer = layers.deserialize(&#123;<span class="string">&#x27;class_name&#x27;</span>: layer.__class__.__name__,</span><br><span class="line">							<span class="string">&#x27;config&#x27;</span>: config&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="非共享层"><a href="#非共享层" class="headerlink" title="非共享层"></a>非共享层</h4><p>如果层仅有一个计算节点（即该层不是共享层），则可以通过下列
方法获得</p>
<ul>
<li>输入张量：<code>layer.input</code></li>
<li>输出张量：<code>layer.output</code></li>
<li>输入数据的形状：<code>layer.input_shape</code></li>
<li>输出数据的形状：<code>layer.output_shape</code></li>
</ul>
<h4 id="共享层"><a href="#共享层" class="headerlink" title="共享层"></a>共享层</h4><p>如果该层有多个计算节点（参考层计算节点和共享层）</p>
<ul>
<li>输入张量：<code>layer.get_input_at(node_index)</code></li>
<li>输出张量：<code>layer.get_output_at(node_index)</code></li>
<li>输入数据形状：<code>layer.get_input_shape_at(node_index)</code></li>
<li>输出数据形状：<code>layer.get_output_shape_at(node_index)</code></li>
</ul>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h4 id="shape类型"><a href="#shape类型" class="headerlink" title="shape类型"></a>shape类型</h4><ul>
<li><p>batch_size</p>
<ul>
<li>batch_size在实际数据输入中为首维（0维）</li>
<li>shape类型参数传递的tuple中一般不包括batch_size维度</li>
<li>输出时使用<code>None</code>表示<code>(batch_size,...)</code></li>
</ul>
</li>
<li><p>time_step</p>
<ul>
<li>对时序数据，time_step在实际数据输入中第二维（1维）</li>
</ul>
</li>
</ul>
<h5 id="input-shape"><a href="#input-shape" class="headerlink" title="input_shape"></a><code>input_shape</code></h5><ul>
<li><p>是<code>Layer</code>的初始化参数，所有<code>Layer</code>子类都具有</p>
</li>
<li><p>如果Layer是首层，需要传递该参数指明输入数据形状，否则
无需传递该参数</p>
<ul>
<li>有些子类有类似于<code>input_dim</code>等参数具有<code>input_shape</code>
部分功能</li>
</ul>
</li>
<li><p><code>None</code>：表示该维度变长</p>
</li>
</ul>
<h3 id="输入、输出"><a href="#输入、输出" class="headerlink" title="输入、输出"></a>输入、输出</h3><ul>
<li><p>channels/depth/features：时间、空间单位上独立的数据，
卷积应该在每个channal分别“独立”进行</p>
<ul>
<li>对1维时序（时间），channels就是每时刻的features</li>
<li>对2维图片（空间），channels就是色彩通道</li>
<li>对3维视频（时空），channels就是每帧色彩通道</li>
<li>中间数据，channnels就是每个filters的输出</li>
</ul>
</li>
<li><p><em>1D</em>：<code>(batch, dim, channels)</code>（<em>channels_last</em>）</p>
</li>
<li><p><em>2D</em>：<code>(batch, dim_1, dim_2, channels)</code>
（<em>channels_last</em>）</p>
</li>
<li><p><em>3D</em>：<code>(batch, dim_1, dim_2, dim_3, channels)</code>
（<em>channels_last</em>）</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">3 minutes read (About 472 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/advanced_activations_layers.html">高级激活层</a></h1><div class="content"><h3 id="LeakyReLU"><a href="#LeakyReLU" class="headerlink" title="LeakyReLU"></a>LeakyReLU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LeakyReLU(alpha=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p>带泄漏的修正线性单元。</p>
<ul>
<li><p>返回值：当神经元未激活时，它仍可以赋予其一个很小的梯度</p>
<ul>
<li><code>x &lt; 0</code>：<code>alpha * x</code></li>
<li><code>x &gt;= 0</code>：<code>x</code></li>
</ul>
</li>
<li><p>输入尺寸</p>
<ul>
<li>可以是任意的。如果将该层作为模型的第一层，需要指定
<code>input_shape</code>参数（整数元组，不包含样本数量的维度）</li>
</ul>
</li>
<li><p>输出尺寸：与输入相同</p>
</li>
<li><p>参数</p>
<ul>
<li><code>alpha</code>：<code>float &gt;= 0</code>，负斜率系数。</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf">Rectifier Nonlinearities Improve Neural Network Acoustic Models</a></li>
</ul>
</li>
</ul>
<h3 id="PReLU"><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.PReLU(</span><br><span class="line">	alpha_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	alpha_regularizer=<span class="literal">None</span>,</span><br><span class="line">	alpha_constraint=<span class="literal">None</span>,</span><br><span class="line">	shared_axes=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>参数化的修正线性单元。</p>
<ul>
<li><p>返回值</p>
<ul>
<li><code>x &lt; 0</code>：<code>alpha * x</code></li>
<li><code>x &gt;= 0</code>：<code>x</code></li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>alpha_initializer</code>: 权重的初始化函数。</li>
<li><code>alpha_regularizer</code>: 权重的正则化方法。</li>
<li><code>alpha_constraint</code>: 权重的约束。</li>
<li><code>shared_axes</code>: 激活函数共享可学习参数的轴。
如果输入特征图来自输出形状为
<code>(batch, height, width, channels)</code>
的2D卷积层，而且你希望跨空间共享参数，以便每个滤波
器只有一组参数，可设置<code>shared_axes=[1, 2]</code></li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
</ul>
</li>
</ul>
<h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ELU(alpha=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p>指数线性单元</p>
<ul>
<li><p>返回值</p>
<ul>
<li><code>x &lt; 0</code>：<code>alpha * (exp(x) - 1.)</code></li>
<li><code>x &gt;= 0</code>：<code>x</code></li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>alpha</code>：负因子的尺度。</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.07289v1">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</a></li>
</ul>
</li>
</ul>
<h3 id="ThresholdedReLU"><a href="#ThresholdedReLU" class="headerlink" title="ThresholdedReLU"></a>ThresholdedReLU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ThresholdedReLU(theta=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p>带阈值的修正线性单元。</p>
<ul>
<li><p>返回值</p>
<ul>
<li><code>x &gt; theta</code>：<code>x</code></li>
<li><code>x &lt;= theta</code>：0</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>theta</code>：<code>float &gt;= 0</code>激活的阈值位。</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1402.3337">Zero-Bias Autoencoders and the Benefits of Co-Adapting Features</a></li>
</ul>
</li>
</ul>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Softmax(axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Softmax激活函数</p>
<ul>
<li>参数<ul>
<li><code>axis</code>: 整数，应用 softmax 标准化的轴。</li>
</ul>
</li>
</ul>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ReLU(max_value=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>ReLU激活函数</p>
<ul>
<li>参数<ul>
<li><code>max_value</code>：浮点数，最大的输出值。</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">11 minutes read (About 1690 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/core_layers.html">常用层</a></h1><div class="content"><p>常用层对应于core模块，core内部定义了一系列常用的网络层，包括
全连接、激活层等</p>
<h2 id="Dense层"><a href="#Dense层" class="headerlink" title="Dense层"></a>Dense层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Dense(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Dense就是常用的全连接层</p>
<ul>
<li><p>用途：实现运算$output = activation(dot(input, kernel)+bias)$</p>
<ul>
<li><code>activation</code>：是逐元素计算的激活函数</li>
<li><code>kernel</code>：是本层的权值矩阵</li>
<li><code>bias</code>：为偏置向量，只有当<code>use_bias=True</code>才会添加</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>units</code>：大于0的整数，代表该层的输出维度。</p>
</li>
<li><p><code>activation</code>：激活函数</p>
<ul>
<li>为预定义的激活函数名（参考激活函数）</li>
<li>逐元素（element-wise）的Theano函数</li>
<li>不指定该参数，将不会使用任何激活函数
（即使用线性激活函数：a(x)=x）</li>
</ul>
</li>
<li><p><code>use_bias</code>: 布尔值，是否使用偏置项</p>
</li>
<li><p><code>kernel_initializer</code>：权值初始化方法</p>
<ul>
<li>预定义初始化方法名的字符串</li>
<li>用于初始化权重的初始化器（参考initializers）</li>
</ul>
</li>
<li><p><code>bias_initializer</code>：偏置向量初始化方法</p>
<ul>
<li>为预定义初始化方法名的字符串</li>
<li>用于初始化偏置向量的初始化器（参考initializers）</li>
</ul>
</li>
<li><p><code>kernel_regularizer</code>：施加在权重上的正则项，为
Regularizer对象</p>
</li>
<li><p><code>bias_regularizer</code>：施加在偏置向量上的正则项，为
Regularizer对象</p>
</li>
<li><p><code>activity_regularizer</code>：施加在输出上的正则项，为
Regularizer对象</p>
</li>
<li><p><code>kernel_constraints</code>：施加在权重上的约束项，为<br>Constraints对象</p>
</li>
<li><p><code>bias_constraints</code>：施加在偏置上的约束项，为
Constraints对象</p>
</li>
</ul>
</li>
<li><p>输入</p>
<ul>
<li>形如<code>(batch_size, ..., input_dim)</code>的NDT，最常见情况
为<code>(batch_size, input_dim)</code>的2DT</li>
<li>数据的维度大于2，则会先被压为与<code>kernel</code>相匹配的大小</li>
</ul>
</li>
<li><p>输出</p>
<ul>
<li>形如<code>(batch_size, ..., units)</code>的NDT，最常见的情况为
$(batch_size, units)$的2DT</li>
</ul>
</li>
</ul>
<h2 id="Activation层"><a href="#Activation层" class="headerlink" title="Activation层"></a>Activation层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Activation(</span><br><span class="line">	activation,</span><br><span class="line">	input_shape</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>激活层对一个层的输出施加激活函数</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>activation</code>：将要使用的激活函数<ul>
<li>预定义激活函数名</li>
<li>Tensorflow/Theano的函数（参考激活函数）</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：任意，使用激活层作为第一层时，要指定<code>input_shape</code></p>
</li>
<li><p>输出：与输入shape相同</p>
</li>
</ul>
<h2 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Dropout(</span><br><span class="line">	rate,</span><br><span class="line">	noise_shape=<span class="literal">None</span>,</span><br><span class="line">	seed=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为输入数据施加Dropout</p>
<ul>
<li><p>说明</p>
<ul>
<li><p>Dropout将在训练过程中每次更新参数时按一定概率<code>rate</code>
随机断开输入神经元</p>
</li>
<li><p>可以用于防止过拟合</p>
</li>
<li><p>参考文献：<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>参数</p>
<ul>
<li><p><code>rate</code>：0~1的浮点数，控制需要断开的神经元的比例</p>
</li>
<li><p><code>noise_shape</code>：整数张量，为将要应用在输入上的二值
Dropout mask的shape</p>
</li>
<li><p><code>seed</code>：整数，使用的随机数种子</p>
</li>
</ul>
</li>
<li><p>输入</p>
<ul>
<li>例：<code>(batch_size, timesteps, features)</code>，希望在各个
时间步上Dropout mask都相同，则可传入
<code>noise_shape=(batch_size, 1, features)</code></li>
</ul>
</li>
</ul>
<h2 id="Flatten层"><a href="#Flatten层" class="headerlink" title="Flatten层"></a>Flatten层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Flatten()</span><br></pre></td></tr></table></figure>
<p>Flatten层用来将输入“压平”，把多维的输入一维化</p>
<ul>
<li>常用在从卷积层到全连接层的过渡</li>
<li>Flatten不影响batch的大小。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Convolution2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>,</span><br><span class="line">            border_mode=<span class="string">&#x27;same&#x27;</span>,</span><br><span class="line">            input_shape=(<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 64, 32, 32)</span></span><br><span class="line"></span><br><span class="line">model.add(Flatten())</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 65536)</span></span><br></pre></td></tr></table></figure>
<h2 id="Reshape层"><a href="#Reshape层" class="headerlink" title="Reshape层"></a>Reshape层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Reshape(</span><br><span class="line">	target_shape,</span><br><span class="line">	input_shape</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Reshape层用来将输入shape转换为特定的shape</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>target_shape</code>：目标shape，为整数的tuple，不包含样本
数目的维度（batch大小）<ul>
<li>包含<code>-1</code>表示推断该维度大小</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：输入的shape必须固定（和<code>target_shape</code>积相同）</p>
</li>
<li><p>输出：<code>(batch_size, *target_shape)</code></p>
</li>
<li><p>例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Reshape((<span class="number">3</span>, <span class="number">4</span>), input_shape=(<span class="number">12</span>,)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 3, 4)</span></span><br><span class="line">	<span class="comment"># note: `None` is the batch dimension</span></span><br><span class="line"></span><br><span class="line">model.add(Reshape((<span class="number">6</span>, <span class="number">2</span>)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 6, 2)</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># also supports shape inference using `-1` as dimension</span></span><br><span class="line">model.add(Reshape((-<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 3, 2, 2)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Permute层"><a href="#Permute层" class="headerlink" title="Permute层"></a>Permute层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Permute(</span><br><span class="line">	dims(<span class="built_in">tuple</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Permute层将输入的维度按照给定模式进行重排</p>
<ul>
<li><p>说明</p>
<ul>
<li>当需要将RNN和CNN网络连接时，可能会用到该层。</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>dims</code>：指定重排的模式，不包含样本数的维度（即下标
从1开始）</li>
</ul>
</li>
<li><p>输出shape</p>
<ul>
<li>与输入相同，但是其维度按照指定的模式重新排列</li>
</ul>
</li>
<li><p>例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Permute((<span class="number">2</span>, <span class="number">1</span>), input_shape=(<span class="number">10</span>, <span class="number">64</span>)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 64, 10)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="RepeatVector层"><a href="#RepeatVector层" class="headerlink" title="RepeatVector层"></a>RepeatVector层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.RepeatVector(</span><br><span class="line">	n(<span class="built_in">int</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>RepeatVector层将输入重复n次</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>n</code>：整数，重复的次数</li>
</ul>
</li>
<li><p>输入：形如<code>(batch_size, features)</code>的张量</p>
</li>
<li><p>输出：形如<code>(bathc_size, n, features)</code>的张量</p>
</li>
<li><p>例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">32</span>, input_dim=<span class="number">32</span>))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 32)</span></span><br><span class="line"></span><br><span class="line">model.add(RepeatVector(<span class="number">3</span>))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 3, 32)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Lambda层"><a href="#Lambda层" class="headerlink" title="Lambda层"></a>Lambda层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Lambda(</span><br><span class="line">	function,</span><br><span class="line">	output_shape=<span class="literal">None</span>,</span><br><span class="line">	mask=<span class="literal">None</span>,</span><br><span class="line">	arguments=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对上一层的输出施以任何Theano/TensorFlow表达式</p>
<ul>
<li><p>参数</p>
<ul>
<li><p><code>function</code>：要实现的函数，该函数仅接受一个变量，即
上一层的输出</p>
</li>
<li><p><code>output_shape</code>：函数应该返回的值的shape，可以是一个
tuple，也可以是一个根据输入shape计算输出shape的函数</p>
</li>
<li><p><code>mask</code>: 掩膜</p>
</li>
<li><p><code>arguments</code>：可选，字典，用来记录向函数中传递的其他
关键字参数</p>
</li>
</ul>
</li>
<li><p>输出：<code>output_shape</code>参数指定的输出shape，使用TF时可自动
推断</p>
</li>
</ul>
<ul>
<li><p>例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.add(Lambda(<span class="keyword">lambda</span> x: x ** <span class="number">2</span>))</span><br><span class="line">	<span class="comment"># add a x -&gt; x^2 layer</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add a layer that returns the concatenation</span></span><br><span class="line"><span class="comment"># of the positive part of the input and</span></span><br><span class="line"><span class="comment"># the opposite of the negative part</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">antirectifier</span>(<span class="params">x</span>):</span></span><br><span class="line">	x -= K.mean(x, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">	x = K.l2_normalize(x, axis=<span class="number">1</span>)</span><br><span class="line">	pos = K.relu(x)</span><br><span class="line">	neg = K.relu(-x)</span><br><span class="line">	<span class="keyword">return</span> K.concatenate([pos, neg], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">antirectifier_output_shape</span>(<span class="params">input_shape</span>):</span></span><br><span class="line">	shape = <span class="built_in">list</span>(input_shape)</span><br><span class="line">	<span class="keyword">assert</span> <span class="built_in">len</span>(shape) == <span class="number">2</span>  <span class="comment"># only valid for 2D tensors</span></span><br><span class="line">	shape[-<span class="number">1</span>] *= <span class="number">2</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">tuple</span>(shape)</span><br><span class="line"></span><br><span class="line">model.add(Lambda(antirectifier,</span><br><span class="line">		 output_shape=antirectifier_output_shape))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="ActivityRegularizer层"><a href="#ActivityRegularizer层" class="headerlink" title="ActivityRegularizer层"></a>ActivityRegularizer层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.ActivityRegularization(</span><br><span class="line">	l1=<span class="number">0.0</span>,</span><br><span class="line">	l2=<span class="number">0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>经过本层的数据不会有任何变化，但会基于其激活值更新损失函数值</p>
<ul>
<li>参数<ul>
<li><code>l1</code>：1范数正则因子（正浮点数）</li>
<li><code>l2</code>：2范数正则因子（正浮点数）</li>
</ul>
</li>
</ul>
<h2 id="Masking层"><a href="#Masking层" class="headerlink" title="Masking层"></a>Masking层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Masking(mask_value=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<p>使用给定的值对输入的序列信号进行“屏蔽”</p>
<ul>
<li><p>说明</p>
<ul>
<li>用以定位需要跳过的时间步</li>
<li>对于输入张量的时间步，如果输入张量在该时间步上都等于
<code>mask_value</code>，则该时间步将在模型接下来的所有层
（只要支持masking）被跳过（屏蔽）。</li>
<li>如果模型接下来的一些层不支持masking，却接受到masking
过的数据，则抛出异常</li>
</ul>
</li>
<li><p>输入：形如<code>(samples,timesteps,features)</code>的张量</p>
</li>
</ul>
<ul>
<li>例：缺少时间步为3和5的信号，希望将其掩盖<ul>
<li>方法：赋值<code>x[:,3,:] = 0., x[:,5,:] = 0.</code></li>
<li>在LSTM层之前插入<code>mask_value=0.</code>的Masking层<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Masking(mask_value=<span class="number">0.</span>, input_shape=(timesteps, features)))</span><br><span class="line">model.add(LSTM(<span class="number">32</span>))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p>.`的Masking层
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Masking(mask_value=<span class="number">0.</span>, input_shape=(timesteps, features)))</span><br><span class="line">model.add(LSTM(<span class="number">32</span>))</span><br></pre></td></tr></table></figure></p>
<p>```</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">2 minutes read (About 229 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/locally_connected_layers.html">LocallyConnceted 局部连接层</a></h1><div class="content"><p>LocallyConnnected和Conv差不多，只是Conv每层共享卷积核，
这里不同位置卷积核独立</p>
<h2 id="LocallyConnected1D层"><a href="#LocallyConnected1D层" class="headerlink" title="LocallyConnected1D层"></a>LocallyConnected1D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.local.LocallyConnected1D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=<span class="number">1</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>类似于<code>Conv1D</code>，单卷积核权重不共享</p>
<h2 id="LocallyConnected2D层"><a href="#LocallyConnected2D层" class="headerlink" title="LocallyConnected2D层"></a>LocallyConnected2D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.local.LocallyConnected2D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>类似<code>Conv2D</code>，区别是不进行权值共享</p>
<ul>
<li>说明<ul>
<li>输出的行列数可能会因为填充方法而改变</li>
</ul>
</li>
<li>例<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(LocallyConnected2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line">	<span class="comment"># apply a 3x3 unshared weights convolution with 64 output filters on a 32x32 image</span></span><br><span class="line">	<span class="comment"># with `data_format=&quot;channels_last&quot;`:</span></span><br><span class="line">	<span class="comment"># now model.output_shape == (None, 30, 30, 64)</span></span><br><span class="line">	<span class="comment"># notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters</span></span><br><span class="line"></span><br><span class="line">model.add(LocallyConnected2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">	<span class="comment"># now model.output_shape == (None, 28, 28, 32)</span></span><br><span class="line">	<span class="comment"># add a 3x3 unshared weights convolution on top, with 32 output filters:</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">2 minutes read (About 374 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/pooling_layers.html">池化层</a></h1><div class="content"><h2 id="MaxPooling1D"><a href="#MaxPooling1D" class="headerlink" title="MaxPooling1D"></a>MaxPooling1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.MaxPooling1D(</span><br><span class="line">	pool_size=<span class="number">2</span>/<span class="built_in">int</span>,</span><br><span class="line">	strides=<span class="literal">None</span>/<span class="built_in">int</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span></span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对时域1D信号进行最大值池化</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>pool_size</code>：整数，池化窗口大小</li>
<li><code>strides</code>：整数或None，下采样因子，例如设2将会使得
输出shape为输入的一半，若为None则默认值为pool_size。</li>
</ul>
</li>
</ul>
<h2 id="MaxPooling2D"><a href="#MaxPooling2D" class="headerlink" title="MaxPooling2D"></a>MaxPooling2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.MaxPooling2D(</span><br><span class="line">	pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">	strides=<span class="literal">None</span>/<span class="built_in">int</span>/(<span class="built_in">int</span>),</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>/<span class="string">&quot;same&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为空域2D信号施加最大值池化 </p>
<h2 id="MaxPooling3D层"><a href="#MaxPooling3D层" class="headerlink" title="MaxPooling3D层"></a>MaxPooling3D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.MaxPooling3D(</span><br><span class="line">	pool_size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">	strides=<span class="literal">None</span>/<span class="built_in">int</span>/(<span class="built_in">int</span>),</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>/<span class="string">&quot;same&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为3D信号（空域或时空域）施加最大值池化</p>
<h2 id="AveragePooling1D层"><a href="#AveragePooling1D层" class="headerlink" title="AveragePooling1D层"></a>AveragePooling1D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.AveragePooling1D(</span><br><span class="line">	pool_size=<span class="number">2</span>,</span><br><span class="line">	strides=<span class="literal">None</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span></span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对1D信号（时域）进行平均值池化</p>
<h2 id="AveragePooling2D层"><a href="#AveragePooling2D层" class="headerlink" title="AveragePooling2D层"></a>AveragePooling2D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.AveragePooling2D(</span><br><span class="line">	pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">	strides=<span class="literal">None</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为2D（空域）信号施加平均值池化</p>
<h2 id="AveragePooling3D层"><a href="#AveragePooling3D层" class="headerlink" title="AveragePooling3D层"></a>AveragePooling3D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.AveragePooling3D(</span><br><span class="line">	pool_size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">	strides=<span class="literal">None</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为3D信号（空域或时空域）施加平均值池化</p>
<h2 id="GlobalMaxPooling1D层"><a href="#GlobalMaxPooling1D层" class="headerlink" title="GlobalMaxPooling1D层"></a>GlobalMaxPooling1D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.GlobalMaxPooling1D(</span><br><span class="line">	data_format=<span class="string">&quot;channels_last&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对于1D（时间）信号的全局最大池化</p>
<h2 id="GlobalAveragePooling1D层"><a href="#GlobalAveragePooling1D层" class="headerlink" title="GlobalAveragePooling1D层"></a>GlobalAveragePooling1D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.GlobalAveragePooling1D(</span><br><span class="line">	data_forma=<span class="string">&quot;channels_last&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为时域信号施加全局平均值池化</p>
<h2 id="GlobalMaxPooling2D层"><a href="#GlobalMaxPooling2D层" class="headerlink" title="GlobalMaxPooling2D层"></a>GlobalMaxPooling2D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.GlobalMaxPooling2D(</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为空域信号施加全局最大值池化</p>
<h2 id="GlobalAveragePooling2D层"><a href="#GlobalAveragePooling2D层" class="headerlink" title="GlobalAveragePooling2D层"></a>GlobalAveragePooling2D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.GlobalAveragePooling2D(</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为2D（空域）信号施加全局平均值池化</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">18 minutes read (About 2770 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/recurrent_layers.html">RNN</a></h1><div class="content"><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.RNN(</span><br><span class="line">	cell,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	unroll=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>循环神经网络层基类：抽象类、无法实例化对象</p>
<ul>
<li><p>参数</p>
<ul>
<li><p><code>cell</code>：RNN单元实例、列表，为RNN单元列表时，单元
 堆叠放置，实现高效堆叠RNN</p>
</li>
<li><p><code>return_sequences</code>：返回输出序列最后值/全部序列</p>
</li>
<li><p><code>return_state</code>：是否返回最后一个状态</p>
</li>
<li><p><code>go_backwards</code>：是否向后处理输入序列并返回相反的序列。</p>
</li>
<li><p><code>stateful</code>：批次中索引<code>i</code>处的每个样品的最后状态将
用作下一批次中索引<code>i</code>样品的初始状态</p>
</li>
<li><p><code>unroll</code>：是否将网络展开（否则将使用符号循环）</p>
<ul>
<li>展开可以加速 RNN，但它往往会占用更多的内存</li>
<li>展开只适用于短序列</li>
</ul>
</li>
<li><p><code>input_dim</code>：输入的维度（整数）</p>
<ul>
<li>将此层用作模型中的第一层时，此参数是必需的
（或者关键字参数 <code>input_shape</code>）</li>
</ul>
</li>
<li><p><code>input_length</code>： 输入序列的长度，在恒定时指定</p>
<ul>
<li>如果你要在上游连接 <code>Flatten</code> 和 <code>Dense</code> 层，则
需要此参数（没有它，无法计算全连接输出的尺寸）</li>
<li>如果循环神经网络层不是模型中的第一层，则需要在
第一层的层级指定输入长度
（或通过关键字参数<code>input_shape</code>）</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch_size, timesteps, input_dim)</code>3D张量</p>
</li>
<li><p>输出</p>
<ul>
<li><code>return_state=True</code>：则返回张量列表，第一个张量为
输出、剩余的张量为最后的状态，每个张量的尺寸为
<code>(batch_size, units)</code>。</li>
<li><code>return_state=False</code>：<code>(batch_size, units)</code>2D张量</li>
</ul>
</li>
</ul>
<p>说明</p>
<ul>
<li><p>屏蔽覆盖：支持以可变数量的时间步长对输入数据进行屏蔽覆盖</p>
</li>
<li><p>使用状态：可以将 RNN 层设置为 <code>stateful</code>（有状态的）</p>
<ul>
<li>这意味着针对一批中的样本计算的状态将被重新用作下一批
样品的初始状态</li>
<li><p>这假定在不同连续批次的样品之间有一对一的映射。</p>
</li>
<li><p>为了使状态有效：</p>
<ul>
<li>在层构造器中指定 <code>stateful=True</code>。</li>
<li>为模型指定一个固定的批次大小<ul>
<li>顺序模型：为模型的第一层传递一个
<code>batch_input_shape=(...)</code> 参数</li>
<li>带有Input层的函数式模型，为的模型的<strong>所有</strong>i
第一层传递一个<code>batch_shape=(...)</code>，这是输入
预期尺寸，包括批量维度</li>
</ul>
</li>
<li>在调用 <code>fit()</code> 是指定 <code>shuffle=False</code>。</li>
</ul>
</li>
<li><p>要重置模型的状态，请在特定图层或整个模型上调用
<code>.reset_states()</code></p>
</li>
</ul>
</li>
<li><p>初始状态</p>
<ul>
<li>通过使用关键字参数<code>initial_state</code>调用它们来符号化地
指定 RNN 层的初始状态（值应该是表示RNN层初始状态的
张量或张量列表）</li>
<li>通过调用带有关键字参数<code>states</code>的<code>reset_states</code>方法
来数字化地指定 RNN 层的初始状态（值应该是一个代表RNN
层初始状态的NDA/[NDA]）</li>
</ul>
</li>
<li><p>RNN单元对象需要具有</p>
<ul>
<li><code>call(input_at_t, states_at_t)</code>方法，它返回
<code>(output_at_t, states_at_t_plus_1)</code>，单元的调用
方法也可以采用可选参数 <code>constants</code></li>
<li><code>state_size</code>属性<ul>
<li>单个整数（单个状态）：在这种情况下，它是循环层
状态大小（应该与单元输出的大小相同）</li>
<li>整数的列表/元组（每个状态一个大小）：第一项应该
与单元输出的大小相同</li>
</ul>
</li>
</ul>
</li>
<li><p>传递外部常量</p>
<ul>
<li>使用<code>RNN.call</code>以及<code>RNN.call</code>的<code>constants</code>关键字
参数将<em>外部</em>常量传递给单元</li>
<li>要求<code>cell.call</code>方法接受相同的关键字参数<code>constants</code>，
这些常数可用于调节附加静态输入（不随时间变化）上的
单元转换，也可用于注意力机制</li>
</ul>
</li>
</ul>
<p>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MinimalRNNCell</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">	<span class="comment"># 定义RNN细胞单元（网络层子类）</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">init</span>(<span class="params">self, units, **kwargs</span>):</span></span><br><span class="line">		self.units = units</span><br><span class="line">		self.state_size = units</span><br><span class="line">		<span class="built_in">super</span>(MinimalRNNCell, self).init(**kwargs)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">		self.kernel = self.add_weight(</span><br><span class="line">			shape=(input_shape[-<span class="number">1</span>], self.units),</span><br><span class="line">			initializer=<span class="string">&quot;uniform&quot;</span>,</span><br><span class="line">			name=<span class="string">&quot;kernel&quot;</span></span><br><span class="line">		)</span><br><span class="line">		self.recurrent_kernel = self.add_weight(</span><br><span class="line">			shape=(self.units, self.units),</span><br><span class="line">			initializer=<span class="string">&quot;uniform&quot;</span>,</span><br><span class="line">			name=<span class="string">&quot;recurrent_kernel&quot;</span>)</span><br><span class="line">		self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, states</span>):</span></span><br><span class="line">		prev_output = states[<span class="number">0</span>]</span><br><span class="line">		h = K.dot(inputs, self.kernel)</span><br><span class="line">		output = h + K.dot(prev_output, self.recurrent_kernel)</span><br><span class="line">		<span class="keyword">return</span> output, [output]</span><br><span class="line"></span><br><span class="line">cell = MinimalRNNCell(<span class="number">32</span>)</span><br><span class="line">	<span class="comment"># 在RNN层使用这个单元：</span></span><br><span class="line">x = keras.Input((<span class="literal">None</span>, <span class="number">5</span>))</span><br><span class="line">layer = RNN(cell)</span><br><span class="line">y = layer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cells = [MinimalRNNCell(<span class="number">32</span>), MinimalRNNCell(<span class="number">64</span>)]</span><br><span class="line">	<span class="comment"># 用单元格构建堆叠的RNN的方法：</span></span><br><span class="line">x = keras.Input((<span class="literal">None</span>, <span class="number">5</span>))</span><br><span class="line">layer = RNN(cells)</span><br><span class="line">y = layer(x)</span><br></pre></td></tr></table></figure>
<h2 id="SimpleRNN"><a href="#SimpleRNN" class="headerlink" title="SimpleRNN"></a>SimpleRNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNN(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&quot;tanh&quot;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&quot;orthogonal&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	unroll=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>完全连接的RNN，其输出将被反馈到输入。</p>
<ul>
<li><p>参数</p>
<ul>
<li><p><code>units</code>：正整数，输出空间的维度。</p>
</li>
<li><p><code>activation</code>：要使用的激活函数</p>
<ul>
<li><code>tanh</code>：默认</li>
<li><code>None</code>：则不使用激活函数，即线性激活：<code>a(x) = x</code></li>
</ul>
</li>
<li><p><code>use_bias</code>：布尔值，该层是否使用偏置向量。</p>
</li>
<li><p><code>kernel_initializer</code>：<em>kernel</em>权值矩阵的初始化器</p>
</li>
<li><p><code>recurrent_initializer</code>：<em>recurrent_kernel</em>权值矩阵</p>
</li>
<li><p><code>bias_initializer</code>：偏置向量的初始化器</p>
</li>
<li><p><code>kernel_regularizer</code>：运用到<em>kernel</em>权值矩阵的正则化
函数</p>
</li>
<li><p><code>recurrent_regularizer</code>：运用到 <code>recurrent_kernel</code> 权值
矩阵的正则化函数</p>
</li>
<li><p><code>bias_regularizer</code>：运用到偏置向量的正则化函数</p>
</li>
<li><p><code>activity_regularizer</code>：运用到层输出（它的激活值）的
正则化函数</p>
</li>
<li><p><code>kernel_constraint</code>：运用到<em>kernel</em>权值矩阵的约束函数</p>
</li>
<li><p><code>recurrent_constraint</code>：运用到<em>recurrent_kernel</em>权值矩阵
的约束函数</p>
</li>
<li><p><code>bias_constraint</code>： 运用到偏置向量的约束函数</p>
</li>
<li><p><code>dropout</code>：单元的丢弃比例，用于输入的线性转换</p>
<ul>
<li>在<em>0-1</em>之间的浮点数</li>
</ul>
</li>
<li><p><code>recurrent_dropout</code>：单元的丢弃比例，用于循环层状态线性
转换</p>
</li>
<li><p><code>return_sequences</code>：返回输出序列中的全部序列</p>
<ul>
<li>默认：返回最后最后一个输出</li>
</ul>
</li>
<li><p><code>return_state</code>：除输出之外是否返回最后一个状态</p>
</li>
<li><p><code>go_backwards</code>：向后处理输入序列并返回相反的序列</p>
</li>
<li><p><code>stateful</code>：批次中索引 i 处的每个样品的最后状态，将用作
下一批次中索引 i 样品的初始状态</p>
</li>
<li><p><code>unroll</code>：展开网络</p>
</li>
</ul>
</li>
</ul>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.GRU(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&quot;tanh&quot;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&quot;hard_sigmoid&quot;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&quot;orthogonal&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	implementation=<span class="number">1</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	unroll=<span class="literal">False</span>,</span><br><span class="line">	reset_after=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>门限循环单元网络</p>
<ul>
<li><p>说明：有两种变体</p>
<ul>
<li><p>默认的基于<em>1406.1078v3</em>，并且在矩阵乘法之前将复位门
应用于隐藏状态</p>
</li>
<li><p>另一种基于<em>1406.1078v1</em>并且顺序倒置</p>
<ul>
<li><p>兼容<em>CuDNNGRU(GPU-only)</em>，并且允许在 CPU 上进行
推理</p>
</li>
<li><p>对于<em>kernel</em>和<em>recurrent_kernel</em>有可分离偏置
<code>reset_after=True</code>和<code>recurrent_activation=sigmoid</code> 。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>recurrent_activation</code>：用于循环时间步的激活函数</p>
</li>
<li><p><code>implementation</code>：实现模式</p>
<ul>
<li><p><code>1</code>：将把它的操作结构化为更多的小的点积和加法操作</p>
</li>
<li><p><code>2</code>：将把它们分批到更少，更大的操作中</p>
</li>
<li><p>这些模式在不同的硬件和不同的应用中具有不同的性能配置
文件</p>
</li>
</ul>
</li>
<li><p><code>reset_after</code>：GRU公约，在矩阵乘法之后使用重置门</p>
</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.1078">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.1259">On the Properties of Neural Machine Translation：Encoder-Decoder Approaches</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1412.3555v1">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></li>
</ul>
</li>
</ul>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTM(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&quot;tanh&quot;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&quot;hard_sigmoid&quot;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&quot;orthogonal&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	implementation=<span class="number">1</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	unroll=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>长短期记忆网络层（Hochreiter 1997）</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>unit_forget_bias</code><ul>
<li><code>True</code>：初始化时，将忘记门的偏置加 1，同时还会强制
<code>bias_initializer=&quot;zeros&quot;</code>（这个建议来自
<a target="_blank" rel="noopener" href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz et al.</a>）</li>
</ul>
</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.bioinf.jku.at/publications/older/2604.pdf">Long short-term memory</a> (original 1997 paper)</li>
<li><a target="_blank" rel="noopener" href="http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015">Learning to forget：Continual prediction with LSTM</a></li>
<li><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~graves/preprint.pdf">Supervised sequence labeling with recurrent neural networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></li>
</ul>
</li>
</ul>
<h2 id="ConvLSTM2D"><a href="#ConvLSTM2D" class="headerlink" title="ConvLSTM2D"></a>ConvLSTM2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ConvLSTM2D(</span><br><span class="line">	filters(<span class="built_in">int</span>),</span><br><span class="line">	kernel_size(<span class="built_in">tuple</span>(<span class="built_in">int</span>, <span class="built_in">int</span>)),</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	activation=<span class="string">&#x27;tanh&#x27;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&#x27;hard_sigmoid&#x27;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>卷积LSTM：它类似于LSTM层，但输入变换和循环变换都是卷积的</p>
<ul>
<li><p>说明</p>
<ul>
<li>当前的实现不包括单元输出的反馈回路</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>dilation_rate</code>：用于膨胀卷积的膨胀率<ul>
<li><code>stride!=1</code>与<code>dilation_rate!=1</code>两者不兼容。</li>
</ul>
</li>
</ul>
</li>
<li><p>输入尺寸</p>
<ul>
<li><code>data_format=&quot;channels_first&quot;</code>：尺寸为
<code>(batch, time, channels, rows, cols)</code></li>
<li><code>data_format=&quot;channels_last&quot;</code>：尺寸为
<code>(batch, time, rows, cols, channels)</code></li>
</ul>
</li>
<li><p>输出尺寸</p>
<ul>
<li><p><code>return_sequences=True</code></p>
<ul>
<li><p><code>data_format=&quot;channels_first&quot;</code>：返回尺寸为
<code>(batch, time, filters, output_row, output_col)</code></p>
</li>
<li><p><code>data_format=&quot;channels_last&quot;</code>：返回尺寸为
<code>(batch, time, output_row, output_col, filters)</code></p>
</li>
</ul>
</li>
<li><p><code>return_seqences=False</code></p>
<ul>
<li><p><code>data_format =&quot;channels_first&quot;</code>：返回尺寸为
<code>(batch, filters, output_row, output_col)</code></p>
</li>
<li><p><code>data_format=&quot;channels_last&quot;</code>：返回尺寸为
<code>(batch, output_row, output_col, filters)</code></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1506.04214v1">Convolutional LSTM Network：A Machine Learning Approach for Precipitation Nowcasting</a></li>
</ul>
</li>
</ul>
<h2 id="SimpleRNNCell"><a href="#SimpleRNNCell" class="headerlink" title="SimpleRNNCell"></a>SimpleRNNCell</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNNCell(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&#x27;tanh&#x27;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><em>SimpleRNN</em>的单元类</p>
<h2 id="GRUCell"><a href="#GRUCell" class="headerlink" title="GRUCell"></a>GRUCell</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.GRUCell(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&#x27;tanh&#x27;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&#x27;hard_sigmoid&#x27;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	implementation=<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><em>GRU</em>层的单元类</p>
<h2 id="LSTMCell"><a href="#LSTMCell" class="headerlink" title="LSTMCell"></a>LSTMCell</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTMCell(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&#x27;tanh&#x27;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&#x27;hard_sigmoid&#x27;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	implementation=<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>LSTM层的单元类</p>
<h2 id="StackedRNNCells"><a href="#StackedRNNCells" class="headerlink" title="StackedRNNCells"></a>StackedRNNCells</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.StackedRNNCells(cells)</span><br></pre></td></tr></table></figure>
<p>将一堆RNN单元表现为一个单元的封装器</p>
<ul>
<li><p>说明</p>
<ul>
<li>用于实现高效堆叠的 RNN。</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>cells</code>：RNN 单元实例的列表</li>
</ul>
</li>
</ul>
<p>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cells = [</span><br><span class="line">    keras.layers.LSTMCell(output_dim),</span><br><span class="line">    keras.layers.LSTMCell(output_dim),</span><br><span class="line">    keras.layers.LSTMCell(output_dim),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">inputs = keras.Input((timesteps, input_dim))</span><br><span class="line">x = keras.layers.RNN(cells)(inputs)</span><br></pre></td></tr></table></figure>
<h2 id="CuDNNGRU"><a href="#CuDNNGRU" class="headerlink" title="CuDNNGRU"></a>CuDNNGRU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.CuDNNGRU(</span><br><span class="line">	units,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>由 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">CuDNN</a> 
支持的快速<em>GRU</em>实现</p>
<ul>
<li><p>说明</p>
<ul>
<li>只能以<em>TensorFlow</em>后端运行在<em>GPU</em>上</li>
</ul>
</li>
</ul>
<h2 id="CuDNNLSTM"><a href="#CuDNNLSTM" class="headerlink" title="CuDNNLSTM"></a>CuDNNLSTM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.CuDNNLSTM(</span><br><span class="line">	units,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>由 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">CuDNN</a>
支持的快速<em>LSTM</em>实现</p>
<ul>
<li><p>说明</p>
<ul>
<li>只能以<em>TensorFlow</em>后端运行在<em>GPU</em>上</li>
</ul>
</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/tags/Python/page/8/">Previous</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/tags/Python/page/10/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/tags/Python/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/tags/Python/page/8/">8</a></li><li><a class="pagination-link is-current" href="/tags/Python/page/9/">9</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/WWxwMVBsOUtoNTdMSlJ4Z1lWVnRISERsd2t6ek9MeVpEUWs0YldlZGxUdXlKdDNmMEZVV1hWaFZFYWFSNmFKL25penZodWx3UzRiaVkxcnREWCtOYUJhZWhNbWpzaUdyc1hPangycUh5RTVjRXFnZnFGdVdSTzZvVzJBcTJHKzl8aXpDK1ROWWl4N080YkFEK3QvbEVWNEJuQjFqdWdxODZQcGNoM1NqbERXST0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>