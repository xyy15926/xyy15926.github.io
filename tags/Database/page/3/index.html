<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: Database - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="follow_it-verification-code" content="SVBypAPPHxjjr7Y4hHfn"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Database</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-19T01:09:11.000Z" title="7/19/2021, 9:09:11 AM">2021-07-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/SQL-DB/">SQL DB</a></span><span class="level-item">3 minutes read (About 394 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/SQL-DB/sql_puzzles.html">SQL数据库Puzzles</a></h1><div class="content"><h2 id="数据迁移"><a href="#数据迁移" class="headerlink" title="数据迁移"></a>数据迁移</h2><h3 id="直接查询、插入"><a href="#直接查询、插入" class="headerlink" title="直接查询、插入"></a>直接查询、插入</h3><h4 id="同库"><a href="#同库" class="headerlink" title="同库"></a>同库</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> dst_tb <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> src_tb;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> dst_tb(field1, field2, ...) <span class="keyword">select</span> (field_a, field_b, ...) <span class="keyword">from</span> src_tb;</span><br></pre></td></tr></table></figure>
<h4 id="异库、同服务器"><a href="#异库、同服务器" class="headerlink" title="异库、同服务器"></a>异库、同服务器</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> db1.dst_db <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> db2.src_db;</span><br><span class="line">	# 插入已有表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> db1.dst_tb <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> db2.src_tb;</span><br><span class="line">	# 创建表并插入数据</span><br><span class="line">rename <span class="keyword">table</span> src_db.src_tb <span class="keyword">to</span> dst_db.dst_tb;</span><br><span class="line">	# 重命名迁移完整表</span><br></pre></td></tr></table></figure>
<h4 id="异服务器"><a href="#异服务器" class="headerlink" title="异服务器"></a>异服务器</h4><h3 id="文件中介、跨实例"><a href="#文件中介、跨实例" class="headerlink" title="文件中介、跨实例"></a>文件中介、跨实例</h3><h4 id="sql"><a href="#sql" class="headerlink" title=".sql"></a><code>.sql</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mysqldump [-u user] -p --single-transaction [--<span class="built_in">where</span>=<span class="string">&quot;&quot;</span>] src_db src_tb &gt; src_db.src_tb.sql</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 导入数据</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 加上`-d`仅导出表结构</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mysql [-u user] -p dst_db &lt; src_db.src_tb.sql</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 导入数据</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source src_db.src_tb.sql;</span><br></pre></td></tr></table></figure>
<h4 id="csv"><a href="#csv" class="headerlink" title=".csv"></a><code>.csv</code></h4><h5 id="secure-file-priv"><a href="#secure-file-priv" class="headerlink" title="secure_file_priv"></a><code>secure_file_priv</code></h5><p><code>load data infile</code>和<code>into outfile</code>需要mysql开启
<code>secure_file_priv</code>选项，可以通过查看</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">global</span> variables <span class="keyword">like</span> `<span class="operator">%</span>secure<span class="operator">%</span>`;</span><br></pre></td></tr></table></figure>
<p>mysql默认值<code>NULL</code>不允许执行，需要更改配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">secure_file_priv=&#x27;&#x27;</span><br></pre></td></tr></table></figure>
<h5 id="本机Server"><a href="#本机Server" class="headerlink" title="本机Server"></a>本机Server</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> src_tb <span class="keyword">into</span> outfile file_name.csv</span><br><span class="line">	fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">	optionally enclosed <span class="keyword">by</span> <span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line">	escaped <span class="keyword">by</span> <span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line">	lines terminated <span class="keyword">by</span> <span class="string">&#x27;\r\n&#x27;</span>;</span><br><span class="line">	# 导出至`.csv`</span><br><span class="line"></span><br><span class="line">load data infile file_name.csv [replace] <span class="keyword">into</span> <span class="keyword">table</span> dst_tb(field1, field2, <span class="variable">@dummy</span>...)</span><br><span class="line">	fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">	optionally enclosed <span class="keyword">by</span> <span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line">	escaped <span class="keyword">by</span> <span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line">	lines terminated <span class="keyword">by</span> <span class="string">&#x27;\r\n&#x27;</span>;</span><br><span class="line">	# 从`.csv`数据导入</span><br><span class="line">	# 表结构不同时可以设置对应字段，多余字段`<span class="variable">@dummy</span>`表示丢弃</span><br></pre></td></tr></table></figure>
<h5 id="异机Server"><a href="#异机Server" class="headerlink" title="异机Server"></a>异机Server</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mysql -h host -u user -p src_db -N -e <span class="string">&quot;select * from src_tb;&quot;</span> &gt; file_name.csv</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 只能通过*shell*查询并导出至文件</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 需要`file`权限</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> `-N`：skip column names</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load data <span class="keyword">local</span> infile filename.csv;</span><br><span class="line">	# 指定`<span class="keyword">local</span>`则从<span class="operator">*</span>client<span class="operator">*</span>读取文件，否则从<span class="operator">*</span>server<span class="operator">*</span>读取</span><br></pre></td></tr></table></figure>
<h3 id="大表分块迁移"><a href="#大表分块迁移" class="headerlink" title="大表分块迁移"></a>大表分块迁移</h3><ul>
<li>容易分块的字段<ul>
<li>自增id</li>
<li>时间</li>
</ul>
</li>
</ul>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T09:13:17.000Z" title="7/16/2021, 5:13:17 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">39 minutes read (About 5813 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/hdfs.html">Hadoop HDFS</a></h1><div class="content"><h2 id="HDFS设计模式"><a href="#HDFS设计模式" class="headerlink" title="HDFS设计模式"></a>HDFS设计模式</h2><ul>
<li><p>数据读取、写入</p>
<ul>
<li>HDFS一般存储不可更新的文件，只能对文件进行数据追加
，也不支持多个写入者的操作</li>
<li>认为一次写入、多次读取是最高效的访问模式</li>
<li>namenode将metadata存储在内存中，所以文件系统所能
存储的文件总数受限于NameNode的内存</li>
</ul>
</li>
<li><p>适合模式</p>
<ul>
<li>每次分析都涉及数据集大部分数据甚至全部，因此读取整个
数据集的大部分数据、甚至全部，因此读取整个数据集的
时间比读取第一条记录时间延迟更重要</li>
<li>HDFS不适合要求地时间延迟的数据访问应用，HDFS是为
高数据吞吐量应用优化的，可能会提高时间延迟</li>
</ul>
</li>
<li><p>硬件：HDFS无需高可靠硬件，HDFS被设计为即使节点故障也能
继续运行，且不让用户察觉</p>
</li>
</ul>
<h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h3><p>和普通文件系统一样，HDFS同样也有块的概念，默认为64MB</p>
<ul>
<li><p>HDFS上的文件也被划分为块大小的多个chunk，作为独立的存储
单元，但是HDFS中小于块大小的文件不会占据整个块空间</p>
</li>
<li><p>对分布式文件系统块进行抽象的好处</p>
<ul>
<li>文件大小可以大于网络中任意一个磁盘的容量</li>
<li>使用抽象块而非整个文件作为存储单元，简化了存储子系统
的设计<ul>
<li>简化存储管理，块大小固定，计算磁盘能存储块数目
相对容易</li>
<li>消除了对元素见的顾虑，文件的元信息（权限等），
不需要和块一起存储，可由其他系统单独管理</li>
</ul>
</li>
</ul>
</li>
<li><p>块非常适合用于数据备份，进而提供数据容错能力、提高可用性</p>
</li>
</ul>
<h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>HDFS系统中的管理者</p>
<ul>
<li><p>集中存储了HDFS的元信息Metadata</p>
<ul>
<li>维护文件系统的文件树、全部的文件和文件夹的元数据</li>
<li>管理文件系统的Namespace：创建、删除、修改、列出所有
文件、目录</li>
</ul>
</li>
<li><p>执行数据块的管理操作</p>
<ul>
<li>把文件映射到所有的数据块</li>
<li>创建、删除数据块</li>
<li>管理副本的Placement、Replication</li>
</ul>
</li>
<li><p>负责DataNode的成员管理</p>
<ul>
<li>接受DataNode的Registration</li>
<li>接受DataNode周期性的Heart Beat</li>
</ul>
</li>
</ul>
<p>Hadoop上层模块，根据NameNode上的元信息，就可以知道每个数据块
有多少副本、存放在哪些节点上，据此分配计算任务，通过
<strong>Move Computation to Data</strong>，而不是移动数据本身，减少数据
移动开销，加快计算过程</p>
<h4 id="Metadata的保存"><a href="#Metadata的保存" class="headerlink" title="Metadata的保存"></a>Metadata的保存</h4><p>为了支持高效的存取操作，NameNode把所有的元信息保存在主内存，
包括文件和数据块的命名空间、文件到数据块的映射、数据块副本
的位置信息。文件命名空间、文件到数据块的映射信息也会持久化
到NameNode本地文件系统</p>
<ul>
<li>FsImage：命名空间镜像文件，保存整个文件系统命名空间、
文件到数据块的映射信息</li>
<li>EditLog：编辑日志文件，是一个Transaction Log文件，记录了
对文件系统元信息的所有更新操作：创建文件、改变文件的
Replication Factor</li>
</ul>
<p>NameNode启动时，读取FsImage、EditLog文件，把EditLog的所有
事务日志应用到从FsImage文件中装载的旧版本元信息上，生成新的
FsImage并保存，然后截短EditLog</p>
<h4 id="NameNode可恢复性"><a href="#NameNode可恢复性" class="headerlink" title="NameNode可恢复性"></a>NameNode可恢复性</h4><h5 id="多个文件系统备份"><a href="#多个文件系统备份" class="headerlink" title="多个文件系统备份"></a>多个文件系统备份</h5><p>备份文件系统元信息的持久化版本</p>
<ul>
<li>在NameNode写入元信息的持久化版本时，同步、atomic写入多个
文件系统（一般是本地磁盘、mount为本地目录的NFS）</li>
</ul>
<h5 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h5><p>运行Secondary NameNode：负责周期性的使用EditLog更新
FsImage，保持EditLog在一定规模内</p>
<ul>
<li><p>Seconadary NameNode保存FsImage、EditLog文件副本，
每个一段时间从NameNode拷贝FsImage，和EditLog文件进行
合并，然后把更新后的FsImage复制回NameNode</p>
</li>
<li><p>若NameNode宕机，可以启动其他机器，从Secondary
NameNode获得FsImage、EditLog，恢复宕机之前的最新的
元信息，当作新的NameNode，也可以直接作为主NameNode</p>
</li>
<li><p>Secondary NameNode保存的出状态总是滞后于主节点，需要
从NFS获取NameNode部分丢失的metadata</p>
</li>
<li><p>Secondary NameNode需要运行在另一台机器，需要和主
NameNode一样规模的CPU计算能力、内存，以便完成元信息
管理</p>
</li>
</ul>
<p>想要从失效的NameNode恢复，需要启动一个拥有文件系统数据副本的
新NameNode，并配置DataNode和客户端以便使用新的NameNode</p>
<ul>
<li>将namespace的映像导入内存中</li>
<li>重做编辑日志</li>
<li>接收到足够多的来自DataNode的数据块报告，并退出安全模式</li>
</ul>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>HDFS中保存数据的节点</p>
<ul>
<li><p>数据被切割为多个数据块，以冗余备份的形式存储在多个
DataNode中，因此不需要再每个节点上安装RAID存储获得硬件上
可靠存储支持。DataNode之间可以拷贝数据副本，从而可以重新
平衡每个节点存储数据量、保证数据可靠性（保证副本数量）</p>
</li>
<li><p>DotaNode定期向NameNode报告其存储的数据块列表，以备使用者
通过直接访问DataNode获得相应数据</p>
</li>
<li><p>所有NameNode和DataNode之间的通讯，包括DataNode的注册、
心跳信息、报告数据块元信息，都是由DataNode发起请求，由
NameNode被动应答和完成管理</p>
</li>
</ul>
<h3 id="HDFS高可用性"><a href="#HDFS高可用性" class="headerlink" title="HDFS高可用性"></a>HDFS高可用性</h3><p>对于大型集群，NN冷启动需要30min甚至更长，因此Hadoop2.x中添加
对高可用性HA（high-availability）的支持</p>
<ul>
<li><p>配置Active-Standby NameNode</p>
<ul>
<li>ANN失效后，SNN就会接管任务并开始服务，没有明显中断</li>
<li>ANN、SNN应该具有相同的硬件配置</li>
</ul>
</li>
<li><p>NN之间需要通过<strong>高可用的共享存储（JounalNode）</strong>实现
Editlog共享</p>
<ul>
<li>JN进程轻量，可以和其他节点部署在同一台机器</li>
<li>JN至少为3个，最好为奇数个，这样JN失效$(n-1)/2$个时
  仍然可以正常工作</li>
<li>SNN接管工作后，将通读共享编辑日志直到末尾，实现与ANN
状态同步</li>
</ul>
</li>
<li><p>DN需要同时向两个NN发送数据块处理报告，因为数据块映射信息
存储在NN内存中</p>
</li>
<li><p>客户端需要使用特定机制处理NN失效问题，且机制对用户透明</p>
</li>
<li><p>如果两个namenode同时失效，同样可以冷启动其他namenode，
此时情况就和<em>no-HA</em>模式冷启动类似</p>
</li>
</ul>
<p>注意：HA模式下，不应该再次配置Secondary NameNode</p>
<blockquote>
<p>   Note that, in an HA cluster, the Standby NameNode also 
    performs checkpoints of the namespace state, and thus it
    is not necessary to run a Secondary NameNode, 
    CheckpointNode, or BackupNode in an HA cluster. In fact,
    to do so would be an error. This also allows one who is
    reconfiguring a non-HA-enabled HDFS cluster to be
    HA-enabled to reuse the hardware which they had
    previously dedicated to the Secondary NameNode.</p>
</blockquote>
<h4 id="Failover-Controller"><a href="#Failover-Controller" class="headerlink" title="Failover Controller"></a>Failover Controller</h4><p>故障转移控制器系统中有一个新实体管理者管理namenode之间切换，</p>
<ul>
<li><p>Failover Controller最初实现基于Zookeeper，可插拔</p>
</li>
<li><p>每个namenode运行着一个Failover Controller，用于监视宿主
namenode是否失效（heart beat机制）， 并在失效时进行故障
切换</p>
<ul>
<li>管理员也可以手动发起故障切换，称为<em>平稳故障转移</em></li>
</ul>
</li>
<li><p>在非平稳故障切换时，无法确切知道失效namenode是否已经停止
运行，如网速慢、网络切割均可能激发故障转移，引入fencing
机制</p>
<ul>
<li>杀死namenode进程</li>
<li>收回对共享存储目录权限</li>
<li>屏蔽相应网络端口</li>
<li>STONITH：shoot the other node in the head，断电</li>
</ul>
</li>
</ul>
<h3 id="联邦HDFS"><a href="#联邦HDFS" class="headerlink" title="联邦HDFS"></a>联邦HDFS</h3><p>NameNode在内存中保存文件系统中每个文件、数据块的引用关系，
所以对于拥有大量文件的超大集群，内存将成为系统扩展的瓶颈，
2.x中引入的联邦HDFS可以添加NameNode实现扩展</p>
<ul>
<li>每个NameNode维护一个namespace volume，包括命名空间的
元数据、命令空间下的文件的所有数据块、数据块池</li>
<li>namespace volume之间相互独立、不通信，其中一个NameNode
失效也不会影响其他NameNode维护的命名空间的可用性</li>
<li>数据块池不再切分，因此集群中的DataNode需要注册到每个
NameNode，并且存储来自多个数据块池的数据块</li>
</ul>
<h2 id="Hadoop文件系统"><a href="#Hadoop文件系统" class="headerlink" title="Hadoop文件系统"></a>Hadoop文件系统</h2><p>Hadoop有一个抽象问的文件系统概念，HDFS只是其中的一个实现，
Java抽象类<code>org.apche.hadoop.fs.FileSystem</code>定义了Hadoop中的
一个文件系统接口，包括以下具体实现</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>文件系统</th>
<th>URI方案</th>
<th>Java实现</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Local</td>
<td><code>file</code></td>
<td><code>fs.LocalFileSystem</code></td>
<td>使用客户端校验和本地磁盘文件系统，没有使用校验和文件系统<code>RawLocalFileSystem</code></td>
</tr>
<tr>
<td>HDFS</td>
<td><code>hdfs</code></td>
<td><code>hdfs.DistributedFileSystem</code></td>
<td>HDFS设计为与MapReduce结合使用实现高性能</td>
</tr>
<tr>
<td>HFTP</td>
<td><code>hftp</code></td>
<td><code>hdfs.HftpFileSystem</code></td>
<td>在HTTP上提供对HDFS只读访问的文件系统，通常与distcp结合使用，以实现在运行不同版本HDFS集群之间复制数据</td>
</tr>
<tr>
<td>HSFTP</td>
<td><code>hsftp</code></td>
<td><code>hdfs.HsftpFileSystem</code></td>
<td>在HTTPS上同以上</td>
</tr>
<tr>
<td>WebHDFS</td>
<td><code>Webhdfs</code></td>
<td><code>hdfs.web.WebHdfsFileSystem</code></td>
<td>基于HTTP，对HDFS提供安全读写访问的文件系统，为了替代HFTP、HFSTP而构建</td>
</tr>
<tr>
<td>HAR</td>
<td><code>har</code></td>
<td><code>fs.HarFileSystem</code></td>
<td>构建于其他文件系统之上，用于文件存档的文件系统，通常用于需要将HDFS中的文件进行存档时，以减少对NN内存的使用</td>
</tr>
<tr>
<td>hfs</td>
<td><code>kfs</code></td>
<td><code>fs.kfs.kosmosFileSystem</code></td>
<td>CloudStore（前身为Kosmos文件系统）类似于HDFS（GFS），<em>C++</em>编写</td>
</tr>
<tr>
<td>FTP</td>
<td><code>ftp</code></td>
<td><code>fs.ftp.FTPFileSystem</code></td>
<td>由FTP服务器支持的文件系统</td>
</tr>
<tr>
<td>S3（原生）</td>
<td><code>S3n</code></td>
<td><code>fs.s3native.NativeS3FileSystem</code></td>
<td>由Amazon S3支持的文件系统</td>
</tr>
<tr>
<td>S3（基于块）</td>
<td><code>S3</code></td>
<td><code>fs.sa.S3FileSystem</code></td>
<td>由Amazon S3支持的文件系统，以块格式存储文件（类似于HDFS），以解决S3Native 5GB文件大小限制</td>
</tr>
<tr>
<td>分布式RAID</td>
<td><code>hdfs</code></td>
<td><code>hdfs.DistributedRaidFileSystem</code></td>
<td>RAID版本的HDFS是为了存档而设计的。针对HDFS中每个文件，创建一个更小的检验文件，并允许数据副本变为2，同时数据丢失概率保持不变。需要在集群中运行一个RaidNode后台进程</td>
</tr>
<tr>
<td>View</td>
<td><code>viewfs</code></td>
<td><code>viewfs.ViewFileSystem</code></td>
<td>针对其他Hadoop文件系统挂载的客户端表，通常用于联邦NN创建挂载点</td>
</tr>
</tbody>
</table>
</div>
<h3 id="文件系统接口"><a href="#文件系统接口" class="headerlink" title="文件系统接口"></a>文件系统接口</h3><p>Hadoop对文件系统提供了许多接口，一般使用<strong>URI方案</strong>选择合适的
文件系统实例进行交互</p>
<h4 id="命令行接口"><a href="#命令行接口" class="headerlink" title="命令行接口"></a>命令行接口</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -copyFromLocal file hdfs://localhost/user/xyy15926/file</span><br><span class="line">	<span class="comment"># 调用Hadoop文件系统的shell命令`fs`</span></span><br><span class="line">	<span class="comment"># `-copyFromLocalFile`则是`fs`子命令</span></span><br><span class="line">	<span class="comment"># 事实上`hfds://localhost`可以省略，使用URI默认设置，即</span></span><br><span class="line">		<span class="comment"># 在`core-site.xml`中的默认设置</span></span><br><span class="line">	<span class="comment"># 类似的默认复制文件路径为HDFS中的`$HOME`</span></span><br><span class="line"></span><br><span class="line">$ hadoop fs -copyToLocal file file</span><br></pre></td></tr></table></figure>
<h4 id="HTTP"><a href="#HTTP" class="headerlink" title="HTTP"></a>HTTP</h4><ul>
<li><p>直接访问：HDFS后台进程直接服务来自于客户端的请求</p>
<ul>
<li>由NN内嵌的web服务器提供目录服务（默认50070端口）</li>
<li>DN的web服务器提供文件数据（默认50075端口）</li>
</ul>
</li>
<li><p>代理访问：依靠独立代理服务器通过HTTP访问HDFS</p>
<ul>
<li>代理服务器可以使用更严格的防火墙策略、贷款限制策略</li>
</ul>
</li>
</ul>
<h4 id="C"><a href="#C" class="headerlink" title="C"></a>C</h4><p>Hadoop提供<code>libhdfs</code>的C语言库，是Java <code>FileSystem</code>接口类的
镜像</p>
<ul>
<li>被写成访问HDFS的C语言库，但其实可以访问全部的Hadoop文件
系统</li>
<li>使用Java原生接口（JNI）调用Java文件系统客户端</li>
</ul>
<h4 id="FUSE"><a href="#FUSE" class="headerlink" title="FUSE"></a>FUSE</h4><p>Filesystem in Userspace允许把按照用户空间实现的文件系统整合
成一个Unix文件系统</p>
<ul>
<li>使用Hadoop Fuse-DFS功能模块，任何一个Hadoop文件系统可以
作为一个标准文件系统进行挂载<ul>
<li>Fuse_DFS使用C语言实现，调用libhdfs作为访问HDFS的接口</li>
</ul>
</li>
<li>然后可以使用Unix工具（<code>ls</code>、<code>cat</code>等）与文件系统交互</li>
<li>还可以使用任何编程语言调用POSIX库访问文件系统</li>
</ul>
<h3 id="读文件"><a href="#读文件" class="headerlink" title="读文件"></a>读文件</h3><ol>
<li><p>客户端程序使用要读取的文件名、Read Range的开始偏移量、
读取范围的程度等信息，询问NameNode</p>
</li>
<li><p>NameNode返回落在读取范围内的数据块的Location信息，根据
与客户端的临近性（Proximity）进行排序，客户端一般选择
最临近的DataNode发送读取请求</p>
</li>
</ol>
<p>具体实现如下</p>
<ol>
<li><p>客户端调用<code>FileSystem</code>对象<code>open</code>方法，打开文件，获得
<code>DistributedFileSystem</code>类的一个实例</p>
</li>
<li><p><code>DistributedFileSystem</code>返回<code>FSDataInputStream</code>类的实例，
支持文件的定位、数据读取</p>
<ul>
<li><code>DistributedFileSystem</code>通过<strong>RPC</strong>调用NameNode，获得
文件首批若干数据块的位置信息（Locations of Blocks）</li>
<li>对每个数据块，NameNode会返回拥有其副本的所有DataNode
地址</li>
<li>其包含一个<code>DFSInputStream</code>对象，负责管理客户端对HDFS
中DataNode、NameNode存取</li>
</ul>
</li>
<li><p>客户端从输入流调用函数<code>read</code>，读取文件第一个数据块，不断
调用<code>read</code>方法从DataNode获取数据</p>
<ul>
<li><code>DFSInputStream</code>保存了文件首批若干数据块所在的
DataNode地址，连接到closest DataNode</li>
<li>当达到数据块末尾时，<code>DFSInputStream</code>关闭到DataNode
的连接，创建到保存其他数据块DataNode的连接</li>
<li>首批数据块读取完毕之后，<code>DFSInputStream</code>向NameNode
询问、提取下一批数据块的DataNode的位置信息</li>
</ul>
</li>
<li><p>客户端完成文件的读取，调用<code>FSDataInputStream</code>实例<code>close</code>
方法关闭文件</p>
</li>
</ol>
<h3 id="写文件"><a href="#写文件" class="headerlink" title="写文件"></a>写文件</h3><ul>
<li><p>客户端询问NameNode，了解应该存取哪些DataNode，然后客户端
直接和DataNode进行通讯，使用Data Transfer协议传输数据，
这个流式数据传输协议可以提高数据传输效率</p>
</li>
<li><p>创建文件时，客户端把文件数据缓存在一个临时的本地文件上，
当本地文件累计超过一个数据块大小时，客户端程序联系
NameNode，NameNode更新文件系统的NameSpace，返回Newly
Allocated数据块的位置信息，客户端根据此信息本文件数据块
从临时文件Flush到DataNode进行保存</p>
</li>
</ul>
<p>具体实现如下：</p>
<ol>
<li><p>客户端调用<code>DistributedFileSystem</code>的<code>create</code>方法</p>
<ul>
<li><code>DistributedFileSystem</code>通过发起RPC告诉NameNode在
其NameSpace创建一个新文件，此时新文件没有任何数据块</li>
<li>NameNode检查：文件是否存在、客户端权限等，检查通过
NameNode为新文件创建新记录、保存其信息，否则文件创建
失败</li>
</ul>
</li>
<li><p><code>DistributedFileSystem</code>返回<code>FSDataOutputStream</code>给客户端</p>
<ul>
<li>其包括一个<code>DFSOutputStream</code>对象，负责和NameNode、
DataNode的通讯</li>
</ul>
</li>
<li><p>客户端调用<code>FSDataOutputStream</code>对象<code>write</code>方法写入数据</p>
<ul>
<li><code>DFSOutputStream</code>把数据分解为数据包Packet，写入内部
Data Queue</li>
<li><code>DataSteamer</code>消费这个队列，写入本地临时文件中</li>
<li>当写入数据超过一个数据块大小时，<code>DataStreamer</code>请求
NameNode为新的数据块分配空间，即选择一系列合适的
DataNode存放各个数据块各个副本</li>
<li>存放各个副本的DataNode形成一个Pipeline，流水线上的
Replica Factor数量的DataNode接收到数据包之后转发给
下个DataNode</li>
<li><code>DFSOutputStream</code>同时维护数据包内部Ack Queue，用于
等待接受DataNode应答信息，只有某个数据包已经被流水线
上所有DataNode应答后，才会从Ack Queue上删除</li>
</ul>
</li>
<li><p>客户端完成数据写入，调用<code>FSDataOutputStream</code>的<code>close</code>
方法</p>
<ul>
<li><code>DFSOutputStream</code>把所有的剩余的数据包发送到DataNode
流水线上，等待应答信息</li>
<li>最后通知NameNode文件结束</li>
<li>NameNode自身知道文件由哪些数据块构成，其等待数据块
复制完成，然后返回文件创建成功</li>
</ul>
</li>
</ol>
<h2 id="Hadoop平台上的列存储"><a href="#Hadoop平台上的列存储" class="headerlink" title="Hadoop平台上的列存储"></a>Hadoop平台上的列存储</h2><p>列存储的优势</p>
<ul>
<li><p>更少的IO操作：读取数据的时候，支持Prject Pushdown，甚至
是Predicate Pushdown，可大大减少IO操作</p>
</li>
<li><p>更大的压缩比：每列中数据类型相同，可以针对性的编码、压缩</p>
</li>
<li><p>更少缓存失效：每列数据相同，可以使用更适合的Cpu Pipline
编码方式，减少CPU cache miss</p>
</li>
</ul>
<h3 id="RCFile"><a href="#RCFile" class="headerlink" title="RCFile"></a>RCFile</h3><p>Record Columnar File Format：FB、Ohio州立、中科院计算所合作
研发的列存储文件格式，首次在Hadoop中引入列存储格式</p>
<ul>
<li><p><em>允许按行查询，同时提供列存储的压缩效率</em>的列存储格式</p>
<ul>
<li>具备相当于行存储的数据加载速度、负载适应能力</li>
<li>读优化可以在扫描表格时，避免不必要的数据列读取</li>
<li>使用列维度压缩，有效提升存储空间利用率</li>
</ul>
</li>
<li><p>具体存储格式</p>
<ul>
<li>首先横向分割表格，生成多个Row Group，大小可以由用户
指定</li>
<li>在RowGroup内部，按照列存储一般做法，按列把数据分开，
分别连续保存<ul>
<li>写盘时，RCFile针对每列数据，使用Zlib/LZO算法进行
压缩，减少空间占用</li>
<li>读盘时，RCFile采用Lazy Decompression策略，即用户
查询只涉及表中部分列时，会跳过不需要列的解压缩、
反序列化的过程</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="ORC存储格式"><a href="#ORC存储格式" class="headerlink" title="ORC存储格式"></a>ORC存储格式</h3><p>Optimized Row Columnar File：对RCFile优化的存储格式</p>
<ul>
<li><p>支持更加丰富的数据类型</p>
<ul>
<li>包括Date Time、Decimal</li>
<li>Hive的各种Complex Type，包括：Struct、List、Map、
Union</li>
</ul>
</li>
<li><p>Self Describing的列存储文件格式</p>
<ul>
<li>为Streaming Read操作进行了优化</li>
<li>支持快速查找少数数据行</li>
</ul>
</li>
<li><p>Type Aware的列存储文件格式</p>
<ul>
<li>文件写入时，针对不同的列的数据类型，使用不同的编码器
进行编码，提高压缩比<ul>
<li>整数类型：Variable Length Compression</li>
<li>字符串类型：Dictionary Encoding</li>
</ul>
</li>
</ul>
</li>
<li><p>引入轻量级索引、基本统计信息</p>
<ul>
<li>包括各数据列的最大/小值、总和、记录数等信息</li>
<li>在查询过程中，通过谓词下推，可以忽略大量不符合查询
条件的记录</li>
</ul>
</li>
</ul>
<h4 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h4><p>一个ORC文件由多个Stripe、一个包含辅助信息的FileFooter、以及
Postscript构成</p>
<h5 id="Stripe"><a href="#Stripe" class="headerlink" title="Stripe"></a>Stripe</h5><p>每个stripe包含index data、row data、stripe footer</p>
<ul>
<li><p>stripe就是ORC File中划分的row group</p>
<ul>
<li>默认大小为256MB，可扩展的长度只受HDFS约束</li>
<li>大尺寸的strip、对串行IO的优化，能提高数据吞吐量、
读取更少的文件，同时能把减轻NN负担</li>
</ul>
</li>
<li><p>Index Data部分</p>
<ul>
<li>包含每个列的极值</li>
<li>一系列Row Index Entry记录压缩模块的偏移量，用于跳转
到正确的压缩块的位置，实现数据的快速读取，缺省可以
跳过10000行</li>
</ul>
</li>
</ul>
<ul>
<li><p>Row Data部分；包含每个列的数据，每列由若干Data Stream
构成</p>
</li>
<li><p>Stripe Footer部分</p>
<ul>
<li>Data Stream位置信息</li>
<li>每列数据的编码方式</li>
</ul>
</li>
</ul>
<h5 id="File-Footer"><a href="#File-Footer" class="headerlink" title="File Footer"></a>File Footer</h5><p>包含该ORCFile文件中所有stripe的元信息</p>
<ul>
<li>每个Stripe的位置</li>
<li>每个Stripe的行数</li>
<li>每列的数据类型</li>
<li>还有一些列级别的聚集结果，如：记录数、极值、总和</li>
</ul>
<h5 id="Postscript"><a href="#Postscript" class="headerlink" title="Postscript"></a>Postscript</h5><ul>
<li>用来存储压缩参数</li>
<li>压缩过后的Footer的长度</li>
</ul>
<h3 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h3><p>灵感来自于Google关于Drenel系统的论文，其介绍了一种支持嵌套
结构的列存储格式，以提升查询性能</p>
<h4 id="支持"><a href="#支持" class="headerlink" title="支持"></a>支持</h4><p>Parquet为hadoop生态系统中的所有项目，提供支持高压缩率的
列存储格式</p>
<ul>
<li><p>兼容各种数据处理框架</p>
<ul>
<li>MapReduce</li>
<li>Spark</li>
<li>Cascading</li>
<li>Crunch</li>
<li>Scalding</li>
<li>Kite</li>
</ul>
</li>
<li><p>支持多种对象模型</p>
<ul>
<li>Avro</li>
<li>Thrift</li>
<li>Protocol Buffers</li>
</ul>
</li>
<li><p>支持各种查询引擎</p>
<ul>
<li>Hive</li>
<li>Impala</li>
<li>Presto</li>
<li>Drill</li>
<li>Tajo</li>
<li>HAWQ</li>
<li>IBM Big SQL</li>
</ul>
</li>
</ul>
<h4 id="Parquet组件"><a href="#Parquet组件" class="headerlink" title="Parquet组件"></a>Parquet组件</h4><ul>
<li><p>Storage Format：存储格式，定义了Parquet内部的数据类型、
存储格式</p>
</li>
<li><p>Object Model Converter：对象转换器，由Parquet-mr实现，
完成对象模型与Parquet数据类型的映射</p>
<ul>
<li>如Parquet-pig子项目，负责把内存中的Pig Tuple序列化
并按存储格式保存为Parquet格式的文件，已经反过来，
把Parquet格式文件的数据反序列化为Pig Tuple</li>
</ul>
</li>
<li><p>Object Model：对象模型，可以理解为内存中的数据表示，包括
Avro、Thrift、Protocal Buffer、Hive Serde、Pig Tuple、
SparkSQL Internal Row等对象模型</p>
</li>
</ul>
<h4 id="Parquet数据schema"><a href="#Parquet数据schema" class="headerlink" title="Parquet数据schema"></a>Parquet数据schema</h4><p>数据schema（结构）可以用一个棵树表达</p>
<ul>
<li><p>有一个根节点，根节点包含多个Feild（子节点），子节点可以
包含子节点</p>
</li>
<li><p>每个field包含三个属性</p>
<ul>
<li><p>repetition：field出现的次数</p>
<ul>
<li><code>required</code>：必须出现1次</li>
<li><code>optional</code>：出现0次或1次</li>
<li><code>repeated</code>：出现0次或多次</li>
</ul>
</li>
<li><p>type：数据类型</p>
<ul>
<li>primitive：原生类惬</li>
<li>group：衍生类型</li>
</ul>
</li>
<li><p>name：field名称</p>
</li>
</ul>
</li>
<li><p>Parquet通过把多个schema结构按树结构组合，提供对复杂类型
支持</p>
<ul>
<li>List、Set：repeated field</li>
<li>Map：包含键值对的Repeated Group（key为Required）</li>
</ul>
</li>
<li><p>schema中有多少<strong>叶子节点</strong>，Parquet格式实际存储多少列，
父节点则是在表头组合成schema的结构</p>
</li>
</ul>
<h4 id="Parquet文件结构"><a href="#Parquet文件结构" class="headerlink" title="Parquet文件结构"></a>Parquet文件结构</h4><ul>
<li>HDFS文件：包含数据和元数据，数据存储在多个block中</li>
<li>HDFS Block：HDFS上最小的存储单位</li>
<li>Row Group：按照行将数据表格划分多个单元，每个行组包含
一定行数，行组包含该行数据各列对应的列块<ul>
<li>一般建议采用更大的行组（512M-1G），此意味着更大的
列块，有毅力在磁盘上串行IO</li>
<li>由于可能依次需要读取整个行组，所以一般让一个行组刚好
在一个HDFS数据块中，HDFS Block需要设置得大于等于行组
大小</li>
</ul>
</li>
<li>Column Chunk：每个行组中每列保存在一个列块中<ul>
<li>行组中所有列连续的存储在行组中</li>
<li>不同列块使用不同压缩算法压缩</li>
<li>列块存储时保存相应统计信息，极值、空值数量，用于加快
查询处理</li>
<li>列块由多个页组成</li>
</ul>
</li>
<li>Page：每个列块划分为多个Page<ul>
<li>Page是压缩、编码的单元</li>
<li>列块的不同页可以使用不同的编码方式</li>
</ul>
</li>
</ul>
<h2 id="HDFS命令"><a href="#HDFS命令" class="headerlink" title="HDFS命令"></a>HDFS命令</h2><h3 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h3><ul>
<li>HDFS的用户就是当前linux登陆的用户</li>
</ul>
<h2 id="Hadoop组件"><a href="#Hadoop组件" class="headerlink" title="Hadoop组件"></a>Hadoop组件</h2><h3 id="Hadoop-Streaming"><a href="#Hadoop-Streaming" class="headerlink" title="Hadoop Streaming"></a>Hadoop Streaming</h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:08.000Z" title="2/17/2019, 11:57:08 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">an hour read (About 7085 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/hadoop_inset.html">Hadoop安装配置</a></h1><div class="content"><h2 id="Hadoop安装"><a href="#Hadoop安装" class="headerlink" title="Hadoop安装"></a>Hadoop安装</h2><h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li><p>Java</p>
<ul>
<li>具体版本<a target="_blank" rel="noopener" href="http://wiki.apache.org/hadoop/HadoopJavaVersions">http://wiki.apache.org/hadoop/HadoopJavaVersions</a></li>
<li>需要配置好java环境（<code>~/.bashrc</code>）</li>
</ul>
</li>
<li><p>ssh：必须安装且保证sshd一直运行，以便使用hadoop脚本管理
远端hadoop守护进程</p>
<ul>
<li>pdsh：建议安装获得更好的ssh资源管理</li>
<li>要设置免密登陆</li>
</ul>
</li>
</ul>
<h3 id="机器环境配置"><a href="#机器环境配置" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc"><a href="#bashrc" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><p>这里所有的设置都只是设置环境变量</p>
<ul>
<li><p>所以这里所有环境变量都可以放在<code>hadoop-env.sh</code>中</p>
</li>
<li><p>放在<code>.bashrc</code>中不是基于用户隔离的考虑</p>
<ul>
<li>因为hadoop中配置信息大部分放在<code>.xml</code>，放在这里无法
实现用户隔离</li>
<li>更多的考虑是给hive等依赖hadoop的应用提供hadoop配置</li>
</ul>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_PREFIX=/opt/hadoop</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 自定义部分</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 此处是直接解压放在`/opt`目录下</span></span><br><span class="line">export HADOOP_HOME=$HADOOP_PREFIX</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_PREFIX</span><br><span class="line"><span class="meta">	#</span><span class="bash"> hadoop common</span></span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_PREFIX</span><br><span class="line"><span class="meta">	#</span><span class="bash"> hdfs</span></span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_PREFIX</span><br><span class="line"><span class="meta">	#</span><span class="bash"> mapreduce</span></span><br><span class="line">export HADOOP_YARN_HOME=$HADOOP_PREFIX</span><br><span class="line"><span class="meta">	#</span><span class="bash"> YARN</span></span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop</span><br><span class="line"></span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export HADOOP_OPTS=&quot;$HADOOP_OPTS -Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR&quot;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里`-Djava`间不能有空格</span></span><br><span class="line"></span><br><span class="line">export CLASSPATH=$CLASS_PATH:$HADOOP_PREFIX/lib/*</span><br><span class="line">export PATH=$PATH:$HADOOP_PREFIX/sbin:$HADOOP_PREFIX/bin</span><br></pre></td></tr></table></figure>
<h4 id="etc-hosts"><a href="#etc-hosts" class="headerlink" title="/etc/hosts"></a><code>/etc/hosts</code></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.31.129 hd-master</span><br><span class="line">192.168.31.130 hd-slave1</span><br><span class="line">192.168.31.131 hd-slave2</span><br><span class="line">127.0.0.1 localhost</span><br></pre></td></tr></table></figure>
<ul>
<li>这里配置的ip地址是各个主机的ip，需要自行配置</li>
<li><code>hd-master</code>、<code>hd-slave1</code>等就是主机ip-主机名映射</li>
<li><h1 id="todo-一定需要在-etc-hostname中设置各个主机名称"><a href="#todo-一定需要在-etc-hostname中设置各个主机名称" class="headerlink" title="todo?一定需要在/etc/hostname中设置各个主机名称"></a>todo?一定需要在<code>/etc/hostname</code>中设置各个主机名称</h1></li>
</ul>
<h4 id="firewalld"><a href="#firewalld" class="headerlink" title="firewalld"></a><code>firewalld</code></h4><p>必须关闭所有节点的防火墙</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo systemctl stop firewalld.service</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo systemctl <span class="built_in">disable</span> firewalld.service</span></span><br></pre></td></tr></table></figure>
<h4 id="文件夹建立"><a href="#文件夹建立" class="headerlink" title="文件夹建立"></a>文件夹建立</h4><ul>
<li>所有节点都需要建立</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir tmp</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p hdfs/data hdfs/name</span></span><br></pre></td></tr></table></figure>
<h3 id="Hadoop配置"><a href="#Hadoop配置" class="headerlink" title="Hadoop配置"></a>Hadoop配置</h3><p>Hadoop<strong>全系列</strong>（包括hive、tez等）配置取决于以下两类配置文件</p>
<ul>
<li><p>只读默认配置文件</p>
<ul>
<li><code>core-defualt.xml</code></li>
<li><code>hdfs-default.xml</code></li>
<li><code>mapred-default.xml</code></li>
</ul>
</li>
<li><p>随站点变化的配置文件</p>
<ul>
<li><code>etc/hadoop/core-site.xml</code></li>
<li><code>etc/hadoop/hdfs-site.xml</code></li>
<li><code>etc/hadoop/mapred-site.xml</code></li>
<li><code>etc/hadoop/yarn-env.xml</code></li>
</ul>
</li>
<li><p>环境设置文件：设置随站点变化的值，从而控制<code>bin/</code>中的
hadoop脚本行为</p>
<ul>
<li><code>etc/hadoop/hadoop-env.sh</code>、</li>
<li><code>etc/hadoop/yarn-env.sh</code></li>
<li><code>etc/hadoop/mapred-env.sh</code></li>
</ul>
<p>中一般是环境变量配置，<strong>补充</strong>在shell中未设置的环境变量</p>
</li>
<li><p>注意</p>
<ul>
<li><p><code>.xml</code>配置信息可在不同应用的配置文件中<strong>继承</strong>使用，
如在tez的配置中可以使用<code>core-site.xml</code>中
<code>$&#123;fs.defaultFS&#125;</code>变量</p>
</li>
<li><p>应用会读取/执行相应的<code>*_CONF_DIR</code>目录下所有
<code>.xml</code>/<code>.sh</code>文件，所以理论上可以在<code>etc/hadoop</code>中存放
所以配置文件，因为hadoop是最底层应用，在其他所有应用
启动前把环境均已设置完毕？？？</p>
</li>
</ul>
</li>
</ul>
<p>Hadoop集群有三种运行模式</p>
<ul>
<li>Standalone Operation</li>
<li>Pseudo-Distributed Operation</li>
<li>Fully-Distributed Operation</li>
</ul>
<p>针对不同的运行模式有，hadoop有三种不同的配置方式</p>
<h4 id="Standalone-Operation"><a href="#Standalone-Operation" class="headerlink" title="Standalone Operation"></a>Standalone Operation</h4><p>hadoop被配置为以非分布模式运行的一个独立Java进程，对调试有
帮助</p>
<ul>
<li>默认为单机模式，无需配置</li>
</ul>
<h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /path/to/hadoop</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cp etc/hadoop/*.xml input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar grep input output <span class="string">&#x27;dfs[a-z.]+&#x27;</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat output/*</span></span><br></pre></td></tr></table></figure>
<h4 id="Pseudo-Distributed-Operation"><a href="#Pseudo-Distributed-Operation" class="headerlink" title="Pseudo-Distributed Operation"></a>Pseudo-Distributed Operation</h4><p>在单节点（服务器）上以所谓的伪分布式模式运行，此时每个Hadoop
守护进程作为独立的Java进程运行</p>
<h5 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a><code>core-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a><code>hdfs-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a><code>mapred-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configruration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">preperty</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configruation</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a><code>yarn-site.xml</code></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="Fully-Distributed-Operation"><a href="#Fully-Distributed-Operation" class="headerlink" title="Fully-Distributed Operation"></a>Fully-Distributed Operation</h4><ul>
<li><strong>单节点配置完hadoop之后，需要将其同步到其余节点</strong></li>
</ul>
<h5 id="core-site-xml-1"><a href="#core-site-xml-1" class="headerlink" title="core-site.xml"></a><code>core-site.xml</code></h5><p>模板：<code>core-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hd-master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode address<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///opt/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>131702<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 为将用户`root`设置为超级代理，代理所有用户，如果是其他用户需要相应的将root修改为其用户名 --&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 是为hive的JDBCServer远程访问而设置，应该有其他情况也需要 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="hdfs-site-xml-1"><a href="#hdfs-site-xml-1" class="headerlink" title="hdfs-site.xml"></a><code>hdfs-site.xml</code></h5><p>模板：<code>hdfs-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///opt/hadoop/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode data directory<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///opt/hadoop/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>datanode data directory<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>replication number<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.directoryscan.throttle.limit.ms.per.sec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!--bug--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="yarn-site-xml-1"><a href="#yarn-site-xml-1" class="headerlink" title="yarn-site.xml"></a><code>yarn-site.xml</code></h5><ul>
<li>模板：<code>yarn-site.xml</code></li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9099<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- container --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>maximum memory allocation per container<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>256<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>minimum memory allocation per container<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- container --&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- node --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>maximium memory allocation per node<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>virtual memmory ratio<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- node --&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>384<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.command-opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xms128m -Xmx256m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="mapred-site-xml-1"><a href="#mapred-site-xml-1" class="headerlink" title="mapred-site.xml"></a><code>mapred-site.xml</code></h5><ul>
<li>模板：<code>mapred-site.xml.template</code></li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">		&lt;value&gt;yarn-tez&lt;/value&gt;</span></span><br><span class="line"><span class="comment">		设置整个hadoop运行在Tez上，需要配置好Tez</span></span><br><span class="line"><span class="comment">		--&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- mapreduce --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>256<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>memory allocation for map task, which should between minimum container and maximum<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>256<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>memory allocation for reduce task, which should between minimum container and maximum<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- mapreduce --&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- java heap size options --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xms128m -Xmx256m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xms128m -Xmx256m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- java heap size options --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h5><ul>
<li><p><code>yarn.scheduler.minimum-allocation-mb</code>：container内存
单位，也是container分配的内存最小值</p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-mb</code>：container内存
最大值，应该为最小值整数倍</p>
</li>
<li><p><code>mapreduce.map.memeory.mb</code>：map task的内存分配</p>
<ul>
<li>hadoop2x中mapreduce构建于YARN之上，资源由YARN统一管理</li>
<li>所以maptask任务的内存应设置container最小值、最大值间</li>
<li>否则分配一个单位，即最小值container</li>
</ul>
</li>
<li><p><code>mapreduce.reduce.memeory.mb</code>：reduce task的内存分配</p>
<ul>
<li>设置一般为map task的两倍</li>
</ul>
</li>
<li><p><code>*.java.opts</code>：JVM进程参数设置</p>
<ul>
<li>每个container（其中执行task）中都会运行JVM进程</li>
<li><code>-Xmx...m</code>：heap size最大值设置，所以此参数应该小于
task（map、reduce）对应的container分配内存的最大值，
如果超出会出现physical memory溢出</li>
<li><code>-Xms...m</code>：heap size最小值？#todo</li>
</ul>
</li>
<li><p><code>yarn.nodemanager.vmem-pmem-ratio</code>：虚拟内存比例</p>
<ul>
<li>以上所有配置都按照此参数放缩</li>
<li>所以在信息中会有physical memory、virtual memory区分</li>
</ul>
</li>
<li><p><code>yarn.nodemanager.resource.memory-mb</code>：节点内存设置</p>
<ul>
<li>整个节点被设置的最大内存，剩余内存共操作系统使用</li>
</ul>
</li>
<li><p><code>yarn.app.mapreduce.am.resource.mb</code>：每个Application
Manager分配的内存大小</p>
</li>
</ul>
<h4 id="主从文件"><a href="#主从文件" class="headerlink" title="主从文件"></a>主从文件</h4><h5 id="masters"><a href="#masters" class="headerlink" title="masters"></a><code>masters</code></h5><ul>
<li>设置主节点地址，根据需要设置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hd-master</span><br></pre></td></tr></table></figure>
<h5 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a><code>slaves</code></h5><ul>
<li>设置从节点地址，根据需要设置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hd-slave1</span><br><span class="line">hd-slave2</span><br></pre></td></tr></table></figure>
<h4 id="环境设置文件"><a href="#环境设置文件" class="headerlink" title="环境设置文件"></a>环境设置文件</h4><ul>
<li>这里环境设置只是起补充作用，在<code>~/.bashrc</code>已经设置的
环境变量可以不设置</li>
<li>但是在这里设置环境变量，然后把整个目录同步到其他节点，
可以保证在其余节点也能同样的设置环境变量</li>
</ul>
<h5 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a><code>hadoop-env.sh</code></h5><p>设置<code>JAVA_HOME</code>为Java安装根路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/opt/java/jdk</span><br></pre></td></tr></table></figure>
<h5 id="hdfs-env-sh"><a href="#hdfs-env-sh" class="headerlink" title="hdfs-env.sh"></a><code>hdfs-env.sh</code></h5><p>设置<code>JAVA_HOME</code>为Java安装根路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/opt/java/jdk</span><br></pre></td></tr></table></figure>
<h5 id="yarn-env-sh"><a href="#yarn-env-sh" class="headerlink" title="yarn-env.sh"></a><code>yarn-env.sh</code></h5><p>设置<code>JAVA_HOME</code>为Java安装根路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/opt/java/jdk</span><br><span class="line">JAVA_HEAP_MAX=Xmx3072m</span><br></pre></td></tr></table></figure>
<h4 id="初始化、启动、测试"><a href="#初始化、启动、测试" class="headerlink" title="初始化、启动、测试"></a>初始化、启动、测试</h4><h5 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h5><ul>
<li><p>格式化、启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs namenode -format</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 格式化文件系统</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-dfs.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 启动NameNode和DataNode</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 此时已可访问NameNode，默认http://localhost:9870/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> stop-dfs.sh</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfsadmin -report</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 应该输出3个节点的情况</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -mkdir /user</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -mkdir /user/&lt;username&gt;</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 创建执行MapReduce任务所需的HDFS文件夹</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -mkdir input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -put etc/hadoop/*.xml input</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 复制文件至分布式文件系统</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar grep input output <span class="string">&#x27;dfs[a-z]+&#x27;</span></span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 执行自带样例</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 样例名称取决于版本</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -get output outut</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat output/*</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 检查输出文件：将所有的输出文件从分布式文件系统复制</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 至本地文件系统，并检查</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -cat output/*</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 或者之间查看分布式文件系统上的输出文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.7.jar \</span></span><br><span class="line"><span class="bash">	-input /path/to/hdfs_file \</span></span><br><span class="line"><span class="bash">	-output /path/to/hdfs_dir \</span></span><br><span class="line"><span class="bash">	-mapper <span class="string">&quot;/bin/cat&quot;</span> \</span></span><br><span class="line"><span class="bash">	-reducer <span class="string">&quot;/user/bin/wc&quot;</span> \</span></span><br><span class="line"><span class="bash">	-file /path/to/local_file \</span></span><br><span class="line"><span class="bash">	-numReduceTasks 1</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sbin/start-yarn.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 启动ResourceManger守护进程、NodeManager守护进程</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 即可访问ResourceManager的web接口，默认：http://localhost:8088/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sbin/stop-yarn.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 关闭守护进程</span></span><br></pre></td></tr></table></figure>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ul>
<li><p><code>hdfs namenode -format</code>甚至可以在datanode节点没有java时
成功格式化</p>
</li>
<li><p>没有关闭防火墙时，整个集群可以正常启动，甚至可以在hdfs里
正常建立文件夹，但是<strong>无法写入文件</strong>，尝试写入文件时报错</p>
</li>
</ul>
<h4 id="可能错误"><a href="#可能错误" class="headerlink" title="可能错误"></a>可能错误</h4><h5 id="节点启动不全"><a href="#节点启动不全" class="headerlink" title="节点启动不全"></a>节点启动不全</h5><ul>
<li><p>原因</p>
<ul>
<li>服务未正常关闭，节点状态不一致</li>
</ul>
</li>
<li><p>关闭服务、删除存储数据的文件夹<code>dfs/data</code>、格式化namenode</p>
</li>
</ul>
<h5 id="文件无法写入"><a href="#文件无法写入" class="headerlink" title="文件无法写入"></a>文件无法写入</h5><blockquote>
<p>   could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and 2 node(s) are excluded in this operation.</p>
</blockquote>
<ul>
<li><p>原因</p>
<ul>
<li>未关闭防火墙</li>
<li>存储空间不够</li>
<li>节点状态不一致、启动不全</li>
<li>在log里面甚至可能会出现一个连接超时1000ms的ERROR</li>
</ul>
</li>
<li><p>处理</p>
<ul>
<li>关闭服务、删除存储数据的文件夹<code>dfs/data</code>、格式化
namenode<ul>
<li>这样处理会丢失数据，不能用于生产环境</li>
</ul>
</li>
<li>尝试修改节点状态信息文件<code>VERSION</code>一致<ul>
<li><code>$&#123;hadoop.tmp.dir&#125;</code></li>
<li><code>$&#123;dfs.namenode.name.dir&#125;</code></li>
<li><code>$&#123;dfs.datanode.data.dir&#125;</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Unhealthy-Node"><a href="#Unhealthy-Node" class="headerlink" title="Unhealthy Node"></a>Unhealthy Node</h5><blockquote>
<p>   1/1 local-dirs are bad: /opt/hadoop/tmp/nm-local-dir; 1/1 log-dirs are bad: /opt/hadoop/logs/userlogs</p>
</blockquote>
<ul>
<li>原因：磁盘占用超过90%</li>
</ul>
<h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/hadoop/etc/hadoop centos2:/opt/hadoop/etc</span><br><span class="line">scp -r /opt/hadoop/etc/hadoop centos3:/opt/hadoop/etc</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 同步配置</span></span><br><span class="line"></span><br><span class="line">scp /root/.bashrc centos2:/root</span><br><span class="line">scp /root/.bashrc centos3:/root</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 同步环境</span></span><br><span class="line"></span><br><span class="line">rm -r /opt/hadoop/tmp /opt/hadoop/hdfs</span><br><span class="line">mkdir -p /opt/hadoop/tmp /opt/hadoop/hdfs</span><br><span class="line">ssh centos2 rm -r /opt/hadoop/tmp /opt/hadoop/hdfs</span><br><span class="line">ssh centos2 mkdir -p /opt/hadoop/tmp /opt/hadoop/hdfs/name /opt/hadoop/hdfs/data</span><br><span class="line">ssh centos3 rm -r /opt/hadoop/tmp /opt/hadoop/hdfs/name /opt/hadoop/data</span><br><span class="line">ssh centos3 mkdir -p /opt/hadoop/tmp /opt/hadoop/hdfs/name /opt/hadoop/hdfs/data</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 同步清除数据</span></span><br><span class="line"></span><br><span class="line">rm -r /opt/hadoop/logs/*</span><br><span class="line">ssh centos2 rm -r /opt/hadoop/logs/*</span><br><span class="line">ssh centos3 rm -r /opt/hadoop/logs/*</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 同步清除<span class="built_in">log</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="依赖-1"><a href="#依赖-1" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>hadoop：配置完成hadoop，则相应java等也配置完成</li>
<li>关系型数据库：mysql、derby等</li>
</ul>
<h3 id="机器环境配置-1"><a href="#机器环境配置-1" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-1"><a href="#bashrc-1" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/opt/hive</span><br><span class="line"><span class="meta">	#</span><span class="bash"> self designed</span></span><br><span class="line">export HIVE_CONF_DIR=$HIVE_HOME/conf</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">export CLASSPATH=$CLASS_PATH:$HIVE_HOME/lib/*</span><br></pre></td></tr></table></figure>
<h4 id="文件夹建立-1"><a href="#文件夹建立-1" class="headerlink" title="文件夹建立"></a>文件夹建立</h4><h5 id="HDFS-1"><a href="#HDFS-1" class="headerlink" title="HDFS"></a>HDFS</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -rm -r /user/hive</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -mkdir -p /user/hive/warehouse /user/hive/tmp /user/hive/logs</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这三个目录与配置文件中对应</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -chmod 777 /user/hive/warehouse /user/hive/tmp /user/hive/logs</span></span><br></pre></td></tr></table></figure>
<h5 id="FS"><a href="#FS" class="headerlink" title="FS"></a>FS</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir data</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> chmod 777 data</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> hive数据存储文件夹</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir logs</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> chmod 777 logs</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">log</span>目录</span></span><br></pre></td></tr></table></figure>
<h3 id="Hive配置"><a href="#Hive配置" class="headerlink" title="Hive配置"></a>Hive配置</h3><h4 id="XML参数"><a href="#XML参数" class="headerlink" title="XML参数"></a>XML参数</h4><h5 id="conf-hive-site-xml"><a href="#conf-hive-site-xml" class="headerlink" title="conf/hive-site.xml"></a><code>conf/hive-site.xml</code></h5><ul>
<li>模板：<code>conf/hive-default.xml.template</code></li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hd-master:3306/metastore_db?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.mariadb.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1234<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">&lt;property&gt;</span></span><br><span class="line"><span class="comment">	&lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span></span><br><span class="line"><span class="comment">	&lt;value&gt;$&#123;system:java.io.tmpdir&#125;/$&#123;system:user.name&#125;&lt;/value&gt;</span></span><br><span class="line"><span class="comment">&lt;/property&gt;</span></span><br><span class="line"><span class="comment">&lt;property&gt;</span></span><br><span class="line"><span class="comment">	&lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</span></span><br><span class="line"><span class="comment">	&lt;valeu&gt;$&#123;system:java.io.tmpdir&#125;/$&#123;hive.session.id&#125;_resources&lt;/value&gt;</span></span><br><span class="line"><span class="comment">&lt;/property&gt;</span></span><br><span class="line"><span class="comment">&lt;property&gt;«</span></span><br><span class="line"><span class="comment">	&lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;«</span></span><br><span class="line"><span class="comment">	&lt;value&gt;$&#123;system:java.io.tmpdir&#125;/$&#123;system:user.name&#125;/operation_logs&lt;/value&gt;«</span></span><br><span class="line"><span class="comment">	&lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt;«</span></span><br><span class="line"><span class="comment">&lt;/property&gt;«</span></span><br><span class="line"><span class="comment">所有`$&#123;system.java.io.tmpdir&#125;`都要被替换为相应的`/opt/hive/tmp`，</span></span><br><span class="line"><span class="comment">可以通过设置这两个变量即可，基本是用于设置路径</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>system:java.io.tmpdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hive/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>system:user.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">&lt;property&gt;</span></span><br><span class="line"><span class="comment">	&lt;name&gt;hive.querylog.location&lt;/name&gt;</span></span><br><span class="line"><span class="comment">	&lt;value&gt;/user/hive/logs&lt;/value&gt;</span></span><br><span class="line"><span class="comment">	&lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;</span></span><br><span class="line"><span class="comment">&lt;/property&gt;</span></span><br><span class="line"><span class="comment">这里应该不用设置，log放在本地文件系统更合适吧</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.31.129:19083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--这个是配置metastore，如果配置此选项，每次启动hive必须先启动metastore，否则hive实可直接启动--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用JDBCServer时需要配置，否则无法自行建立log文件夹，然后报错，手动创建可行，但是每次查询都会删除文件夹，必须查一次建一次 --&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>/user</code>开头的路径一般表示hdfs中的路径，而<code>$&#123;&#125;</code>变量开头
的路径一般表示本地文件系统路径</p>
<ul>
<li>变量<code>system:java.io.tmpdir</code>、<code>system:user.name</code>在
文件中需要自己设置，这样就避免需要手动更改出现这些
变量的地方</li>
<li><code>hive.querylog.location</code>设置在本地更好，这个日志好像
只在hive启动时存在，只是查询日志，不是hive运行日志，
hive结束运行时会被删除，并不是没有生成日志、<code>$&#123;&#125;</code>表示
HDFS路径</li>
</ul>
</li>
<li><p>配置中出现的目录（HDFS、locaL）有些手动建立</p>
<ul>
<li>HDFS的目录手动建立？</li>
<li>local不用</li>
</ul>
</li>
<li><p><code>hive.metastore.uris</code>若配置，则hive会通过metastore服务
访问元信息</p>
<ul>
<li>使用hive前需要启动metastore服务</li>
<li>并且端口要和配置文件中一样，否则hive无法访问</li>
</ul>
</li>
</ul>
<h4 id="环境设置文件-1"><a href="#环境设置文件-1" class="headerlink" title="环境设置文件"></a>环境设置文件</h4><h5 id="conf-hive-env-sh"><a href="#conf-hive-env-sh" class="headerlink" title="conf/hive-env.sh"></a><code>conf/hive-env.sh</code></h5><ul>
<li>模板：<code>conf/hive-env.sh.template</code></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/java/jdk</span><br><span class="line">export HADOOP_HOME=/opt/hadoop</span><br><span class="line">export HIVE_CONF_DIR=/opt/hive/conf</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 以上3者若在`~/.bashrc`中设置，则无需再次设置</span></span><br><span class="line">export HIVE_AUX_JARS_PATH=/opt/hive/lib</span><br></pre></td></tr></table></figure>
<h5 id="conf-hive-exec-log4j2-properties"><a href="#conf-hive-exec-log4j2-properties" class="headerlink" title="conf/hive-exec-log4j2.properties"></a><code>conf/hive-exec-log4j2.properties</code></h5><ul>
<li><p>模板：<code>hive-exec-log4j2.properties.template</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">property.hive.log.dir=/opt/hive/logs</span><br><span class="line">	# 原为`$&#123;sys:java.io.tmpdir&#125;/$&#123;sys:user.name&#125;`</span><br><span class="line">	# 即`/tmp/root`（root用户执行）</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="conf-hive-log4j2-properties"><a href="#conf-hive-log4j2-properties" class="headerlink" title="conf/hive-log4j2.properties"></a><code>conf/hive-log4j2.properties</code></h5><ul>
<li>模板：<code>hive-log4j2.properties.template</code></li>
</ul>
<h4 id="MetaStore"><a href="#MetaStore" class="headerlink" title="MetaStore"></a>MetaStore</h4><h5 id="MariaDB"><a href="#MariaDB" class="headerlink" title="MariaDB"></a>MariaDB</h5><ul>
<li><p>安装MariaDB</p>
</li>
<li><p>修改MariaDB配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp /user/share/mysql/my-huge.cnf /etc/my.cnf</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建用户，注意新创建用户可能无效，见mysql配置</p>
<ul>
<li>需要注意用户权限：创建数据库权限、修改表权限</li>
<li>初始化时Hive要自己创建数据库（<code>hive-site</code>中配置），
所以对权限比较严格的环境下，可能需要先行创建同名
数据库、赋权、删库</li>
</ul>
</li>
<li><p>下载<code>mariadb-java-client-x.x.x-jar</code>包，复制到<code>lib</code>中</p>
</li>
</ul>
<h5 id="初始化数据库"><a href="#初始化数据库" class="headerlink" title="初始化数据库"></a>初始化数据库</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> schematool -initSchema -dbType mysql</span></span><br></pre></td></tr></table></figure>
<p>这个命令要在所有配置完成之后执行</p>
<h4 id="服务设置"><a href="#服务设置" class="headerlink" title="服务设置"></a>服务设置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hive --service metastore -p 19083 &amp;</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 启动metastore服务，端口要和hive中的配置相同</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 否则hive无法连接metastore服务，无法使用</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 终止metastore服务只能根据进程号`<span class="built_in">kill</span>`</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hive --service hiveserver2 --hiveconf hive.server2.thrift.port =10011 &amp;</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 启动JDBC Server</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 此时可以通过JDBC Client（如beeline）连接JDBC Server对</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> Hive中数据进行操作</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hive --service hiveserver2 --stop</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 停止JDBC Server</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 或者直接<span class="built_in">kill</span></span></span><br></pre></td></tr></table></figure>
<h4 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h4><h5 id="Hive可用性"><a href="#Hive可用性" class="headerlink" title="Hive可用性"></a>Hive可用性</h5><p>需要先启动hdfs、YARN、metastore database（mysql），如果有
设置独立metastore server，还需要在正确端口启动</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span>	<span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> words(id <span class="type">INT</span>, word STRING)</span><br><span class="line">		<span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot; &quot;</span><br><span class="line">		lines terminated <span class="keyword">by</span> &quot;\n&quot;;</span><br><span class="line">hive<span class="operator">&gt;</span>	load data <span class="keyword">local</span> inpath &quot;/opt/hive-test.txt&quot; overwrite <span class="keyword">into</span></span><br><span class="line">		<span class="keyword">table</span> words;</span><br><span class="line">hive<span class="operator">&gt;</span>	<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> words;</span><br></pre></td></tr></table></figure>
<h5 id="JDBCServer可用性"><a href="#JDBCServer可用性" class="headerlink" title="JDBCServer可用性"></a>JDBCServer可用性</h5><ul>
<li><p>命令行连接</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> beeline -u jdbc:hive2://localhost:10011 -n hive -p 1234</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>beeline中连接</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> beeline</span></span><br><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://localhost:10011</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 然后输入用户名、密码（metastore数据库用户名密码）</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h3><h4 id="可能错误-1"><a href="#可能错误-1" class="headerlink" title="可能错误"></a>可能错误</h4><blockquote>
<p>   Failed with exception Unable to move source file</p>
</blockquote>
<ul>
<li>linux用户权限问题，无法操作原文件</li>
<li>hdfs用户权限问题，无法写入目标文件</li>
<li>hdfs配置问题，根本无法向hdfs写入：参见hdfs问题</li>
</ul>
<blockquote>
<p>   org.apache.hive.service.cli.HiveSQLException: Couldn’t find log associated with operation handle: </p>
</blockquote>
<ul>
<li><p>原因：hiveserver2查询日志文件夹不存在</p>
</li>
<li><p>可以在hive中通过</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">set</span> hive.server2.logging.operation.log.location;</span><br></pre></td></tr></table></figure>
<p>查询日志文件夹，建立即可，默认为
<code>$&#123;system:java.io.tmpdir&#125;/$&#123;system:user.name&#125;/operation_logs</code>
，并设置权限为777</p>
<ul>
<li>好像如果不设置权限为777，每次查询文件夹被删除，每
查询一次建立一次文件夹？#todo</li>
<li>在<code>hive-sitex.xml</code>中配置允许自行创建？</li>
</ul>
</li>
</ul>
<blockquote>
<p>   User: root is not allowed to impersonate hive</p>
</blockquote>
<ul>
<li><p>原因：当前用户（不一定是root）不被允许通过代理操作
hadoop用户、用户组、主机</p>
<ul>
<li>hadoop引入安全伪装机制，不允许上层系统直接将实际用户
传递给超级代理，此代理在hadoop上执行操作，避免客户端
随意操作hadoop</li>
</ul>
</li>
<li><p>配置hadoop的<code>core-site.xml</code>，使得当前用户作为超级代理</p>
</li>
</ul>
<h2 id="Tez"><a href="#Tez" class="headerlink" title="Tez"></a>Tez</h2><h3 id="依赖-2"><a href="#依赖-2" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>hadoop</li>
</ul>
<h3 id="机器环境配置-2"><a href="#机器环境配置-2" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-2"><a href="#bashrc-2" class="headerlink" title=".bashrc"></a><code>.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">export TEZ_HOME=/opt/tez</span><br><span class="line">export TEZ_CONF_DIR=$TEZ_HOME/conf</span><br><span class="line"></span><br><span class="line">for jar in `ls $TEZ_HOME | grep jar`; do</span><br><span class="line">	export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_HOME/$jar</span><br><span class="line">done</span><br><span class="line">for jar in `ls $TEZ_HOME/lib`; do</span><br><span class="line">	export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_HOME/lib/$jar</span><br><span class="line">done</span><br><span class="line"><span class="meta">	#</span><span class="bash"> this part could be replaced with line bellow</span></span><br><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_HOME/*:$TEZ_HOME/lib/*</span><br><span class="line"><span class="meta">	#</span><span class="bash"> `hadoop-env.sh`中说`HADOOP_CLASSPATH`是Extra Java CLASSPATH</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> elements</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这意味着hadoop组件只需要把其jar包加到`HADOOP_CLASSPATH`中既可</span></span><br></pre></td></tr></table></figure>
<h4 id="HDFS-2"><a href="#HDFS-2" class="headerlink" title="HDFS"></a>HDFS</h4><ul>
<li><p>上传<code>$TEZ_HOME/share/tez.tar.gz</code>至HDFS中</p>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -mkdir /apps</span><br><span class="line">$ hdfs dfs -copyFromLocal tez.tar.gz /apps</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="HadoopOnTez"><a href="#HadoopOnTez" class="headerlink" title="HadoopOnTez"></a>HadoopOnTez</h3><p>在hadoop中配置Tez</p>
<ul>
<li><p>侵入性较强，对已有的hadoop集群全体均有影响</p>
</li>
<li><p>所有hadoop集群执行的MapReduce任务都通过tez执行</p>
<ul>
<li>这里所有的任务应该是指直接在hadoop上执行、能在
webRM上看到的任务</li>
<li>hive这样的独立组件需要独立配置</li>
</ul>
</li>
</ul>
<h4 id="XML参数-1"><a href="#XML参数-1" class="headerlink" title="XML参数"></a>XML参数</h4><h5 id="tez-site-xml"><a href="#tez-site-xml" class="headerlink" title="tez-site.xml"></a><code>tez-site.xml</code></h5><ul>
<li>模板：<code>conf/tez-default-tmplate.xml</code></li>
<li>好像还是需要复制到hadoop的配置文件夹中</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;tez.lib.uris&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;$&#123;fs.defaultFS&#125;/apps/tez.tar.gz&lt;/value&gt;</span><br><span class="line">	&lt;!--设置tez安装包位置--&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;tez.container.max.java.heap.fraction&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;0.2&lt;/value&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">内存不足时--&gt;</span><br></pre></td></tr></table></figure>
<h5 id="mapred-site-xml-2"><a href="#mapred-site-xml-2" class="headerlink" title="mapred-site.xml"></a><code>mapred-site.xml</code></h5><ul>
<li>修改<code>mapred-site.xml</code>文件：配置mapreduce基于<code>yarn-tez</code>，
（配置修改在hadoop部分也有）</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn-tez<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="环境参数"><a href="#环境参数" class="headerlink" title="环境参数"></a>环境参数</h4><h3 id="HiveOnTez"><a href="#HiveOnTez" class="headerlink" title="HiveOnTez"></a>HiveOnTez</h3><ul>
<li><p>此模式下Hive可以在mapreduce、tez计算模型下自由切换？</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.execution.engine<span class="operator">=</span>tez;</span><br><span class="line">	# 切换查询引擎为tez</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.execution.engine<span class="operator">=</span>mr;</span><br><span class="line">	# 切换查询引擎为mapreduce</span><br><span class="line">	# 这些命令好像没用，只能更改值，不能更改实际查询模型</span><br></pre></td></tr></table></figure>
</li>
<li><p>只有Hive会受到影响，其他基于hadoop平台的mapreduce作业
仍然使用tez计算模型</p>
</li>
</ul>
<h4 id="Hive设置"><a href="#Hive设置" class="headerlink" title="Hive设置"></a>Hive设置</h4><ul>
<li>若已经修改了<code>mapred-site.xml</code>设置全局基于tez，则无需复制
jar包，直接修改<code>hive-site.xml</code>即可</li>
</ul>
<h5 id="Jar包复制"><a href="#Jar包复制" class="headerlink" title="Jar包复制"></a>Jar包复制</h5><p>复制<code>$TEZ_HOME</code>、<code>$TEZ_HOME/lib</code>下的jar包到<code>$HIVE_HOME/lib</code>
下即可</p>
<h5 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a><code>hive-site.xml</code></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hive.execution.engine&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;tez&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<h3 id="其他-2"><a href="#其他-2" class="headerlink" title="其他"></a>其他</h3><h4 id="可能错误-2"><a href="#可能错误-2" class="headerlink" title="可能错误"></a>可能错误</h4><blockquote>
<p>   SLF4J: Class path contains multiple SLF4J bindings.</p>
</blockquote>
<ul>
<li>原因：包冲突的</li>
<li>解决方案：根据提示冲突包删除即可</li>
</ul>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="依赖-3"><a href="#依赖-3" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
<li>scala</li>
<li>python：一般安装anaconda，需要额外配置<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PYTHON_HOME=/opt/anaconda3</span><br><span class="line">export PATH=$PYTHON_HOME/bin:$PATH</span><br></pre></td></tr></table></figure></li>
<li>相应资源管理框架，如果不以standalone模式运行</li>
</ul>
<h3 id="机器环境配置-3"><a href="#机器环境配置-3" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-3"><a href="#bashrc-3" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/opt/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin</span><br><span class="line">export PYTHON_PATH=$PYTHON_PATH:$SPARK_HOME/python:$SPARK_HOME/python/lib/*</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 把`pyshark`、`py4j`模块对应的zip文件添加进路径</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里用的是`*`通配符应该也可以，手动添加所有zip肯定可以</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 否则无法在一般的python中对spark进行操作</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 似乎只要master节点有设置`/lib/*`添加`pyspark`、`py4j`就行</span></span><br></pre></td></tr></table></figure>
<h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><h4 id="环境设置文件-2"><a href="#环境设置文件-2" class="headerlink" title="环境设置文件"></a>环境设置文件</h4><h5 id="conf-spark-env-sh"><a href="#conf-spark-env-sh" class="headerlink" title="conf/spark-env.sh"></a><code>conf/spark-env.sh</code></h5><ul>
<li>模板：<code>conf/spark-env.sh.template</code></li>
</ul>
<p>这里应该有些配置可以省略、移除#todo</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/jdk</span><br><span class="line">export HADOOP_HOME=/opt/hadoop</span><br><span class="line">export hADOOP_CONF_DIR=/opt/hadoop/etc/hadoop</span><br><span class="line">export HIVE_HOME=/opt/hive</span><br><span class="line"></span><br><span class="line">export SCALA_HOME=/opt/scala</span><br><span class="line">export SCALA_LIBRARY=$SPARK_HOME/lib</span><br><span class="line"><span class="meta">	#</span><span class="bash"> `~/.bashrc`设置完成之后，前面这段应该就这个需要设置</span></span><br><span class="line"></span><br><span class="line">export SPARK_HOME=/opt/spark</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(hadoop classpath)</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里是执行命令获取classpath</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> todo</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里看文档的意思，应该也是类似于`<span class="variable">$HADOOP_CLASSPATH</span>`</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 可以直接添加进`<span class="variable">$CLASSPATH</span>`而不必设置此变量</span></span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_HOME/lib</span><br><span class="line"></span><br><span class="line">export SPARK_MASTER_HOST=hd-master</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line">export SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line">export SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line">export SPARK_WORKER_MEMORY=1024m</span><br><span class="line"><span class="meta">	#</span><span class="bash"> spark能在一个container内执行多个task</span></span><br><span class="line">export SPARK_LOCAL_DIRS=$SPARK_HOME/data</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 需要手动创建</span></span><br><span class="line"></span><br><span class="line">export SPARK_MASTER_OPTS=</span><br><span class="line">export SPARK_WORKER_OPTS=</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=</span><br><span class="line">export SPARK_DAEMON_MEMORY=</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=</span><br></pre></td></tr></table></figure>
<h5 id="文件夹建立-2"><a href="#文件夹建立-2" class="headerlink" title="文件夹建立"></a>文件夹建立</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir /opt/spark/spark_data</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="keyword">for</span> `<span class="variable">$SPARK_LOCAL_DIRS</span>`</span></span><br></pre></td></tr></table></figure>
<h4 id="Spark配置"><a href="#Spark配置" class="headerlink" title="Spark配置"></a>Spark配置</h4><h5 id="conf-slaves"><a href="#conf-slaves" class="headerlink" title="conf/slaves"></a><code>conf/slaves</code></h5><p>文件不存在，则在当前主机单节点运行</p>
<ul>
<li>模板：<code>conf/slaves.template</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hd-slave1</span><br><span class="line">hd-slave2</span><br></pre></td></tr></table></figure>
<h5 id="conf-hive-site-xml-1"><a href="#conf-hive-site-xml-1" class="headerlink" title="conf/hive-site.xml"></a><code>conf/hive-site.xml</code></h5><p>这里只是配置Spark，让Spark作为“thrift客户端”能正确连上
metastore server</p>
<ul>
<li>模板：<code>/opt/hive/conf/hive-site.xml</code></li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.31.129:19083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>Thrift URI for the remote metastor. Used by metastore client to connect to remote metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>10011<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!--配置spark对外界thrift服务，以便可通过JDBC客户端存取spark--&gt;</span></span><br><span class="line">	<span class="comment">&lt;!--这里启动端口同hive的配置，所以两者不能默认同时启动--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="测试-2"><a href="#测试-2" class="headerlink" title="测试"></a>测试</h4><h5 id="启动Spark服务"><a href="#启动Spark服务" class="headerlink" title="启动Spark服务"></a>启动Spark服务</h5><p>需要启动hdfs、正确端口启动的metastore server</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> start-master.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 在执行**此命令**机器上启动master实例</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-slaves.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 在`conf/slaves`中的机器上启动worker实例</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-slave.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 在执行**此命令**机器上启动worker实例</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> stop-master.sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> stop-slaves.sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> stop-slave.sh</span></span><br></pre></td></tr></table></figure>
<h5 id="启动Spark-Thrift-Server"><a href="#启动Spark-Thrift-Server" class="headerlink" title="启动Spark Thrift Server"></a>启动Spark Thrift Server</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> start-thriftserver.sh --master spark://hd-master:7077 \</span></span><br><span class="line"><span class="bash">	--hiveconf hive.server2.thrift.bind.host hd-master \</span></span><br><span class="line"><span class="bash">	--hiveconf hive.server2.thrift.port 10011</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里在命令行启动thrift server时动态指定host、port</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> 如果在`conf/hive-site.xml`有配置，应该不需要</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 然后使用beeline连接thrift server，同hive</span></span><br></pre></td></tr></table></figure>
<h5 id="Spark-Sql测试"><a href="#Spark-Sql测试" class="headerlink" title="Spark-Sql测试"></a>Spark-Sql测试</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ spark<span class="operator">-</span><span class="keyword">sql</span> <span class="comment">--master spark://hd-master:7077</span></span><br><span class="line">	# 在含有配置文件的节点上启动时，配置文件中已经指定`MASTER`</span><br><span class="line">		# 因此不需要指定后面配置</span><br><span class="line"></span><br><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span><span class="operator">&gt;</span> <span class="keyword">set</span> spark.sql.shuffle.partitions<span class="operator">=</span><span class="number">20</span>;</span><br><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span><span class="operator">&gt;</span> <span class="keyword">select</span> id, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> words <span class="keyword">group</span> <span class="keyword">by</span> id <span class="keyword">order</span> <span class="keyword">by</span> id;</span><br></pre></td></tr></table></figure>
<h5 id="pyspark测试"><a href="#pyspark测试" class="headerlink" title="pyspark测试"></a>pyspark测试</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ MASTER=spark://hd-master:<span class="number">7077</span> pyspark</span><br><span class="line">	<span class="comment"># 这里应该是调用`$PATH`中第一个python，如果未默认指定</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext</span><br><span class="line">sql_ctxt = HiveContext(sc)</span><br><span class="line">	<span class="comment"># 此`sc`是pyspark启动时自带的，是`SparkContext`类型实例</span></span><br><span class="line">	<span class="comment"># 每个连接只能有一个此实例，不能再次创建此实例</span></span><br><span class="line"></span><br><span class="line">ret = sql_ctxt.sql(<span class="string">&quot;show tables&quot;</span>).collect()</span><br><span class="line">	<span class="comment"># 这里语句结尾不能加`;` </span></span><br><span class="line"></span><br><span class="line">file = sc.textFile(<span class="string">&quot;hdfs://hd-master:9000/user/root/input/capacity-scheduler.xml&quot;</span>)</span><br><span class="line">file.count()</span><br><span class="line">file.first()</span><br></pre></td></tr></table></figure>
<h5 id="Scala测试"><a href="#Scala测试" class="headerlink" title="Scala测试"></a>Scala测试</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="type">MASTER</span>=spark:<span class="comment">//hd-master:7077 spark-shell \</span></span><br><span class="line">	executor-memory <span class="number">1024</span>m \</span><br><span class="line">	--total-executor-cores <span class="number">2</span> \</span><br><span class="line">	--excutor-cores <span class="number">1</span> \</span><br><span class="line">	# 添加参数启动`spark-shell`</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span>(sc)</span><br><span class="line">sqlContext.sql(<span class="string">&quot;select * from words&quot;</span>).collect().foreach(println)</span><br><span class="line">sqlContext.sql(<span class="string">&quot;select id, word from words order by id&quot;</span>).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">&quot;insert into words values(7, \&quot;jd\&quot;)&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.sql(<span class="string">&quot;select * from words&quot;</span>);</span><br><span class="line">df.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> df = spark.read.json(<span class="string">&quot;file:///opt/spark/example/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
<h3 id="Spark-on-YARN"><a href="#Spark-on-YARN" class="headerlink" title="Spark on YARN"></a>Spark on YARN</h3><h3 id="其他-3"><a href="#其他-3" class="headerlink" title="其他"></a>其他</h3><h4 id="可能错误-3"><a href="#可能错误-3" class="headerlink" title="可能错误"></a>可能错误</h4><blockquote>
<p>   Initial job has not accepted any resources;</p>
</blockquote>
<ul>
<li><p>原因：内存不足，spark提交application时内存超过分配给
worker节点内存</p>
</li>
<li><p>说明</p>
<ul>
<li>根据结果来看，<code>pyspark</code>、<code>spark-sql</code>需要内存比
<code>spark-shell</code>少？
（设置worker内存512m，前两者可以正常运行）</li>
<li>但是前两者的内存分配和scala不同，scala应该是提交任务
、指定内存大小的方式，这也可以从web-ui中看出来，只有
spark-shell开启时才算是<em>application</em></li>
</ul>
</li>
<li><p>解决方式</p>
<ul>
<li>修改<code>conf/spark-env.sh</code>中<code>SPARK_WORKER_MEMORY</code>更大，
（spark默认提交application内存为1024m）</li>
<li>添加启动参数<code>--executor-memory XXXm</code>不超过分配值</li>
</ul>
</li>
</ul>
<blockquote>
<pre><code>ERROR KeyProviderCache:87 - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider
</code></pre></blockquote>
<ul>
<li>无影响</li>
</ul>
<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><h3 id="依赖-4"><a href="#依赖-4" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
<li>hadoop</li>
<li>zookeeper：建议，否则日志不好管理</li>
</ul>
<h3 id="机器环境"><a href="#机器环境" class="headerlink" title="机器环境"></a>机器环境</h3><h4 id="bashrc-4"><a href="#bashrc-4" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/opt/hbase</span><br><span class="line">export PATH=$PAHT:$HBASE_HOME/bin</span><br><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_HOME/lib/*</span><br></pre></td></tr></table></figure>
<h4 id="建立目录"><a href="#建立目录" class="headerlink" title="建立目录"></a>建立目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir /tmp/hbase/tmpdir</span></span><br></pre></td></tr></table></figure>
<h3 id="HBase配置"><a href="#HBase配置" class="headerlink" title="HBase配置"></a>HBase配置</h3><h4 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h4><h5 id="conf-hbase-env-sh"><a href="#conf-hbase-env-sh" class="headerlink" title="conf/hbase-env.sh"></a><code>conf/hbase-env.sh</code></h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 不使用自带zookeeper</span></span><br></pre></td></tr></table></figure>
<h5 id="conf-zoo-cfg"><a href="#conf-zoo-cfg" class="headerlink" title="conf/zoo.cfg"></a><code>conf/zoo.cfg</code></h5><p>若设置使用独立zookeeper，需要复制zookeeper配置至HBase配置
文件夹中</p>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp /opt/zookeeper/conf/zoo.cfg /opt/hbase/conf</span><br></pre></td></tr></table></figure>
<h4 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h4><h5 id="conf-hbase-site-xml"><a href="#conf-hbase-site-xml" class="headerlink" title="conf/hbase-site.xml"></a><code>conf/hbase-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;HBASE_HOME&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/zookeeper/zkdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="Pseudo-Distributed模式"><a href="#Pseudo-Distributed模式" class="headerlink" title="Pseudo-Distributed模式"></a>Pseudo-Distributed模式</h4><h5 id="conf-hbase-site-xml-1"><a href="#conf-hbase-site-xml-1" class="headerlink" title="conf/hbase-site.xml"></a><code>conf/hbase-site.xml</code></h5><ul>
<li>在Standalone配置上修改</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">proeperty</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hd-master:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="Fully-Distributed模式"><a href="#Fully-Distributed模式" class="headerlink" title="Fully-Distributed模式"></a>Fully-Distributed模式</h4><h5 id="conf-hbase-site-xml-2"><a href="#conf-hbase-site-xml-2" class="headerlink" title="conf/hbase-site.xml"></a><code>conf/hbase-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hd-master:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master,hd-slave1,hd-slave2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/zookeeper/zkdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="测试-3"><a href="#测试-3" class="headerlink" title="测试"></a>测试</h4><ul>
<li>需要首先启动HDFS、YARN</li>
<li>使用独立zookeeper还需要先行在每个节点启动zookeeper</li>
</ul>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ start-hbase.sh</span><br><span class="line"><span class="code">	# 启动HBase服务</span></span><br><span class="line"><span class="code">$ local-regionservers.sh start 2 3 4 5</span></span><br><span class="line"><span class="code">	# 启动额外的4个RegionServer</span></span><br><span class="line"><span class="code">$ hbase shell</span></span><br><span class="line"><span class="code">hbase&gt; create &#x27;test&#x27;, &#x27;cf&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; list &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; put &#x27;test&#x27;, &#x27;row7&#x27;, &#x27;cf:a&#x27;, &#x27;value7a&#x27;</span></span><br><span class="line"><span class="code">	put &#x27;test&#x27;, &#x27;row7&#x27;, &#x27;cf:b&#x27;, &#x27;value7b&#x27;</span></span><br><span class="line"><span class="code">	put &#x27;test&#x27;, &#x27;row7&#x27;, &#x27;cf:c&#x27;, &#x27;value7c&#x27;</span></span><br><span class="line"><span class="code">	put &#x27;test&#x27;, &#x27;row8&#x27;, &#x27;cf:b&#x27;, &#x27;value8b&#x27;,</span></span><br><span class="line"><span class="code">	put &#x27;test&#x27;, &#x27;row9&#x27;, &#x27;cf:c&#x27;, &#x27;value9c&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; scan &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; get &#x27;test&#x27;, &#x27;row7&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; disable &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; enable &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbaee&gt; drop &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; quit</span></span><br></pre></td></tr></table></figure>
<h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><h3 id="依赖-5"><a href="#依赖-5" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li><p>java</p>
</li>
<li><p>注意：zookeeper集群中工作超过半数才能对外提供服务，所以
一般配置服务器数量为奇数</p>
</li>
</ul>
<h3 id="机器环境-1"><a href="#机器环境-1" class="headerlink" title="机器环境"></a>机器环境</h3><h4 id="bashrc-5"><a href="#bashrc-5" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export ZOOKEEPER_HOME=/opt/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$ZOOKEEPER_HOME/lib</span><br></pre></td></tr></table></figure>
<h4 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h4><ul>
<li>在所有节点都需要创建相应文件夹、<code>myid</code>文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /tmp/zookeeper/zkdata /tmp/zookeeper/zkdatalog</span><br><span class="line">echo 0 &gt; /tmp/zookeeper/zkdatalog/myid</span><br><span class="line"></span><br><span class="line">ssh centos2 mkdir -p /tmp/zookeeper/zkdata /tmp/zookeeper/zkdatalog</span><br><span class="line">ssh centos3 mkdir -p /tmp/zookeeper/zkdata /tmp/zookeeper/zkdatalog</span><br><span class="line">ssh centos2 &quot;echo 2 &gt; /tmp/zookeeper/zkdata/myid&quot;</span><br><span class="line">ssh centos3 &quot;echo 3 &gt; /tmp/zookeeper/zkdata/myid&quot;</span><br></pre></td></tr></table></figure>
<h3 id="Zookeeper配置"><a href="#Zookeeper配置" class="headerlink" title="Zookeeper配置"></a>Zookeeper配置</h3><h4 id="Conf"><a href="#Conf" class="headerlink" title="Conf"></a>Conf</h4><h5 id="conf-zoo-cfg-1"><a href="#conf-zoo-cfg-1" class="headerlink" title="conf/zoo.cfg"></a><code>conf/zoo.cfg</code></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">	# The number of milliseconds of each tick</span><br><span class="line">initLimit=10</span><br><span class="line">	# The number of ticks that the initial</span><br><span class="line">	# synchronization phase can take</span><br><span class="line">syncLimit=5</span><br><span class="line">	# The number of ticks that can pass between</span><br><span class="line">	# sending a request and getting an acknowledgement</span><br><span class="line">dataDir=/tmp/zookeeper/zkdata</span><br><span class="line">dataLogDir=/tmp/zookeeper/zkdatalog</span><br><span class="line">	# the directory where the snapshot is stored.</span><br><span class="line">	# do not use /tmp for storage, /tmp here is just</span><br><span class="line">	# example sakes.</span><br><span class="line">clientPort=2181</span><br><span class="line">	# the port at which the clients will connect</span><br><span class="line"></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line">	# Be sure to read the maintenance section of the</span><br><span class="line">	# administrator guide before turning on autopurge.</span><br><span class="line">	# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span><br><span class="line">	# The number of snapshots to retain in dataDir</span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line">	# Purge task interval in hours</span><br><span class="line">	# Set to &quot;0&quot; to disable auto purge feature</span><br><span class="line"></span><br><span class="line">server.0=hd-master:2888:3888</span><br><span class="line">server.1=hd-slave1:2888:3888</span><br><span class="line">server.2=hd-slave2:2888:3888</span><br><span class="line">	# Determine the zookeeper servers</span><br><span class="line">	# fromation: server.NO=HOST:PORT1:PORT2</span><br><span class="line">		# PORT1: port used to communicate with leader</span><br><span class="line">		# PORT2: port used to reelect leader when current leader fail</span><br></pre></td></tr></table></figure>
<h5 id="dataDir-myid"><a href="#dataDir-myid" class="headerlink" title="$dataDir/myid"></a><code>$dataDir/myid</code></h5><ul>
<li><code>$dataDir</code>是<code>conf/zoo.cfg</code>中指定目录</li>
<li><code>myid</code>文件里就一个id，指明当前zookeeper server的id，服务
启动时读取文件确定其id，需要自行创建</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure>
<h4 id="启动、测试、清理"><a href="#启动、测试、清理" class="headerlink" title="启动、测试、清理"></a>启动、测试、清理</h4><p>启动zookeeper</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> zkServer.sh start</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 开启zookeeper服务</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> zookeeper服务要在各个节点分别手动启动</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> zkServer.sh status</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 查看服务状态</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> zkCleanup.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 清理旧的快照、日志文件</span></span><br></pre></td></tr></table></figure>
<h2 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h2><h3 id="依赖-6"><a href="#依赖-6" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
</ul>
<h3 id="机器环境配置-4"><a href="#机器环境配置-4" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-6"><a href="#bashrc-6" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/opt/flume/bin</span><br></pre></td></tr></table></figure>
<h3 id="Flume配置"><a href="#Flume配置" class="headerlink" title="Flume配置"></a>Flume配置</h3><h4 id="环境设置文件-3"><a href="#环境设置文件-3" class="headerlink" title="环境设置文件"></a>环境设置文件</h4><h5 id="conf-flume-env-sh"><a href="#conf-flume-env-sh" class="headerlink" title="conf/flume-env.sh"></a><code>conf/flume-env.sh</code></h5><ul>
<li>模板：<code>conf/flume-env.sh.template</code></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/opt/jdk</span><br></pre></td></tr></table></figure>
<h4 id="Conf文件"><a href="#Conf文件" class="headerlink" title="Conf文件"></a>Conf文件</h4><h5 id="conf-flume-conf"><a href="#conf-flume-conf" class="headerlink" title="conf/flume.conf"></a><code>conf/flume.conf</code></h5><ul>
<li>模板：<code>conf/flume-conf.properties.template</code></li>
</ul>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">agent1.channels.ch1.type=memory</span><br><span class="line"><span class="code">	# define a memory channel called `ch1` on `agent1`</span></span><br><span class="line"><span class="code">agent1.sources.avro-source1.channels=ch1</span></span><br><span class="line"><span class="code">agent1.sources.avro-source1.type=avro</span></span><br><span class="line"><span class="code">agent1.sources.avro-source1.bind=0.0.0.0</span></span><br><span class="line"><span class="code">agent1.sources.avro-source1.prot=41414</span></span><br><span class="line"><span class="code">	# define an Avro source called `avro-source1` on `agent1` and tell it</span></span><br><span class="line"><span class="code">agent1.sink.log-sink1.channels=ch1</span></span><br><span class="line"><span class="code">agent1.sink.log-sink1.type=logger</span></span><br><span class="line"><span class="code">	# define a logger sink that simply logs all events it receives</span></span><br><span class="line"><span class="code">agent1.channels=ch1</span></span><br><span class="line"><span class="code">agent1.sources=avro-source1</span></span><br><span class="line"><span class="code">agent1.sinks=log-sink1</span></span><br><span class="line"><span class="code">	# Finally, all components have been defined, tell `agent1` which one to activate</span></span><br></pre></td></tr></table></figure>
<h4 id="启动、测试"><a href="#启动、测试" class="headerlink" title="启动、测试"></a>启动、测试</h4><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ flume-ng agent --conf /opt/flume/conf \</span><br><span class="line"><span class="code">	-f /conf/flume.conf \</span></span><br><span class="line"><span class="code">	-D flume.root.logger=DEBUG,console \</span></span><br><span class="line"><span class="code">	-n agent1</span></span><br><span class="line"><span class="code">	# the agent name specified by -n agent1` must match an agent name in `-f /conf/flume.conf`</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">$ flume-ng avro-client --conf /opt/flume/conf \</span><br><span class="line"><span class="code">	-H localhost -p 41414 \</span></span><br><span class="line"><span class="code">	-F /opt/hive-test.txt \</span></span><br><span class="line"><span class="code">	-D flume.root.logger=DEBUG, Console</span></span><br><span class="line"><span class="code">	# 测试flume</span></span><br></pre></td></tr></table></figure>
<h3 id="其他-4"><a href="#其他-4" class="headerlink" title="其他"></a>其他</h3><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><h3 id="依赖-7"><a href="#依赖-7" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
<li>zookeeper</li>
</ul>
<h3 id="机器环境变量"><a href="#机器环境变量" class="headerlink" title="机器环境变量"></a>机器环境变量</h3><h4 id="bashrc-7"><a href="#bashrc-7" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/opt/kafka/bin</span><br><span class="line">export KAFKA_HOME=/opt/kafka</span><br></pre></td></tr></table></figure>
<h3 id="多brokers配置"><a href="#多brokers配置" class="headerlink" title="多brokers配置"></a>多brokers配置</h3><h4 id="Conf-1"><a href="#Conf-1" class="headerlink" title="Conf"></a>Conf</h4><h5 id="config-server-1-properties"><a href="#config-server-1-properties" class="headerlink" title="config/server-1.properties"></a><code>config/server-1.properties</code></h5><ul>
<li>模板：<code>config/server.properties</code></li>
<li>不同节点<code>broker.id</code>不能相同</li>
<li>可以多编写几个配置文件，在不同节点使用不同配置文件启动</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">broker.id=0</span><br><span class="line">listeners=PLAINTEXT://:9093</span><br><span class="line">zookeeper.connect=hd-master:2181, hd-slave1:2181, hd-slave2:2181</span><br></pre></td></tr></table></figure>
<h4 id="测试-4"><a href="#测试-4" class="headerlink" title="测试"></a>测试</h4><ul>
<li>启动zookeeper</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-server-start.sh /opt/kafka/config/server.properties &amp;</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 开启kafka服务（broker）</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里是指定使用单个默认配置文件启动broker</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> 启动多个broker需要分别使用多个配置启动多次</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-server-stop.sh /opt/kafka/config/server.properties</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics.sh --create --zookeeper localhost:2181 \</span></span><br><span class="line"><span class="bash">	--replication-factor 1 \</span></span><br><span class="line"><span class="bash">	--partitions 1 \</span></span><br><span class="line"><span class="bash">	--topic test1</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 开启话题</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics.sh --list zookeeper localhost:2181</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics.shd --delete --zookeeper localhost:2181</span></span><br><span class="line">	--topic test1</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 关闭话题</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-producer.sh --broker-list localhost:9092 \</span></span><br><span class="line"><span class="bash">	--topic test1</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 新终端开启producer，可以开始发送消息</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer.sh --bootstrap-server localhost:9092 \</span></span><br><span class="line"><span class="bash">	--topic test1 \</span></span><br><span class="line"><span class="bash">	--from-beginning</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer.sh --zookeeper localhost:2181 \</span></span><br><span class="line"><span class="bash">	--topic test1 \</span></span><br><span class="line"><span class="bash">	--from beginning</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 新终端开启consumer，可以开始接收信息</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这个好像是错的</span></span><br></pre></td></tr></table></figure>
<h3 id="其他-5"><a href="#其他-5" class="headerlink" title="其他"></a>其他</h3><h2 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h2><h3 id="依赖-8"><a href="#依赖-8" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
<li>zookeeper</li>
<li>python2.6+</li>
<li>ZeroMQ、JZMQ</li>
</ul>
<h3 id="机器环境配置-5"><a href="#机器环境配置-5" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-8"><a href="#bashrc-8" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export STORM_HOME=/opt/storm</span><br><span class="line">export PAT=$PATH:$STORM_HOME/bin</span><br></pre></td></tr></table></figure>
<h3 id="Storm配置"><a href="#Storm配置" class="headerlink" title="Storm配置"></a>Storm配置</h3><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><h5 id="conf-storm-yaml"><a href="#conf-storm-yaml" class="headerlink" title="conf/storm.yaml"></a><code>conf/storm.yaml</code></h5><ul>
<li>模板：<code>conf/storm.yarml</code></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">storm.zookeeper.servers:</span><br><span class="line">	-hd-master</span><br><span class="line">	-hd-slave1</span><br><span class="line">	-hd-slave2</span><br><span class="line">storm.zookeeper.port: 2181</span><br><span class="line"></span><br><span class="line">nimbus.seeds: [hd-master]</span><br><span class="line">storm.local.dir: /tmp/storm/tmp</span><br><span class="line">nimbus.host: hd-master</span><br><span class="line">supervisor.slots.ports:</span><br><span class="line">	-6700</span><br><span class="line">	-6701</span><br><span class="line">	-6702</span><br><span class="line">	-6703</span><br></pre></td></tr></table></figure>
<h4 id="启动、测试-1"><a href="#启动、测试-1" class="headerlink" title="启动、测试"></a>启动、测试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">storm nimbus &amp;&gt; /dev/null &amp;</span><br><span class="line">storm logviewer &amp;&gt; /dev/null &amp;</span><br><span class="line">storm ui &amp;&gt; /dev/null &amp;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> master节点启动nimbus</span></span><br><span class="line"></span><br><span class="line">storm sueprvisor &amp;&gt; /dev/null &amp;</span><br><span class="line">storm logviewer &amp;&gt; /dev/nulla &amp;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> worker节点启动</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">storm jar /opt/storm/example/..../storm-start.jar \</span><br><span class="line">	storm.starter.WordCountTopology</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 测试用例</span></span><br><span class="line">stom kill WordCountTopology</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r3.1.1">http://hadoop.apache.org/docs/r3.1.1</a></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/tags/Database/page/2/">Previous</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/tags/Database/page/4/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/tags/Database/">1</a></li><li><a class="pagination-link" href="/tags/Database/page/2/">2</a></li><li><a class="pagination-link is-current" href="/tags/Database/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/WWxwMVBsOUtoNTdMSlJ4Z1lWVnRISERsd2t6ek9MeVpEUWs0YldlZGxUdXlKdDNmMEZVV1hWaFZFYWFSNmFKL25penZodWx3UzRiaVkxcnREWCtOYUJhZWhNbWpzaUdyc1hPangycUh5RTVjRXFnZnFGdVdSTzZvVzJBcTJHKzl8aXpDK1ROWWl4N080YkFEK3QvbEVWNEJuQjFqdWdxODZQcGNoM1NqbERXST0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>