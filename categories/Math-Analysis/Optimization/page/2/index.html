<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: Optimization - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="follow_it-verification-code" content="SVBypAPPHxjjr7Y4hHfn"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/Math-Analysis/">Math Analysis</a></li><li class="is-active"><a href="#" aria-current="page">Optimization</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">7 minutes read (About 1121 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/unconstrained_specials.html">无约束优化特殊问题</a></h1><div class="content"><h2 id="正定二次目标函数"><a href="#正定二次目标函数" class="headerlink" title="正定二次目标函数"></a>正定二次目标函数</h2><script type="math/tex; mode=display">
min f(x) = \frac 1 2 x^T G x + r^T x + \sigma</script><h2 id="非线性最小二乘"><a href="#非线性最小二乘" class="headerlink" title="非线性最小二乘"></a>非线性最小二乘</h2><script type="math/tex; mode=display">\begin{align*}
f(x) & = \frac 1 2 \sum_{i=1}^m r^2_i(x) \\
& = \frac 1 2 r(x) r^T(x)
\end{align*}</script><blockquote>
<ul>
<li>$r_i(x)$：通常为非线性函数</li>
<li>$r(x) = (r_1(x), \cdots, r_n(x))^T$</li>
<li>$x \in R^n, m \geq n$</li>
</ul>
</blockquote>
<p>则</p>
<script type="math/tex; mode=display">\begin{align*}
\nabla f(x) & = \sum_{i=1}^m \nabla r_i(x)
    r_i(x) \\
& = J(x)^T r(x) \\

\nabla^2 f(x) & = \sum_{i=1}^m \nabla r_i(x)
    r_i(x) + \sum_{i=1}^m r_i \nabla^2 r_i(x) \\
& = J(x)^T J(x) + \sum_{i=1}^m r_i(x) \nabla^2 r_i(x)
\end{align*}</script><blockquote>
<ul>
<li><script type="math/tex; mode=display">
  J(x) = \begin{bmatrix}
  \frac {\partial r_1} {\partial x_1} &
      \frac {\partial r_1} {\partial x_2} & \cdots &
      \frac {\partial r_1} {\partial x_n} \\
  \frac {\partial r_2} {\partial x_1} &
      \frac {\partial r_2} {\partial x_2} & \cdots &
      \frac {\partial r_2} {\partial x_n} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac {\partial r_m} {\partial x_1} &
      \frac {\partial r_m} {\partial x_2} & \cdots &
      \frac {\partial r_m} {\partial x_n}
  \end{bmatrix}
  = \begin{bmatrix}
  \nabla r_1(x)^T \\
  \nabla r_2(x)^T \\
  \vdots \\
  \nabla r_m(x)^T \\
  \end{bmatrix}</script>  为$r(x)$的Jacobi矩阵</li>
</ul>
</blockquote>
<h3 id="Gauss-Newton法"><a href="#Gauss-Newton法" class="headerlink" title="Gauss-Newton法"></a>Gauss-Newton法</h3><p>Newton法中为简化计算，略去其Hesse矩阵中
$\sum_{i=1}^m r_i(x) \nabla^2 r_i(x)$项，即直接求解
方程组</p>
<script type="math/tex; mode=display">
J(x^{(k)})^T J(x^{(k)}) d = -J(x^{(k)})^T r(x^{(k)})</script><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>同Newton法，仅求解Newton方程改为求解以上方程组</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>实际问题中</p>
<ul>
<li>局部解$x^{ <em> }$对应的目标函数值$f(x^{ </em> })$接近0
时，$|r(x^{(k)})|$较小</li>
<li>曲线$r_i(x)$接近直线，
$\nabla^2 r_i(x) \approx 0$</li>
</ul>
<p>采用Gauss-Newton法效果较好，否则效果一般</p>
</li>
<li><p>矩阵$J(x^{(k)})^T J(x^{(k)})$是半正定矩阵，当Jacobi矩阵
列满秩时为正定矩阵，此时虽然$d^{(k)}$是下降方向，但仍需
类似修正牛顿法增加一维搜索策略保证目标函数值不上升</p>
</li>
</ul>
<h3 id="Levenberg-Marquardt方法"><a href="#Levenberg-Marquardt方法" class="headerlink" title="Levenberg-Marquardt方法"></a>Levenberg-Marquardt方法</h3><p>但$J(x^{(k)})$中各列线性相关、接近线性相关，则求解
Newton-Gauss方法中的方程组会出现困难，可以改为求解</p>
<script type="math/tex; mode=display">
(J(x^{(k)})^T J(x^{(k)}) + vI) d = -J(x^{(k)})^T r(x^{(k)})</script><blockquote>
<ul>
<li>$v$：迭代过程中需要调整的参数，LM方法的关键即如何调整</li>
</ul>
</blockquote>
<h4 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h4><blockquote>
<ul>
<li>若$d(v)$是以上方程组的解，则$|d(v)|^2$是$v$的连续下降
  函数，且$v \rightarrow +\infty, |d(v)| \rightarrow 0$</li>
</ul>
</blockquote>
<ul>
<li><p>$J(x^{(k)})^T J(x^{(k)})$是对称半正定矩阵，则存在正交阵</p>
<script type="math/tex; mode=display">
(P^{(k)})^T J(x^{(k)})^T J(x^{(k)}) P^{(k)} =
   \Lambda^{(k)}</script></li>
<li><p>则可以解出$|d(v)|^2$</p>
</li>
</ul>
<blockquote>
<ul>
<li>增大$v$可以限制$|d^{(k)}|$，所以LM方法也被称为阻尼最小
  二乘法</li>
</ul>
</blockquote>
<h4 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h4><blockquote>
<ul>
<li>若$d(v)$是以上方程的解，则$d(v)$是$f(x)$在$x^{(k)}$处的
  下降方向，且$v \rightarrow + \infty$时，$d(v)$的方向与
  $-J(x^{(k)})^T r(x^{(k)})$方向一致</li>
</ul>
</blockquote>
<ul>
<li>下降方向：$\nabla f(x^{(k)}) d(v) &lt; 0$即可</li>
<li>方向一致：夹角余弦</li>
</ul>
<blockquote>
<ul>
<li>$v$充分大时，LM方法产生的搜索方向$d^{(k)}$和负梯度方向
  一致</li>
</ul>
</blockquote>
<h4 id="参数调整方法"><a href="#参数调整方法" class="headerlink" title="参数调整方法"></a>参数调整方法</h4><p>使用梯度、近似Hesse矩阵定义二次函数</p>
<script type="math/tex; mode=display">
q(d) = f(x^{(k)}) + (J(x^{(k)})^T r(x^{(k)}))^T d +
    \frac 1 2 d^T (J(x^{(k)})^T J(x^{(k)})) d</script><blockquote>
<ul>
<li><p>其增量为</p>
<script type="math/tex; mode=display">\begin{align*}
  \Delta q^{(k)} & = q(d^{(k)}) - q(0) \\
  & = (J(x^{(k)})^T r(x^{(k)}))^T d^{(k)} + \frac 1 2
      (d^{(k)})^T (J(x^{(k)})^T J(x^{(k)})) d^{(k)}
  \end{align*}</script></li>
<li><p>目标函数增量</p>
<script type="math/tex; mode=display">\begin{align*}
  \Delta f^{(k)} & = f(x^{(k)} + d^{(k)}) - f(x^{(k)}) \\
  & = f(x^{(k+1)}) - f(x^{(k)})
  \end{align*}</script></li>
<li><p>定义$\gamma_k = \frac {\Delta f^{(k)}} {\Delta q^{(k)}}$</p>
</li>
</ul>
</blockquote>
<ul>
<li><p>$\gamma_k$接近1说明$\Delta f^{(k)}$接近$\Delta q^{(k)}$</p>
<ul>
<li>即$f(x^{(k)} + d^{(k+1)})$接近$q(d^{(k)})$</li>
<li>即$f(x)$在$x^{(k)}$附近接近二次函数</li>
<li>即使用Gauss-Newton方法求解最小二乘问题效果较好</li>
<li>即LM方法求解时$v$参数应该较小</li>
</ul>
</li>
<li><p>$\gamma_k$接近0说明$\Delta f^{(k)}$与$\Delta q^{(k)}$
近似程度不好</p>
<ul>
<li>$d^{(k)}$不应取得过大，应减少$d^{(k)}$得模长</li>
<li>应该增加参数$v$进行限制</li>
<li>迭代方向趋近于负梯度方向</li>
</ul>
</li>
<li><p>$\gamma_k$适中时，认为参数$v$选取合适，不做调整</p>
<ul>
<li>临界值通常为0.25、0.75</li>
</ul>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始点$x^{(1)}$、初始参数$v$（小值）、精度要求$\epsilon$
，置k=k+1</p>
</li>
<li><p>若$|J(x^{(k)})^T r(x^{(k)})| &lt; \epsilon$，则停止计算，
得到问题解$x^{(k)}$，否则求解线性方程组</p>
<script type="math/tex; mode=display">
(J(x^{(k)})^T J(x^{(k)}) + v_kI) d = -J(x^{(k)})^T
  r(x^{(k)})</script><p>得到$d^{(k)}$</p>
</li>
<li><p>置$x^{(k+1)} = x^{(k)} + d^{(k)}$，计算$\gamma_k$</p>
</li>
<li><p>若</p>
<ul>
<li>$\gamma &lt; 0.25$，置$v_{k+1} = 4 v_k$</li>
<li>$\gamma &gt; 0.75$，置$v_{k+1} = v_k / 2$</li>
<li>否则置$v_{k+1} = v_k$</li>
</ul>
</li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-26T17:01:10.000Z" title="6/27/2019, 1:01:10 AM">2019-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-06-26T17:01:10.000Z" title="6/27/2019, 1:01:10 AM">2019-06-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">7 minutes read (About 999 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/lagrange_duality.html">Lagrange 对偶</a></h1><div class="content"><h2 id="Langrangian-Duality"><a href="#Langrangian-Duality" class="headerlink" title="Langrangian Duality"></a><em>Langrangian Duality</em></h2><p>拉格朗日对偶</p>
<ul>
<li><p>考虑优化问题：找到$f(x)$满足约束的最好下界</p>
<script type="math/tex; mode=display">
z^{*} = \min_{x} f(x) \\
\begin{align*}
s.t. \quad & g_i(x) \leq 0, i=1,2,\cdots,m \\
   & x \in X
\end{align*}</script></li>
<li><p>考虑方程组</p>
<script type="math/tex; mode=display">
\left \{ \begin{array}{l}
f(x) < v \\
g_i(x) \leq 0, i=1,2,\cdots,m
\end{array} \right.</script><ul>
<li><p><strong>方程组无解</strong>：$v$是优化问题的一个下界</p>
</li>
<li><p><strong>方程组有解</strong>：则可以推出</p>
<script type="math/tex; mode=display">
\forall \lambda \geq 0, \exists x, 
f(x) + \sum_{i=1}^m \lambda_ig_i(x) < v</script><blockquote>
<ul>
<li>显然，取$g_1 + g_2 = 0, g_1(x) &gt; 0$是反例，不能
推出原方程有解</li>
</ul>
</blockquote>
</li>
<li><p>由以上方程组有解逆否命题：方程组无解<strong>充分条件</strong>如下</p>
<script type="math/tex; mode=display">
\exists \lambda \geq 0,
\min_{x} f(x) + \sum _{i=1}^m \lambda_ig_i(x) \geq v</script></li>
</ul>
</li>
<li><p>由此方法推出的最好下界，即拉格朗日对偶问题</p>
<script type="math/tex; mode=display">
v^{*} = \max_{\lambda \geq 0} \min_{x} f(x) +
   \sum_{i=1}^m \lambda_ig_i(x)</script></li>
</ul>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><ul>
<li><p>拉格朗日对偶对实数域上的优化问题都存在，对目标函数、
约束函数都没有要求</p>
</li>
<li><p>强对偶定理：$v^{<em>} = z^{</em>}$，需要$f,g$满足特定条件才成立</p>
<ul>
<li>线性规划</li>
<li>半正定规划</li>
<li>凸优化</li>
</ul>
<blockquote>
<ul>
<li>即需要给约束条件加以限制，使得<script type="math/tex; mode=display">
 \forall \lambda \geq 0, \exists x, 
 f(x) + \sum_{i=1}^m \lambda_ig_i(x) < v</script> 是上述方程组有解的冲要条件</li>
</ul>
</blockquote>
</li>
<li><p>弱对偶定理：$v^{<em>} \leq z^{</em>}$，永远成立（以上即可证）</p>
<ul>
<li>通过弱对偶定理，可以得到原问题的一个下界</li>
<li>对求解原问题有帮助，比如：分支界限法中快速求下界</li>
</ul>
</li>
<li><p>对偶问题相关算法往往原问题算法在实际应用中往往更加有效</p>
<ul>
<li><em>dual-simplex</em></li>
<li><em>primal-dual interior point method</em></li>
<li><em>augmented Lagrangian Method</em></li>
</ul>
</li>
</ul>
<h2 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h2><p>约束最优化问题</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min_{x \in R^n} & f(x) \\
s.t. & c_i(x) \leq 0, i = 1,2,\cdots,k \\
& h_j(x) = 0, j = 1,2,\cdots,l
\end{array}</script><h3 id="Generalized-Lagrange-Function"><a href="#Generalized-Lagrange-Function" class="headerlink" title="Generalized Lagrange Function"></a><em>Generalized Lagrange Function</em></h3><ul>
<li><p>引入<em>Generalized Lagrange Function</em></p>
<script type="math/tex; mode=display">
L(x, \alpha, \beta) = f(x) + \sum_{i=1}^k \alpha_i
   c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><blockquote>
<ul>
<li>$x=(x_1, x_2, \cdots, x_n) \in R^n$</li>
<li>$\alpha_i \geq 0, \beta_j$：拉格朗日乘子</li>
</ul>
</blockquote>
</li>
<li><p>考虑关于x的函数</p>
<script type="math/tex; mode=display">
\theta_P(x) = \max_{\alpha, \beta: \alpha_i \geq 0}
   L(x, \alpha, \beta)</script><blockquote>
<ul>
<li>$P$：primal，原始问题</li>
</ul>
</blockquote>
<ul>
<li><p>若x满足原始问题的两组约束条件，则$\theta_P(x)=f(x)$</p>
</li>
<li><p>若x违反等式约束j，取$\beta_j \rightarrow \infty$，
则有$\theta_P(x) \rightarrow \infty$</p>
</li>
<li><p>若x违反不等式约束i，取$\alpha_i \rightarrow \infty$
，则有$\theta_P(x) \rightarrow \infty$</p>
</li>
</ul>
<p>则有</p>
<script type="math/tex; mode=display">\theta_P(x) = \left \{ \begin{array}{l}
f(x), & x 满足原始问题约束条件 \\
+\infty, & 其他
\end{array} \right.</script></li>
<li><p>则极小化问题，称为广义拉格朗日函数的极小极大问题</p>
<script type="math/tex; mode=display">
\min_x \theta_P(x) = \max_{\alpha, \beta: \alpha_i \geq 0}
   L(x, \alpha, \beta)</script><p>与原始最优化问题等价，两问题最优值相同，记为</p>
<script type="math/tex; mode=display">
p^{*} = \min_x \theta_P(x)</script></li>
</ul>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><ul>
<li><p>定义</p>
<script type="math/tex; mode=display">
\theta_D (\alpha, \beta) = \min_x L(x, \alpha, \beta)</script></li>
<li><p>再考虑极大化$\theta_D(\alpha, \beta)$，得到广义拉格朗日
函数的极大极小问题，即</p>
<script type="math/tex; mode=display">
\max_{\alpha, \beta: \alpha \geq 0}
   \theta_D(\alpha, \beta) =
   \max_{\alpha, \beta: \alpha \geq 0} \min_x
   L(x, \alpha, \beta)</script><p>表示为约束最优化问题如下</p>
<script type="math/tex; mode=display">\begin{align*}
\max_{\alpha, \beta} & \theta_D(\alpha, \beta) =
   \max_{\alpha, \beta} \min_x L(x, \alpha, \beta) \\
s.t. & \alpha_i \geq 0, i=1,2,\cdots,k
\end{align*}</script><p>称为原始问题的对偶问题，其最优值定义记为</p>
<script type="math/tex; mode=display">
d^{*} = \max_{\alpha, \beta: \alpha \geq 0}
   \theta_D(\alpha, \beta)</script></li>
</ul>
<h2 id="原始、对偶问题关系"><a href="#原始、对偶问题关系" class="headerlink" title="原始、对偶问题关系"></a>原始、对偶问题关系</h2><h3 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h3><blockquote>
<ul>
<li>若原始问题、对偶问题都有最优值，则<script type="math/tex; mode=display">
  d^{*} = \max_{\alpha, \beta: \alpha \geq 0} \min_x
      L(x, \alpha, \beta) \leq
  \min_x \max_{\alpha, \beta: \alpha \geq 0}
      L(x, \alpha, \beta) = p^{*}</script></li>
</ul>
</blockquote>
<ul>
<li><p>$\forall x, \alpha, \beta$有</p>
<script type="math/tex; mode=display">
\theta_D(\alpha, \beta) = \min_x L(x, \alpha, \beta)
   \leq L(x, \alpha, \beta) \leq
   \max_{\alpha, \beta: \alpha \geq 0} = \theta_P(x)</script><p>即</p>
<script type="math/tex; mode=display">
\theta_D(\alpha, \beta) \leq \theta_P(x)</script></li>
<li><p>而原始、对偶问题均有最优值，所以得证</p>
</li>
</ul>
<blockquote>
<ul>
<li>设$x^{<em>}$、$\alpha^{</em>}, \beta^{<em>}$分别是原始问题、对偶
  问题的可行解，且$d^{</em>} = p^{*}$，则其分别是原始问题、
  对偶问题的最优解</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:26:19.000Z" title="8/4/2021, 11:26:19 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">16 minutes read (About 2327 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/gredient_based.html">Gradient Descent Method</a></h1><div class="content"><h2 id="思想：最速下降-amp-牛顿"><a href="#思想：最速下降-amp-牛顿" class="headerlink" title="思想：最速下降&amp;牛顿"></a>思想：最速下降&amp;牛顿</h2><p>对目标函数$f(x)$在$x^{(1)}$进行展开</p>
<script type="math/tex; mode=display">
f(x) = f(x^{(1)}) + \nabla f(x^{(1)})(x - x^{(1)})+
    \frac 1 2 \nabla^2 f(x^{(1)})(x - x^{(1)})^2 +
    o((x - x^{(1)})^2)</script><blockquote>
<ul>
<li>最速下降法：只保留一阶项，即使用线性函数近似原目标函数</li>
<li>Newton法：保留一阶、二阶项，即使用二次函数近似</li>
</ul>
</blockquote>
<ul>
<li><p>利用近似函数求解元素问题极小值</p>
<ul>
<li>最速下降法：<strong>线性函数无极值，需要确定步长、迭代</strong></li>
<li>Newton法：<strong>二次函数有极值，直接求导算出极值、迭代</strong></li>
</ul>
</li>
<li><p>最速下降法</p>
<ul>
<li>只考虑一阶导：甚至说根本没有考虑拟合原目标函数</li>
</ul>
</li>
<li><p>Newton法</p>
<ul>
<li>考虑二阶导：每步迭代还考虑了二阶导，即当前更新完毕
后，下一步能够更好的更新（二阶导的意义）</li>
<li>甚至从后面部分可以看出，Newton法甚至考虑是全局特征，
不只是局部性质（前提目标函数性质足够好）</li>
<li>二次函数拟合更接近函数极值处的特征</li>
</ul>
</li>
</ul>
<h2 id="最速下降算法"><a href="#最速下降算法" class="headerlink" title="最速下降算法"></a>最速下降算法</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>设$x=x(t)$为最优点$x$从初始点、沿负梯度方向经过的曲线，
则有</p>
<script type="math/tex; mode=display">\left \{ \begin{array}{l}
& \frac {dx(t)} {dt} = -\nabla f(x(t)) \\
& x(t_1) = x^{(1)}
\end{array} \right.</script><blockquote>
<ul>
<li>$t_1, x^{(1)}$：初始时刻、初始位置</li>
</ul>
</blockquote>
</li>
<li><p>可以证明，$x(t)$解存在，且$t \rightarrow \infty$时，有
$x(t) \rightarrow x^{ * }$，即得到无约束问题最优解</p>
</li>
<li><p>但微分方程组求解可能很麻烦，可能根本无法求解</p>
<ul>
<li>考虑将以上曲线离散化，每次前进到“不应该”前进为止</li>
<li>然后更换方向，逐步迭代得到最优解</li>
</ul>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>搜索方向最速下降方向：负梯度方向</li>
<li>终止准则：$\nabla f(x^{(k)})=0$</li>
</ul>
</blockquote>
<ol>
<li><p>取初始点$x^{(1)}$，置k=1</p>
</li>
<li><p>若$\nabla f(x^{(k)})=0$，则停止计算，得到最优解，
否则置</p>
<script type="math/tex; mode=display">d^{(k)} = -\nabla f(x^{(k)})</script><p>以负梯度作为前进方向</p>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) =
  f(x^{(k)} + \alpha d^{(k)})</script><p>得$\alpha_k$前进步长，置</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}</script></li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
<blockquote>
<ul>
<li>最速下降算法不具有二次终止性</li>
</ul>
</blockquote>
<h2 id="叠加惯性"><a href="#叠加惯性" class="headerlink" title="叠加惯性"></a>叠加惯性</h2><p>模拟物体运动时惯性：指数平滑更新步长</p>
<p><img src="/imgs/momentum.png" alt="momentum"></p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a><em>Momentum</em></h3><p>冲量方法：在<strong>原始更新步</strong>上叠加上次更新步，类似指数平滑</p>
<script type="math/tex; mode=display">
v^{(t)} = \gamma v^{(t-1)} + (1 - \gamma) \eta
    \bigtriangledown_\theta L(\theta^{(t-1)}) \\
\theta^{(t)} = \theta^{(t-1)} - v^{(t)}</script><blockquote>
<ul>
<li>$v^{(t)}$：第$t$步时第k个参数更新步</li>
<li>$L(\theta)$：往往是batch损失函数</li>
</ul>
</blockquote>
<ul>
<li>更新参数时，一定程度<strong>保持</strong>上次更新方向</li>
<li>可以在一定程度上保持稳定性，学习速度更快</li>
<li>能够越过部分局部最优解</li>
</ul>
<h3 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a><em>Nesterov Momentum</em></h3><p><em>NGA</em>：在使用冲量修正最终方向基础上，使用冲量对当前
<strong>参数位置</strong>进行修正，即使用“未来”位置计算梯度</p>
<ul>
<li>先使用冲量更新一步</li>
<li>再在更新后位置计算新梯度进行第二步更新</li>
</ul>
<script type="math/tex; mode=display">
v^{(t)} = \gamma v^{(t-1)} + \eta \bigtriangledown_\theta
    L(\theta^{(t-1)} - \gamma v^{(t-1)}) \\

\theta^{(t)} = \theta^{(t-1)} - v^{(t)}</script><h2 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h2><ul>
<li>学习率太小收敛速率缓慢、过大则会造成较大波动</li>
<li>在训练过程中动态调整学习率大小较好</li>
</ul>
<blockquote>
<ul>
<li>模拟退火思想：达到一定迭代次数、损失函数小于阈值时，减小
  学习速率</li>
</ul>
</blockquote>
<p><img src="/imgs/param_estimation_comparion_1.png" alt="param_estimation_comparion_1">
<img src="/imgs/param_estimation_comparion_2.png" alt="param_estimation_comparion_2"></p>
<h3 id="Vanilla-Gradient-Descent"><a href="#Vanilla-Gradient-Descent" class="headerlink" title="Vanilla Gradient Descent"></a><em>Vanilla Gradient Descent</em></h3><p>每次迭代减小学习率$\eta$</p>
<script type="math/tex; mode=display">
\eta^{(t)} = \frac \eta {\sqrt {t+1}} \\

\theta^{(t)} = \theta^{(t-1)} - \eta^{(t)}
    \bigtriangledown_\theta L(\theta^{(t-1)})</script><ul>
<li>学习率逐渐减小，避免学习后期参数在最优解附近反复震荡</li>
</ul>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a><em>Adagrad</em></h3><p><em>adaptive gradient</em>：训练中<strong>不同参数</strong>学习率随着迭代次数、
梯度动态变化，使得参数收敛更加平稳</p>
<script type="math/tex; mode=display">
v^{(t)}_k = \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\

\theta^{(t)}_k = \theta^{(t-1)}_k - \frac \eta
    {\sqrt {\sum_{i=0}^{t-1} (v^{(i)}_k)^2 + \epsilon}}
    v^{(t)}_k</script><blockquote>
<ul>
<li>$\epsilon$：fuss factor，避免分母为0</li>
<li>$\theta^{(t)}_k$：第t轮迭代完成后待估参数第k个分量
  （之前未涉及参数间不同，统一为向量）</li>
</ul>
</blockquote>
<ul>
<li><p>特点</p>
<ul>
<li>较大梯度参数真正学习率会被拉小；较小梯度真正学习率
参数被拉小幅度较小</li>
<li>可以和异步更新参数结合使用，给不常更新参数更大学习率</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>在训练后期，分母中梯度平方累加很大，学习步长趋于0，
收敛速度慢（可能触发阈值，提前结束训练）</li>
</ul>
</li>
</ul>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a><em>RMSprop</em></h3><p><em>root mean square prop</em>：指数平滑更新学习率分母</p>
<script type="math/tex; mode=display">
v^{(t)}_k = \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\

\theta^{(t)}_k = \theta^{(t-1)}_k - \frac \eta
    {\sqrt { \gamma \sum_{i=1}^{t-1}(v^{(i)}_k)^2 +
        (1 - \gamma)((v^{(t)})^2 + \epsilon}
    } v^{(t)}</script><ul>
<li>赋予当前梯度更大权重，减小学习率分母，避免学习速率下降
太快</li>
</ul>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><em>Adam</em></h3><p><em>adptive moment estimation</em>：指数平滑更新步、学习率分母</p>
<script type="math/tex; mode=display">
\begin{align*}
v^{(t)}_k & = \gamma_1 v^{(t-1)}_k + (1 - \gamma_1)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\
s^{(t)}_k & = \gamma_2 s^{(t-1)}_k + (1 - \gamma_2)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\hat{v^{(t)}_k} & = \frac {v^{(t)}_k} {1 - \gamma_1^t} \\
\hat{s^{(t)}_k} & = \frac {s^{(t)}_k} {1 - \gamma_2^t} \\

\theta^{(t)}_k & = \theta^{(t-1)}_k - \frac \eta
    {\sqrt{\hat{s^{(t)}_k} + \epsilon}} \hat{v^{(t)}_k}
\end{align*}</script><blockquote>
<ul>
<li>$\gamma_1$：通常为0.9</li>
<li>$\gamma_2$：通常为0.99</li>
<li>$\hat{v^{(t)}_k} = \frac {v^{(t)}_k} {1 - \gamma_1^t}$
  ：权值修正，使得过去个时间步，小批量随机梯度权值之和为1</li>
</ul>
</blockquote>
<ul>
<li><p>利用梯度的一阶矩$v^{(t)}$、二阶矩$s^{(t)}$动态调整每个
参数学习率</p>
</li>
<li><p>类似于<em>mommentum</em>、<em>RMSprop</em>结合</p>
</li>
<li><p>经过偏执矫正后，每次迭代学习率都有确定范围，参数比较平稳</p>
</li>
</ul>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a><em>Adadelta</em></h3><p>指数平滑更新学习率（分子）、学习率分母</p>
<script type="math/tex; mode=display">
\begin{align*}
s^{(t)}_k & = \gamma_1 s^{(t-1)}_k + (1 - \gamma_1)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\hat{v^{(t)}_k} & = \sqrt {\frac {\Delta \theta^{(t-1)}_k + \epsilon}
    {s^{(t)}_k + \epsilon}}
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\Delta \theta^{(t)}_k & = \gamma_1 \Delta \theta^{(t-1)}_k +
    (1 - \gamma_1) \hat{v^{(t)}_k}^2 \\

\theta^{(t)}_k & = \theta^{(t)}_k - \hat{v^{(t)}_k}
\end{align*}</script><blockquote>
<ul>
<li>$s, \Delta \theta$共用超参$\gamma_1$</li>
</ul>
</blockquote>
<ul>
<li>在<em>RMSprop</em>基础上，使用$\sqrt {\Delta \theta}$作为学习率</li>
<li>$\hat v$：中超参$\gamma_1$在分子、分母“抵消”，模型对
超参不敏感</li>
</ul>
<h2 id="样本量"><a href="#样本量" class="headerlink" title="样本量"></a>样本量</h2><h3 id="Singular-Loss-Stocastic-Gradient-Descent"><a href="#Singular-Loss-Stocastic-Gradient-Descent" class="headerlink" title="Singular Loss/Stocastic Gradient Descent"></a>Singular Loss/Stocastic Gradient Descent</h3><p><em>SGD</em>：用模型在某个样本点上的损失极小化目标函数、计算梯度、
更新参数</p>
<ul>
<li><p>单点损失度量模型“一次”预测的好坏</p>
<ul>
<li>代表模型在单点上的优劣，无法代表模型在总体上性质</li>
<li>具有很强随机性</li>
</ul>
</li>
<li><p>单点损失不常用，SGD范围也不局限于单点损失</p>
</li>
</ul>
<blockquote>
<ul>
<li>损失函数具体参见<em>ml_xxxxx</em></li>
</ul>
</blockquote>
<h3 id="全局估计"><a href="#全局估计" class="headerlink" title="全局估计"></a>全局估计</h3><p>全局损失：用模型在全体样本点上损失极小化目标函数、计算梯度、
更新参数</p>
<script type="math/tex; mode=display">
\theta^{(t)} = \theta^{(t-1)} - \eta \bigtriangledown_\theta
    L_{total}(\theta_{(t-1)})</script><blockquote>
<ul>
<li>$\theta^{(t)}$：第t步迭代完成后待估参数</li>
<li>$\eta$：学习率</li>
<li>$L<em>{total}(\theta) = \sum</em>{i=1}^N L(\theta, x_i, y_i)$：
  训练样本整体损失</li>
<li>$N$：训练样本数量</li>
</ul>
</blockquote>
<ul>
<li><p>若损失函数有解析解、样本量不大，可<strong>一步更新（计算）</strong>
完成（传统参数估计场合）</p>
<ul>
<li>矩估计</li>
<li>最小二乘估计</li>
<li>极大似然估计</li>
</ul>
</li>
<li><p>否则需要迭代更新参数</p>
<ul>
<li>样本量较大场合</li>
<li>并行计算</li>
</ul>
</li>
</ul>
<h3 id="Mini-Batch-Loss"><a href="#Mini-Batch-Loss" class="headerlink" title="Mini-Batch Loss"></a>Mini-Batch Loss</h3><p><em>mini-batch loss</em>：用模型在某个batch上的损失极小化目标函数、
计算梯度、更新参数</p>
<script type="math/tex; mode=display">
\theta^{(t)} = \theta^{(t-1)} - \eta \bigtriangledown_\theta
    L_{batch}(\theta^{(t-1)})</script><blockquote>
<ul>
<li>$L<em>{batch}(\theta)=\sum</em>{i \in B} L(\theta, x_i, y_i)$：
  当前batch整体损失</li>
<li>$B$：当前更新步中，样本组成的集合batch</li>
</ul>
</blockquote>
<ul>
<li><p>batch-loss是模型在batch上的特征，对整体的代表性取决于
batch大小</p>
<ul>
<li>batch越大对整体代表性越好，越稳定；越小对整体代表
越差、不稳定、波动较大、难收敛</li>
<li>batch大小为1时，就是SGD</li>
<li>batch大小为整个训练集时，就是经验（结构）风险</li>
</ul>
</li>
<li><p>batch-loss是学习算法中最常用的loss，SGD优化常指此</p>
<ul>
<li>实际中往往是使用batch-loss替代整体损失，表示经验风险
极小化</li>
<li>batch-loss同样可以带正则化项，表示结构风险极小化</li>
<li>损失极值：SVM（几何间隔最小）</li>
</ul>
</li>
</ul>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>适合样本量较大、无法使用样本整体估计使用</li>
<li>一定程度能避免局部最优（随机batch可能越过局部极值）</li>
<li>开始阶段收敛速度快</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li><p>限于每次只使用单batch中样本更新参数，batch-size较小时，
结果可能不稳定，往往很难得到最优解</p>
</li>
<li><p>无法保证良好的收敛性，学习率小收敛速度慢，学习率过大
则损失函数可能在极小点反复震荡</p>
</li>
<li><p>对所有参数更新应用相同学习率，没有对低频特征有优化
（更的学习率）</p>
</li>
<li><p>依然容易陷入局部最优点</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">5 minutes read (About 752 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/newtons.html">Newton&#039;s Method</a></h1><div class="content"><h2 id="Newton法"><a href="#Newton法" class="headerlink" title="Newton法"></a>Newton法</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>若$x^{ * }$是无约束问题局部解，则有</p>
<script type="math/tex; mode=display">\nabla f(x^{ * }) = 0</script><p>可求解此问题，得到无约束问题最优解</p>
</li>
<li><p>原始问题是非线性，考虑求解其线性逼近，在初始点$x^{(1)}$
处泰勒展开</p>
<script type="math/tex; mode=display">
\nabla f(x) \approx \nabla f(x^{(1)})
   + \nabla^2 f(x^{(1)})(x - x^{(1)})</script><p>解得</p>
<script type="math/tex; mode=display">
x^{(2)} = x^{(1)} - (\nabla^2 f(x^{(1)}))^{-1}
   \nabla f(x^{(1)})</script><p>作为$x^{ * }$的第二次近似</p>
</li>
<li><p>不断迭代，得到如下序列</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + d^{(k)}</script><blockquote>
<ul>
<li>$d^{(k)}$：Newton方向，即以下方程解<script type="math/tex; mode=display">
 \nabla^2 f(x^{(k)}) d = -\nabla
     f(x^{(k)})</script></li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ul>
<li><p>初始点 $x^{(1)}$、精度要求 $\epsilon$，置 $k=1$</p>
</li>
<li><p>考虑 $|\nabla f(x^{(k)})| \leq \epsilon$</p>
<ul>
<li>若满足，则停止计算，得到最优解 $x^{(k)}$</li>
<li><p>否则求解如下方程，得到 $d^{(k)}$</p>
<script type="math/tex; mode=display">
\nabla^2 f(x^{(k)}) d = -\nabla f(x^{(k)})</script></li>
</ul>
</li>
<li><p>如下设置，并转2</p>
<script type="math/tex; mode=display">x^{(k+1)} = x^{(k)} + d^{(k)}, k = k+1</script></li>
</ul>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>优点</p>
<ul>
<li>产生点列 ${x^{k}}$ 若收敛，则具有二阶收敛速率</li>
<li>具有二次终止性，事实上对正定二次函数，一步即可收敛</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>可能会在某步迭代时目标函数值上升</li>
<li>当初始点 $x^{(1)}$ 距离最优解 $x^{ * }$ 时，产生的点列
可能不收敛，或者收敛到鞍点</li>
<li>需要计算 <em>Hesse</em> 矩阵<ul>
<li>计算量大</li>
<li><em>Hesse</em> 矩阵可能不可逆，算法终止</li>
<li><em>Hesse</em> 矩阵不正定，<em>Newton</em> 方向可能不是下降方向</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="阻尼-修正-Newton-法"><a href="#阻尼-修正-Newton-法" class="headerlink" title="阻尼/修正 Newton 法"></a>阻尼/修正 <em>Newton</em> 法</h2><ul>
<li>克服 <em>Newton</em> 法目标函数值上升的缺点</li>
<li>一定程度上克服点列可能不收敛缺点</li>
</ul>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul>
<li><p>初始点 $x^{(1)}$、精度要求 $\epsilon$，置 $k=1$</p>
</li>
<li><p>考虑 $|\nabla f(x^{(k)})| \leq \epsilon$</p>
<ul>
<li>若满足，停止计算，得到最优解 $x^{(k)}$</li>
<li>否则求解如下方程得到 $d^{(k)}$<script type="math/tex; mode=display">
\nabla^2 f(x^{(k)}) d = -\nabla f(x^{(k)})</script></li>
</ul>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) = f(x^{(k)} + \alpha d^{(k)})</script><p>得到 $\alpha_k$，如下设置并转2</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}, k = k+1</script></li>
</ul>
<h2 id="其他改进"><a href="#其他改进" class="headerlink" title="其他改进"></a>其他改进</h2><ul>
<li><em>Newton</em> 法、修正 <em>Newton</em> 法的改进方向<ul>
<li>结合最速下降方向修正迭代方向</li>
<li><em>Hesse</em> 矩阵不正定情形下的替代</li>
</ul>
</li>
</ul>
<h3 id="结合最速下降方向"><a href="#结合最速下降方向" class="headerlink" title="结合最速下降方向"></a>结合最速下降方向</h3><blockquote>
<ul>
<li>将 <em>Newton</em> 方向和最速下降方向结合</li>
</ul>
</blockquote>
<ul>
<li><p>设 $\theta_k$ 是 $<d^{(k)}, -\nabla f(x^{(k)})>$ 之间夹角，显然希望 $\theta &lt; \frac \pi 2$</p>
</li>
<li><p>则置限制条件 $\eta$，取迭代方向</p>
<script type="math/tex; mode=display">
d^{(k)} = \left \{ \begin{array}{l}
   d^{(k)}, & cos\theta_k \geq \eta \\
   -\nabla f(x^{(k)}), & 其他
\end{array} \right.</script></li>
</ul>
<h3 id="Negative-Curvature"><a href="#Negative-Curvature" class="headerlink" title="Negative Curvature"></a><em>Negative Curvature</em></h3><blockquote>
<ul>
<li>当 <em>Hesse</em> 矩阵非正定时，选择负曲率下降方向 $d^{(k)}$（一定存在）</li>
</ul>
</blockquote>
<ul>
<li><p><em>Hesse</em> 矩阵非正定时，一定存在负特征值、相应特征向量 $u$</p>
<ul>
<li><p>取负曲率下降方向作为迭代方向</p>
<script type="math/tex; mode=display">
d^{(k)} = -sign(u^T \nabla f(x^{(k)})) u</script></li>
<li><p>$x^{(k)}$ 处负曲率方向 $d^{(k)}$ 满足</p>
<script type="math/tex; mode=display">
{d^{(k)}}^T \nabla^2 f(x^{(k)}) d^{(k)} < 0</script></li>
</ul>
</li>
</ul>
<h3 id="修正-Hesse-矩阵"><a href="#修正-Hesse-矩阵" class="headerlink" title="修正 Hesse 矩阵"></a>修正 <em>Hesse</em> 矩阵</h3><ul>
<li><p>取迭代方向 $d^{(k)}$ 为以下方程的解</p>
<script type="math/tex; mode=display">
(\nabla^2 f(x^{(k)}) + v_k I) d = -\nabla f(x^{k})</script></li>
</ul>
<blockquote>
<ul>
<li>$v_k$：大于 $\nabla^2 f(x^{(k)})$ 最大负特征值绝对值</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-04-22T17:32:09.000Z" title="4/23/2019, 1:32:09 AM">2019-04-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-04-22T17:32:09.000Z" title="4/23/2019, 1:32:09 AM">2019-04-23</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">12 minutes read (About 1864 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/quasi_newtons.html">Quasi-Newton Method/Variable Metric Method</a></h1><div class="content"><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>拟Newton法/变度量法：不需要求解Hesse矩阵，使用一阶导构造
二阶信息的近似矩阵</p>
<ul>
<li><p>使用迭代过程中信息，创建近似矩阵$B^{(k)}$代替Hesse矩阵</p>
</li>
<li><p>用以下方程组替代Newton方程，其解$d^{(k)}$作为搜索方向</p>
<script type="math/tex; mode=display">
B^{(k)} d = - \triangledown f(x^{(k)})</script></li>
</ul>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>考虑$\triangledown f(x)$在$x^{(k+1)}$处泰勒展开</p>
<script type="math/tex; mode=display">
\triangledown f(x) \approx \triangledown f(x^{(k+1)})
   + \triangledown^2 f(x^{(k+1)})(x - x^{(k+1)})</script></li>
<li><p>取$x = x^{(k)}$，有</p>
<script type="math/tex; mode=display">\begin{align*}
\triangledown f(x^{(k+1)}) - \triangledown f(x^{(k)})
   & \approx \triangledown^2 f(x^{(x+1)})
   (x^{(k+1) } - x^{(k)}) \\
\triangledown^2 f(x^{k+1}) s^{(k)} & \approx y^{(k)}
\end{align*}</script><blockquote>
<ul>
<li>$s^{(k)} = x^{(k+1)} - x^{(k)}$</li>
<li>$y^{(k)} = \triangledown f(x^{(k+1)}) - \triangledown f(x^{(k)})$</li>
</ul>
</blockquote>
</li>
<li><p>要求$B^{(k)}$近似$\triangledown^2 f(x^{(k)})$，带入并将
$\approx$改为$=$，得到拟Newton方程</p>
<script type="math/tex; mode=display">
B^{(k+1)} s^{(k)} = y^{(k)}</script><p>并假设$B^{(k)}$对称</p>
</li>
<li><p>拟Newton方程不能唯一确定$B^{(k+1)}$，需要附加条件，自然
的想法就是$B^{(k+1)}$可由$B^{(k)}$修正得到，即</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} + \Delta B^{(k)}</script><p>且修正项$\Delta B^{(k)}$具有“简单形式”</p>
</li>
</ul>
<h2 id="Hesse矩阵修正"><a href="#Hesse矩阵修正" class="headerlink" title="Hesse矩阵修正"></a>Hesse矩阵修正</h2><h3 id="对称秩1修正"><a href="#对称秩1修正" class="headerlink" title="对称秩1修正"></a>对称秩1修正</h3><p>认为简单指矩阵秩小：即认为$\Delta B^{(k)}$秩为最小值1</p>
<ul>
<li><p>设$\Delta B^{(k)} = u v^T$，带入有</p>
<script type="math/tex; mode=display">\begin{align*}
y^{(k)} & = B^{(k+1)} s^{(k)} \\
& = B^{(k)} s^{(k)} + (v^T s^{(k)}) u \\
y^{(k)} - B^{(k)} s^{(k)} & = (v^T s^{(k)}) u
\end{align*}</script><ul>
<li>这里有的书会设$\Delta B^{(k)} = \alpha u v^T$，
其实对向量没有必要</li>
<li>$v^T s^{(k)}$是数，所以$u$必然与共线，同理也没有必要
考虑系数，直接取相等即可</li>
<li>而且系数不会影响最终结果</li>
</ul>
</li>
<li><p><strong>可取</strong>$u = y^{(k)} - B^{(k)} s{(k)}$，取$v$满足
$v^T s^{(k)}  = 1$</p>
</li>
<li><p>由$B^{(k)}$的对称性、并希望$B^{(k+1)}$保持对称，需要
$u, v$共线，则有</p>
<script type="math/tex; mode=display">\begin{align*}
v & = \lambda u = \lambda (y^{(k)} - B^{(k)} s^{(k)}) \\
1 & = \lambda (y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)}
\end{align*}</script></li>
<li><p>得到$B^{(k)}$的对称秩1修正公式</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} + \frac {(y^{(k) - B^{(k)} s^{(k)}})
   (y^{(k)} - B^{(k)} s^{(k)})^T}
   {(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)}}</script></li>
</ul>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始点$x^{(1)}$、初始矩阵$B^{(1)} = I$、精度要求
$\epsilon$、置$k=1$</p>
</li>
<li><p>若$|\triangledown f(x^{(k)})| \leq \epsilon$，停止计算
，得到解$x^{(k)}$，否则求解以下方程得到$d^{(k)}$</p>
<script type="math/tex; mode=display">
B^{(k)} d = -\triangle f(x^{(k)})</script></li>
<li><p>一维搜索，求解</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha)=f(x^{(k)} + \alpha d^{(k)})</script><p>得到$\alpha_k$，置$x^{(k+1)}=x^{(k)} + \alpha_k d^{(k)}$</p>
</li>
<li><p>修正$B^{(k)}$</p>
<script type="math/tex; mode=display">\begin{align*}
s^{(k)} & = x^{(k+1)} - x^{(k)} \\
y^{(k)} & = \triangledown f(x^{(k+1)}) -
  \triangledown f(x^{(k)}) \\
B^{(k+1)} & = B^{(k)} + \frac {(y^{(k) - B^{(k)} s^{(k)}})
  (y^{(k)} - B^{(k)} s^{(k)})^T}
  {(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)}}
\end{align*}</script></li>
<li><p>置$k = k+1$，转2</p>
</li>
</ol>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>缺点</p>
<ul>
<li><p>要求$(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)} \neq 0$，
否则无法继续计算</p>
</li>
<li><p>不能保证正定性传递，只有
$(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)} &gt; 0$才能保证
$B^{(k+1)}$也正定</p>
</li>
<li><p>即使$(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)} &gt; 0$，
也可能很小，容易产生较大的舍入误差</p>
</li>
</ul>
</li>
</ul>
<h3 id="对称秩2修正"><a href="#对称秩2修正" class="headerlink" title="对称秩2修正"></a>对称秩2修正</h3><ul>
<li><p>为克服秩1修正公式缺点，考虑$\Delta B^{(k)}$秩为2，设</p>
<script type="math/tex; mode=display">
\Delta B^{(k)} = u^{(1)} (v^{(1)})^T
   + u^{(2)} (v^{(2)})^T</script></li>
<li><p>带入拟Newton方程有</p>
<script type="math/tex; mode=display">
B^{(k)} s^{(k)} + ((v^{(1)})^T s^{(k)}) u^{(1)} +
   ((v^{(2)})^T s^{(k)}) u^{(2)} = y^{(k)}</script></li>
<li><p>类似的取</p>
<script type="math/tex; mode=display">\left \{ \begin{array}{l}
u^{(1)} = y^{(k)} \\
(v^{(1)})^T s^{(k)} = 1
\end{array} \right.</script><script type="math/tex; mode=display">\left \{ \begin{array}{l}
u^{(2)} = -B^{(k)} s^{(k)} \\
(v^{(2)})^T s^{(k)} = 1
\end{array} \right.</script></li>
<li><p>同秩1公式保持对称性推导，得到对称秩2修正公式/BFGS公式</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} - \frac {B^{(k)} s^{(k)}
   (s^{(k)})^T B^{(k)}} {(s^{(k)})^T B^{(k)} s^{(k)}}
   + \frac {y^{(k)} (y^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script></li>
</ul>
<h3 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h3><p>类似同秩1修正算法，仅第4步使用对称秩2修正公式</p>
<h2 id="Hesse逆修正"><a href="#Hesse逆修正" class="headerlink" title="Hesse逆修正"></a>Hesse逆修正</h2><h3 id="对称秩2修正-1"><a href="#对称秩2修正-1" class="headerlink" title="对称秩2修正"></a>对称秩2修正</h3><ul>
<li><p>考虑直接构造近似于$(\triangledown^2 f(x^{(k)}))^{-1}$的
矩阵$H^{(k)}$</p>
</li>
<li><p>这样无需求解线性方程组，直接计算</p>
<script type="math/tex; mode=display">
d^{(k)} = -H^{(k)} \triangledown f(x^{(k)})</script></li>
<li><p>相应拟Newton方程为</p>
<script type="math/tex; mode=display">
H^{(k+1)} y^{(k)} = s^{(k)}</script></li>
<li><p>可得$H^{(k)}$的对称秩1修正公式</p>
<script type="math/tex; mode=display">
H^{(k+1)} = H^{(k)} + \frac {(s^{(k)} - H^{(k)} y^{(k)})
   (s^{(k)} - H^{(k)} y^{(k)})T}
   {(s^{(k)} - H^{(k)} y^{(k)})^T y^{(k)}}</script></li>
<li><p>可得$H^{(k)}$的对称秩2修正公式/DFP公式</p>
<script type="math/tex; mode=display">
H^{(k+1)} = H^{(k)} - \frac {H^{(k)} y^{(k)} (y^{(k)})^T
   H^{(k)}} {(y^{(k)})^T H^{(k)} y^{(k)}} +
   \frac {s^{(k)} (s^{(k)})^T} {(s^{(k)})^T y^{(k)}}</script></li>
</ul>
<h4 id="DFP算法"><a href="#DFP算法" class="headerlink" title="DFP算法"></a>DFP算法</h4><p>类似BFGS算法，只是</p>
<ul>
<li>使用$H^{(k)}$计算更新方向</li>
<li>使用$H^{(k)}$的对称秩2修正公式修正</li>
</ul>
<blockquote>
<ul>
<li>对正定二次函数，BFGS算法和DFP算法效果相同</li>
<li>对一般可微（非正定二次函数），一般认为BFGS算法在收敛性质
  、数值计算方面均由于DFP算法</li>
</ul>
</blockquote>
<h3 id="Hesse逆的BFGS算法"><a href="#Hesse逆的BFGS算法" class="headerlink" title="Hesse逆的BFGS算法"></a>Hesse逆的BFGS算法</h3><ul>
<li><p>考虑</p>
<script type="math/tex; mode=display">\begin{align*}
B^{(k+1)} & = B^{(k)} + u^{(1)} (v^{(1)})^T +
   u^{(2)} (v^{(2)})^T \\
H^{(k+1)} & = (B^{(k+1)})^{-1} \\
& = (B^{(k)} + u^{(1)} (v^{(1)})^T + u^{(2)}
   (v^{(2)})^T)^{-1} \\
\end{align*}</script></li>
<li><p>两次利用<em>Sherman-Morrison</em>公式，可得</p>
<script type="math/tex; mode=display">
H^{(k+1)} = (I - \frac {s^{(k)} (y^{(k)})^T} 
   {(y^{(k)})^T s^{(k)}})
   H^{(k)}
   (I - \frac {s^{(k)} (y^{(k)})^T}
       {(y^{(k)})^T s^{(k)}})^T
   + \frac {s^{(k)} (s^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script></li>
</ul>
<h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><ul>
<li><p>还可以进一步展开</p>
<script type="math/tex; mode=display">
H^{(k+1)} = H^{(k)} + (\frac 1 {(s^{(k)})^T y^{(k)}} +
   \frac {(y^{(k)})^T H^{(k)} y^{(k)}}
   {((s^{(k)})^T y^{(k)})^2}) s^{(k)} (s^{(k)})^T
   - \frac 1 {(s^{(k)})^T y^{(k)}}
   (H^{(k)} y^{(k)} (s^{(k)})^T +
   s^{(k)} (y^{(k)})^T H^{(k)})</script></li>
</ul>
<h2 id="变度量法的基本性质"><a href="#变度量法的基本性质" class="headerlink" title="变度量法的基本性质"></a>变度量法的基本性质</h2><h3 id="算法的下降性"><a href="#算法的下降性" class="headerlink" title="算法的下降性"></a>算法的下降性</h3><h4 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h4><blockquote>
<ul>
<li>设$B^{(k)}$（$H^{(k)}$）是正定对称矩阵，且有
  $(s^{(k)})^T y^{(k)} &gt; 0$，则由BFGS（DFS）公式构造的
  $B^{(k+1)}$（$H^{(k+1)}$）是正定对称的</li>
</ul>
</blockquote>
<ul>
<li><p>考虑$B^{(k)}$对称正定，有
$B^{(k)} = (B^{(k)})^{1/2} (B^{(k)})^{1/2}$</p>
</li>
<li><p>带入利用柯西不等式即可证</p>
</li>
</ul>
<blockquote>
<ul>
<li>中间插入正定矩阵的向量内积不等式也称为广义柯西不等式</li>
</ul>
</blockquote>
<h4 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h4><blockquote>
<ul>
<li>若$d^{(k)}$v是下降方向，且<strong>一维搜索是精确的</strong>，设
  $B^{(k)}$（$H^{(k)}$）是正定对称矩阵，则有BFGS（DFP）
  公式构造的$B^{(k+1)}$（$H^{(k+1)}$）是正定对称的</li>
</ul>
</blockquote>
<ul>
<li>精确一维搜索$(d^{(k)})^T \triangledown f(x^{(k+1)}) = 0$</li>
<li>则有$(s^{(k)})^T y^{(k)} &gt; 0$</li>
</ul>
<h4 id="定理3"><a href="#定理3" class="headerlink" title="定理3"></a>定理3</h4><blockquote>
<ul>
<li>若用BFGS算法（DFP算法）求解无约束问题，设初始矩阵
  $B^{(1)}$（$H^{(1)}$）是正定对称矩阵，且一维搜索是精确的
  ，若$\triangledown f(x^{(k)}) \neq 0$，则产生搜索方向
  $d^{(k)}$是下降方向</li>
</ul>
</blockquote>
<ul>
<li>结合上2个结论，数学归纳法即可</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li><p>若每步迭代一维搜索精确，或满足$(s^{(k)})^T y^{(k)} &gt; 0$</p>
<ul>
<li>停止在某一稳定点</li>
<li>或产生严格递减的序列${f(x^{(k)})}$</li>
</ul>
</li>
<li><p>若目标函数满足一定条件我，可以证明变度量法产生的点列
${x^{(k)}}$收敛到极小点，且收敛速率超线性</p>
</li>
</ul>
<h3 id="搜索方向共轭性"><a href="#搜索方向共轭性" class="headerlink" title="搜索方向共轭性"></a>搜索方向共轭性</h3><blockquote>
<ul>
<li>用变度量法BFGS（DFP）算法求解正定二次函数</li>
</ul>
</blockquote>
<pre><code>$$
min f(x) = \frac 1 2 x^T G x + r^T x + \sigma
$$

若一维搜索是精确的，假设已经进行了m次迭代，则
</code></pre><blockquote>
<ul>
<li><p>搜索方向$d^{(1)}, \cdots, d^{(m)}$是m个非零的G共轭方向</p>
</li>
<li><p>对于$j = 1, 2, \cdots, m$，有</p>
</li>
</ul>
</blockquote>
<pre><code>$$
B^&#123;(m+1)&#125; s^&#123;(j)&#125; = y^&#123;(j)&#125;
(H^&#123;(m+1)&#125; y^&#123;(j)&#125; = s^&#123;(j)&#125;)
$$

且$m = n$时有吧

$$
B^&#123;(n+1)&#125; = G(H^&#123;(n+1)&#125; = G^&#123;-1&#125;)
$$
</code></pre><h3 id="变度量法二次终止"><a href="#变度量法二次终止" class="headerlink" title="变度量法二次终止"></a>变度量法二次终止</h3><blockquote>
<ul>
<li>若一维搜索是精确的，则变度量法（BFGS、DFP）具有二次终止</li>
</ul>
</blockquote>
<ul>
<li><p>若$\triangle f(x^{(k)}) = 0, k \leq n$，则得到最优解
$x^{(k)}$</p>
</li>
<li><p>否则得到的搜索方向是共轭的，由扩展空间子定理，
$x^{(n+1)}$是最优解</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:25:02.000Z" title="8/4/2021, 11:25:02 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">11 minutes read (About 1640 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/conjugate_gradient.html">Conjugate Gradient Method</a></h1><div class="content"><h2 id="共轭方向"><a href="#共轭方向" class="headerlink" title="共轭方向"></a>共轭方向</h2><blockquote>
<ul>
<li><p>设G为$n * n$阶正定对称矩阵，若$d^{(1)}, d^{(2)}$满足</p>
<script type="math/tex; mode=display">(d^{(1)})^T G d^{(2)} = 0</script><p>  则称$d^{(1)}, d^{(2)}$关于G共轭</p>
</li>
<li><p>类似正交方向，若$d^{(1)},\cdots,d^{(k)}(k \leq n)$关于
  G两两共轭，则称其为G的k个共轭方向</p>
</li>
</ul>
</blockquote>
<ul>
<li>特别的，$G=I$时，共轭方向就是正交方向</li>
</ul>
<h3 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h3><blockquote>
<ul>
<li>设目标函数为<script type="math/tex; mode=display">
  f(w) = \frac 1 2 w^T w + r^T w + \sigma</script>  $q^{(1)}, \cdots, q^{(k)}$是$k, k \leq n$个非零正交方向
  ，从任意初始点$w^{(1)}$出发，依次沿着以上正交方向做
  <strong>精确一维搜索</strong>，得到$w^{(1)}, \cdots, w^{(k+1)}$，
  则$w^{(k+1)}$是$f(w)$在线性流形<script type="math/tex; mode=display">
  \bar W_k = \{w = w^{(1)} + \sum_{i=1}^k \alpha_i q^{(i)}
      | -\infty < \alpha_i < +\infty \}</script>  上的唯一极小点，特别的k=n时，$w^{(n+1)}$是$f(w)$在整个
  空间上的唯一极小点</li>
</ul>
</blockquote>
<ul>
<li><p>$\bar W_k$上的存在唯一极小点$\hat w^{(k)}$，在所有方向
都是极小点，所以有</p>
<script type="math/tex; mode=display">
<\triangledown f(\hat w^{(k)}), q^{(i)}> = 0, i=1,2,..</script></li>
<li><p>将$\hat w^{(k)}$由正交方向表示带入梯度，求出系数表达式</p>
</li>
<li><p>解精确搜索步长，得到$w^{(k+1)}$系数表达式</p>
</li>
</ul>
<h3 id="扩展子空间定理"><a href="#扩展子空间定理" class="headerlink" title="扩展子空间定理"></a>扩展子空间定理</h3><blockquote>
<ul>
<li>设目标函数为<script type="math/tex; mode=display">
  f(w) = \frac 1 2 x^T G x + r^T x + \sigma</script>  $d^{(1)}, \cdots, d^{(k)}$是$k, k \leq n$个非零正交方向
  ，从任意初始点$x^{(1)}$出发，依次沿着以上正交方向做
  <strong>精确一维搜索</strong>，得到$x^{(1)}, \cdots, x^{(k+1)}$，
  则$x^{(k+1)}$是$f(x)$在线性流形<script type="math/tex; mode=display">
  \bar x_k = \{x = x^{(1)} + \sum_{i=1}^k \alpha_i d^{(i)}
      | -\infty < \alpha_i < +\infty \}</script>  上的唯一极小点，特别的k=n时，$x^{(n+1)}$是$f(x)$在整个
  空间上的唯一极小点</li>
</ul>
</blockquote>
<ul>
<li>引进变换$w = \sqrt G x$即可证</li>
</ul>
<blockquote>
<ul>
<li>在以上假设下，有<script type="math/tex; mode=display">
  <\triangledown f(x^{(k+1)}), d^{(i)}> = 0, i=1,2...</script></li>
</ul>
</blockquote>
<h2 id="Conjugate-Gradient-Method"><a href="#Conjugate-Gradient-Method" class="headerlink" title="Conjugate Gradient Method"></a><em>Conjugate Gradient Method</em></h2><p>共轭梯度法</p>
<h3 id="对正定二次函数函数"><a href="#对正定二次函数函数" class="headerlink" title="对正定二次函数函数"></a>对正定二次函数函数</h3><script type="math/tex; mode=display">
f(x) = \frac 1 2 x^T G x + r^T x + \sigma</script><ul>
<li><p>任取初始点$x^{(1)}$，若$\triangledown f(x^{(1)}) = 0$，
停止计算，得到极小点$x^{(1)}$，否则取</p>
<script type="math/tex; mode=display">
d^{(1)} = -\triangledown f(x^{(1)})</script></li>
<li><p>沿着$d^{(1)}$方向进行精确一维搜索得到$x^{(2)}$，若
$\triangledown f(x^{(2)}) \neq 0$，令</p>
<script type="math/tex; mode=display">
d^{(2)} = -\triangledown f(x^{(2)}) + \beta_1^{(2)}
   d^{(1)}</script><p>且满足$(d^{(1)})^T G d^{(2)} = 0$，即二者共轭，可得</p>
<script type="math/tex; mode=display">
\beta_1^{(2)} = \frac {(d^{(1)})^T G \triangledown
   f(x^{(2)})} {((d^{(1)})^T G d^{(1)})}</script><ul>
<li>这里$d^{(2)}$方向的构造方式是为类似构造后面$d^{(k)}$
，得到能方便表示的系数</li>
<li>类似于将向量组$\triangledown f(x^{(i)})$正交化</li>
</ul>
</li>
<li><p>如此重复搜索，若$\triangledown f^(x^{i)}) \neq 0$，构造
$x^{(k)}$处搜索方向$d^{(k)}$如下</p>
<script type="math/tex; mode=display">\begin{align*}
0 & = (d^{(i)})^T G d^{(k)} \\
& = -(d^{(i)})T G \triangledown f(x^{(k)}) +
   \sum_{j=1}^{k-1} \beta_j^{(k)} (d^{(i)})^T G d^{(j)} \\
& = -(d^{(i)})^T G \triangledown f(x^{(k)}) +
   \beta_i^{(k)} (d^{(i)})^T G d^{(i)}
\end{align*}</script><p>可得</p>
<script type="math/tex; mode=display">
\beta_i^{(k)} = \frac {(d^{(i)})^T G \triangledown
   f(x^{(k)})} {(d^{(i)})^T G d^{(i)}}</script><p>此时$d^{(k)}$与前k-1个方向均关于G共轭，此k个方向是G的k个
共轭方向，由扩展空间子定理，$x^{(k+1)}$是整个空间上极小</p>
</li>
</ul>
<h4 id="计算公式简化"><a href="#计算公式简化" class="headerlink" title="计算公式简化"></a>计算公式简化</h4><p>期望简化$d^{(k)}$的计算公式</p>
<ul>
<li><p>由扩展子空间定理推论有
$\triangledown f(x^{(k)})^T d^{(i)} = 0, i=1,2…,k-1$
结合以上$d^{(k)}$的构造公式，有</p>
<script type="math/tex; mode=display">\begin{align*}
& \triangledown f(x^{(k)})^T \triangledown f(x^{(i)}) \\
= & \triangledown f(x^{(k)})^T ( -d^{(i)} +
   \beta_1^{(i)} d^{(1)} + \cdots +
   \beta_{i-1}^{(i)} d^{(i-1)} ) \\
= & 0, i=1,2,...,k-1
\end{align*}</script></li>
<li><p>则有</p>
<script type="math/tex; mode=display">\begin{align*}
(d^{(i)})^T G \triangledown f(x^{(k)}) & =
   \triangledown f(x^{(k)})^T G d^{(i)} \\
& = \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   G (x^{(i+1)} - x^{(i)}) \\
& = \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   (\triangledown f(x^{(i+1)}) -
   \triangledown f(x^{(i)})) \\
& = 0, i=1,2,\cdots,k-2
\end{align*}</script><blockquote>
<ul>
<li>$d^{(k)} = \frac 1 {\alpha_i} x^{(i+1)} - x^{(i)}$</li>
</ul>
</blockquote>
</li>
<li><p>所以上述$d^{(k)}$构造公式可以简化为</p>
<script type="math/tex; mode=display">
d^{(k)} = -\triangledown f(x^{(k)}) + \beta_{k-1}
   d^{(k-1)}</script></li>
<li><p>类似以上推导有</p>
<script type="math/tex; mode=display">\begin{align*}
(d^{(k-1)})^T G \triangledown f(x^{(k)}) & =
   \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)})) \\
& = \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   \triangledown f(x^{(k)}) \\
\end{align*}</script><script type="math/tex; mode=display">\begin{align*}
(d^{(k-1)})^T G d^{(k-1)} & = \frac 1 {\alpha_i}
   (d^{(k-1)})^T (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)})) \\
& = -\frac 1 {\alpha_i} (d^{(k-1)})^T
   \triangledown f(x^{(x-1)}) \\
& = -\frac 1 {\alpha_i} (\triangledown f(x^{(k-1)}) -
   \beta_{k-2}d^{(k-2)})^T \triangledown f(x^{(x-1)}) \\
& = -\frac 1 {\alpha_i} \triangledown f(x^{(k-1)})^T
   \triangledown f(x^{(k-1)})
\end{align*}</script><p>最终的得到简化后系数$\beta_{k-1}, k&gt;1$的PRP公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\triangledown f(x^{(k)})^T
   (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)}))}
   {\triangledown f(x^{(k-1)})^T
       \triangledown f(x^{(k-1)})}</script><p>或FR公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\|\triangledown f(x^{(k)})\|^2}
   {\|\triangledown f(x^{(k-1)}) \|^2}</script></li>
</ul>
<blockquote>
<ul>
<li><p>以上推导虽然是根据正定二次函数得出的推导，但是仍适用于
  一般可微函数</p>
</li>
<li><p>$\beta _ {k-1}$给出两种计算方式，应该是考虑到目标函数
  可能不是标准正定二次函数、一维搜索数值计算不精确性</p>
</li>
<li><p>将$\beta _ {k-1}$分子、分母推导到不同程度可以得到其他
  公式</p>
</li>
</ul>
</blockquote>
<ul>
<li><p>Growder-Wolfe公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\triangledown f(x^{(k)})^T
   (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)}))}
   {(d^{(k-1)})^T (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)}))}</script></li>
<li><p>Dixon公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\triangledown f(x^{(k)})^T
   \triangledown f(x^{(k)})}
   {(d^{(k-1)})^T \triangledown f(x^{(k-1)})}</script></li>
</ul>
<h3 id="FR-PRP算法"><a href="#FR-PRP算法" class="headerlink" title="FR/PRP算法"></a>FR/PRP算法</h3><ol>
<li><p>初始点$x^{(1)}$、精度要求$\epsilon$，置k=1</p>
</li>
<li><p>若$|\triangledown f(x^{(k)}) | \leq \epsilon$，停止
计算，得到解$x^{(k)}$，否则置</p>
<script type="math/tex; mode=display">
d^{(k)} = -\triangledown f(x^{(k)}) + \beta_{k-1}d^{(k-1)}</script><p>其中$\beta_{k-1}=0, k=1$，或由上述公式计算</p>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) = f(x^{(k)} -
  \alpha d^{(k)})</script><p>得$\alpha_k$，置$x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}$</p>
</li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
<blockquote>
<ul>
<li>实际计算中，n步重新开始的FR算法优于原始FR算法</li>
<li>PRP算法中
  $\triangledown f(x^{(k)}) \approx \triangledown f(x^{(k-1)})$
  时，有$\beta_{k-1} \approx 0$，即
  $d^{(k)} \approx -\triangledown f(x^{(k)})$，自动重新开始</li>
<li>试验表明，对大型问题，PRP算法优于FR算法</li>
</ul>
</blockquote>
<h3 id="共轭方向下降性"><a href="#共轭方向下降性" class="headerlink" title="共轭方向下降性"></a>共轭方向下降性</h3><blockquote>
<ul>
<li>设$f(x)$具有连续一阶偏导，假设一维搜索是精确的，使用共轭
  梯度法求解无约束问题，若$\triangledown f(x^{(k)}) \neq 0$
  则搜索方向$d^{(k)}$是$x^{(k)}$处的下降方向</li>
</ul>
</blockquote>
<ul>
<li>将$d^{(k)}$导入即可</li>
</ul>
<h3 id="算法二次终止性"><a href="#算法二次终止性" class="headerlink" title="算法二次终止性"></a>算法二次终止性</h3><blockquote>
<ul>
<li>若一维搜索是精确的，则共轭梯度法具有二次终止性</li>
</ul>
</blockquote>
<ul>
<li><p>对正定二次函数，共轭梯度法至多n步终止，否则</p>
<ul>
<li>目标函数不是正定二次函数</li>
<li>或目标函数没有进入正定二次函数区域，</li>
</ul>
</li>
<li><p>此时共轭没有意义，搜索方向应该重新开始，即令</p>
<script type="math/tex; mode=display">
d^{(k)} = -\triangledown f(x^{(k)})</script><p>即算法每n次重新开始一次，称为n步重新开始策略</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-03-16T02:42:12.000Z" title="3/16/2019, 10:42:12 AM">2019-03-16</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">14 minutes read (About 2091 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/line_search.html">Line Search</a></h1><div class="content"><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>一维搜索/线搜索：单变量函数最优化，即求一维问题</p>
<script type="math/tex; mode=display">
\arg\min _ {\alpha} \phi(\alpha) = f(x^{(k)} +
    \alpha d^{(k)})</script><p>最优解的$\alpha_k$的数值方法</p>
<ul>
<li><p><em>exact line search</em>：精确一维搜索，求得<strong>最优步长</strong>
$\alpha_k$使得目标函数沿着$d^{(k)}$方向达到极小，即</p>
</li>
<li><p><em>inexact line search</em>：非精确一维搜索，求得$\alpha_k$
使得</p>
<script type="math/tex; mode=display">\begin{align*}
f(x^{(k)} + \alpha_k d^{(k)}) & < f(x^{(k)}) \\
\phi(\alpha_k) & < \phi(0)
\end{align*}</script></li>
</ul>
<h3 id="一维搜索基本结构"><a href="#一维搜索基本结构" class="headerlink" title="一维搜索基本结构"></a>一维搜索基本结构</h3><ul>
<li>确定搜索区间</li>
<li>用某种方法缩小搜索区间</li>
<li>得到所需解</li>
</ul>
<h3 id="搜索区间"><a href="#搜索区间" class="headerlink" title="搜索区间"></a>搜索区间</h3><blockquote>
<ul>
<li>搜索区间：设$\alpha^{ <em> }$是$\phi(\alpha)$极小点，若存在
  <strong>闭区间</strong>$[a, b]$使得$\alpha^{ </em> } \in [a, b]$，则称
  $[a, b]$是$phi(\alpha)$的搜索区间</li>
</ul>
</blockquote>
<h4 id="确定搜索区间的进退法"><a href="#确定搜索区间的进退法" class="headerlink" title="确定搜索区间的进退法"></a>确定搜索区间的进退法</h4><ol>
<li><p>取初始步长$\alpha$，置初始值</p>
<script type="math/tex; mode=display">
\mu_3 = 0, \phi_3 = \phi(\mu_3), k = 0</script></li>
<li><p>置</p>
<script type="math/tex; mode=display">
\mu = \mu_3 + \alpha, \phi = \phi(\mu), k = k+1</script></li>
<li><p>若$\phi &lt; \phi_3$，置</p>
<script type="math/tex; mode=display">
\mu_2 = \mu_3, \phi_2 = \phi_3 \\
\mu_3 = \mu, \phi_3 = \phi \\
\alpha = 2\alpha, k = k+1</script></li>
<li><p>若k =1，置</p>
<script type="math/tex; mode=display">
\mu_2 = mu, \phi_2 = \phi, \alpha = -\alpha</script><p>转2，否则置</p>
<script type="math/tex; mode=display">
\mu_1 = \mu_2, \phi_1 = \phi_2 \\
\mu_2 = \mu_3, \phi_2 = \phi_3 \\
\mu_3 = \mu, \phi_3 = \phi</script><p>并令$a=min{\mu_1,\mu_3}, b=max{\mu_1,\mu_3}$，停止搜索</p>
</li>
</ol>
<blockquote>
<ul>
<li>通常认为目标函数此算法得到搜索区间就是单峰函数</li>
</ul>
</blockquote>
<h2 id="试探法"><a href="#试探法" class="headerlink" title="试探法"></a>试探法</h2><ul>
<li>在搜索区间内选择<strong>两个点</strong>，计算目标函数值<ul>
<li>需要获得两个点取值才能判断极值点的所属区间</li>
</ul>
</li>
<li>去掉<strong>函数值较大者至离其较近端点</strong>段</li>
</ul>
<h3 id="0-618法"><a href="#0-618法" class="headerlink" title="0.618法"></a>0.618法</h3><ol>
<li><p>置初始搜索区间$[a, b]$，置精度要求$\epsilon$，计算左右
试探点</p>
<script type="math/tex; mode=display">\begin{align*}
a_l & = a + (1 - \tau)(b - a) \\
a_r & = a + \tau(b - a)
\end{align*}</script><p>其中$\tau = \frac {\sqrt 5 - 1} 2$，及相应函数值</p>
<script type="math/tex; mode=display">\begin{align*}
\phi_l & = \phi(a_l) \\
\phi_r & = \phi(a_r)
\end{align*}</script></li>
<li><p>若$\phi_l&lt;\phi_r$，置</p>
<script type="math/tex; mode=display">
b= a_r, a_r = a_l, \phi_l = \phi_l</script><p>并计算</p>
<script type="math/tex; mode=display">
a_l = a + (1 - \tau)(b - a), \phi_l = \phi(a_l)</script><p>否则置</p>
<script type="math/tex; mode=display">
a = a_l, a_l = a_r, \phi_l = \phi_r</script><p>并计算</p>
<script type="math/tex; mode=display">
a_r = a + \tau(b - a), \phi_r = \phi(a_r)</script></li>
<li><p>若$|b - a| \geq \epsilon$</p>
<ul>
<li>若$\phi_l &lt; \phi_r$，置$\mu = a_l$</li>
<li>否则置$\mu = \alpha_r$
得到问题解$\mu$，否则转2</li>
</ul>
</li>
</ol>
<ul>
<li>0.618法除第一次外，每次只需要计算一个新试探点、一个新
函数值，大大提高了算法效率</li>
<li>收敛速率线性，收敛比为$\tau = \frac {\sqrt 5 - 1} 2$常数</li>
</ul>
<h3 id="Fibonacci方法"><a href="#Fibonacci方法" class="headerlink" title="Fibonacci方法"></a>Fibonacci方法</h3><ol>
<li><p>置初始搜索区间$[a, b]$，置精度要求$\epsilon$，选取分离
间隔$\sigma &lt; \epsilon$，求最小正整数n，使得
$F_n &gt; \frac {b - a} \epsilon$，计算左右试探点</p>
<p>$\begin{align<em>}
a<em>l &amp; = a + \frac {F</em>{n-2}} {F<em>n} (b - a)\
a_r &amp; = a + \frac {F</em>{n-1}} {F_n} (b - a)
\end{align</em>}</p>
</li>
<li><p>置n=n-1</p>
</li>
<li><p>若$\phi_l &lt; \phi_r$，置</p>
<script type="math/tex; mode=display">
b = a_r, a_r = a_l , \phi_r = \phi_l</script><ul>
<li><p>若n&gt;2，计算</p>
<script type="math/tex; mode=display">\begin{align*}
a_l & = a + \frac {F_{n-2}} {F_n} (b - a) \\
\phi_r & = \phi(a_l)
\end{align*}</script></li>
<li><p>否则计算</p>
<script type="math/tex; mode=display">\begin{align*}
a_l & = a_r - \sigma \\
\phi_l & = \phi(a_l)
\end{align*}</script></li>
</ul>
</li>
<li><p>若$\phi_l \geq \phi_r$，置</p>
<script type="math/tex; mode=display">
a = a_l, a_l = a_r , \phi_l = \phi_r</script><ul>
<li><p>若n&gt;2，计算</p>
<script type="math/tex; mode=display">\begin{align*}
a_l & = a + \frac {F_{n-1}} {F_n} (b - a) \\
\phi_r & = \phi(a_r)
\end{align*}</script></li>
<li><p>否则计算</p>
<script type="math/tex; mode=display">\begin{align*}
a_r & = a_l + \sigma \\
\phi_r & = \phi(a_r)
\end{align*}</script></li>
</ul>
</li>
<li><p>若n=1</p>
<ul>
<li>若$\phi_l &lt; \phi_r$，置$\mu = a_r$</li>
<li>否则置$\mu = a_r$</li>
</ul>
<p>得到极小点$\mu$，停止计算，否则转2</p>
</li>
</ol>
<ul>
<li>Finonacci方法是选取实验点的最佳策略，即在实验点个数相同
情况下，最终的极小区间最小的策略</li>
</ul>
<blockquote>
<ul>
<li>Finonacci法最优性质可通过设最终区间长度为1，递推使得原始
  估计区间最大的取实验点方式，得出</li>
</ul>
</blockquote>
<h2 id="插值法"><a href="#插值法" class="headerlink" title="插值法"></a>插值法</h2><ul>
<li>利用搜索区间上某点的信息构造插值多项式（通常不超过3次）
$\hat \phi(\alpha)$</li>
<li>逐步用$\hat \phi(\alpha)$的极小点逼近$\phi(\alpha)$
极小点$\alpha^{*}$</li>
</ul>
<blockquote>
<ul>
<li>$\phi^{ * }$解析性质比较好时，插值法较试探法效果好</li>
</ul>
</blockquote>
<h3 id="三点二次插值法"><a href="#三点二次插值法" class="headerlink" title="三点二次插值法"></a>三点二次插值法</h3><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>以过三个点$(\mu_1,\phi_1), (\mu_2,\phi_2), (\mu_3,\phi_3)$
的二次插值函数逼近目标函数</p>
<script type="math/tex; mode=display">\begin{align*}
\hat \phi(\alpha) & = \phi_1 \frac {(\alpha - \mu_2)
    (\alpha - \mu_3)} {(\mu_1 - \mu_2)(\mu_1 - \mu_3)} \\
& + \phi_2 \frac {(\alpha - \mu_1) (\alpha - \mu_3)}
    {(\mu_2 - \mu_1)(\mu_2 - \mu_3)} \\
& + \phi_3 \frac {(\alpha - \mu_1) (\alpha - \mu_2)}
    {(\mu_3 - \mu_1)(\mu_3 - \mu_2)}
\end{align*}</script><ul>
<li><p>求导，得到$\hat \phi(\alpha)$的极小点</p>
<script type="math/tex; mode=display">
\mu = \frac {2[\phi_1(\mu_2-\mu_3) + \phi_2(\mu_3-\mu_1)
   + \phi_3(\mu_1 - \mu_2)]}
   {[\phi_1 (\mu_2^2-\mu_3^2) + \phi_2(\mu_3^2-\mu_1^2)
   + \phi_3(\mu_1^2 - \mu_2^2)]}</script></li>
<li><p>若插值结果不理想，继续构造插值函数求极小点近似值</p>
</li>
</ul>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>取初始点$\mu_1&lt;\mu_2&lt;\mu_3$，计算$\phi_i=\phi(\mu_i)$，
且满足$\phi_1 &gt; \phi_2, \phi_3 &gt; \phi_2$，置精度要求
$\epsilon$</p>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">
A = 2[\phi_1(\mu_2 - \mu_3) + \phi_2(\mu_3 - \mu_1) +
  \phi_3(\mu_1 - \mu_2)]</script><ul>
<li>若A=0，置$\mu = \mu_2, \phi = \phi_2$，停止计算，
输出$\mu, \phi$</li>
</ul>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">
\mu = [\phi_1 (\mu_2^2 - \mu_3^2) + \phi_2(\mu_3^2 -
  \mu_1^2) + \phi_3(\mu_1^2 - \mu_2^2)] / A</script><ul>
<li>若$\mu&lt;\mu_1 或 \mu&gt;\mu_3,\mu \notin (\mu_1,\mu_3)$
，停止计算，输出$\mu, \phi$</li>
</ul>
</li>
<li><p>计算$\phi = \phi(\mu)$，若$|\mu - \mu_2| &lt; \epsilon$，
停止计算，得到极小点$\mu$</p>
</li>
<li><p>若$\mu \in (\mu_2, \mu_3)$</p>
<ul>
<li>若$\phi &lt; \phi_2$，置<script type="math/tex; mode=display">
\mu_1=\mu_2, \phi_1=\phi_2, \mu_2=\mu, \phi_2=\phi</script></li>
<li>否则置<script type="math/tex; mode=display">\mu_3 = \mu, \phi_3 = \phi</script></li>
</ul>
<p>否则</p>
<ul>
<li><p>若$\phi &lt; \phi_2$，置</p>
<script type="math/tex; mode=display">
\mu_3=\mu_2, \phi_3=\phi_2, \mu_2=\mu, \phi_2=\phi</script></li>
<li><p>否则置</p>
<script type="math/tex; mode=display">
\mu_1 = \mu, \phi_1 = \phi</script></li>
</ul>
</li>
<li><p>转2</p>
</li>
</ol>
<h3 id="两点二次插值法"><a href="#两点二次插值法" class="headerlink" title="两点二次插值法"></a>两点二次插值法</h3><h4 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h4><p>以$\phi(\alpha)$在两点处$\mu_1, \mu_2$函数值
$\phi_1=\phi(\mu_1)$、一点处导数值
$\phi_1^{‘}=\phi^{‘}(\mu_1) &lt; 0$构造二次函数逼近原函数</p>
<script type="math/tex; mode=display">\begin{align*}
\hat \phi(\alpha) & = A(\alpha - \mu_1)^2 + B(\alpha - \mu_1)
    + C \\
A & = \frac {\phi_2 - \phi_1 - \phi_1^{'}(\mu_2 - \mu_1)}
    {(\mu_2 - \mu_1)^2} \\
B & = \phi_1^{'} \\
C & = \phi_1
\end{align*}</script><ul>
<li><p>为保证$[\mu_1, \mu_2]$中极小点，须有
$\phi_2 &gt; \phi_1 + \phi_1^{‘}(\mu_2 - \mu_1)$</p>
</li>
<li><p>求解，得到$\hat \phi (\mu)$极小值为</p>
<script type="math/tex; mode=display">
\mu = \mu_1 - \frac {\phi_1^{'}(\mu_2 - \mu_1)^2}
   {2[\phi_2 - \phi_1 - \phi_1^{'}(\mu_2 - \mu_1)]}</script></li>
<li><p>若插值不理想，继续构造插值函数求极小点的近似值</p>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始点$\mu_1$、初始步长$d$、步长缩减因子$\rho$、精度要求
$\epsilon$，计算</p>
<script type="math/tex; mode=display">\phi_1 = \phi(\mu_1), \phi_2 = \phi_(\mu_2)</script></li>
<li><p>若$\phi_1^{‘} &lt; 0$，置$d = |d|$，否则置$d = -|d|$</p>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">\mu_2 = \mu_1 + d, \phi_2 = \phi(\mu_2)</script></li>
<li><p>若$\phi_2 \leq \phi_1 + \phi_1^{‘}(\mu_2 - \mu_1)$，置
$d = 2d$，转3</p>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">
\mu = \mu_1 - \frac {\phi_1^{'}(\mu_2 - \mu_1)^2}
  {2[\phi_2 - \phi_1 - \phi_1^{'}(\mu_2 - \mu_1)]} \\
\phi = \phi(\mu), \phi^{'} = \phi^{'}(\mu)</script></li>
<li><p>若$|phi^{‘}| \leq \epsilon$，停止计算，得到极小点$\mu$，
否则置</p>
<script type="math/tex; mode=display">\mu_1 = \mu, \phi_1 = \phi, \phi_1^{'} = \phi^{'},
  \alpha = \rho \alpha</script></li>
</ol>
<blockquote>
<ul>
<li>其中通常取$d = 1, \rho = 0.1$</li>
</ul>
</blockquote>
<h3 id="两点三次插值法"><a href="#两点三次插值法" class="headerlink" title="两点三次插值法"></a>两点三次插值法</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>以两点$\mu_1, \mu_2$处函数值$\phi_i = \phi(\mu_i)$和其导数值
$\phi_i^{‘} = \phi^{‘}(\mu_i)$，由<em>Himiter</em>插值公式可以构造
三次插值多项式$\hat \phi(\alpha)$</p>
<ul>
<li><p>求导置0，得到$\hat \phi(\alpha)$极小点</p>
<script type="math/tex; mode=display">\begin{align*}
\mu & = \mu_1 + (\mu_2 - \mu_1)(1 - \frac {\phi_2^{'}
   + w + z} {\phi_2^{'} - \phi_1^{'} + 2w}) \\
z & = \frac {3(\phi_2 - \phi_1)} {\mu_2 - \mu_1} -
   \phi_1^{'} - \phi_2^{'} \\
w & = sign(\mu_2 - \mu_1) \sqrt {z^2 -
   \phi_1^{'} \phi_2^{'}}
\end{align*}</script></li>
</ul>
<h4 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始值$\mu_1$、初始步长$d$、步长缩减因子$\rho$、精度要求
$\epsilon$，计算</p>
<script type="math/tex; mode=display">\phi_1 = \phi(\mu_1), \phi_1^{'} = \phi^{'}(\mu_1)</script></li>
<li><p>若$\phi_1^{‘} &gt; 0$，置$d = -|d|$，否则置$d = |d|$</p>
</li>
<li><p>置$\mu_2 = \mu_1 + \alpha$，计算</p>
<script type="math/tex; mode=display">\phi_2 = \phi(\mu_2), \phi_2^{'} = \phi^{'}(\mu_2)</script></li>
<li><p>若$\phi_1^{‘} \phi_2{‘} &gt; 0$，置</p>
<script type="math/tex; mode=display">
d = 2d, \mu_1 = \mu_2, \phi_1 = \phi_2,
  \phi_1^{'} = \phi_2^{'}</script><p>转3</p>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">\begin{align*}
\mu & = \mu_1 + (\mu_2 - \mu_1)(1 - \frac {\phi_2^{'}
  + w + z} {\phi_2^{'} - \phi_1^{'} + 2w}) \\
z & = \frac {3(\phi_2 - \phi_1)} {\mu_2 - \mu_1} -
  \phi_1^{'} - \phi_2^{'} \\
w & = sign(\mu_2 - \mu_1) \sqrt {z^2 -
  \phi_1^{'} \phi_2^{'}} \\
\phi & = \phi(\mu) \\
\phi^{'} = \phi^{'}(\mu)
\end{align*}</script></li>
<li><p>若$|\phi^{‘}| &lt; \epsilon$，停止计算，得到极小点$\mu$，
否则置</p>
<script type="math/tex; mode=display">
d = \rho d, \mu_1 = \mu, \phi_1 = \phi,
  \phi_1^{'} = \phi^{'}</script><p>转2</p>
</li>
</ol>
<blockquote>
<ul>
<li>通常取$d = 1, \rho = 0.1$</li>
</ul>
</blockquote>
<h2 id="非精确一维搜索"><a href="#非精确一维搜索" class="headerlink" title="非精确一维搜索"></a>非精确一维搜索</h2><ul>
<li><p>对无约束问题整体而言，又是不要求得到极小点，只需要一定
下降量，缩短一维搜索时间，使整体效果最好</p>
</li>
<li><p>求满足$\phi(\mu) &lt; \phi(0)$、大小合适的$\mu$</p>
<ul>
<li>$\mu$过大容易不稳定</li>
<li>$\mu$过小速度慢</li>
</ul>
</li>
</ul>
<h3 id="GoldStein方法"><a href="#GoldStein方法" class="headerlink" title="GoldStein方法"></a>GoldStein方法</h3><h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><ul>
<li><p>预先指定精度要求$0&lt; \beta_1 &lt; \beta_2 &lt; 1$</p>
</li>
<li><p>以以下不等式限定步长</p>
<script type="math/tex; mode=display">\begin{align*}
\phi(\mu) & \leq \phi(0) + \mu\beta_1 \phi^{'}(0) \\
\phi(\mu) & \geq \phi(0) + \mu\beta_2 \phi^{'}(0)
\end{align*}</script></li>
</ul>
<p><img src="/imgs/line_search_goldstein.png" alt="line_search_goldstein"></p>
<h4 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始试探点$\mu$，置$\mu<em>{min} = 0, \mu</em>{max} = \infty$，
置精度要求$0 &lt; \beta_1 &lt; \beta_2 &lt; 1$</p>
</li>
<li><p>对$\phi(mu)$</p>
<ul>
<li><p>若$\phi(\mu) &gt; \phi(0) + \beta<em>1 \phi^{‘}(0) \mu$，
置$\mu</em>{max} = \mu$</p>
</li>
<li><p>否则若$\phi(\mu) &gt; \phi(0) + \beta_2 \phi^{‘}(0)\mu$
，则停止计算，得到非精确最优解$\mu$</p>
</li>
<li><p>否则置$\mu_{min} = \mu$</p>
</li>
</ul>
</li>
<li><p>若$\mu<em>{max} &lt; \infty$，置
$\mu = \frac 1 2 (\mu</em>{min} + \mu<em>{max})$，否则置
$\mu = 2 \mu</em>{min}$</p>
</li>
<li><p>转2</p>
</li>
</ol>
<h3 id="Armijo方法"><a href="#Armijo方法" class="headerlink" title="Armijo方法"></a>Armijo方法</h3><p>Armijo方法是Goldstein方法的变形</p>
<ul>
<li><p>预先取$M &gt; 1, 0 &lt; \beta_1 &lt; 1$</p>
</li>
<li><p>选取$\mu$使得其满足以下，而$M\mu$不满足</p>
<script type="math/tex; mode=display">\phi(\mu) \leq \phi(0) + \mu \beta_1 \phi^{'}(0)</script></li>
</ul>
<blockquote>
<ul>
<li>M通常取2至10</li>
</ul>
</blockquote>
<p><img src="/imgs/line_search_armijo.png" alt="line_search_armijo"></p>
<h3 id="Wolfe-Powell方法"><a href="#Wolfe-Powell方法" class="headerlink" title="Wolfe-Powell方法"></a>Wolfe-Powell方法</h3><ul>
<li><p>预先指定参数$0 &lt; \beta_1 &lt; \beta_2 &lt;1$</p>
</li>
<li><p>选取$\mu$满足</p>
<script type="math/tex; mode=display">\begin{align*}
\phi(\mu) & \leq \phi(0) + \mu \beta_1 \phi^{'}(0) \\
\phi^{'}(\mu) & \geq \beta_2 \phi^{'}(0)
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li>能保证可接受解中包含最优解，而Goldstein方法不能保证</li>
</ul>
</blockquote>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/Math-Analysis/Optimization/">Previous</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/categories/Math-Analysis/Optimization/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/Math-Analysis/Optimization/">1</a></li><li><a class="pagination-link is-current" href="/categories/Math-Analysis/Optimization/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/WWxwMVBsOUtoNTdMSlJ4Z1lWVnRISERsd2t6ek9MeVpEUWs0YldlZGxUdXlKdDNmMEZVV1hWaFZFYWFSNmFKL25penZodWx3UzRiaVkxcnREWCtOYUJhZWhNbWpzaUdyc1hPangycUh5RTVjRXFnZnFGdVdSTzZvVzJBcTJHKzl8aXpDK1ROWWl4N080YkFEK3QvbEVWNEJuQjFqdWdxODZQcGNoM1NqbERXST0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>