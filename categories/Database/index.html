<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: Database - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"Hexo","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">Database</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-02T15:17:39.000Z" title="8/2/2019, 11:17:39 PM">2019-08-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-02T09:31:39.000Z" title="8/2/2021, 5:31:39 PM">2021-08-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Spark/">Spark</a></span><span class="level-item">12 minutes read (About 1826 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Spark/spark_core.html">Spark Core</a></h1><div class="content"><h2 id="Spark特性"><a href="#Spark特性" class="headerlink" title="Spark特性"></a>Spark特性</h2><h3 id="数据处理速度快"><a href="#数据处理速度快" class="headerlink" title="数据处理速度快"></a>数据处理速度快</h3><p>得益于Spark的内存处理技术、DAG执行引擎</p>
<h4 id="内存计算"><a href="#内存计算" class="headerlink" title="内存计算"></a>内存计算</h4><ul>
<li><p>Spark尽量把数据（中间结果等）驻留在内存中，必要时才写入
磁盘，避免I/O操作，提高处理效率</p>
</li>
<li><p>支持保存部分数据在内存中，剩下部分保存在磁盘中</p>
</li>
<li><p>数据完全驻留于内存时，数据处理达到hadoop系统的
几十~上百倍，数据存在磁盘上时，处理速度能够达到hadoop的
10倍左右</p>
</li>
</ul>
<h4 id="DAG执行引擎"><a href="#DAG执行引擎" class="headerlink" title="DAG执行引擎"></a>DAG执行引擎</h4><ul>
<li><p>Spark执行任务前，根据任务之间依赖关系生成DAG图，优化数据
处理流程（减少任务数量）、减少I/O操作</p>
</li>
<li><p>除了简单的map、reduce，Spark提供了超过80个数据处理的
Operator Primitives</p>
</li>
<li><p>对于数据查询操作，Spark采用Lazy Evaluation方式执行，
帮助优化器对整个数据处力工作流进行优化</p>
</li>
</ul>
<h3 id="易用性-API支持"><a href="#易用性-API支持" class="headerlink" title="易用性/API支持"></a>易用性/API支持</h3><ul>
<li><p>Spark使用Scala编写，经过编译后在JVM上运行</p>
</li>
<li><p>支持各种编程语言，提供了简洁、一致的编程接口</p>
<ul>
<li>Scala</li>
<li>Java</li>
<li>Python</li>
<li>Clojure</li>
<li>R</li>
</ul>
</li>
</ul>
<h3 id="通用性"><a href="#通用性" class="headerlink" title="通用性"></a>通用性</h3><ul>
<li><p>Spark是通用平台，支持以DAG（有向无环图）形式表达的复杂
数据处理流程，能够对数据进行复杂的处理操作，而不用将复杂
任务分解成一系列MapReduce作业</p>
</li>
<li><p>Spark生态圈DBAS（Berkely Data Analysis Stack）包含组件，
支持批处理、流数据处理、图数据处理、机器学习</p>
</li>
</ul>
<h3 id="兼容性"><a href="#兼容性" class="headerlink" title="兼容性"></a>兼容性</h3><ul>
<li><p>DataStorage</p>
<ul>
<li>一般使用HDFS、Amazon S3等分布式系统存储数据</li>
<li>支持Hive、Hbase、Cassandra等数据源</li>
<li>支持Flume、Kafka、Twitter等流式数据</li>
</ul>
</li>
<li><p>Resource Management</p>
<ul>
<li>能以YARN、Mesos等分布式资源管理框架为资源管理器</li>
<li>也可以使用自身资源的管理器以Standalone Mode独立运行</li>
</ul>
</li>
<li><p>使用支持</p>
<ul>
<li>可以使用shell程序，交互式的对数据进行查询</li>
<li>支持流处理、批处理</li>
</ul>
</li>
<li><p>数据类型、计算表达能力</p>
<ul>
<li>Spark可以管理各种类型的数据集：文本</li>
</ul>
</li>
</ul>
<h2 id="Spark架构"><a href="#Spark架构" class="headerlink" title="Spark架构"></a>Spark架构</h2><h3 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h3><blockquote>
<ul>
<li><em>Spark Streaming</em>、<em>Spark SQL</em>、<em>Spark GraphX</em>、
  <em>Spark MLLib</em>为BDAS所包含的组件</li>
</ul>
</blockquote>
<ul>
<li><p><em>Spark Streaming</em>：提供对实时数据流高吞吐、高容错、可
扩展的流式处理系统</p>
<ul>
<li>采用Micro Batch数据处理方式，实现更细粒度资源分配，
实现动态负载均衡</li>
<li>可以对多种数据源（Kafka、Flume、Twitter、ZeroMQ），进行
包括map、reduce、join等复杂操作</li>
</ul>
</li>
<li><p><em>Spark SQL</em>：结构化数据查询模块</p>
<ul>
<li>通过JDBC API暴露Spark数据集，让客户程序可以在其上
直接执行SQL查询</li>
<li>可以连接传统的BI、可视化工具至数据集</li>
</ul>
<blockquote>
<ul>
<li>前身<em>Shark</em>即为<em>Hive on Spark</em>，后出于维护、优化、
 性能考虑放弃</li>
</ul>
</blockquote>
</li>
<li><p><em>Spark GraphX</em>：图数据的并行处理模块</p>
<ul>
<li>扩展RDD为<em>Resilient Distributed Property Graph</em>，
将属性赋予各个节点、边的有向图</li>
<li>可利用此模块对图数据进行ExploratoryAnalysis、Iterative
Graph Computation</li>
</ul>
</li>
<li><p><em>Spark MLLib</em>：可扩展的机器学习模块</p>
<ul>
<li>大数据平台使得在全量数据上进行学习成为可能</li>
<li>实现包括以下算法<ul>
<li>Classification</li>
<li>Regression</li>
<li>Clustering</li>
<li>Collaborative Filtering</li>
<li>Dimensionality Reduction</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="周围组件"><a href="#周围组件" class="headerlink" title="周围组件"></a>周围组件</h3><ul>
<li><p><em>BlinkDB</em>：近似查询处理引擎</p>
<ul>
<li>可以在大规模数据集上，交互式执行SQL查询</li>
<li>允许用户在查询精度、响应时间之间做出折中<ul>
<li>用户可以指定查询响应时间、查询结果精度要求之一</li>
<li>BlinkDB在Data Sample上执行查询，获得近似结果</li>
<li>查询结果会给Error Bar标签，帮助决策</li>
</ul>
</li>
</ul>
</li>
<li><p><em>Tachyon</em>：基于内存的分布式文件系统</p>
<ul>
<li>支持不同处理框架<ul>
<li>可在不同计算框架之间实现可靠的文件共享</li>
<li>支持不同的上层查询处理框架，可以以极高的速度对集群
内存中的文件进行访问</li>
</ul>
</li>
<li>将workset文件缓存在内存中，避免对磁盘的存取，如果数据集
不断被扫描、处理，数据处理速度将极大提升</li>
</ul>
</li>
</ul>
<h2 id="Spark实体"><a href="#Spark实体" class="headerlink" title="Spark实体"></a>Spark实体</h2><p><img src="/imgs/spark_entities.png" alt="spark_entities"></p>
<ul>
<li><p><em>Spark Context</em>：负责和CM的交互、协调应用</p>
<ul>
<li>所有的Spark应用作为独立进程运行，由各自的SC协调</li>
<li>SC向CM申请在集群中worker创建executor执行应用</li>
</ul>
</li>
<li><p><em>Driver</em>：执行应用主函数、创建<em>Spark Context</em>的节点</p>
</li>
<li><p><em>Worker</em>：数据处理的节点</p>
</li>
<li><p><em>Cluster Manager</em>：为每个driver中的应用分配资源</p>
<ul>
<li>以下3种资源管理器，在sceduling、security、monitoring
有所不同，根据需要选择不同的CM<ul>
<li>Standalone</li>
<li>Mesos</li>
<li>YARN</li>
</ul>
</li>
<li>CM对Spark是agnostic</li>
</ul>
</li>
</ul>
<h2 id="Spark-Context"><a href="#Spark-Context" class="headerlink" title="Spark Context"></a>Spark Context</h2><h3 id="spark-SparkContext"><a href="#spark-SparkContext" class="headerlink" title="spark.SparkContext"></a><code>spark.SparkContext</code></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkConf</span>&#123;</span></span><br><span class="line">	<span class="comment">// 设置Spark应用名</span></span><br><span class="line">	<span class="function">def <span class="title">setAppName</span><span class="params">(appName: String)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">	<span class="comment">// 设置集群地址：yarn master节点地址、&quot;local[4]&quot;本地standalone</span></span></span><br><span class="line"><span class="function">	def <span class="title">setMaster</span><span class="params">(master: String)</span></span></span><br><span class="line"><span class="function">&#125;</span></span><br><span class="line"><span class="function">class <span class="title">SparkContext</span><span class="params">(?conf: SparkConf)</span></span>&#123;</span><br><span class="line">	<span class="comment">// 将driver中节点分块</span></span><br><span class="line">	<span class="function">def <span class="title">parallelize</span><span class="params">(?val: ?AnyRef, ?numPartition: Int)</span></span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><code>SparkContext</code>是Spark应用执行环境封装，任何应用都需要、
  也只能拥有一个活动实例，有些shell可能默认已经实例化</li>
</ul>
</blockquote>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">val conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">&quot;app name&quot;</span>)</span><br><span class="line">	.setMaster(<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line">val sc = <span class="keyword">new</span> SparkContext(conf)</span><br></pre></td></tr></table></figure>
<h3 id="Share-Variable"><a href="#Share-Variable" class="headerlink" title="Share Variable"></a><em>Share Variable</em></h3><p>共享变量：可以是一个值、也可以是一个数据集，Spark提供了两种
共享变量</p>
<h4 id="Broadcast-Variable"><a href="#Broadcast-Variable" class="headerlink" title="Broadcast Variable"></a><em>Broadcast Variable</em></h4><p>广播变量：缓存在各个节点上，而不随着计算任务调度的发送变量
拷贝，可以避免大量的数据移动</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val broadcastVal = sc.breadcast(Array(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">println(broadcastVal.value)</span><br></pre></td></tr></table></figure>
<h4 id="Accumulator"><a href="#Accumulator" class="headerlink" title="Accumulator"></a><em>Accumulator</em></h4><p>收集变量/累加器：用于实现计数器、计算总和</p>
<ul>
<li><p>集群上各个任务可以向变量执行增加操作，但是不能读取值，
只有Driver Program（客户程序）可以读取</p>
</li>
<li><p>累加符合结合律，所以集群对收集变量的累加没有顺序要求，
能够高效应用于并行环境</p>
</li>
<li><p>Spark原生支持数值类型累加器，可以自定义类型累加器</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建数值类型累加器</span></span><br><span class="line">val accum = sc.accumulator(<span class="number">0</span>, <span class="string">&quot;accumulator&quot;</span>)</span><br><span class="line">sc.parallelize(Array(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)).foreach(x =&gt; accum += x)</span><br><span class="line">println(accum.value)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义向量累加器工具</span></span><br><span class="line">object VectorAccumulatorParam extends AccumulatorParam[Vector]&#123;</span><br><span class="line">	def zero(initialValue: Vector): Vector = &#123;</span><br><span class="line">		Vector.zeros(initialValue.size)</span><br><span class="line">	&#125;</span><br><span class="line">	def addInPlace(v1: Vector, v2: Vector)&#123;</span><br><span class="line">		v1 += v2</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 创建向量累加器</span></span><br><span class="line">val vecAccum = sc.accumulator(<span class="keyword">new</span> Vector(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))(VectorAccumulator)</span><br></pre></td></tr></table></figure>
<h3 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 按行读取文本文件</span></span><br><span class="line">def sc.textFile(?fileName: String, ?slices: Int): RDD[String]</span><br><span class="line"><span class="comment">// 读取包含多个小文件的目录</span></span><br><span class="line">def sc.wholeTextFile(?directoryName: String): Map[String, RDD[String]]</span><br><span class="line"><span class="comment">// #todo</span></span><br><span class="line">def sc.SequenceFiles(fileName: String)</span><br><span class="line">def sc.hadoopRDD()</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><code>slices</code>：切片数目，缺省为每个文件块创建切片，不能设置
  小于文件块数目的切片值</li>
</ul>
</blockquote>
<ul>
<li><p>Spark基于文件的方法，原生支持</p>
<ul>
<li>文件目录</li>
<li>压缩文件：<code>gz</code></li>
<li><p>简单通配符</p>
<p>|通配符|含义|
|——-|——-|
|<code>[]</code>：范围|
|<code>[^]</code>|范围补集|
|<code>?</code>|单个字符|
|<code>*</code>|0、多个字符|
|<code>&#123;&#125;</code>|<strong>整体或选择</strong>|</p>
</li>
</ul>
</li>
</ul>
<h2 id="Spark-Session"><a href="#Spark-Session" class="headerlink" title="Spark Session"></a>Spark Session</h2><p><code>SparkSession</code>：Spark2.0中新入口，封装有<code>SparkConf</code>、
<code>SparkContext</code>、<code>SQLContext</code>、<code>HiveContext</code>等接口</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> warehouseLocation = <span class="string">&quot;file:$&#123;system:user.dir&#125;/spark-warehouse&quot;</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">	.builder()</span><br><span class="line">	.appName(<span class="string">&quot;App&quot;</span>)</span><br><span class="line">	.config(<span class="string">&quot;spark.sql.warehouse.dir&quot;</span>, warehouseLocation)</span><br><span class="line">	.enableHiveSupport()</span><br><span class="line">	.getOrCreate()</span><br><span class="line">spark.conf.set(<span class="string">&quot;spark.executor.memory&quot;</span>, <span class="string">&quot;2g&quot;</span>)</span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-02T15:17:39.000Z" title="8/2/2019, 11:17:39 PM">2019-08-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-19T01:03:11.000Z" title="7/19/2021, 9:03:11 AM">2021-07-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Spark/">Spark</a></span><span class="level-item">17 minutes read (About 2506 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Spark/spark_rdd.html">Resilient Distributed Dataset</a></h1><div class="content"><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD：容错的、immutable、分布式、确定可重复计算的数据集</p>
<ul>
<li><p>RDD可分配在集群中的多个节点中以支持并行处理</p>
<ul>
<li>隶属于同一RDD数据，被划分为不同的Partition，以此为
单位分布到集群环境中各个节点上</li>
</ul>
</li>
<li><p>RDD是<strong>无结构的数据表</strong>，可以存放任何数据类型</p>
</li>
<li><p>RDD immutable</p>
<ul>
<li>对RDD进行转换，不会修改原RDD，只是返回新RDD</li>
<li>这也是基于Lineage容错机制的要求</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>是Spark软件系统的核心概念</li>
</ul>
</blockquote>
<h3 id="RDD容错机制"><a href="#RDD容错机制" class="headerlink" title="RDD容错机制"></a>RDD容错机制</h3><ul>
<li><p>RDD采用基于Lineage的容错机制</p>
<ul>
<li>每个RDD记住确定性操作的lineage，即从其他RDD转换而来
的路径</li>
<li>若所有RDD的转换操作是确定的，则最终转换数据一致，
无论机器发生的错误</li>
<li>当某个RDD损坏时，Spark可以从上游RDD重新计算、创建
其数据</li>
</ul>
</li>
<li><p>容错语义</p>
<ul>
<li>输入源文件：Spark运行在HDFS、S3等容错文件系统上，从
任何容错数据而来的RDD都是容错的</li>
<li>receiver：可靠接收者告知可靠数据源，保证所有数据总是
会被恰好处理一次</li>
<li>输出：输出操作可能会使得数据被重复写出，但文件会被
之后写入覆盖</li>
</ul>
</li>
<li><p>故障类型</p>
<ul>
<li>worker节点故障：节点中内存数据丢失，其上接收者缓冲
数据丢失</li>
<li>driver节点故障：spark context丢失，所有执行算子丢失</li>
</ul>
</li>
</ul>
<h2 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br></pre></td></tr></table></figure>
<h3 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h3><p>转换：从已有数据集创建新数据集</p>
<ul>
<li><p>返回新RDD，原RDD保持不变</p>
</li>
<li><p>转换操作Lazy</p>
<ul>
<li>仅记录转换操作作用的基础数据集</li>
<li>仅当某个<strong>动作</strong>被Driver Program（客户操作）调用DAG
的动作操作时，动作操作的一系列proceeding转换操作才会
被启动</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Transformation</th>
<th>RDD</th>
<th>DStream</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>map(func)</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>flatMap(func)</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>filter(func)</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>reduceByKey(func[, numTasks])</code></td>
<td>包含<code>(K, V)</code>键值对，返回按键聚集键值对</td>
<td></td>
</tr>
<tr>
<td><code>groupByKey()</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>aggregateByKey()</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>pipe()</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>coalesce()</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>repartition(numPartitions)</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>union(other)</code></td>
<td>无</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<ul>
<li><code>XXXByKey</code>：RDD中应为<code>(K, V)</code>键值对</li>
</ul>
</blockquote>
<p><img src="/imgs/spark_rdd_transformation.png" alt="spark_rdd_transformation"></p>
<blockquote>
<ul>
<li>绿色、黑色：单、多RDD窄依赖转换</li>
<li>紫色：KV shuffle转换</li>
<li>黄色：重分区转换</li>
<li>蓝色：特例转换</li>
</ul>
</blockquote>
<h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><p>动作：在数据集上进行计算后返回值到驱动程序</p>
<ul>
<li>施加于一个RDD，通过对RDD数据集的计算返回新的结果<ul>
<li>默认RDD会在每次执行动作时重新计算，但可以使用
<code>cache</code>、<code>persist</code>持久化RDD至内存、硬盘中，加速下次
查询速度</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Action</th>
<th>RDD</th>
<th>DStream</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>count()</code></td>
<td></td>
<td>返回包含单元素RDD的DStream</td>
</tr>
<tr>
<td><code>collect()</code></td>
<td>将RDD数据聚集至本地</td>
<td></td>
</tr>
<tr>
<td><code>countByValue()</code></td>
<td>返回<code>(T, long)</code>键值对</td>
<td></td>
</tr>
<tr>
<td><code>countByKey()</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>first()</code></td>
<td></td>
<td>返回包含单元素RDDd的DStream</td>
</tr>
<tr>
<td><code>reduce(func)</code></td>
<td></td>
<td>返回包含单元素RDD的DStream</td>
</tr>
<tr>
<td><code>take(func)</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>foreach(func)</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>foreachPartition(func)</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>join(other[, numTasks])</code></td>
<td>包含<code>(K,V)</code>，与另一<code>(K,W)</code>连接</td>
<td></td>
</tr>
<tr>
<td><code>cogroup(other[, numTasks])</code></td>
<td>包含<code>(K,V)</code>、输入<code>(K,W)</code>，返回<code>(K, Seq(V), Seq(W)</code></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<ul>
<li><code>numTasks</code>：默认使用Spark默认并发数目</li>
</ul>
</blockquote>
<h3 id="DStream-RDD"><a href="#DStream-RDD" class="headerlink" title="DStream RDD"></a>DStream RDD</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RDD级`map`：`func`以RDD为参数，自定义转换操作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>(func)</span><br><span class="line"><span class="comment">// RDD级`foreach`</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachRDD</span></span>(func)</span><br><span class="line"></span><br><span class="line"><span class="comment">// RDD级`reduce`</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateStateByKey</span></span>[<span class="type">S</span>: <span class="type">ClassTag</span>](</span><br><span class="line">	<span class="comment">// `K`、`Seq[V]`：当前RDD中键`K`对应值`V`集合</span></span><br><span class="line">	<span class="comment">// `Option[S]`：上个RDD结束后键`K`对应状态</span></span><br><span class="line">	updateFunc: (<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>])]) =&gt; <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">S</span>)],</span><br><span class="line">	<span class="comment">// 分区算法</span></span><br><span class="line"></span><br><span class="line">	partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">	<span class="comment">// 是否在接下来Streaming执行过程中产生的RDD使用相同分区算法</span></span><br><span class="line">	remmemberPartition: <span class="type">Boolean</span>,</span><br><span class="line">	<span class="comment">// 键值对的初始状态</span></span><br><span class="line">	initRDD: <span class="type">RDD</span>[(<span class="type">K</span>,<span class="type">S</span>)]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>RDD分布在多个worker节点上，对不可序列化传递对象，需要在
每个worker节点独立创建</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">	rdd.foreachPartition(partitionOfRecords =&gt; &#123;</span><br><span class="line">		<span class="comment">// 为每个partition创建不可序列化网络连接</span></span><br><span class="line">		<span class="comment">// 为每个record创建成本过高</span></span><br><span class="line">		<span class="keyword">val</span> connection = createNewConnnection()</span><br><span class="line">		<span class="comment">// 进一步可以维护静态连接对象池</span></span><br><span class="line">		<span class="comment">// val connection = ConnectionPool.getConnection()</span></span><br><span class="line">		partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">		connection.close()</span><br><span class="line">	&#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="DStream-Window-Action"><a href="#DStream-Window-Action" class="headerlink" title="DStream Window Action"></a>DStream Window Action</h3><div class="table-container">
<table>
<thead>
<tr>
<th>Window Action</th>
<th>DStream</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>window(windowLength, slideInterval)</code></td>
<td>基于DStream产生的窗口化批数据产生DStream</td>
</tr>
<tr>
<td><code>countByWindow(windowLenght, slideInterval)</code></td>
<td>返回滑动窗口数</td>
</tr>
<tr>
<td><code>reduceByWindow(func, windowLength, slideInterval)</code></td>
<td></td>
</tr>
<tr>
<td><code>reduceByKeyAndWindow(func, windowLength, slidenInterval[, numTasks])</code></td>
<td></td>
</tr>
<tr>
<td><code>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval[, numTasks])</code></td>
<td>须提供<code>invFunc</code>消除离开窗口RDD对reduce结果影响</td>
</tr>
<tr>
<td><code>countByValueAndWindow(windowLength, slideInterval[, numTasks])</code></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<ul>
<li><code>windowLength</code>：窗口长度</li>
<li><p><code>slideInterval</code>：滑动间隔</p>
</li>
<li><p>以上操作默认会持久化RDD至内存，无需手动调用<code>persist</code>等方法</p>
</li>
</ul>
</blockquote>
<p><img src="/imgs/spark_streaming_dstream_window_based_transformation.png" alt="spark_streaming_dstream_window_based_operation"></p>
<h3 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h3><div class="table-container">
<table>
<thead>
<tr>
<th>Output Operation</th>
<th>RDD</th>
<th>DStream</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>print</code></td>
<td>打印前10条元素</td>
<td>每个RDD打印前10条元素</td>
</tr>
<tr>
<td><code>saveAsObjectFile(prefix[, suffix])</code></td>
<td>保存为序列化文件</td>
<td>命名为<code>&lt;prefix&gt;-TIME_IN_M.&lt;suffix&gt;</code></td>
</tr>
<tr>
<td><code>saveAsTextFile(prefix[, suffix])</code></td>
<td>保存为文本文件</td>
<td></td>
</tr>
<tr>
<td><code>saveAsHadoopFile(prefix[, suffix])</code></td>
<td>保存为Hadoop文件</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence</h3><div class="table-container">
<table>
<thead>
<tr>
<th>Persistence</th>
<th>RDD</th>
<th>DStream</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>persist()</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>cache()</code></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id="Directed-Asycled-Graph"><a href="#Directed-Asycled-Graph" class="headerlink" title="Directed Asycled Graph"></a>Directed Asycled Graph</h2><p>Spark中DAG：可以看作由RDD、转换操作、动作操作构成，用于表达
复杂计算</p>
<ul>
<li><p>当需要<strong>执行</strong>某个操作时，将重新从上游RDD进行计算</p>
</li>
<li><p>也可以对RDD进行缓存、持久化，以便再次存取，获得更高查询
速度</p>
<ul>
<li>In-mem Storage as Deserialized Java Objects</li>
<li>In-mem Storage as Serialized Data</li>
<li>On-Disk Storage</li>
</ul>
</li>
</ul>
<h3 id="DAG工作流示例"><a href="#DAG工作流示例" class="headerlink" title="DAG工作流示例"></a>DAG工作流示例</h3><p><img src="/imgs/spark_dag_procedure.png" alt="spark_dag_procedure"></p>
<ul>
<li>从HDFS装载数据至两个RDD中</li>
<li>对RDD（和中间生成的RDD）施加一系列转换操作</li>
<li>最后动作操作施加于最后的RDD生成最终结果、存盘</li>
</ul>
<h3 id="宽依赖、窄依赖"><a href="#宽依赖、窄依赖" class="headerlink" title="宽依赖、窄依赖"></a>宽依赖、窄依赖</h3><p><img src="/imgs/spark_dag_wide_narrow_dependecies.png" alt="spark_dag_wide_narrow_dependencies"></p>
<ul>
<li><p>窄依赖：每个父RDD最多被一个子RDD分区使用</p>
<ul>
<li>即单个父RDD分区经过转换操作生成子RDD分区</li>
<li>窄依赖可以在一台机器上处理，无需Data Shuffling，
在网络上进行数据传输</li>
</ul>
</li>
<li><p>宽依赖：多个子RDD分区，依赖同一个父RDD分区</p>
<ul>
<li>涉及宽依赖操作<ul>
<li><code>groupByKey</code></li>
<li><code>reduceByKey</code></li>
<li><code>sortByKey</code></li>
</ul>
</li>
<li>宽依赖一般涉及Data Shuffling</li>
</ul>
</li>
</ul>
<h3 id="DAG-Scheduler"><a href="#DAG-Scheduler" class="headerlink" title="DAG Scheduler"></a>DAG Scheduler</h3><p><em>DAG Scheduler</em>：<em>Stage-Oriented</em>的DAG执行调度器</p>
<p><img src="/imgs/spark_dag_job_stage.png" alt="spark_dag_job_stage"></p>
<ul>
<li><p>使用Job、Stage概念进行作业调度</p>
<ul>
<li>作业：一个提交到DAG Scheduler的工作项目，表达成DAG，
以一个RDD结束</li>
<li>阶段：一组并行任务，每个任务对应RDD一个分区，是作业
的一部分、数据处理基本单元，负责计算部分结果，</li>
</ul>
</li>
<li><p>DAG Scheduler检查依赖类型</p>
<ul>
<li>把一系列窄依赖RDD组织成一个阶段<ul>
<li>所以说阶段中并行的每个任务对应RDD一个分区</li>
</ul>
</li>
<li>宽依赖需要跨越连续阶段<ul>
<li>因为宽依赖子RDD分区依赖多个父RDD分区，涉及
Data Shuffling，数据处理不能在单独节点并行执行</li>
<li>或者说阶段就是根据宽依赖进行划分</li>
</ul>
</li>
</ul>
</li>
<li><p>DAG Scheduler对整个DAG进行分析</p>
<ul>
<li>为作业产生一系列阶段、其间依赖关系</li>
<li>确定需要持久化的RDD、阶段的输出</li>
<li>找到作业运行最小代价的执行调度方案、根据Cache Status
确定的运行每个task的优选位置，把信息提交给
Task Sheduler执行</li>
</ul>
</li>
<li><p>容错处理</p>
<ul>
<li>DAG Scheduler负责对shuffle output file丢失情况进行
处理，把已经执行过的阶段重新提交，以便重建丢失的数据</li>
<li>stage内其他失败情况由Task Scheduler本身进行处理，
将尝试执行任务一定次数、直到取消整个阶段</li>
</ul>
</li>
</ul>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><ul>
<li><p><em>DataFrame</em>：类似关系型数据库中表，数据被组织到具名列中</p>
<ul>
<li>相较于RDD是对分布式、结构化数据集的高层次抽象，提供
特定领域的专用API进行处理</li>
<li>静态类型安全：相较于SQL查询语句，在编译时即可发现
语法错误</li>
</ul>
</li>
<li><p><em>Dataset</em>：有明确类型数据、或无明确数据类型集合，相应API
也分为两类</p>
<ul>
<li>相较于DataFrame，也可组织半结构化数据，同样提供方便
易用的结构化API</li>
<li>静态类型、运行时类型安全：相较于DataFrame，集合元素
有明确类型，在编译时即可发现分析错误</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>Spark2.0中二者API统一</li>
<li>DataFrame可视为无明确数据类型<code>Dataset[Row]</code>别名，每行是
  无类型JVM对象</li>
</ul>
</blockquote>
<h3 id="创建方式"><a href="#创建方式" class="headerlink" title="创建方式"></a>创建方式</h3><ul>
<li><p><code>.toDF</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="comment">// `.toDF` + `Seq`创建</span></span><br><span class="line"><span class="keyword">val</span> df = <span class="type">Seq</span>(</span><br><span class="line">	(<span class="number">1</span>, <span class="string">&quot;F&quot;</span>, java.sql.<span class="type">Date</span>.valueOf(<span class="string">&quot;2019-08-02&quot;</span>)),</span><br><span class="line">	(<span class="number">2</span>, <span class="string">&quot;G&quot;</span>, java.sql.<span class="type">Date</span>.valueOf(<span class="string">&quot;2019-08-01&quot;</span>))</span><br><span class="line">).toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;level&quot;</span>, <span class="string">&quot;date&quot;</span>)</span><br><span class="line"><span class="comment">// 不指定列名，则默认为`_1`、`_2`等</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// `.toDF` + `case Class`创建</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="keyword">val</span> people = sc.textFile(<span class="string">&quot;people.txt&quot;</span>)</span><br><span class="line">	.map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">	.map(p =&gt; <span class="type">Person</span>(p(<span class="number">0</span>),p(<span class="number">1</span>).trim.toInt))</span><br><span class="line">	.toDF()</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>.createDataFrame</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StrucType</span>(<span class="type">List</span>(</span><br><span class="line">	<span class="type">StructField</span>(<span class="string">&quot;id&quot;</span>, <span class="type">IntegerType</span>, nullable=<span class="type">False</span>),</span><br><span class="line">	<span class="type">StructField</span>(<span class="string">&quot;level&quot;</span>, <span class="type">StringType</span>, nullable=<span class="type">False</span>),</span><br><span class="line">	<span class="type">StructField</span>(<span class="string">&quot;date&quot;</span>, <span class="type">DateType</span>, nullable=<span class="type">False</span>)</span><br><span class="line">))</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Seq</span>(</span><br><span class="line">	(<span class="number">1</span>, <span class="string">&quot;F&quot;</span>, java.sql.<span class="type">Date</span>.valueOf(<span class="string">&quot;2019-08-02&quot;</span>)),</span><br><span class="line">	(<span class="number">2</span>, <span class="string">&quot;G&quot;</span>, java.sql.<span class="type">Date</span>.valueOf(<span class="string">&quot;2019-08-01&quot;</span>))</span><br><span class="line">))</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.createDataFrame(rdd, schema)</span><br></pre></td></tr></table></figure>
</li>
<li><p>读取文件创建</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read.parquet(<span class="string">&quot;hdfs:/peopole.parq&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;people.json&quot;</span>)</span><br><span class="line"><span class="comment">// 读取csv仅2.0版本`SparkSession`后可</span></span><br><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;com.databricks.spark.csv&quot;</span>)</span><br><span class="line">	.option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">	.option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;DROPMALFORMED&quot;</span>)</span><br><span class="line">	.load(<span class="string">&quot;people.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="三种数据集对比"><a href="#三种数据集对比" class="headerlink" title="三种数据集对比"></a>三种数据集对比</h3><ul>
<li><p>空间、时间效率：DataFrame &gt;= Dataset &gt; RDD</p>
<ul>
<li>DataFrame、Dataset基于SparkSQL引擎构建，使用Catalyst
生成优化后的逻辑、物理查询计划；无类型DataFrame相较
有类型Dataset运行更快</li>
<li>Spark作为编译器可以理解Dataset类型JVM对象，会使用
编码器将其映射为Tungsten内存内部表示</li>
</ul>
</li>
<li><p>RDD适合场景</p>
<ul>
<li>对数据集进行最基本的转换、处理、控制</li>
<li>希望以函数式编程而不是以特定领域表达处理数据</li>
<li>数据为非结构化，如：流媒体、字符流</li>
</ul>
</li>
<li><p>DataFrame、Dataset使用场景</p>
<ul>
<li>需要语义丰富、高级抽象、通用平台、特定领域API</li>
<li>需要对半结构化数据进行高级处理，如：filter、SQL查询</li>
<li>需要编译时/运行时类型安全、Catalyst优化、内存优化</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Spark/">Spark</a></span><span class="level-item">7 minutes read (About 1067 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Spark/spark_streaming.html">Spark Streaming</a></h1><div class="content"><h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><p><em>Spark Streaming</em>：提供对实时数据流高吞吐、高容错、可扩展的
流式处理系统</p>
<ul>
<li><p>可以对多种数据源（Kafka、Flume、Twitter、ZeroMQ），进行
包括map、reduce、join等复杂操作</p>
</li>
<li><p>采用Micro Batch数据处理方式，实现更细粒度资源分配，实现
动态负载均衡</p>
<ul>
<li>离散化数据流为为小的RDDs得到DStream交由Spark引擎处理</li>
<li>数据存储在内存实现数据流处理、批处理、交互式一体化</li>
<li>故障节点任务将均匀分散至集群中，实现更快的故障恢复</li>
</ul>
</li>
</ul>
<h3 id="streaming-StreamingContext"><a href="#streaming-StreamingContext" class="headerlink" title="streaming.StreamingContext"></a><code>streaming.StreamingContext</code></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StreamingContext</span>(<span class="params">?conf: <span class="type">SparkConf</span>, ?slices: <span class="type">Int</span></span>)</span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 开始接受、处理流式数据</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">start</span></span>()</span><br><span class="line">	<span class="comment">// 结束流式处理过程</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>(?stop_spark_context=<span class="type">True</span>)</span><br><span class="line">	<span class="comment">// 等待计算完成</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">awaitTermination</span></span>()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">Seconds</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;app name&quot;</span>).setMaster(master)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h2 id="Discreted-Stream"><a href="#Discreted-Stream" class="headerlink" title="Discreted Stream"></a><em>Discreted Stream</em></h2><p><em>DStream</em>：代表持续性的数据流，是Spark Streaming的基础抽象</p>
<p><img src="/imgs/spark_streaming_dstream_transformation.png" alt="spark_streaming_dstream_transformation"></p>
<ul>
<li><p>可从外部输入源、已有DStream转换得到</p>
<ul>
<li>可在流应用中并行创建多个输入DStream接收多个数据流</li>
</ul>
</li>
<li><p>在内部实现</p>
<ul>
<li>DStream由时间上连续的RDD表示，每个RDD包含特定时间
间隔内的数据流</li>
<li>对DStream中各种操作也是<strong>映射到内部RDD上分别进行</strong>
（部分如<code>transform</code>等则以RDD为基本单元）<ul>
<li>转换操作仍然得到DStream</li>
<li>最终结果也是以批量方式生成的batch</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>对DStream操作参见<em>tools/spark/spark_rdd</em></li>
</ul>
</blockquote>
</li>
<li><p>（大部分）输入流DStream和一个<em>Receiver</em>对象相关联</p>
<ul>
<li><code>Recevier</code>对象作为长期任务运行，会占用独立计算核，
若分配核数量不够，系统将只能接收数据而不能处理</li>
<li><em>reliable receiver</em>：可靠的receiver正确应答可靠源，
数据收到、且被正确复制至Spark</li>
<li><em>unreliable receiver</em>：不可靠recevier不支持应答</li>
</ul>
</li>
</ul>
<h3 id="Basic-Sources"><a href="#Basic-Sources" class="headerlink" title="Basic Sources"></a><em>Basic Sources</em></h3><p>基本源：在<code>StreamingContext</code>中直接可用</p>
<ul>
<li>套接字连接</li>
<li>Akka中Actor</li>
<li>RDD队列数据流</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 套接字连接TCP源获取数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssc</span>.<span class="title">socketTextStream</span></span>(?host: <span class="type">String</span>, ?port: <span class="type">Int</span>): <span class="type">DStream</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义actor流</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssc</span>.<span class="title">actorStream</span></span>(actorProps: ?, actorName: <span class="type">String</span>): <span class="type">DStream</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// RDD队列流</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssc</span>.<span class="title">queueStream</span></span>(queueOfRDDs: <span class="type">Seq</span>[<span class="type">RDD</span>])</span><br></pre></td></tr></table></figure>
<h4 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 文件流获取数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssc</span>.<span class="title">fileStream</span></span>[keyClass, valueClass, inputFormatClass]</span><br><span class="line">	(dataDirectory: <span class="type">String</span>): <span class="type">DStream</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssc</span>.<span class="title">textFileStream</span></span>(dataDirectory)</span><br></pre></td></tr></table></figure>
<p>文件系统：<code>StreamingContext</code>将监控目标目录，处理目录下任何
文件（不包括嵌套目录）</p>
<ul>
<li>文件须具有相同数据格式</li>
<li>文件需要直接位于目标目录下</li>
<li>已处理文件追加数据不被处理</li>
</ul>
<blockquote>
<ul>
<li>文件流无需运行<code>receiver</code></li>
</ul>
</blockquote>
<h3 id="Advanced-Sources"><a href="#Advanced-Sources" class="headerlink" title="Advanced Sources"></a><em>Advanced Sources</em></h3><p>高级源：需要额外的依赖</p>
<ul>
<li>Flume</li>
<li>Kinesis</li>
<li>Twitter</li>
</ul>
<h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建多个Kafka输入流</span></span><br><span class="line"><span class="keyword">val</span> kafkaStreams = (<span class="number">1</span> to numStreams).map(_ =&gt; kafkaUtils.createStream())</span><br><span class="line"><span class="keyword">val</span> unifiedStream = streamingContext.union(kafkaStreams)</span><br></pre></td></tr></table></figure>
<h3 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h3><ul>
<li><p>减少批数据处理时间</p>
<ul>
<li>创建多个<em>receiver</em>接收输入流，提高数据接受并行水平</li>
<li>提高数据处理并行水平</li>
<li>减少输入数据序列化、RDD数据序列化成本</li>
<li>减少任务启动开支</li>
</ul>
</li>
<li><p>设置正确的批容量，保证系统能正常、稳定处理数据</p>
</li>
<li><p>内存调优，调整内存使用、应用的垃圾回收行为</p>
</li>
</ul>
<h2 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a><em>Checkpoint</em></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置checkpoint存储信息目录</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ssc</span>.<span class="title">checkpoint</span></span>(?checkpointDirectory: <span class="type">String</span>)</span><br><span class="line"><span class="comment">// 从checkpoint中恢复（若目录存在）、或创建新streaming上下文</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">StreamingContext</span>.<span class="title">getOrCreate</span></span>(?checkPointDirectory: <span class="type">String</span>, ?functionToCreateContext: () =&gt; <span class="type">StreamingContext</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>为保证流应用程序全天运行，需要checkpoint足够信息到容错
存储系统，使得系统能从程序逻辑无关错误中恢复</p>
<ul>
<li><em>metadata checkpointing</em>：流计算的定义信息，用于恢复
worker节点故障</li>
<li><em>configuration</em>：streaming程序配置</li>
<li><em>DStream operation</em>：streaming程序操作集合</li>
<li><em>imcomplete batches</em>：操作队列中未完成批</li>
<li><em>data checkpointing</em>：中间生成的RDD，在有状态的转换
操作中必须，避免RDD依赖链无限增长</li>
</ul>
</li>
<li><p>需要开启checkpoint场合</p>
<ul>
<li>使用有状态转换操作：<code>updateStateByKey</code>、
<code>reduceByKeyAndWindow</code>等</li>
<li>从程序的driver故障中恢复</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">	<span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">	<span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf)</span><br><span class="line">	<span class="comment">// other streaming setting</span></span><br><span class="line">	ssc.checkpoint(<span class="string">&quot;checkpointDirectory&quot;</span>)</span><br><span class="line">	ssc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-10T16:51:41.000Z" title="7/11/2019, 12:51:41 AM">2019-07-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-10T16:51:41.000Z" title="7/11/2019, 12:51:41 AM">2019-07-11</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Spark/">Spark</a></span><span class="level-item">a minute read (About 169 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Spark/spark_graphx.html">Spark GraphX</a></h1><div class="content"><h2 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h2><p><em>Spark GraphX</em>：图数据的并行处理模块</p>
<ul>
<li><p>GraphX扩展RDD为<em>Resilient Distributed Property Graph</em>，
边、顶点都有属性的有向图</p>
</li>
<li><p>可利用此模块对图数据进行ExploratoryAnalysis、Iterative
Graph Computation</p>
</li>
<li><p>GraphX提供了包括：子图、顶点连接、信息聚合等操作在内的
基础原语，并且对的Pregel API提供了优化变量的</p>
</li>
<li><p>GraphX包括了正在不断增加的一系列图算法、构建方法，用于
简化图形分析任务</p>
<ul>
<li><p>提供了一系列操作</p>
<ul>
<li>Sub Graph：子图</li>
<li>Join Vertices：顶点连接</li>
<li>Aggregate Message：消息聚集</li>
<li>Pregel API变种</li>
</ul>
</li>
<li><p>经典图处理算法</p>
<ul>
<li>PageRank</li>
</ul>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-10T16:51:41.000Z" title="7/11/2019, 12:51:41 AM">2019-07-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-19T01:01:22.000Z" title="7/19/2021, 9:01:22 AM">2021-07-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Spark/">Spark</a></span><span class="level-item">6 minutes read (About 869 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Spark/spark_mllib.html">Spark MLLib</a></h1><div class="content"><h2 id="MLLib"><a href="#MLLib" class="headerlink" title="MLLib"></a>MLLib</h2><p><em>Spark MLLib</em>：Spark平台的机器学习库</p>
<ul>
<li><p>能直接操作RDD数据集，可以和其他BDAS其他组件无缝集成，
使得在全量数据上进行学习成为可能</p>
</li>
<li><p>实现包括以下算法</p>
<ul>
<li>Classification</li>
<li>Regression</li>
<li>Clustering</li>
<li>Collaborative Filtering</li>
<li>Dimensionality Reduction</li>
</ul>
</li>
<li><p>MLLib是MLBase中的一部分</p>
<ul>
<li>MLLib</li>
<li>MLI</li>
<li>MLOptimizer</li>
<li>MLRuntime</li>
</ul>
</li>
<li><p>从Spark1.2起被分为两个模块</p>
<ul>
<li><code>spark.mllib</code>：包含基于RDD的原始算法API</li>
<li><code>spark.ml</code>：包含基于DataFrame的高层次API<ul>
<li>可以用于构建机器学习PipLine</li>
<li>ML PipLine API可以方便的进行数据处理、特征转换、
正则化、联合多个机器算法，构建单一完整的机器学习
流水线</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>MLLib算法代码可以在<code>examples</code>目录下找到，数据则在<code>data</code>
  目录下</li>
<li>机器学习算法往往需要多次迭代到收敛为止，Spark内存计算、
  DAG执行引擎象相较MapReduce更理想</li>
<li>由于Spark核心模块的高性能、通用性，Mahout已经放弃
  MapReduce计算模型，选择Spark作为执行引擎</li>
</ul>
</blockquote>
<h2 id="mllib-classification"><a href="#mllib-classification" class="headerlink" title="mllib.classification"></a><code>mllib.classification</code></h2><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><h4 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> \</span><br><span class="line">	LogisticRegressionWithLBFGS, LogisticRegressionModel</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabledPoint</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_point</span>(<span class="params">line</span>):</span></span><br><span class="line">	value = [<span class="built_in">float</span>(i) <span class="keyword">for</span> i line.split(<span class="string">&quot;, \r\n\t&quot;</span>)</span><br><span class="line"></span><br><span class="line">data = sc.textFile(<span class="string">&quot;data/mllib/sample_svm_data.txt&quot;</span>)</span><br><span class="line">parsed_data = data.<span class="built_in">map</span>(parse_point)</span><br><span class="line">	<span class="comment"># map `parse_point` to all data</span></span><br><span class="line"></span><br><span class="line">model = LogisticRegressionWithLBFGS.train(parsed_data)</span><br><span class="line">labels_and_preds = parsed_data.<span class="built_in">map</span>(<span class="keyword">lambda</span> p: (p.label, model.predict(p.features)))</span><br><span class="line">train_err = labels_and_preds \</span><br><span class="line">	.<span class="built_in">filter</span>(<span class="keyword">lambda</span> lp: lp[<span class="number">0</span>] != lp[<span class="number">1</span>]) \</span><br><span class="line">	.count() / <span class="built_in">float</span>(parsed_data.count())</span><br><span class="line"></span><br><span class="line">model.save(sc, <span class="string">&quot;model_path&quot;</span>)</span><br><span class="line">same_model = LogisticRegressionModel.load(sc, <span class="string">&quot;model.path&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Decision Tree</li>
<li>Random Forest</li>
<li>Gradient</li>
<li>boosted tree</li>
<li>Multilaye Perceptron</li>
<li>Support Vector Machine</li>
<li>One-vs-Rest Classifier</li>
<li>Naive Bayes</li>
</ul>
<h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><h4 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.clustering <span class="keyword">import</span> KMeans, KMeansModel</span><br><span class="line"></span><br><span class="line">data = sc.textFile(<span class="string">&quot;data/mllib/kmeans_data.txt&quot;</span>)</span><br><span class="line">parsed_data = data.<span class="built_in">map</span>(<span class="keyword">lambda</span> line: np.array([<span class="built_in">float</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> line.split()]))</span><br><span class="line"></span><br><span class="line">cluster_model = KMeans.train(</span><br><span class="line">	parsed_data,</span><br><span class="line">	maxIteration=<span class="number">10</span>,</span><br><span class="line">	initializationMode=<span class="string">&quot;random&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error</span>(<span class="params">point</span>):</span></span><br><span class="line">	center = cluster_model.centers[cluster.predict(point)]</span><br><span class="line">	<span class="keyword">return</span> np.sqrt(<span class="built_in">sum</span>([i**<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> (point - center)]))</span><br><span class="line">WSSSE = parsed_data \</span><br><span class="line">	.<span class="built_in">map</span>(<span class="keyword">lambda</span> point.error(point)) \</span><br><span class="line">	.reduce(lambd x, y: x + y)</span><br><span class="line"></span><br><span class="line">cluster_model.save(sc, <span class="string">&quot;model_path&quot;</span>)</span><br><span class="line">same_model = KMeansModel.load(sc, <span class="string">&quot;model_path&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="Gaussian-Mixture-Model-GMM"><a href="#Gaussian-Mixture-Model-GMM" class="headerlink" title="Gaussian Mixture Model(GMM)"></a>Gaussian Mixture Model(GMM)</h4><ul>
<li>混合密度模型<ul>
<li>有限混合模型：正态分布混合模型可以模拟所有分布</li>
<li>迪利克莱混合模型：类似于泊松过程</li>
</ul>
</li>
<li>应用<ul>
<li>聚类：检验聚类结果是否合适</li>
<li>预测：<h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1></li>
</ul>
</li>
</ul>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from pyspark.mllib.clustering import GussianMixture, \</span><br><span class="line"><span class="code">	GussianMixtureModel</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">data = sc.textFile(&quot;data/mllib/gmm<span class="emphasis">_data.txt&quot;)</span></span><br><span class="line"><span class="emphasis">parsed_</span>data = data.map(lambda line: np.array[float(i) for i in line.strip()]))</span><br><span class="line"></span><br><span class="line">gmm = GaussianMixture.train(parsed<span class="emphasis">_data, 2)</span></span><br><span class="line"><span class="emphasis">for w, g in zip(gmm.weights, gmm.gaussians):</span></span><br><span class="line"><span class="emphasis">	print(&quot;weight = &quot;, w,</span></span><br><span class="line"><span class="emphasis">		&quot;mu = &quot;, g.mu,</span></span><br><span class="line"><span class="emphasis">		&quot;sigma = &quot;, g.sigma.toArray())</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">gmm.save(sc, &quot;model_</span>path&quot;)</span><br><span class="line">same<span class="emphasis">_model = GussainMixtureModel.load(sc, &quot;model_</span>path&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="Latent-Dirichlet-Allocation-LDA"><a href="#Latent-Dirichlet-Allocation-LDA" class="headerlink" title="Latent Dirichlet Allocation(LDA)"></a>Latent Dirichlet Allocation(LDA)</h4><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.mllib.clustering import LDA, LDAModel</span><br><span class="line">from pyspark.mllib.linalg import Vectors</span><br><span class="line"></span><br><span class="line">data = sc.textFile(&quot;data/mllib/sample<span class="emphasis">_lda_</span>data.txt&quot;)</span><br><span class="line">parsed<span class="emphasis">_data = data.map(lambda line: Vector.dense([float(i) for i in line.strip()]))</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">corpus = parsed_</span>data.zipWithIndex() \</span><br><span class="line"><span class="code">	.map(lambda x: [x[1], x[0]).cache()</span></span><br><span class="line"><span class="code">ldaModel = LDA.train(corpus, k=3)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">topics = ldaModel.topicsMatrix()</span><br><span class="line"></span><br><span class="line">for word in range(0, ldaModel.vocabSize()):</span><br><span class="line"><span class="code">	for topic in word:</span></span><br><span class="line"><span class="code">		print(topic)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">ldaModel.save(sc, &quot;model<span class="emphasis">_path&quot;)</span></span><br><span class="line"><span class="emphasis">same_</span>model = LDAModel.load(&quot;model<span class="emphasis">_path&quot;)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Disecting K-means</li>
</ul>
<h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><ul>
<li>耗时长、无法计算解析解（无意义）</li>
<li>使用MSE作为极小化目标函数，使用SGD算法求解</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabledPoint, \</span><br><span class="line">	LinearRegressionWithSGD, LinearRegressionModel</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_point</span>(<span class="params">line</span>):</span></span><br><span class="line">	value = [<span class="built_in">float</span>(i) <span class="keyword">for</span> i line.split(<span class="string">&quot;, \r\n\t&quot;</span>)</span><br><span class="line"></span><br><span class="line">data = sc.textFile(<span class="string">&quot;data/mllib/ridge-data/lpsa.data&quot;</span>)</span><br><span class="line">parsed_data = data.<span class="built_in">map</span>(parse_point)</span><br><span class="line">	<span class="comment"># map `parse_point` to all data</span></span><br><span class="line"></span><br><span class="line">model = LinearRegressionWithSGD.train(</span><br><span class="line">	parsed_data,</span><br><span class="line">	iteration=<span class="number">100</span>,</span><br><span class="line">	step=<span class="number">0.00000001</span></span><br><span class="line">)</span><br><span class="line">values_and_preds = parsed_data.<span class="built_in">map</span>(<span class="keyword">lambda</span> p:(p.label, model.predict(p.features)))</span><br><span class="line">MSE = values_and_preds \</span><br><span class="line">	.<span class="built_in">map</span>(<span class="keyword">lambda</span> vp: (vp[<span class="number">0</span>] - vp[<span class="number">1</span>]) ** <span class="number">2</span>) \</span><br><span class="line">	.reduce(<span class="keyword">lambda</span> x, y: x + y) / values_and_preds.count()</span><br><span class="line"></span><br><span class="line">model.save(sc, <span class="string">&quot;model_path&quot;</span>)</span><br><span class="line">	<span class="comment"># save model</span></span><br><span class="line">same_model = LinearRegressionModel.load(sc, <span class="string">&quot;model_path&quot;</span>)</span><br><span class="line">	<span class="comment"># load saved model</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Generalized Linear Regression</li>
<li>Decision Tree Regression</li>
<li>Random Forest Regression</li>
<li>Gradient-boosted Tree Regression</li>
<li>Survival Regression</li>
<li>Isotonic Regression</li>
</ul>
<h3 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-10T16:51:41.000Z" title="7/11/2019, 12:51:41 AM">2019-07-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-02T09:32:31.000Z" title="8/2/2021, 5:32:31 PM">2021-08-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Spark/">Spark</a></span><span class="level-item">5 minutes read (About 758 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Spark/spark_sql.html">Spark SQL</a></h1><div class="content"><h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><p><em>Spark SQL</em>：结构化数据查询模块</p>
<ul>
<li><p>内置JDBC服务器，通过JDBC API暴露Spark数据集，让客户程序
可以在其上直接执行SQL查询</p>
</li>
<li><p>通过ETL工具从不同格式数据源装载数据，并运行一些
Ad-Hoc Query</p>
</li>
<li><p>可以连接传统的BI、可视化工具至数据集</p>
</li>
</ul>
<blockquote>
<ul>
<li>前身<em>Shark</em>即为<em>Hive on Spark</em>，后出于维护、优化、
  性能考虑放弃</li>
<li><em>Extraction Transformation Loading</em>：ETL</li>
</ul>
</blockquote>
<p><img src="/imgs/spark_structure.png" alt="sparksql_structure"></p>
<h3 id="sql-SQLContext"><a href="#sql-SQLContext" class="headerlink" title="sql.SQLContext"></a><em>sql.SQLContext</em></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">SQLContext</span>, <span class="type">HiveContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SQLContext</span></span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 缓存使用柱状格式的表</span></span><br><span class="line">	<span class="comment">// Spark将仅仅浏览需要的列、自动压缩数据减少内存使用</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">cacheTable</span></span>(tableName: <span class="type">String</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 将普通RDD转换为SchemaRDD</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">implicit</span> <span class="title">createSchemaRDD</span></span>(rdd: <span class="type">RDD</span>): <span class="type">SchemaRDD</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 载入parquet格式文件</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parquetFile</span></span>(fileName: <span class="type">String</span>): <span class="type">SchemaRDD</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 载入json格式文件</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">jsonFile</span></span>(fileName: <span class="type">String</span>): <span class="type">SchemaRDD</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">jsonRDD</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">SchemaRDD</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 执行SQL query</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">sql</span></span>(query: <span class="type">String</span>): <span class="type">SchemeRDD</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><code>HiveContext</code>支持<code>SQLContext</code>支持功能的超集，增加在
  MetaStore发现表、利用HiveSQL写查询功能</li>
</ul>
</blockquote>
<h2 id="sql-SchemaRDD"><a href="#sql-SchemaRDD" class="headerlink" title="sql.SchemaRDD"></a><code>sql.SchemaRDD</code></h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SchemaRDD</span>&#123;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 存储为parquet文件</span></span><br><span class="line">	<span class="function">def <span class="title">saveAsParquetFile</span><span class="params">(fileName: String)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">	<span class="comment">// 注册为临时表，然后可以使用SQL语句查询</span></span></span><br><span class="line"><span class="function">	def <span class="title">registerTempTable</span><span class="params">(tableName: String)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">	<span class="comment">// 打印表schema</span></span></span><br><span class="line"><span class="function">	def <span class="title">printSchema</span><span class="params">()</span></span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure>
<p>在数据存储层面对数据进行结构化描述的schema</p>
<ul>
<li><p>由SchemaRDD（上个版本）发展而来，在其上增加schema层
，以便对各个数据列命名、数据类型描述</p>
</li>
<li><p>可以通过DF API把过程性处理、Relational Processing
（对表格的选择、投影、连接等操作）集成</p>
</li>
<li><p>DF API操作是Lazy的，使得Spark可以对关系操作、数据处理
工作流进行深入优化</p>
</li>
<li><p>结构化的DF可以通过调用DF API重新转换为无结构的RDD数据集</p>
</li>
<li><p>可以通过不同Data Source创建DF</p>
<ul>
<li>已经存在的RDD数据集</li>
<li>结构化数据文件</li>
<li>JSON数据集</li>
<li>Hive表格</li>
<li>外部数据库表</li>
</ul>
</li>
</ul>
<h3 id="Data-Source"><a href="#Data-Source" class="headerlink" title="Data Source"></a><em>Data Source</em></h3><p>数据源：通过DS API可以存取不同格式保存的结构化数据</p>
<ul>
<li>Parquet</li>
<li>JSON</li>
<li>Apache Avro数据序列化格式</li>
<li>JDBC DS：可以通过JDBC读取关系型数据库</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">SQLContext</span>, <span class="type">StructType</span>, <span class="type">StructField</span>, <span class="type">Row</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">HiveContext</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="keyword">import</span> sqlContext.createSchemeRDD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 通过反射推断包含特定对象类型的RDD的模式</span></span><br><span class="line"><span class="comment">// 需要编写时已知模式</span></span><br><span class="line"><span class="comment">// 代码更简洁、工作更好</span></span><br><span class="line"><span class="keyword">val</span> people: <span class="type">RDD</span>[<span class="type">Person</span>] = sc.textFile(<span class="string">&quot;people.txt&quot;</span>)</span><br><span class="line">	.map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">	.map(p =&gt; <span class="type">Person</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim.toInt))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 编程指定模式：构建模式，然后在已经存在的RDDs上使用</span></span><br><span class="line"><span class="comment">// 运行在运行期前不知道列、列类型情况下构造SchemaRDDs</span></span><br><span class="line"><span class="keyword">val</span> schemaString = <span class="string">&quot;name age&quot;</span></span><br><span class="line"><span class="keyword">val</span> people = sc.textFile(<span class="string">&quot;people.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(schemaString.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">	.map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, <span class="literal">true</span>))</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> rowRDD = people.map(_.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">	.map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>), p(<span class="number">1</span>).trim))</span><br><span class="line"><span class="keyword">val</span> peopleSchemaRDD = sqlContext.applySchema(rowRDD, schema)</span><br><span class="line">peopleSchemaRDD.registerTempTable(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 查询语言集成</span></span><br><span class="line"><span class="keyword">val</span> teenagers = people.where(<span class="string">&quot;age &gt;= 13&quot;</span>).select(<span class="string">&quot;name&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">people.registerTempTable(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> teenagers = sqlContext.sql(<span class="string">&quot;SELECT name FORM people WHERE age &gt;= 13&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> apRDD = sc.parallelize(</span><br><span class="line">	<span class="string">&quot;&quot;&quot;&#123;&quot;name&quot;: &quot;Tom&quot;, &quot;address&quot;: &#123; &quot;city&quot;: &quot;Columbus&quot;, &quot;state&quot;: &quot;Ohio&quot; &#125;&#125;&quot;&quot;&quot;</span> :: <span class="type">Nil</span>)</span><br><span class="line"><span class="keyword">val</span> anotherPeople = sqlContext.jsonRDD(apRDD)</span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-10T16:51:41.000Z" title="7/11/2019, 12:51:41 AM">2019-07-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T08:17:23.000Z" title="8/4/2021, 4:17:23 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Spark/">Spark</a></span><span class="level-item">10 minutes read (About 1483 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Spark/sparksql_catalyst.html">Catalyst 优化器</a></h1><div class="content"><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>Catalyst优化器：利用Scala模式匹配和quasiquotes机制构建的
可扩展查询优化器</p>
<p><img src="/imgs/sparksql_optimization.png" alt="sparksql_optimization"></p>
<p><img src="/imgs/sparksql_procedure.png" alt="sparksql_procedure"></p>
<ul>
<li>SparkSQL Pipeline的中间核心部分</li>
</ul>
<h3 id="Parser模块"><a href="#Parser模块" class="headerlink" title="Parser模块"></a><em>Parser</em>模块</h3><p>Parser模块：将SQL语句切分为token，根据一定语义规则解析为AST</p>
<p><img src="/imgs/sql_parser_ast.png" alt="sql_parser_ast"></p>
<ul>
<li><p>Spark1.x使用Scala原生Parser Combinator构建的词法、语法
分析器</p>
</li>
<li><p>Spark2.x使用采用第三方语法解析器工具ANTLR4</p>
<ul>
<li><p>ANTLR4根据语法文件<code>SqlBase.g4</code>自动解析生成两个Java类
，将sql语句解析成ParseTree的语法结构</p>
<ul>
<li><code>SqlBaseLexer</code>：词法解析器</li>
<li><code>SqlBaseParser</code>：语法解析器</li>
</ul>
</li>
<li><p>随后ParsePlan过程，使用<code>AstBuilder.scala</code>将ParseTree
转换为catalyst表达式逻辑计划<em>Unresovled Logical Plan</em></p>
<ul>
<li>Unsolved Relation</li>
<li>Unsolved Function</li>
<li>Unsolved Attribute</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Analyzer模块"><a href="#Analyzer模块" class="headerlink" title="Analyzer模块"></a><em>Analyzer</em>模块</h3><p><em>Analyzer</em>模块：使用<em>Analysis Rules</em>，借助数据元数据
（<em>session catalog</em>、<em>hive metastore</em>）将<em>ULP</em>解析为
<em>Logical Plan</em></p>
<p><img src="/imgs/sparksql_catalyst_analyzer.png" alt="sparksql_catalyst_analyzer"></p>
<ul>
<li><p>ULP虽然具备基本骨架，但是系统对表的字段信息不清楚，需要
基本元数据信息表达ULP中token</p>
</li>
<li><p>遍历整个AST，对树上每个结点进行数据类型绑定、函数绑定，
得到LP</p>
</li>
</ul>
<h4 id="Schema-Catalog"><a href="#Schema-Catalog" class="headerlink" title="Schema Catalog"></a><em>Schema Catalog</em></h4><p>元数据信息：表的模式信息</p>
<ul>
<li>表的基本定义：表名、列名、数据类型</li>
<li>表的数据格式：json、text、parquet、压缩格式</li>
<li>表的物理位置</li>
</ul>
<h3 id="Optimizer模块"><a href="#Optimizer模块" class="headerlink" title="Optimizer模块"></a><em>Optimizer</em>模块</h3><p><em>Optimizer</em>模块：使用Optimization Rules对<em>LP</em>进行合并、列
裁剪、过滤器下推等优化工作，生成等价<em>Optimized Logical Plan</em></p>
<ul>
<li>分为RBO、CBO两种优化策略，是catalyst核心</li>
</ul>
<h3 id="Spark-Planner"><a href="#Spark-Planner" class="headerlink" title="Spark Planner"></a><em>Spark Planner</em></h3><p><em>Spark Planner</em>模块：将<em>OLP</em>转换为spark能够理解的
<em>Physical Plan</em></p>
<p><img src="/imgs/sql_optimization_physical_plan.png" alt="sql_optimization_physical_plan"></p>
<ul>
<li>将逻辑上可行的执行计划变为Spark真正可以执行的物理计划</li>
<li>物理计划实际上是逻辑计划中耗时最小的算法实现</li>
</ul>
<h2 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h2><h3 id="Join类型"><a href="#Join类型" class="headerlink" title="Join类型"></a>Join类型</h3><p>SparkSQL目前支持三种join算符</p>
<ul>
<li><em>shuffle hash join</em></li>
<li><em>broadcast hash join</em></li>
<li><em>sort merge join</em></li>
</ul>
<h4 id="Broadcast-Hash-Join"><a href="#Broadcast-Hash-Join" class="headerlink" title="Broadcast Hash Join"></a><em>Broadcast Hash Join</em></h4><p><em>broadcast hash join</em>：将小表广播分发到大表所在的结点上，
并行在各节点上进行hash join</p>
<p><img src="/imgs/sparksql_broadcast_hash_join.png" alt="sparksql_broadcast_hash_join"></p>
<ul>
<li><p>适合小表很小，可以直接广播的场合</p>
<blockquote>
<ul>
<li><code>spark.sql.autoBroadcastJoinThreshold</code>设置广播小表
 大小上限</li>
</ul>
</blockquote>
</li>
<li><p><em>broadcast</em>阶段：将所小表广播分发到大表所在的所有主机</p>
<ul>
<li>driver分发</li>
<li>p2p分发</li>
</ul>
</li>
<li><p><em>hash join</em>结点：各结点独立并行hash join</p>
<ul>
<li>小表构建hash表</li>
<li>各结点本地大表试探</li>
</ul>
</li>
</ul>
<h4 id="Shuffle-Hash-Join"><a href="#Shuffle-Hash-Join" class="headerlink" title="Shuffle Hash Join"></a><em>Shuffle Hash Join</em></h4><p><em>shuffle hash join</em>：按照join key分区，在每个结点独立并行
进行hash join</p>
<p><img src="/imgs/sparksql_shuffle_hash_join.png" alt="sparksql_shuffle_hash_join"></p>
<ul>
<li><p>类似分布式GHJ，不同块位于不同结点</p>
</li>
<li><p><em>shuffle</em>阶段：将表按照join key分区，将具有相同join key
的记录重分布到同一结点</p>
</li>
<li><p><em>hash jon</em>阶段：各结点使用本地数据独立并行hash join</p>
</li>
</ul>
<h4 id="Sort-Merge-Join"><a href="#Sort-Merge-Join" class="headerlink" title="Sort Merge Join"></a><em>Sort Merge Join</em></h4><p><em>SMJ</em>：按照join key分区，在各节点独立并行<em>SMJ</em></p>
<p><img src="/imgs/sparksql_sort_merge_join.png" alt="sparksql_sort_merge_join"></p>
<ul>
<li><p><em>shuffle</em>阶段：将表按照join key分区，将具有相同join key
的记录重分布到同一结点</p>
</li>
<li><p><em>sort</em>阶段：各节点并行对本地数据排序</p>
<ul>
<li>spark当前shuffle算法使用<em>sort-based shuffle</em>算法</li>
<li>理论上shuffle过后各分区数据已经排序完毕，无需再次
sort，效率很高</li>
</ul>
</li>
<li><p><em>merge</em>阶段：各节点对排序好表数据执行join操作</p>
</li>
</ul>
<h3 id="Join-Reorder"><a href="#Join-Reorder" class="headerlink" title="Join Reorder"></a>Join Reorder</h3><ul>
<li><p>基于CBO的join重排序优化：用统计信息预估的基修正join顺序</p>
</li>
<li><p>使用动态规划算法，考虑所有可能组合，选择代价最小的方案</p>
<ul>
<li><p>单个join操作成本，join树的成本是所有中间join成本总和</p>
<script type="math/tex; mode=display">
cost = weight * cardinality + (1 - weight)*size</script><blockquote>
<ul>
<li>carinality：对应CPU成本</li>
<li>size：对应IO成本</li>
</ul>
</blockquote>
</li>
<li><p>没有任何join条件同时包含左、右子树时，修剪笛卡尔积
减少搜索范围</p>
</li>
</ul>
</li>
</ul>
<h2 id="Statistics-Collection-Framework"><a href="#Statistics-Collection-Framework" class="headerlink" title="Statistics Collection Framework"></a><em>Statistics Collection Framework</em></h2><p>CBO依赖统计细节信息优化查询计划</p>
<ul>
<li>CBO自下而上遍历LP，统计信息可以随之传播到上层算子</li>
</ul>
<h3 id="统计信息类型"><a href="#统计信息类型" class="headerlink" title="统计信息类型"></a>统计信息类型</h3><ul>
<li><p>Numeric、Date、Timestamp</p>
<ul>
<li>Distinct Count</li>
<li>Max</li>
<li>Min</li>
<li>Null Count</li>
<li>Average Length：定长</li>
<li>Max Length：定长</li>
</ul>
</li>
<li><p>String、Binary</p>
<ul>
<li>Distinct Count</li>
<li>Null Count</li>
<li>Average Length</li>
<li>Max Length</li>
</ul>
</li>
</ul>
<h3 id="统计方式"><a href="#统计方式" class="headerlink" title="统计方式"></a>统计方式</h3><ul>
<li>扫描全表：简单、统计信息准确，代价大</li>
<li>抽样统计：</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><h4 id="Filter-Selectivity"><a href="#Filter-Selectivity" class="headerlink" title="Filter Selectivity"></a><em>Filter Selectivity</em></h4><p>过滤选择率：估计应用谓词表达式过滤的选择率</p>
<h5 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h5><ul>
<li><p><code>AND</code>：左侧过滤条件选择率、右侧过滤条件选择率之积</p>
<script type="math/tex; mode=display">
fs(a AND b) = fs(a) * fs(b)</script></li>
<li><p><code>OR</code>：左侧、右侧过滤条件选择率之和，减去其乘积</p>
<script type="math/tex; mode=display">
fs(a OR b) = fs(a) + fs(b) - fs(a) * fs(b)</script></li>
<li><p><code>NOT</code>：1减去原始过滤条件选择率</p>
<script type="math/tex; mode=display">
fs(NOT a) = 1.0 - fs(a)</script></li>
</ul>
<h5 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h5><ul>
<li><p><code>=</code>：等于条件</p>
<ul>
<li>若常数取值在当前列取值范围之外，则过滤选择率为0</li>
<li>否则根据柱状图、均匀分布得到过滤选择率</li>
</ul>
</li>
<li><p><code>&lt;</code>：小于条件</p>
<ul>
<li>若常数取值小于当前列最小值，则过滤选择率为0</li>
<li>否则根据柱状图、均匀分数得到过滤选择率</li>
</ul>
</li>
</ul>
<h4 id="Join-Carinality"><a href="#Join-Carinality" class="headerlink" title="Join Carinality"></a><em>Join Carinality</em></h4><p>联接基：估计联接操作结果的基</p>
<ul>
<li><p><em>inner</em>：其他基估计值可由inner join计算</p>
<script type="math/tex; mode=display">
num(A IJ B) = \frac {num(A) * num(B)}
   {max(distinct(A.k), distinct(B.k))}</script><blockquote>
<ul>
<li><code>num(A)</code>：join操作前表A的有效记录数</li>
<li><code>distinct(A.k)</code>：表A中列k唯一值数量</li>
</ul>
</blockquote>
</li>
<li><p><em>left-outer</em>：取inner join、左表中基较大者</p>
<script type="math/tex; mode=display">
num(A LOJ B) = max(num(A IJ B), num(A))</script></li>
<li><p><em>right-outer</em>：取inner join、右表中基较大者</p>
<script type="math/tex; mode=display">
num(A ROJ B) = max(num(A IJ B), num(B))</script></li>
<li><p><em>full-outer</em></p>
<script type="math/tex; mode=display">
num(A FOJ B) = num(A ROJ B) + num(A ROJ B) - num(A IJ B)</script></li>
</ul>
<p>B)</p>
<pre><code>$$
</code></pre></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-09T16:48:33.000Z" title="7/10/2019, 12:48:33 AM">2019-07-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-19T01:04:42.000Z" title="7/19/2021, 9:04:42 AM">2021-07-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/SQL-DB/">SQL DB</a></span><span class="level-item">7 minutes read (About 996 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/SQL-DB/db_intro.html">数据库背景</a></h1><div class="content"><h2 id="数据库术语"><a href="#数据库术语" class="headerlink" title="数据库术语"></a>数据库术语</h2><h3 id="数据库结构"><a href="#数据库结构" class="headerlink" title="数据库结构"></a>数据库结构</h3><ul>
<li>数据源：多源数据继承</li>
<li>数据仓库继承工具：FTL工具、MapReduce</li>
<li>数据仓库服务器：列存储数据库引擎</li>
<li>数据即使：数据仓库的数据子集、聚集数据继</li>
<li>OLAP服务器：提供多维数据视图</li>
<li>前台数据分析工具<ul>
<li>报表工具</li>
<li>多维分析工具</li>
<li>数据挖掘工具</li>
</ul>
</li>
</ul>
<h3 id="数据库类型"><a href="#数据库类型" class="headerlink" title="数据库类型"></a>数据库类型</h3><ul>
<li>传统关系型数据库</li>
<li>MPP大规模并行处理数据库</li>
<li>NoSQL数据库</li>
<li>图数据库</li>
<li>NewSQL数据库</li>
<li>GPU数据库</li>
</ul>
<h2 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h2><ul>
<li><p>Project Pushdown：投影下推</p>
<ul>
<li>只读取、查询需要的<strong>列</strong></li>
<li>减少每次查询的IO数据量</li>
</ul>
</li>
<li><p>Predicate Pushdown：谓词下推</p>
<ul>
<li>将过滤条件尽快执行，跳过不满足条件<strong>行</strong></li>
</ul>
</li>
</ul>
<h2 id="数据压缩算法"><a href="#数据压缩算法" class="headerlink" title="数据压缩算法"></a>数据压缩算法</h2><ul>
<li>Run Length Encoding：重复数据</li>
<li>Delta Encoding：有序数据集</li>
<li>Dictionary Encoding：小规模数据集合</li>
<li>Prefix Encoding：字符串版Delta Encoding</li>
</ul>
<h2 id="数据库调优"><a href="#数据库调优" class="headerlink" title="数据库调优"></a>数据库调优</h2><p>数据库调优：是数据库具有更高的吞吐量、更快响应</p>
<ul>
<li>被调优对象是数据库整体</li>
<li>需要考虑很多资源、数据库配置</li>
</ul>
<h3 id="人工调优"><a href="#人工调优" class="headerlink" title="人工调优"></a>人工调优</h3><ul>
<li>依赖人工，效率低下</li>
<li>要求操作者理解查询原理，对应用、DBMS、操作系统、硬件有
一定理解</li>
</ul>
<h3 id="基于案例调优"><a href="#基于案例调优" class="headerlink" title="基于案例调优"></a>基于案例调优</h3><ul>
<li>总结典型应用案例情况中数据库参数推荐配置值、逻辑层设计等
情况，为用户调优工作提供参考、借鉴</li>
<li>忽略系统动态性、不同系统之间差异</li>
</ul>
<h3 id="自调优"><a href="#自调优" class="headerlink" title="自调优"></a>自调优</h3><ul>
<li>为DBMS建立模型，根据“影响数据库性能效率的因素”，自动进行
参数配置</li>
<li>部分商业数据库实现了自调优技术</li>
</ul>
<h3 id="需求分析期"><a href="#需求分析期" class="headerlink" title="需求分析期"></a>需求分析期</h3><h4 id="应用情况估算"><a href="#应用情况估算" class="headerlink" title="应用情况估算"></a>应用情况估算</h4><ul>
<li>应用使用方式<ul>
<li>将业务逻辑转换为读写分布逻辑，以读多写少、读写均衡
区分OLAP、OLTP</li>
<li>应用对数据库的并发情况、并发是否可池化</li>
</ul>
</li>
<li>数据量</li>
<li>对数据库压力、峰值压力</li>
</ul>
<h4 id="系统选型策略"><a href="#系统选型策略" class="headerlink" title="系统选型策略"></a>系统选型策略</h4><ul>
<li>确定适合的数据库：开源、商业；集群、单机</li>
<li>操作系统、中间件、硬件、网络选型</li>
</ul>
<h3 id="项目设计期"><a href="#项目设计期" class="headerlink" title="项目设计期"></a>项目设计期</h3><h4 id="数据模型设计"><a href="#数据模型设计" class="headerlink" title="数据模型设计"></a>数据模型设计</h4><p>根据业务逻辑，从以下角度考虑表结构</p>
<ul>
<li>E-R模型设计：遵循E-R模型设计原理，适当非规范化可以改善
系统查询性能</li>
<li>数据逻辑分布策略：减少数据请求中不必要的数据量<ul>
<li>分区</li>
<li>利用E-R模型分表</li>
</ul>
</li>
<li>数据物理存储策略：减少IO操作<ul>
<li>启用压缩</li>
<li>分开存储索引、表数据</li>
<li>不同表数据分布在不同表空间</li>
<li>不同表空间分布在不同物理存储，尤其是读写量大的表空间
分布在不同物理存储上</li>
<li>日志、索引、数据分布在不同物理存储上</li>
</ul>
</li>
<li>索引：在查询频繁的对象上建立<strong>恰当</strong>索引</li>
</ul>
<h3 id="开发期"><a href="#开发期" class="headerlink" title="开发期"></a>开发期</h3><h4 id="SQL设计"><a href="#SQL设计" class="headerlink" title="SQL设计"></a>SQL设计</h4><ul>
<li>编写正确、查询效率高的SQL语句，依据“查询重写规则”<ul>
<li>有意识地保障SQL能利用到索引</li>
</ul>
</li>
</ul>
<h4 id="数据库功能启用"><a href="#数据库功能启用" class="headerlink" title="数据库功能启用"></a>数据库功能启用</h4><ul>
<li>查询重用</li>
<li>数据库参数设计</li>
</ul>
<h3 id="测试、试运行、上线、维护"><a href="#测试、试运行、上线、维护" class="headerlink" title="测试、试运行、上线、维护"></a>测试、试运行、上线、维护</h3><h4 id="模型系统预运行"><a href="#模型系统预运行" class="headerlink" title="模型系统预运行"></a>模型系统预运行</h4><ul>
<li>在备用系统上模型实际运行环境，加大压力进行相似测试</li>
</ul>
<h4 id="系统监控分析"><a href="#系统监控分析" class="headerlink" title="系统监控分析"></a>系统监控分析</h4><ul>
<li>应用系统表示：收集用户使用意见、系统存在问题</li>
<li>OS环境监控：实时监控CPU、内存、IO等，对比历史正常情况</li>
<li>数据库内部状态监控：系统表、视图、工具、锁的情况</li>
<li>日志分析：在数据库的日志、操作系统日志中找出异常</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-09T16:48:33.000Z" title="7/10/2019, 12:48:33 AM">2019-07-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-09T16:48:33.000Z" title="7/10/2019, 12:48:33 AM">2019-07-10</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/SQL-DB/">SQL DB</a></span><span class="level-item">3 minutes read (About 448 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/SQL-DB/hivesql.html">HiveSQL</a></h1><div class="content"><h2 id="命令行参数"><a href="#命令行参数" class="headerlink" title="命令行参数"></a>命令行参数</h2><ul>
<li><code>-d/--define &lt;key=value&gt;</code>：替换脚本中shell形式变量</li>
<li><code>--hivevar &lt;key=value&gt;</code>：替换脚本中shell形式变量<ul>
<li>结合hive脚本中设置shell变量使用</li>
</ul>
</li>
<li><code>-h &lt;hostname&gt;</code>：hive服务器</li>
<li><code>-p &lt;port&gt;</code>：hive服务器端口</li>
<li><code>-database &lt;database&gt;</code>：连接数据库</li>
<li><code>-e &lt;quoted-query-string&gt;</code>：从命令行获取、执行hive脚本</li>
<li><code>-f &lt;filename&gt;</code>：从文件获取、执行hive脚本</li>
<li><code>-i &lt;filename&gt;</code>：初始化hive脚本</li>
<li><code>--hiveconf &lt;property=value&gt;</code>：设置hive参数</li>
<li><code>-S/--slient</code>：安静模式启动交互hive shell</li>
<li><code>-v/--verbose</code>：详细模式</li>
<li><code>-H/--help</code>：帮助</li>
</ul>
<h2 id="辅助语句"><a href="#辅助语句" class="headerlink" title="辅助语句"></a>辅助语句</h2><h3 id="结果输出"><a href="#结果输出" class="headerlink" title="结果输出"></a>结果输出</h3><ul>
<li><code>INSERT INTO/OVERWRITE</code>：查询结果追加/覆盖在hive表中</li>
<li><code>INSERT INTO/OVERWRITE [LOCAL] DIRECTORY</code>：查询结果追加/
覆盖本地/HDFS目录</li>
</ul>
<blockquote>
<ul>
<li>有分区情况下，仅覆盖当前分区</li>
</ul>
</blockquote>
<h2 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h2><h3 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h3><ul>
<li><code>collect_set()</code>：配合<code>group by</code>合并、消除重复字段，返回
<code>array</code></li>
<li><code>concat_ws()</code>：连接字符串</li>
<li><code>if(&lt;condition&gt;, &lt;true_value&gt;, &lt;false_value&gt;)</code>：判断条件</li>
<li><code>size()</code>：返回<code>array</code>长度</li>
<li><code>length()</code>：返回字符串大小</li>
</ul>
<h2 id="配置相关语句"><a href="#配置相关语句" class="headerlink" title="配置相关语句"></a>配置相关语句</h2><h3 id="文本分隔符"><a href="#文本分隔符" class="headerlink" title="文本分隔符"></a>文本分隔符</h3><ul>
<li>记录分隔：<code>\n</code></li>
<li>字段分隔：<code>\001</code>（八进制）ASCII码1字符</li>
<li>Array、Struct、Map等集合中元素分隔：<code>\002</code>ASCII码1字符</li>
<li>Map中键值对分隔：<code>\003</code>ASCII码1字符</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">line terminated <span class="keyword">by</span> `\n`</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> `\<span class="number">001</span>`</span><br><span class="line">collection items terminated <span class="keyword">by</span> `\<span class="number">002</span>`</span><br><span class="line">map keys terminated <span class="keyword">by</span> `\<span class="number">003</span>`</span><br></pre></td></tr></table></figure>
<h3 id="空值"><a href="#空值" class="headerlink" title="空值"></a>空值</h3><ul>
<li><p>hive中空值一般有两种存储方式</p>
<ul>
<li><code>NULL</code>：底层存储<code>NULL</code>，查询显示为<code>NULL</code></li>
<li><code>\N</code>：底层存储<code>\N</code>，查询显示为<code>NULL</code>，查询输出为<code>\N</code></li>
</ul>
</li>
<li><p>空值查询：<code>&lt;field&gt; is NULL</code></p>
<ul>
<li><code>NULL</code>：也可<code>&lt;field&gt; = &#39;NULL&#39;</code></li>
<li><code>\N</code>：也可<code>&lt;field&gt; = &#39;\\N&#39;</code>（转义）</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>底层存储设置参见<strong>表存储</strong></li>
<li>空字符串不是空值，需要用<code>&lt;field&gt; = &#39;&#39;</code>查询</li>
</ul>
</blockquote>
<h2 id="表存储配置"><a href="#表存储配置" class="headerlink" title="表存储配置"></a>表存储配置</h2><h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><h4 id="serdeproperties"><a href="#serdeproperties" class="headerlink" title="serdeproperties"></a><code>serdeproperties</code></h4><ul>
<li><p>设置空值存储方式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="operator">&lt;</span><span class="keyword">table</span><span class="operator">&gt;</span> <span class="keyword">SET</span> serdeproperites(<span class="string">&#x27;serialization.null.format&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;\N&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-09T16:48:33.000Z" title="7/10/2019, 12:48:33 AM">2019-07-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-19T01:06:40.000Z" title="7/19/2021, 9:06:40 AM">2021-07-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/SQL-DB/">SQL DB</a></span><span class="level-item">12 minutes read (About 1771 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/SQL-DB/mysql.html">Mysql/Mariadb安装配置</a></h1><div class="content"><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="Mysql"><a href="#Mysql" class="headerlink" title="Mysql"></a>Mysql</h3><p>大部分系统常用源中包含mysql，直接使用自带的包管理工具安装
即可，对于源中无法找到mysql的系统可以访问<a target="_blank" rel="noopener" href="https://dev.mysql.com/downloads/" title="mysql">官网</a>获取
安装方法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo zypper install mysql-server mysql-client</span></span><br></pre></td></tr></table></figure>
<h4 id="CentOS7"><a href="#CentOS7" class="headerlink" title="CentOS7"></a>CentOS7</h4><p>CentOS7的常用源中即不含mysql，安装mysql则需要添加mysql源，
同样在<a target="_blank" rel="noopener" href="https://dev.mysql.com/downloads/" title="mysql">官网</a>中找到添加方式：</p>
<ol>
<li><a target="_blank" rel="noopener" href="http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpm" title="mysql_rpm">下载</a>的是RPM源rpm包</li>
<li><code>$ sudo yum localintall</code>安装后即添加源</li>
<li>使用yum直接安装mysql，需要注意的是默认情况下只有最新版本
mysql源是enabled，其他版本的需要<code>--enablerepo</code>指定或者
<code>/etc/yum.repo.d</code>下修改文件</li>
</ol>
<h3 id="Mariadb"><a href="#Mariadb" class="headerlink" title="Mariadb"></a>Mariadb</h3><p>mariadb和mysql大部分兼容，添加了一些新的特性，是mysql的一个
开源分支，由mysql的作者维护，避免mysql在被Oracle收购后闭源。</p>
<ul>
<li>大部分情况下，mariadb可以完全看作是mysql</li>
<li>甚至在某些系统中，mariadb服务有别名mysql</li>
<li>mariadb控制台命令也是<code>mysql</code></li>
</ul>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><ul>
<li><p><code>/etc/mysql/my.cnf</code>:mysql主配置文件</p>
<ul>
<li>mysql的此配置文件内包含有具体的配置</li>
<li>mariadb此文件中则不包含具体配置，而是导入配置文件<figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!includedir /etc/mysql/conf.d/</span><br><span class="line">!includedir /etc/mysql/mariadb.cond.d/</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><code>~/.my.cnf</code></p>
<ul>
<li>一般不存在，需要自行建文件，注意需要设置文件权限，
只能root用户可写，否则mysql会忽略此配置文件</li>
<li>mysqld服务启动需要root权限，因此<code>~</code>目录下的配置文件
基本不可能影响mysql-server状态，即<code>[server]</code>下的配置
基本是无效的</li>
</ul>
</li>
</ul>
<h3 id="数据位置"><a href="#数据位置" class="headerlink" title="数据位置"></a>数据位置</h3><ul>
<li>数据库存放数据位置：<code>/var/lib/mysql/db_name/</code></li>
</ul>
<h2 id="Mysql-Client"><a href="#Mysql-Client" class="headerlink" title="Mysql-Client"></a>Mysql-Client</h2><h3 id="登陆"><a href="#登陆" class="headerlink" title="登陆"></a>登陆</h3><p>@todo
一个问题，我安装的mariadb，默认的root用户无法在一般用户账户
登陆，必须sudo才能正常登陆</p>
<h4 id="参数登陆"><a href="#参数登陆" class="headerlink" title="参数登陆"></a>参数登陆</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mysql -h host -P port -u user -p</span></span><br></pre></td></tr></table></figure>
<p>mysql不带参数启动则是默认<code>cur_user_name@localhost:3306</code>，
表示使用当前用户名作为用户名登陆，如果该用户设置密码，<code>-p</code>
参数不能省略</p>
<h4 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mysql --defaults-file=file_name</span></span><br></pre></td></tr></table></figure>
<p>文件内容格式类似于配置文件</p>
<figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[client]</span></span><br><span class="line"><span class="attr">host</span>=</span><br><span class="line"><span class="attr">user</span>=</span><br><span class="line"><span class="attr">password</span>=</span><br><span class="line"><span class="attr">database</span>=（可选）</span><br></pre></td></tr></table></figure>
<h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ul>
<li>mysql中默认存在一个用户名为空的账户，只要在本地，可以
不用账户、密码登陆mysql，因为这个账号存在，使用新建用户
无法通过密码登陆，需要在<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ use mysql;</span><br><span class="line">$ <span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">User</span><span class="operator">=</span>&quot;&quot;;</span><br><span class="line">$ flush privileges;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Mysql交互命令"><a href="#Mysql交互命令" class="headerlink" title="Mysql交互命令"></a>Mysql交互命令</h3><h4 id="Show信息类"><a href="#Show信息类" class="headerlink" title="Show信息类"></a>Show信息类</h4><ul>
<li><code>SHOW DATABASES</code>：列出MySQLServer数据库。</li>
<li><code>SHOW TABLES [FROM db_name]</code>：列出数据库数据表。</li>
<li><code>SHOW TABLE STATUS [FROM db_name]</code>：列出数据表及表状态信息。</li>
<li><code>SHOW COLUMNS FROM tbl_name [FROM db_name]</code>：列出资料表字段<ul>
<li><code>DESC tbl_name</code>：同</li>
</ul>
</li>
<li><code>SHOW FIELDS FROM tbl_name [FROM db_name]</code></li>
<li><code>DESCRIBE tbl_name [col_name]</code></li>
<li><code>SHOW FULL COLUMNS FROM tbl_name [FROM db_name]</code>：列出字段及详情</li>
<li><code>SHOW FULL FIELDS FROM tbl_name [FROM db_name]</code>：列出字段完整属性</li>
<li><code>SHOW INDEX FROM tbl_name [FROM db_name]</code>：列出表索引</li>
<li><code>SHOW STATUS</code>：列出 DB Server 状态</li>
<li><code>SHOW VARIABLES [like pattern]</code>：列出 MySQL 系统环境变量</li>
<li><code>SHOW PROCESSLIST</code>：列出执行命令。</li>
<li><code>SHOW GRANTS FOR user</code>：列出某用户权限</li>
</ul>
<h4 id="User"><a href="#User" class="headerlink" title="User"></a>User</h4><ul>
<li><p><code>CREATE USER &#39;USER_NAME&#39;@&#39;ADDRESS&#39; IDENTIFIED BY &#39;PASSWORD&#39;</code></p>
<ul>
<li><code>IDENTIFIED BY PASSWORD</code>这个语法好像已经被丢弃了</li>
</ul>
</li>
<li><p><code>SET PASSWORD FOR &#39;USER_NAME&#39;@&#39;ADDRESS&#39; = PASSWORD(&#39;NEW_PWD&#39;)</code></p>
</li>
<li><p><code>SET PASSWORD = PASSWORD(&#39;NEW_PWD&#39;)</code>：修改当前用户密码</p>
</li>
<li><p><code>GRANT PRIVILEGES ON DB_NAME.TBL_NAME TO &#39;USER_NAME&#39;@&#39;ADDRESS&#39;
[WITH GRANT OPTION]</code></p>
<ul>
<li><code>WITH GRANT OPTION</code>：允许用户授权</li>
</ul>
</li>
<li><p><code>REVOKE PRIVILIEGES ON DB_NAME.TBL_NAME TO &#39;USER_NAME&#39;@&#39;ADDRES%&#39;</code></p>
</li>
<li><p><code>DROP &#39;USER_NAME&#39;@&#39;ADDRESS&#39;</code></p>
</li>
</ul>
<p>说明</p>
<ul>
<li><p>revoke和grant中的权限需要一致才能取消相应授权</p>
<ul>
<li><code>grant select</code>不能通过<code>revoke all</code>取消<code>select</code></li>
<li><code>grant all</code>也不能通过<code>revoke select</code>取消<code>select</code></li>
</ul>
</li>
<li><p>特殊符号</p>
<ul>
<li><code>%</code>：所有address，也可以是ip部分，如：111.111.111.%<ul>
<li>这个其实在sql语句中可以表示任意长度字符串</li>
</ul>
</li>
<li><code>*</code>：所有数据库、表</li>
</ul>
</li>
</ul>
<h4 id="Priviledges"><a href="#Priviledges" class="headerlink" title="Priviledges"></a>Priviledges</h4><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>权限</th>
</tr>
</thead>
<tbody>
<tr>
<td>alter</td>
<td>alter table</td>
</tr>
<tr>
<td>alter routine</td>
<td>alter or drop routines</td>
</tr>
<tr>
<td>create</td>
<td>create table</td>
</tr>
<tr>
<td>create routine</td>
<td>create routine</td>
</tr>
<tr>
<td>create temporary table</td>
<td>create temporary table</td>
</tr>
<tr>
<td>create user</td>
<td>create, drop, rename users and revoke all privilieges</td>
</tr>
<tr>
<td>create view</td>
<td>create view</td>
</tr>
<tr>
<td>delete</td>
<td>delete</td>
</tr>
<tr>
<td>drop</td>
<td>drop table</td>
</tr>
<tr>
<td>execute</td>
<td>run stored routines</td>
</tr>
<tr>
<td>file</td>
<td>select info outfile and load data infile</td>
</tr>
<tr>
<td>index</td>
<td>create index and drop index</td>
</tr>
<tr>
<td>insert</td>
<td>insert</td>
</tr>
<tr>
<td>lock tables</td>
<td>lock tables on tables for which select is granted</td>
</tr>
<tr>
<td>process</td>
<td>show full processlist</td>
</tr>
<tr>
<td>reload</td>
<td>use flush</td>
</tr>
<tr>
<td>replicati on client</td>
<td>ask where slave or master server are</td>
</tr>
<tr>
<td>replicati on slave</td>
<td></td>
</tr>
<tr>
<td>select</td>
<td>select</td>
</tr>
<tr>
<td>show databases</td>
<td>show databases</td>
</tr>
<tr>
<td>show view</td>
<td>show view</td>
</tr>
<tr>
<td>shutdown</td>
<td>use mysqladmin shutdown</td>
</tr>
<tr>
<td>super</td>
<td>change master, kill, purge master logs,</td>
</tr>
</tbody>
</table>
</div>
<pre><code>set global sql statements,    use mysqladmin
debug command, create an extra connection
even reach the maximum amount|
</code></pre><p>|update|update|
|usage|connect without any specific priviliege|</p>
<h3 id="执行Sql脚本"><a href="#执行Sql脚本" class="headerlink" title="执行Sql脚本"></a>执行Sql脚本</h3><ul>
<li><p>shell内执行</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h host -D db_name -u user -p &lt; file_name.sql;</span><br></pre></td></tr></table></figure>
</li>
<li><p>mysql命令行执行</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source file_name.sql;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="导入、导出数据"><a href="#导入、导出数据" class="headerlink" title="导入、导出数据"></a>导入、导出数据</h3><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><ul>
<li><p>shell内</p>
</li>
<li><p>mysql命令行内</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">load data [<span class="keyword">local</span>] infile <span class="string">&#x27;/path/to/file&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> tbl_name</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;sep_char&#x27;</span></span><br><span class="line">optionally enclosed <span class="keyword">by</span> <span class="string">&#x27;closure_char&#x27;</span></span><br><span class="line">escaped <span class="keyword">by</span> <span class="string">&#x27;esc_char&#x27;</span></span><br><span class="line">lines terminated <span class="keyword">by</span> `\r\n`;</span><br></pre></td></tr></table></figure>
<ul>
<li>若<code>/path/to/file</code>不是绝对路径，则被认为是相对于当前
数据库存储数据目录的相对路径，而不是当前目录</li>
<li>关键字<code>local</code>表示从客户端主机导入数据，否则从服务器
导入数据</li>
</ul>
</li>
</ul>
<h4 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h4><ul>
<li><p>shell内</p>
</li>
<li><p>mysql命令行内</p>
</li>
</ul>
<p><strong>注意</strong>：远程登陆mysql时，两种方式导出数据不同，shell导出
数据可以导出至client，而mysql命令行内导出至server</p>
<h2 id="Mysql-Server"><a href="#Mysql-Server" class="headerlink" title="Mysql-Server"></a>Mysql-Server</h2><h3 id="数据库字符编码方式"><a href="#数据库字符编码方式" class="headerlink" title="数据库字符编码方式"></a>数据库字符编码方式</h3><h4 id="查看"><a href="#查看" class="headerlink" title="查看"></a>查看</h4><ul>
<li><p>只查看<strong>数据库编码</strong>方式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> &quot;character_set_database;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看数据库<strong>相关的编码</strong>方式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> &quot;character%&quot;;</span><br></pre></td></tr></table></figure>
<p>|variable_name|value|
|——-|——-|
|character_set_client|latin1|
|character_set_connection|latin1|
|character_set_database|latin1|
|character_set_filesystem|binary|
|character_set_results|latin1|
|character_set_server|latin1|
|character_set_system|utf8|
|character_sets_dir|/usr/share/mysql/charsets/|</p>
</li>
<li><p>另一种查询数据库编码方式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> &quot;collation%&quot;;</span><br></pre></td></tr></table></figure>
<p>|Variable_name|Value|
|——-|——-|
|collation_connection|utf8mb4_general_ci|
|collation_database|utf8mb4_general_ci|
|collation_server|utf8mb4_general_ci|</p>
</li>
</ul>
<h4 id="相关变量说明"><a href="#相关变量说明" class="headerlink" title="相关变量说明"></a>相关变量说明</h4><ul>
<li><code>character_set_client</code>：客户端编码方式</li>
<li><code>character_set_connection</code>：建立连接是所用编码</li>
<li><code>character_set_database</code>：数据库编码</li>
<li><code>character_set_results</code>：结果集编码</li>
<li><code>character_set_server</code>：服务器编码</li>
</ul>
<p>保证以上编码方式相同则不会出现乱码问题，还需要注意其他连接
数据库的方式不一定同此编码方式，可能需要额外指定</p>
<h4 id="修改编码方式"><a href="#修改编码方式" class="headerlink" title="修改编码方式"></a>修改编码方式</h4><h5 id="修改数据库默认编码方式"><a href="#修改数据库默认编码方式" class="headerlink" title="修改数据库默认编码方式"></a>修改数据库默认编码方式</h5><p>修改mysql配置文件（<code>/etc/mysql/my.cnf</code>）</p>
<pre><code>#todo
mysql配置文件的优先级
utf8mb4意义
</code></pre><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[client]</span></span><br><span class="line"><span class="attr">default-character-set</span>=utf8mb4</span><br><span class="line"><span class="section">[mysqld]</span></span><br><span class="line"><span class="attr">default-character-set</span>=utf8mb4</span><br><span class="line"><span class="attr">init_connect</span>=<span class="string">&quot;SET NAMES utf8mb4&quot;</span></span><br></pre></td></tr></table></figure>
<p>重启mysql即可</p>
<h5 id="修改单个数据库"><a href="#修改单个数据库" class="headerlink" title="修改单个数据库"></a>修改单个数据库</h5><ul>
<li>创建时<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database db_name <span class="type">character</span> <span class="keyword">set</span> utf8 <span class="keyword">collate</span> utf8_general_ci;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> db_name defualt charater <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="脚本、窗口"><a href="#脚本、窗口" class="headerlink" title="脚本、窗口"></a>脚本、窗口</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> names gbk;</span><br></pre></td></tr></table></figure>
<p>只修改<code>character_set_client</code>、<code>character_set_connection</code>、
<code>character_set_results</code>编码方式，且只对当前窗口、脚本有效，
不影响数据库底层编码方式</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/Database/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/Database/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/Database/">1</a></li><li><a class="pagination-link" href="/categories/Database/page/2/">2</a></li><li><a class="pagination-link" href="/categories/Database/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6371777973" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>