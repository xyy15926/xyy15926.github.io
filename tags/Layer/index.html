<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: Layer - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="follow_it-verification-code" content="SVBypAPPHxjjr7Y4hHfn"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Layer</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">14 minutes read (About 2173 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/convolutional_layers.html">卷积层</a></h1><div class="content"><h2 id="Conv1D"><a href="#Conv1D" class="headerlink" title="Conv1D"></a>Conv1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Conv1D(</span><br><span class="line">	filters(<span class="built_in">int</span>),</span><br><span class="line">	kernel_size(<span class="built_in">int</span>),</span><br><span class="line">	strides=<span class="number">1</span>,</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	dilation_rate=<span class="number">1</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>一维卷积层（即时域卷积）</p>
<ul>
<li><p>说明</p>
<ul>
<li>用以在一维输入信号上进行邻域滤波</li>
<li>作为首层时，需要提供关键字参数<code>input_shape</code></li>
<li>该层生成将输入信号与卷积核按照单一的空域（或时域）
方向进行卷积</li>
<li>可以将Convolution1D看作Convolution2D的快捷版</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>filters</code>：卷积核的数目（即输出的维度）</p>
</li>
<li><p><code>kernel_size</code>：整数或由单个整数构成的list/tuple，
卷积核的空域或时域窗长度</p>
</li>
<li><p><code>strides</code>：整数或由单个整数构成的list/tuple，为卷积
步长</p>
<ul>
<li>任何不为1的strides均与任何不为1的dilation_rate
均不兼容</li>
</ul>
</li>
<li><p><code>padding</code>：补0策略</p>
</li>
<li><p><code>activation</code>：激活函数</p>
</li>
<li><p><code>dilation_rate</code>：整数或由单个整数构成的list/tuple，
指定dilated convolution中的膨胀比例</p>
<ul>
<li>任何不为1的dilation_rate均与任何不为1的strides
均不兼容</li>
</ul>
</li>
<li><p><code>use_bias</code>：布尔值，是否使用偏置项</p>
</li>
<li><p><code>kernel_initializer</code>：权值初始化方法</p>
<ul>
<li>预定义初始化方法名的字符串</li>
<li>用于初始化权重的初始化器（参考initializers）</li>
</ul>
</li>
<li><p><code>bias_initializer</code>：偏置初始化方法</p>
<ul>
<li>为预定义初始化方法名的字符串</li>
<li>用于初始化偏置的初始化器</li>
</ul>
</li>
<li><p><code>kernel_regularizer</code>：施加在权重上的正则项，为
Regularizer对象</p>
</li>
<li><p><code>bias_regularizer</code>：施加在偏置向量上的正则项</p>
</li>
<li><p><code>activity_regularizer</code>：施加在输出上的正则项</p>
</li>
<li><p><code>kernel_constraints</code>：施加在权重上的约束项</p>
</li>
<li><p><code>bias_constraints</code>：施加在偏置上的约束项</p>
</li>
</ul>
</li>
<li><p>输入：形如<code>(batch, steps, input_dim)</code>的3D张量</p>
</li>
<li><p>输出：形如<code>(batch, new_steps, filters)</code>的3D张量</p>
<ul>
<li>因为有向量填充的原因，<code>steps</code>的值会改变</li>
</ul>
</li>
</ul>
<h2 id="Conv2D"><a href="#Conv2D" class="headerlink" title="Conv2D"></a>Conv2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Conv2D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>二维卷积层，即对图像的空域卷积</p>
<ul>
<li><p>说明</p>
<ul>
<li>该层对二维输入进行滑动窗卷积</li>
<li>当使用该层作为第一层时，应提供</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>filters</code>：卷积核的数目（即输出的维度）</p>
</li>
<li><p><code>kernel_size</code>：单个整数或由两个整数构成的list/tuple，
卷积核的宽度和长度</p>
<ul>
<li>如为单个整数，则表示在各个空间维度的相同长度</li>
</ul>
</li>
<li><p><code>strides</code>：单个整数或由两个整数构成的list/tuple，
卷积的步长</p>
<ul>
<li>如为单个整数，则表示在各个空间维度的相同步长</li>
<li>任何不为1的strides均与任何不为1的dilation_rate
均不兼容</li>
</ul>
</li>
<li><p><code>padding</code>：补0策略</p>
</li>
<li><p><code>activation</code>：激活函数</p>
</li>
<li><p><code>dilation_rate</code>：单个或两个整数构成的list/tuple，
指定dilated convolution中的膨胀比例</p>
<ul>
<li>任何不为1的dilation_rate均与任何不为1的strides
均不兼容</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch, channels, rows, cols)</code>
（”channels_first”）4D张量</p>
</li>
<li><p>输出：<code>(batch, filters, new_rows, new_cols)</code>
（”channels_first”）4D张量</p>
<ul>
<li>输出的行列数可能会因为填充方法而改变</li>
</ul>
</li>
</ul>
<h2 id="SeparableConv2D"><a href="#SeparableConv2D" class="headerlink" title="SeparableConv2D"></a>SeparableConv2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.SeparableConv2D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	depth_multiplier=<span class="number">1</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	depthwise_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	pointwise_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	depthwise_regularizer=<span class="literal">None</span>,</span><br><span class="line">	pointwise_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	depthwise_constraint=<span class="literal">None</span>,</span><br><span class="line">	pointwise_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>该层是在深度方向上的可分离卷积。</p>
<ul>
<li><p>说明</p>
<ul>
<li>首先按深度方向进行卷积（对每个输入通道分别卷积）</li>
<li>然后逐点卷积，将上步卷积结果混合到输出通道中</li>
<li>直观来说，可分离卷积可以看做讲一个卷积核分解为两个小
卷积核，或看作Inception模块的一种极端情况</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>depth_multiplier</code>：按深度卷积的步骤中，每个输入通道
使用（产生）多少个输出通道</p>
</li>
<li><p><code>depthwise_regularizer</code>：按深度卷积的权重上的正则项</p>
</li>
<li><p><code>pointwise_regularizer</code>：按点卷积的权重上的正则项</p>
</li>
<li><p><code>depthwise_constraint</code>：按深度卷积权重上的约束项</p>
</li>
<li><p><code>pointwise_constraint</code>：在按点卷积权重的约束项</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>输入：<code>(batch, channels, rows, cols)</code>4DT
（”channels_first”)</p>
</li>
<li><p>输出：<code>(batch, filters, new_rows, new_cols)</code>4DTK
（”channels_first”）</p>
<ul>
<li>输出的行列数可能会因为填充方法而改变</li>
</ul>
</li>
</ul>
<h2 id="Conv2DTranspose"><a href="#Conv2DTranspose" class="headerlink" title="Conv2DTranspose"></a>Conv2DTranspose</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Conv2DTranspose(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	output_padding=<span class="literal">None</span>/<span class="built_in">int</span>/<span class="built_in">tuple</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>该层是反卷积操作（转置卷积）</p>
<ul>
<li><p>说明</p>
<ul>
<li>通常发生在用户想要对普通卷积的结果做反方向的变换</li>
<li>参考文献<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.07285">A guide to convolution arithmetic for deep learning</a></li>
<li><a target="_blank" rel="noopener" href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic">Transposed convolution arithmetic</a></li>
<li><a target="_blank" rel="noopener" href="http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf">Deconvolutional Networks</a></li>
</ul>
</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>output_padding</code>：指定输出的长、宽padding<ul>
<li>必须小于相应的<code>stride</code></li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch, rows, cols, channels)</code>4DT
（”channels_last”)</p>
</li>
<li><p>输出：<code>(batch, new_rows, new_cols, filters)</code>4DT
（”channels_last”）</p>
<ul>
<li>输出的行列数可能会因为填充方法而改变</li>
</ul>
</li>
</ul>
<h2 id="Conv3D"><a href="#Conv3D" class="headerlink" title="Conv3D"></a>Conv3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Conv3D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	dilation_rate=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>三维卷积对三维的输入（视频）进行滑动窗卷积</p>
<ul>
<li>输入：<code>(batch, channels, conv_dim1, conv_dim2, conv_dim3)</code>
5D张量（”channnels_first”）</li>
</ul>
<h2 id="Cropping1D"><a href="#Cropping1D" class="headerlink" title="Cropping1D"></a>Cropping1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Cropping1D(</span><br><span class="line">	cropping=(<span class="number">1</span>, <span class="number">1</span>)/<span class="built_in">tuple</span>/<span class="built_in">int</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在时间轴上对1D输入（即时间序列）进行裁剪</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>cropping</code>：指定在序列的首尾要裁剪掉多少个元素<ul>
<li>单值表示首尾裁剪相同</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch, axis_to_crop, features)</code>的3DT</p>
</li>
<li><p>输出：<code>(batch, cropped_axis, features)</code>的3DT</p>
</li>
</ul>
<h2 id="Cropping2D"><a href="#Cropping2D" class="headerlink" title="Cropping2D"></a>Cropping2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Cropping2D(</span><br><span class="line">	cropping=((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>)),</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对2D输入（图像）进行裁剪</p>
<ul>
<li><p>说明</p>
<ul>
<li>将在空域维度，即宽和高的方向上裁剪</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>cropping</code>：长为2的整数tuple，分别为宽和高方向上头部
与尾部需要裁剪掉的元素数<ul>
<li>单值表示宽高、首尾相同</li>
<li>单元组类似</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch, rows, cols, channels)</code>4DT（”channels_last”）</p>
</li>
<li><p>输出：<code>(batch, cropped_rows, cropped_cols, channels)</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">	<span class="comment"># Crop the input 2D images or feature maps</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Cropping2D(cropping=((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                     input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">3</span>)))</span><br><span class="line">	<span class="comment"># now model.output_shape == (None, 24, 20, 3)</span></span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">model.add(Cropping2D(cropping=((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>))))</span><br><span class="line">	<span class="comment"># now model.output_shape == (None, 20, 16, 64)</span></span><br></pre></td></tr></table></figure>
<h2 id="Cropping3D"><a href="#Cropping3D" class="headerlink" title="Cropping3D"></a>Cropping3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.Cropping3D(</span><br><span class="line">	cropping=((<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对3D输入（空间、时空）进行裁剪</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>cropping</code>：长为3的整数tuple，分别为三个方向上头部
与尾部需要裁剪掉的元素数</li>
</ul>
</li>
<li><p>输入：<code>(batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)</code>
（”channels_first”）</p>
</li>
<li><p>输出：<code>(batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)</code></p>
</li>
</ul>
<h2 id="UpSampling1D"><a href="#UpSampling1D" class="headerlink" title="UpSampling1D"></a>UpSampling1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.UpSampling1D(</span><br><span class="line">	size=<span class="number">2</span>/integer</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在时间轴上，将每个时间步重复<code>size</code>次</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>size</code>：轴上采样因子</li>
</ul>
</li>
<li><p>输入：<code>(batch, steps, features)</code>的3D张量</p>
</li>
<li><p>输出：<code>(batch, upsampled_steps, feature)</code>的3D张量</p>
</li>
</ul>
<h2 id="UpSampling2D"><a href="#UpSampling2D" class="headerlink" title="UpSampling2D"></a>UpSampling2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.UpSampling2D(</span><br><span class="line">	size=(<span class="number">2</span>, <span class="number">2</span>)/<span class="built_in">tuple</span>/<span class="built_in">int</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将数据的行和列分别重复<code>size[0]</code>和<code>size[1]</code>次</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>size</code>：分别为行和列上采样因子</li>
</ul>
</li>
<li><p>输入：<code>(batch, channels, rows, cols)</code>的4D张量
（”channels_first”）</p>
</li>
<li><p>输出：<code>(batch, channels, upsampled_rows, upsampled_cols)</code></p>
</li>
</ul>
<h2 id="UpSampling3D"><a href="#UpSampling3D" class="headerlink" title="UpSampling3D"></a>UpSampling3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.UpSampling3D(</span><br><span class="line">	size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)/<span class="built_in">tuple</span>/<span class="built_in">int</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将数据的三个维度上分别重复<code>size</code>次</p>
<ul>
<li><p>说明</p>
<ul>
<li>本层目前只能在使用Theano为后端时可用</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>size</code>：代表在三个维度上的上采样因子</li>
</ul>
</li>
</ul>
<ul>
<li><p>输入：<code>(batch, dim1, dim2, dim3, channels)</code>5DT
（”channels_last”）</p>
</li>
<li><p>输出：<code>(batch, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)</code></p>
</li>
</ul>
<h2 id="ZeroPadding1D"><a href="#ZeroPadding1D" class="headerlink" title="ZeroPadding1D"></a>ZeroPadding1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.ZeroPadding1D(</span><br><span class="line">	padding=<span class="number">1</span>/<span class="built_in">int</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对1D输入的首尾端（如时域序列）填充0</p>
<ul>
<li><p>说明</p>
<ul>
<li>以控制卷积以后向量的长度</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>padding</code>：整数，在axis 1起始和结束处填充0数目</li>
</ul>
</li>
<li><p>输入：<code>(batch, axis_to_pad, features)</code>3DT</p>
</li>
<li><p>输出：<code>(batch, paded_axis, features)</code>3DT</p>
</li>
</ul>
<h2 id="ZeroPadding2D"><a href="#ZeroPadding2D" class="headerlink" title="ZeroPadding2D"></a>ZeroPadding2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.ZeroPadding2D(</span><br><span class="line">	padding=(<span class="number">1</span>, <span class="number">1</span>)/<span class="built_in">tuple</span>/<span class="built_in">int</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对2D输入（如图片）的边界填充0</p>
<ul>
<li><p>说明</p>
<ul>
<li>以控制卷积以后特征图的大小</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>padding</code>：在要填充的轴的起始和结束处填充0的数目</li>
</ul>
</li>
</ul>
<h2 id="ZeroPadding3D"><a href="#ZeroPadding3D" class="headerlink" title="ZeroPadding3D"></a>ZeroPadding3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.ZeroPadding3D(</span><br><span class="line">	padding=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将数据的三个维度上填充0</p>
<ul>
<li>说明<ul>
<li>本层目前只能在使用Theano为后端时可用</li>
</ul>
</li>
</ul>
<p>结束处填充0的数目</p>
<h2 id="ZeroPadding3D-1"><a href="#ZeroPadding3D-1" class="headerlink" title="ZeroPadding3D"></a>ZeroPadding3D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.convolutional.ZeroPadding3D(</span><br><span class="line">	padding=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将数据的三个维度上填充0</p>
<ul>
<li>说明<ul>
<li>本层目前只能在使用Theano为后端时可用</li>
</ul>
</li>
</ul>
<p>?时可用</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:42:51.000Z" title="8/4/2021, 11:42:51 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">4 minutes read (About 578 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/about_layers.html">Layers 总述</a></h1><div class="content"><h3 id="Layer方法"><a href="#Layer方法" class="headerlink" title="Layer方法"></a>Layer方法</h3><p>所有的Keras层对象都有如下方法：</p>
<ul>
<li><p><code>layer.get_weights()</code>：返回层的权重NDA</p>
</li>
<li><p><code>layer.set_weights(weights)</code>：从NDA中将权重加载到该层中
，要求NDA的形状与<code>layer.get_weights()</code>的形状相同</p>
</li>
<li><p><code>layer.get_config()</code>：返回当前层配置信息的字典，层也可以
借由配置信息重构</p>
</li>
<li><p><code>layer.from_config(config)</code>：根据<code>config</code>配置信息重构层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">layer = Dense(<span class="number">32</span>)</span><br><span class="line">config = layer.get_config()</span><br><span class="line">reconstructed_layer = Dense.from_config(config)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">config = layer.get_config()</span><br><span class="line">layer = layers.deserialize(&#123;<span class="string">&#x27;class_name&#x27;</span>: layer.__class__.__name__,</span><br><span class="line">							<span class="string">&#x27;config&#x27;</span>: config&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="非共享层"><a href="#非共享层" class="headerlink" title="非共享层"></a>非共享层</h4><p>如果层仅有一个计算节点（即该层不是共享层），则可以通过下列
方法获得</p>
<ul>
<li>输入张量：<code>layer.input</code></li>
<li>输出张量：<code>layer.output</code></li>
<li>输入数据的形状：<code>layer.input_shape</code></li>
<li>输出数据的形状：<code>layer.output_shape</code></li>
</ul>
<h4 id="共享层"><a href="#共享层" class="headerlink" title="共享层"></a>共享层</h4><p>如果该层有多个计算节点（参考层计算节点和共享层）</p>
<ul>
<li>输入张量：<code>layer.get_input_at(node_index)</code></li>
<li>输出张量：<code>layer.get_output_at(node_index)</code></li>
<li>输入数据形状：<code>layer.get_input_shape_at(node_index)</code></li>
<li>输出数据形状：<code>layer.get_output_shape_at(node_index)</code></li>
</ul>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><h4 id="shape类型"><a href="#shape类型" class="headerlink" title="shape类型"></a>shape类型</h4><ul>
<li><p>batch_size</p>
<ul>
<li>batch_size在实际数据输入中为首维（0维）</li>
<li>shape类型参数传递的tuple中一般不包括batch_size维度</li>
<li>输出时使用<code>None</code>表示<code>(batch_size,...)</code></li>
</ul>
</li>
<li><p>time_step</p>
<ul>
<li>对时序数据，time_step在实际数据输入中第二维（1维）</li>
</ul>
</li>
</ul>
<h5 id="input-shape"><a href="#input-shape" class="headerlink" title="input_shape"></a><code>input_shape</code></h5><ul>
<li><p>是<code>Layer</code>的初始化参数，所有<code>Layer</code>子类都具有</p>
</li>
<li><p>如果Layer是首层，需要传递该参数指明输入数据形状，否则
无需传递该参数</p>
<ul>
<li>有些子类有类似于<code>input_dim</code>等参数具有<code>input_shape</code>
部分功能</li>
</ul>
</li>
<li><p><code>None</code>：表示该维度变长</p>
</li>
</ul>
<h3 id="输入、输出"><a href="#输入、输出" class="headerlink" title="输入、输出"></a>输入、输出</h3><ul>
<li><p>channels/depth/features：时间、空间单位上独立的数据，
卷积应该在每个channal分别“独立”进行</p>
<ul>
<li>对1维时序（时间），channels就是每时刻的features</li>
<li>对2维图片（空间），channels就是色彩通道</li>
<li>对3维视频（时空），channels就是每帧色彩通道</li>
<li>中间数据，channnels就是每个filters的输出</li>
</ul>
</li>
<li><p><em>1D</em>：<code>(batch, dim, channels)</code>（<em>channels_last</em>）</p>
</li>
<li><p><em>2D</em>：<code>(batch, dim_1, dim_2, channels)</code>
（<em>channels_last</em>）</p>
</li>
<li><p><em>3D</em>：<code>(batch, dim_1, dim_2, dim_3, channels)</code>
（<em>channels_last</em>）</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">3 minutes read (About 472 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/advanced_activations_layers.html">高级激活层</a></h1><div class="content"><h3 id="LeakyReLU"><a href="#LeakyReLU" class="headerlink" title="LeakyReLU"></a>LeakyReLU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LeakyReLU(alpha=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p>带泄漏的修正线性单元。</p>
<ul>
<li><p>返回值：当神经元未激活时，它仍可以赋予其一个很小的梯度</p>
<ul>
<li><code>x &lt; 0</code>：<code>alpha * x</code></li>
<li><code>x &gt;= 0</code>：<code>x</code></li>
</ul>
</li>
<li><p>输入尺寸</p>
<ul>
<li>可以是任意的。如果将该层作为模型的第一层，需要指定
<code>input_shape</code>参数（整数元组，不包含样本数量的维度）</li>
</ul>
</li>
<li><p>输出尺寸：与输入相同</p>
</li>
<li><p>参数</p>
<ul>
<li><code>alpha</code>：<code>float &gt;= 0</code>，负斜率系数。</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf">Rectifier Nonlinearities Improve Neural Network Acoustic Models</a></li>
</ul>
</li>
</ul>
<h3 id="PReLU"><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.PReLU(</span><br><span class="line">	alpha_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	alpha_regularizer=<span class="literal">None</span>,</span><br><span class="line">	alpha_constraint=<span class="literal">None</span>,</span><br><span class="line">	shared_axes=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>参数化的修正线性单元。</p>
<ul>
<li><p>返回值</p>
<ul>
<li><code>x &lt; 0</code>：<code>alpha * x</code></li>
<li><code>x &gt;= 0</code>：<code>x</code></li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>alpha_initializer</code>: 权重的初始化函数。</li>
<li><code>alpha_regularizer</code>: 权重的正则化方法。</li>
<li><code>alpha_constraint</code>: 权重的约束。</li>
<li><code>shared_axes</code>: 激活函数共享可学习参数的轴。
如果输入特征图来自输出形状为
<code>(batch, height, width, channels)</code>
的2D卷积层，而且你希望跨空间共享参数，以便每个滤波
器只有一组参数，可设置<code>shared_axes=[1, 2]</code></li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
</ul>
</li>
</ul>
<h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ELU(alpha=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p>指数线性单元</p>
<ul>
<li><p>返回值</p>
<ul>
<li><code>x &lt; 0</code>：<code>alpha * (exp(x) - 1.)</code></li>
<li><code>x &gt;= 0</code>：<code>x</code></li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>alpha</code>：负因子的尺度。</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.07289v1">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</a></li>
</ul>
</li>
</ul>
<h3 id="ThresholdedReLU"><a href="#ThresholdedReLU" class="headerlink" title="ThresholdedReLU"></a>ThresholdedReLU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ThresholdedReLU(theta=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p>带阈值的修正线性单元。</p>
<ul>
<li><p>返回值</p>
<ul>
<li><code>x &gt; theta</code>：<code>x</code></li>
<li><code>x &lt;= theta</code>：0</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>theta</code>：<code>float &gt;= 0</code>激活的阈值位。</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1402.3337">Zero-Bias Autoencoders and the Benefits of Co-Adapting Features</a></li>
</ul>
</li>
</ul>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Softmax(axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>Softmax激活函数</p>
<ul>
<li>参数<ul>
<li><code>axis</code>: 整数，应用 softmax 标准化的轴。</li>
</ul>
</li>
</ul>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ReLU(max_value=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>ReLU激活函数</p>
<ul>
<li>参数<ul>
<li><code>max_value</code>：浮点数，最大的输出值。</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">11 minutes read (About 1690 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/core_layers.html">常用层</a></h1><div class="content"><p>常用层对应于core模块，core内部定义了一系列常用的网络层，包括
全连接、激活层等</p>
<h2 id="Dense层"><a href="#Dense层" class="headerlink" title="Dense层"></a>Dense层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Dense(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Dense就是常用的全连接层</p>
<ul>
<li><p>用途：实现运算$output = activation(dot(input, kernel)+bias)$</p>
<ul>
<li><code>activation</code>：是逐元素计算的激活函数</li>
<li><code>kernel</code>：是本层的权值矩阵</li>
<li><code>bias</code>：为偏置向量，只有当<code>use_bias=True</code>才会添加</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>units</code>：大于0的整数，代表该层的输出维度。</p>
</li>
<li><p><code>activation</code>：激活函数</p>
<ul>
<li>为预定义的激活函数名（参考激活函数）</li>
<li>逐元素（element-wise）的Theano函数</li>
<li>不指定该参数，将不会使用任何激活函数
（即使用线性激活函数：a(x)=x）</li>
</ul>
</li>
<li><p><code>use_bias</code>: 布尔值，是否使用偏置项</p>
</li>
<li><p><code>kernel_initializer</code>：权值初始化方法</p>
<ul>
<li>预定义初始化方法名的字符串</li>
<li>用于初始化权重的初始化器（参考initializers）</li>
</ul>
</li>
<li><p><code>bias_initializer</code>：偏置向量初始化方法</p>
<ul>
<li>为预定义初始化方法名的字符串</li>
<li>用于初始化偏置向量的初始化器（参考initializers）</li>
</ul>
</li>
<li><p><code>kernel_regularizer</code>：施加在权重上的正则项，为
Regularizer对象</p>
</li>
<li><p><code>bias_regularizer</code>：施加在偏置向量上的正则项，为
Regularizer对象</p>
</li>
<li><p><code>activity_regularizer</code>：施加在输出上的正则项，为
Regularizer对象</p>
</li>
<li><p><code>kernel_constraints</code>：施加在权重上的约束项，为<br>Constraints对象</p>
</li>
<li><p><code>bias_constraints</code>：施加在偏置上的约束项，为
Constraints对象</p>
</li>
</ul>
</li>
<li><p>输入</p>
<ul>
<li>形如<code>(batch_size, ..., input_dim)</code>的NDT，最常见情况
为<code>(batch_size, input_dim)</code>的2DT</li>
<li>数据的维度大于2，则会先被压为与<code>kernel</code>相匹配的大小</li>
</ul>
</li>
<li><p>输出</p>
<ul>
<li>形如<code>(batch_size, ..., units)</code>的NDT，最常见的情况为
$(batch_size, units)$的2DT</li>
</ul>
</li>
</ul>
<h2 id="Activation层"><a href="#Activation层" class="headerlink" title="Activation层"></a>Activation层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Activation(</span><br><span class="line">	activation,</span><br><span class="line">	input_shape</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>激活层对一个层的输出施加激活函数</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>activation</code>：将要使用的激活函数<ul>
<li>预定义激活函数名</li>
<li>Tensorflow/Theano的函数（参考激活函数）</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：任意，使用激活层作为第一层时，要指定<code>input_shape</code></p>
</li>
<li><p>输出：与输入shape相同</p>
</li>
</ul>
<h2 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Dropout(</span><br><span class="line">	rate,</span><br><span class="line">	noise_shape=<span class="literal">None</span>,</span><br><span class="line">	seed=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为输入数据施加Dropout</p>
<ul>
<li><p>说明</p>
<ul>
<li><p>Dropout将在训练过程中每次更新参数时按一定概率<code>rate</code>
随机断开输入神经元</p>
</li>
<li><p>可以用于防止过拟合</p>
</li>
<li><p>参考文献：<a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>参数</p>
<ul>
<li><p><code>rate</code>：0~1的浮点数，控制需要断开的神经元的比例</p>
</li>
<li><p><code>noise_shape</code>：整数张量，为将要应用在输入上的二值
Dropout mask的shape</p>
</li>
<li><p><code>seed</code>：整数，使用的随机数种子</p>
</li>
</ul>
</li>
<li><p>输入</p>
<ul>
<li>例：<code>(batch_size, timesteps, features)</code>，希望在各个
时间步上Dropout mask都相同，则可传入
<code>noise_shape=(batch_size, 1, features)</code></li>
</ul>
</li>
</ul>
<h2 id="Flatten层"><a href="#Flatten层" class="headerlink" title="Flatten层"></a>Flatten层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Flatten()</span><br></pre></td></tr></table></figure>
<p>Flatten层用来将输入“压平”，把多维的输入一维化</p>
<ul>
<li>常用在从卷积层到全连接层的过渡</li>
<li>Flatten不影响batch的大小。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Convolution2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>,</span><br><span class="line">            border_mode=<span class="string">&#x27;same&#x27;</span>,</span><br><span class="line">            input_shape=(<span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 64, 32, 32)</span></span><br><span class="line"></span><br><span class="line">model.add(Flatten())</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 65536)</span></span><br></pre></td></tr></table></figure>
<h2 id="Reshape层"><a href="#Reshape层" class="headerlink" title="Reshape层"></a>Reshape层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Reshape(</span><br><span class="line">	target_shape,</span><br><span class="line">	input_shape</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Reshape层用来将输入shape转换为特定的shape</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>target_shape</code>：目标shape，为整数的tuple，不包含样本
数目的维度（batch大小）<ul>
<li>包含<code>-1</code>表示推断该维度大小</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：输入的shape必须固定（和<code>target_shape</code>积相同）</p>
</li>
<li><p>输出：<code>(batch_size, *target_shape)</code></p>
</li>
<li><p>例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Reshape((<span class="number">3</span>, <span class="number">4</span>), input_shape=(<span class="number">12</span>,)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 3, 4)</span></span><br><span class="line">	<span class="comment"># note: `None` is the batch dimension</span></span><br><span class="line"></span><br><span class="line">model.add(Reshape((<span class="number">6</span>, <span class="number">2</span>)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 6, 2)</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># also supports shape inference using `-1` as dimension</span></span><br><span class="line">model.add(Reshape((-<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 3, 2, 2)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Permute层"><a href="#Permute层" class="headerlink" title="Permute层"></a>Permute层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Permute(</span><br><span class="line">	dims(<span class="built_in">tuple</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Permute层将输入的维度按照给定模式进行重排</p>
<ul>
<li><p>说明</p>
<ul>
<li>当需要将RNN和CNN网络连接时，可能会用到该层。</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>dims</code>：指定重排的模式，不包含样本数的维度（即下标
从1开始）</li>
</ul>
</li>
<li><p>输出shape</p>
<ul>
<li>与输入相同，但是其维度按照指定的模式重新排列</li>
</ul>
</li>
<li><p>例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Permute((<span class="number">2</span>, <span class="number">1</span>), input_shape=(<span class="number">10</span>, <span class="number">64</span>)))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 64, 10)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="RepeatVector层"><a href="#RepeatVector层" class="headerlink" title="RepeatVector层"></a>RepeatVector层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.RepeatVector(</span><br><span class="line">	n(<span class="built_in">int</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>RepeatVector层将输入重复n次</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>n</code>：整数，重复的次数</li>
</ul>
</li>
<li><p>输入：形如<code>(batch_size, features)</code>的张量</p>
</li>
<li><p>输出：形如<code>(bathc_size, n, features)</code>的张量</p>
</li>
<li><p>例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">32</span>, input_dim=<span class="number">32</span>))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 32)</span></span><br><span class="line"></span><br><span class="line">model.add(RepeatVector(<span class="number">3</span>))</span><br><span class="line">	<span class="comment"># now: model.output_shape == (None, 3, 32)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Lambda层"><a href="#Lambda层" class="headerlink" title="Lambda层"></a>Lambda层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Lambda(</span><br><span class="line">	function,</span><br><span class="line">	output_shape=<span class="literal">None</span>,</span><br><span class="line">	mask=<span class="literal">None</span>,</span><br><span class="line">	arguments=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对上一层的输出施以任何Theano/TensorFlow表达式</p>
<ul>
<li><p>参数</p>
<ul>
<li><p><code>function</code>：要实现的函数，该函数仅接受一个变量，即
上一层的输出</p>
</li>
<li><p><code>output_shape</code>：函数应该返回的值的shape，可以是一个
tuple，也可以是一个根据输入shape计算输出shape的函数</p>
</li>
<li><p><code>mask</code>: 掩膜</p>
</li>
<li><p><code>arguments</code>：可选，字典，用来记录向函数中传递的其他
关键字参数</p>
</li>
</ul>
</li>
<li><p>输出：<code>output_shape</code>参数指定的输出shape，使用TF时可自动
推断</p>
</li>
</ul>
<ul>
<li><p>例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.add(Lambda(<span class="keyword">lambda</span> x: x ** <span class="number">2</span>))</span><br><span class="line">	<span class="comment"># add a x -&gt; x^2 layer</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add a layer that returns the concatenation</span></span><br><span class="line"><span class="comment"># of the positive part of the input and</span></span><br><span class="line"><span class="comment"># the opposite of the negative part</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">antirectifier</span>(<span class="params">x</span>):</span></span><br><span class="line">	x -= K.mean(x, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">	x = K.l2_normalize(x, axis=<span class="number">1</span>)</span><br><span class="line">	pos = K.relu(x)</span><br><span class="line">	neg = K.relu(-x)</span><br><span class="line">	<span class="keyword">return</span> K.concatenate([pos, neg], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">antirectifier_output_shape</span>(<span class="params">input_shape</span>):</span></span><br><span class="line">	shape = <span class="built_in">list</span>(input_shape)</span><br><span class="line">	<span class="keyword">assert</span> <span class="built_in">len</span>(shape) == <span class="number">2</span>  <span class="comment"># only valid for 2D tensors</span></span><br><span class="line">	shape[-<span class="number">1</span>] *= <span class="number">2</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">tuple</span>(shape)</span><br><span class="line"></span><br><span class="line">model.add(Lambda(antirectifier,</span><br><span class="line">		 output_shape=antirectifier_output_shape))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="ActivityRegularizer层"><a href="#ActivityRegularizer层" class="headerlink" title="ActivityRegularizer层"></a>ActivityRegularizer层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.ActivityRegularization(</span><br><span class="line">	l1=<span class="number">0.0</span>,</span><br><span class="line">	l2=<span class="number">0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>经过本层的数据不会有任何变化，但会基于其激活值更新损失函数值</p>
<ul>
<li>参数<ul>
<li><code>l1</code>：1范数正则因子（正浮点数）</li>
<li><code>l2</code>：2范数正则因子（正浮点数）</li>
</ul>
</li>
</ul>
<h2 id="Masking层"><a href="#Masking层" class="headerlink" title="Masking层"></a>Masking层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.core.Masking(mask_value=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
<p>使用给定的值对输入的序列信号进行“屏蔽”</p>
<ul>
<li><p>说明</p>
<ul>
<li>用以定位需要跳过的时间步</li>
<li>对于输入张量的时间步，如果输入张量在该时间步上都等于
<code>mask_value</code>，则该时间步将在模型接下来的所有层
（只要支持masking）被跳过（屏蔽）。</li>
<li>如果模型接下来的一些层不支持masking，却接受到masking
过的数据，则抛出异常</li>
</ul>
</li>
<li><p>输入：形如<code>(samples,timesteps,features)</code>的张量</p>
</li>
</ul>
<ul>
<li>例：缺少时间步为3和5的信号，希望将其掩盖<ul>
<li>方法：赋值<code>x[:,3,:] = 0., x[:,5,:] = 0.</code></li>
<li>在LSTM层之前插入<code>mask_value=0.</code>的Masking层<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Masking(mask_value=<span class="number">0.</span>, input_shape=(timesteps, features)))</span><br><span class="line">model.add(LSTM(<span class="number">32</span>))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p>.`的Masking层
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(Masking(mask_value=<span class="number">0.</span>, input_shape=(timesteps, features)))</span><br><span class="line">model.add(LSTM(<span class="number">32</span>))</span><br></pre></td></tr></table></figure></p>
<p>```</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">2 minutes read (About 229 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/locally_connected_layers.html">LocallyConnceted 局部连接层</a></h1><div class="content"><p>LocallyConnnected和Conv差不多，只是Conv每层共享卷积核，
这里不同位置卷积核独立</p>
<h2 id="LocallyConnected1D层"><a href="#LocallyConnected1D层" class="headerlink" title="LocallyConnected1D层"></a>LocallyConnected1D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.local.LocallyConnected1D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=<span class="number">1</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>类似于<code>Conv1D</code>，单卷积核权重不共享</p>
<h2 id="LocallyConnected2D层"><a href="#LocallyConnected2D层" class="headerlink" title="LocallyConnected2D层"></a>LocallyConnected2D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.local.LocallyConnected2D(</span><br><span class="line">	filters,</span><br><span class="line">	kernel_size,</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	activation=<span class="literal">None</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>类似<code>Conv2D</code>，区别是不进行权值共享</p>
<ul>
<li>说明<ul>
<li>输出的行列数可能会因为填充方法而改变</li>
</ul>
</li>
<li>例<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(LocallyConnected2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line">	<span class="comment"># apply a 3x3 unshared weights convolution with 64 output filters on a 32x32 image</span></span><br><span class="line">	<span class="comment"># with `data_format=&quot;channels_last&quot;`:</span></span><br><span class="line">	<span class="comment"># now model.output_shape == (None, 30, 30, 64)</span></span><br><span class="line">	<span class="comment"># notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters</span></span><br><span class="line"></span><br><span class="line">model.add(LocallyConnected2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>)))</span><br><span class="line">	<span class="comment"># now model.output_shape == (None, 28, 28, 32)</span></span><br><span class="line">	<span class="comment"># add a 3x3 unshared weights convolution on top, with 32 output filters:</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">2 minutes read (About 374 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/pooling_layers.html">池化层</a></h1><div class="content"><h2 id="MaxPooling1D"><a href="#MaxPooling1D" class="headerlink" title="MaxPooling1D"></a>MaxPooling1D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.MaxPooling1D(</span><br><span class="line">	pool_size=<span class="number">2</span>/<span class="built_in">int</span>,</span><br><span class="line">	strides=<span class="literal">None</span>/<span class="built_in">int</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span></span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对时域1D信号进行最大值池化</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>pool_size</code>：整数，池化窗口大小</li>
<li><code>strides</code>：整数或None，下采样因子，例如设2将会使得
输出shape为输入的一半，若为None则默认值为pool_size。</li>
</ul>
</li>
</ul>
<h2 id="MaxPooling2D"><a href="#MaxPooling2D" class="headerlink" title="MaxPooling2D"></a>MaxPooling2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.MaxPooling2D(</span><br><span class="line">	pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">	strides=<span class="literal">None</span>/<span class="built_in">int</span>/(<span class="built_in">int</span>),</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>/<span class="string">&quot;same&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为空域2D信号施加最大值池化 </p>
<h2 id="MaxPooling3D层"><a href="#MaxPooling3D层" class="headerlink" title="MaxPooling3D层"></a>MaxPooling3D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.MaxPooling3D(</span><br><span class="line">	pool_size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">	strides=<span class="literal">None</span>/<span class="built_in">int</span>/(<span class="built_in">int</span>),</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>/<span class="string">&quot;same&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为3D信号（空域或时空域）施加最大值池化</p>
<h2 id="AveragePooling1D层"><a href="#AveragePooling1D层" class="headerlink" title="AveragePooling1D层"></a>AveragePooling1D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.AveragePooling1D(</span><br><span class="line">	pool_size=<span class="number">2</span>,</span><br><span class="line">	strides=<span class="literal">None</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span></span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对1D信号（时域）进行平均值池化</p>
<h2 id="AveragePooling2D层"><a href="#AveragePooling2D层" class="headerlink" title="AveragePooling2D层"></a>AveragePooling2D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.AveragePooling2D(</span><br><span class="line">	pool_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">	strides=<span class="literal">None</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为2D（空域）信号施加平均值池化</p>
<h2 id="AveragePooling3D层"><a href="#AveragePooling3D层" class="headerlink" title="AveragePooling3D层"></a>AveragePooling3D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.AveragePooling3D(</span><br><span class="line">	pool_size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">	strides=<span class="literal">None</span>,</span><br><span class="line">	padding=<span class="string">&quot;valid&quot;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为3D信号（空域或时空域）施加平均值池化</p>
<h2 id="GlobalMaxPooling1D层"><a href="#GlobalMaxPooling1D层" class="headerlink" title="GlobalMaxPooling1D层"></a>GlobalMaxPooling1D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.GlobalMaxPooling1D(</span><br><span class="line">	data_format=<span class="string">&quot;channels_last&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>对于1D（时间）信号的全局最大池化</p>
<h2 id="GlobalAveragePooling1D层"><a href="#GlobalAveragePooling1D层" class="headerlink" title="GlobalAveragePooling1D层"></a>GlobalAveragePooling1D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.GlobalAveragePooling1D(</span><br><span class="line">	data_forma=<span class="string">&quot;channels_last&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为时域信号施加全局平均值池化</p>
<h2 id="GlobalMaxPooling2D层"><a href="#GlobalMaxPooling2D层" class="headerlink" title="GlobalMaxPooling2D层"></a>GlobalMaxPooling2D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.GlobalMaxPooling2D(</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为空域信号施加全局最大值池化</p>
<h2 id="GlobalAveragePooling2D层"><a href="#GlobalAveragePooling2D层" class="headerlink" title="GlobalAveragePooling2D层"></a>GlobalAveragePooling2D层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.pooling.GlobalAveragePooling2D(</span><br><span class="line">	data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为2D（空域）信号施加全局平均值池化</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-02-20T15:58:15.000Z" title="2/20/2019, 11:58:15 PM">2019-02-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:07.000Z" title="2/17/2019, 11:57:07 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/Keras/">Keras</a></span><span class="level-item">18 minutes read (About 2770 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/Keras/recurrent_layers.html">RNN</a></h1><div class="content"><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.RNN(</span><br><span class="line">	cell,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	unroll=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>循环神经网络层基类：抽象类、无法实例化对象</p>
<ul>
<li><p>参数</p>
<ul>
<li><p><code>cell</code>：RNN单元实例、列表，为RNN单元列表时，单元
 堆叠放置，实现高效堆叠RNN</p>
</li>
<li><p><code>return_sequences</code>：返回输出序列最后值/全部序列</p>
</li>
<li><p><code>return_state</code>：是否返回最后一个状态</p>
</li>
<li><p><code>go_backwards</code>：是否向后处理输入序列并返回相反的序列。</p>
</li>
<li><p><code>stateful</code>：批次中索引<code>i</code>处的每个样品的最后状态将
用作下一批次中索引<code>i</code>样品的初始状态</p>
</li>
<li><p><code>unroll</code>：是否将网络展开（否则将使用符号循环）</p>
<ul>
<li>展开可以加速 RNN，但它往往会占用更多的内存</li>
<li>展开只适用于短序列</li>
</ul>
</li>
<li><p><code>input_dim</code>：输入的维度（整数）</p>
<ul>
<li>将此层用作模型中的第一层时，此参数是必需的
（或者关键字参数 <code>input_shape</code>）</li>
</ul>
</li>
<li><p><code>input_length</code>： 输入序列的长度，在恒定时指定</p>
<ul>
<li>如果你要在上游连接 <code>Flatten</code> 和 <code>Dense</code> 层，则
需要此参数（没有它，无法计算全连接输出的尺寸）</li>
<li>如果循环神经网络层不是模型中的第一层，则需要在
第一层的层级指定输入长度
（或通过关键字参数<code>input_shape</code>）</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：<code>(batch_size, timesteps, input_dim)</code>3D张量</p>
</li>
<li><p>输出</p>
<ul>
<li><code>return_state=True</code>：则返回张量列表，第一个张量为
输出、剩余的张量为最后的状态，每个张量的尺寸为
<code>(batch_size, units)</code>。</li>
<li><code>return_state=False</code>：<code>(batch_size, units)</code>2D张量</li>
</ul>
</li>
</ul>
<p>说明</p>
<ul>
<li><p>屏蔽覆盖：支持以可变数量的时间步长对输入数据进行屏蔽覆盖</p>
</li>
<li><p>使用状态：可以将 RNN 层设置为 <code>stateful</code>（有状态的）</p>
<ul>
<li>这意味着针对一批中的样本计算的状态将被重新用作下一批
样品的初始状态</li>
<li><p>这假定在不同连续批次的样品之间有一对一的映射。</p>
</li>
<li><p>为了使状态有效：</p>
<ul>
<li>在层构造器中指定 <code>stateful=True</code>。</li>
<li>为模型指定一个固定的批次大小<ul>
<li>顺序模型：为模型的第一层传递一个
<code>batch_input_shape=(...)</code> 参数</li>
<li>带有Input层的函数式模型，为的模型的<strong>所有</strong>i
第一层传递一个<code>batch_shape=(...)</code>，这是输入
预期尺寸，包括批量维度</li>
</ul>
</li>
<li>在调用 <code>fit()</code> 是指定 <code>shuffle=False</code>。</li>
</ul>
</li>
<li><p>要重置模型的状态，请在特定图层或整个模型上调用
<code>.reset_states()</code></p>
</li>
</ul>
</li>
<li><p>初始状态</p>
<ul>
<li>通过使用关键字参数<code>initial_state</code>调用它们来符号化地
指定 RNN 层的初始状态（值应该是表示RNN层初始状态的
张量或张量列表）</li>
<li>通过调用带有关键字参数<code>states</code>的<code>reset_states</code>方法
来数字化地指定 RNN 层的初始状态（值应该是一个代表RNN
层初始状态的NDA/[NDA]）</li>
</ul>
</li>
<li><p>RNN单元对象需要具有</p>
<ul>
<li><code>call(input_at_t, states_at_t)</code>方法，它返回
<code>(output_at_t, states_at_t_plus_1)</code>，单元的调用
方法也可以采用可选参数 <code>constants</code></li>
<li><code>state_size</code>属性<ul>
<li>单个整数（单个状态）：在这种情况下，它是循环层
状态大小（应该与单元输出的大小相同）</li>
<li>整数的列表/元组（每个状态一个大小）：第一项应该
与单元输出的大小相同</li>
</ul>
</li>
</ul>
</li>
<li><p>传递外部常量</p>
<ul>
<li>使用<code>RNN.call</code>以及<code>RNN.call</code>的<code>constants</code>关键字
参数将<em>外部</em>常量传递给单元</li>
<li>要求<code>cell.call</code>方法接受相同的关键字参数<code>constants</code>，
这些常数可用于调节附加静态输入（不随时间变化）上的
单元转换，也可用于注意力机制</li>
</ul>
</li>
</ul>
<p>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MinimalRNNCell</span>(<span class="params">keras.layers.Layer</span>):</span></span><br><span class="line">	<span class="comment"># 定义RNN细胞单元（网络层子类）</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">init</span>(<span class="params">self, units, **kwargs</span>):</span></span><br><span class="line">		self.units = units</span><br><span class="line">		self.state_size = units</span><br><span class="line">		<span class="built_in">super</span>(MinimalRNNCell, self).init(**kwargs)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">build</span>(<span class="params">self, input_shape</span>):</span></span><br><span class="line">		self.kernel = self.add_weight(</span><br><span class="line">			shape=(input_shape[-<span class="number">1</span>], self.units),</span><br><span class="line">			initializer=<span class="string">&quot;uniform&quot;</span>,</span><br><span class="line">			name=<span class="string">&quot;kernel&quot;</span></span><br><span class="line">		)</span><br><span class="line">		self.recurrent_kernel = self.add_weight(</span><br><span class="line">			shape=(self.units, self.units),</span><br><span class="line">			initializer=<span class="string">&quot;uniform&quot;</span>,</span><br><span class="line">			name=<span class="string">&quot;recurrent_kernel&quot;</span>)</span><br><span class="line">		self.built = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, states</span>):</span></span><br><span class="line">		prev_output = states[<span class="number">0</span>]</span><br><span class="line">		h = K.dot(inputs, self.kernel)</span><br><span class="line">		output = h + K.dot(prev_output, self.recurrent_kernel)</span><br><span class="line">		<span class="keyword">return</span> output, [output]</span><br><span class="line"></span><br><span class="line">cell = MinimalRNNCell(<span class="number">32</span>)</span><br><span class="line">	<span class="comment"># 在RNN层使用这个单元：</span></span><br><span class="line">x = keras.Input((<span class="literal">None</span>, <span class="number">5</span>))</span><br><span class="line">layer = RNN(cell)</span><br><span class="line">y = layer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cells = [MinimalRNNCell(<span class="number">32</span>), MinimalRNNCell(<span class="number">64</span>)]</span><br><span class="line">	<span class="comment"># 用单元格构建堆叠的RNN的方法：</span></span><br><span class="line">x = keras.Input((<span class="literal">None</span>, <span class="number">5</span>))</span><br><span class="line">layer = RNN(cells)</span><br><span class="line">y = layer(x)</span><br></pre></td></tr></table></figure>
<h2 id="SimpleRNN"><a href="#SimpleRNN" class="headerlink" title="SimpleRNN"></a>SimpleRNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNN(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&quot;tanh&quot;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&quot;orthogonal&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	unroll=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>完全连接的RNN，其输出将被反馈到输入。</p>
<ul>
<li><p>参数</p>
<ul>
<li><p><code>units</code>：正整数，输出空间的维度。</p>
</li>
<li><p><code>activation</code>：要使用的激活函数</p>
<ul>
<li><code>tanh</code>：默认</li>
<li><code>None</code>：则不使用激活函数，即线性激活：<code>a(x) = x</code></li>
</ul>
</li>
<li><p><code>use_bias</code>：布尔值，该层是否使用偏置向量。</p>
</li>
<li><p><code>kernel_initializer</code>：<em>kernel</em>权值矩阵的初始化器</p>
</li>
<li><p><code>recurrent_initializer</code>：<em>recurrent_kernel</em>权值矩阵</p>
</li>
<li><p><code>bias_initializer</code>：偏置向量的初始化器</p>
</li>
<li><p><code>kernel_regularizer</code>：运用到<em>kernel</em>权值矩阵的正则化
函数</p>
</li>
<li><p><code>recurrent_regularizer</code>：运用到 <code>recurrent_kernel</code> 权值
矩阵的正则化函数</p>
</li>
<li><p><code>bias_regularizer</code>：运用到偏置向量的正则化函数</p>
</li>
<li><p><code>activity_regularizer</code>：运用到层输出（它的激活值）的
正则化函数</p>
</li>
<li><p><code>kernel_constraint</code>：运用到<em>kernel</em>权值矩阵的约束函数</p>
</li>
<li><p><code>recurrent_constraint</code>：运用到<em>recurrent_kernel</em>权值矩阵
的约束函数</p>
</li>
<li><p><code>bias_constraint</code>： 运用到偏置向量的约束函数</p>
</li>
<li><p><code>dropout</code>：单元的丢弃比例，用于输入的线性转换</p>
<ul>
<li>在<em>0-1</em>之间的浮点数</li>
</ul>
</li>
<li><p><code>recurrent_dropout</code>：单元的丢弃比例，用于循环层状态线性
转换</p>
</li>
<li><p><code>return_sequences</code>：返回输出序列中的全部序列</p>
<ul>
<li>默认：返回最后最后一个输出</li>
</ul>
</li>
<li><p><code>return_state</code>：除输出之外是否返回最后一个状态</p>
</li>
<li><p><code>go_backwards</code>：向后处理输入序列并返回相反的序列</p>
</li>
<li><p><code>stateful</code>：批次中索引 i 处的每个样品的最后状态，将用作
下一批次中索引 i 样品的初始状态</p>
</li>
<li><p><code>unroll</code>：展开网络</p>
</li>
</ul>
</li>
</ul>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.GRU(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&quot;tanh&quot;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&quot;hard_sigmoid&quot;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&quot;orthogonal&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	implementation=<span class="number">1</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	unroll=<span class="literal">False</span>,</span><br><span class="line">	reset_after=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>门限循环单元网络</p>
<ul>
<li><p>说明：有两种变体</p>
<ul>
<li><p>默认的基于<em>1406.1078v3</em>，并且在矩阵乘法之前将复位门
应用于隐藏状态</p>
</li>
<li><p>另一种基于<em>1406.1078v1</em>并且顺序倒置</p>
<ul>
<li><p>兼容<em>CuDNNGRU(GPU-only)</em>，并且允许在 CPU 上进行
推理</p>
</li>
<li><p>对于<em>kernel</em>和<em>recurrent_kernel</em>有可分离偏置
<code>reset_after=True</code>和<code>recurrent_activation=sigmoid</code> 。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><p><code>recurrent_activation</code>：用于循环时间步的激活函数</p>
</li>
<li><p><code>implementation</code>：实现模式</p>
<ul>
<li><p><code>1</code>：将把它的操作结构化为更多的小的点积和加法操作</p>
</li>
<li><p><code>2</code>：将把它们分批到更少，更大的操作中</p>
</li>
<li><p>这些模式在不同的硬件和不同的应用中具有不同的性能配置
文件</p>
</li>
</ul>
</li>
<li><p><code>reset_after</code>：GRU公约，在矩阵乘法之后使用重置门</p>
</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.1078">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.1259">On the Properties of Neural Machine Translation：Encoder-Decoder Approaches</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1412.3555v1">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></li>
</ul>
</li>
</ul>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTM(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&quot;tanh&quot;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&quot;hard_sigmoid&quot;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&quot;glorot_uniform&quot;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&quot;orthogonal&quot;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&quot;zeros&quot;</span>,</span><br><span class="line">	unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	implementation=<span class="number">1</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	unroll=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>长短期记忆网络层（Hochreiter 1997）</p>
<ul>
<li><p>参数</p>
<ul>
<li><code>unit_forget_bias</code><ul>
<li><code>True</code>：初始化时，将忘记门的偏置加 1，同时还会强制
<code>bias_initializer=&quot;zeros&quot;</code>（这个建议来自
<a target="_blank" rel="noopener" href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz et al.</a>）</li>
</ul>
</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.bioinf.jku.at/publications/older/2604.pdf">Long short-term memory</a> (original 1997 paper)</li>
<li><a target="_blank" rel="noopener" href="http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015">Learning to forget：Continual prediction with LSTM</a></li>
<li><a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~graves/preprint.pdf">Supervised sequence labeling with recurrent neural networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1512.05287">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></li>
</ul>
</li>
</ul>
<h2 id="ConvLSTM2D"><a href="#ConvLSTM2D" class="headerlink" title="ConvLSTM2D"></a>ConvLSTM2D</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.ConvLSTM2D(</span><br><span class="line">	filters(<span class="built_in">int</span>),</span><br><span class="line">	kernel_size(<span class="built_in">tuple</span>(<span class="built_in">int</span>, <span class="built_in">int</span>)),</span><br><span class="line">	strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	padding=<span class="string">&#x27;valid&#x27;</span>,</span><br><span class="line">	data_format=<span class="literal">None</span>,</span><br><span class="line">	dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">	activation=<span class="string">&#x27;tanh&#x27;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&#x27;hard_sigmoid&#x27;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	go_backwards=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>卷积LSTM：它类似于LSTM层，但输入变换和循环变换都是卷积的</p>
<ul>
<li><p>说明</p>
<ul>
<li>当前的实现不包括单元输出的反馈回路</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>dilation_rate</code>：用于膨胀卷积的膨胀率<ul>
<li><code>stride!=1</code>与<code>dilation_rate!=1</code>两者不兼容。</li>
</ul>
</li>
</ul>
</li>
<li><p>输入尺寸</p>
<ul>
<li><code>data_format=&quot;channels_first&quot;</code>：尺寸为
<code>(batch, time, channels, rows, cols)</code></li>
<li><code>data_format=&quot;channels_last&quot;</code>：尺寸为
<code>(batch, time, rows, cols, channels)</code></li>
</ul>
</li>
<li><p>输出尺寸</p>
<ul>
<li><p><code>return_sequences=True</code></p>
<ul>
<li><p><code>data_format=&quot;channels_first&quot;</code>：返回尺寸为
<code>(batch, time, filters, output_row, output_col)</code></p>
</li>
<li><p><code>data_format=&quot;channels_last&quot;</code>：返回尺寸为
<code>(batch, time, output_row, output_col, filters)</code></p>
</li>
</ul>
</li>
<li><p><code>return_seqences=False</code></p>
<ul>
<li><p><code>data_format =&quot;channels_first&quot;</code>：返回尺寸为
<code>(batch, filters, output_row, output_col)</code></p>
</li>
<li><p><code>data_format=&quot;channels_last&quot;</code>：返回尺寸为
<code>(batch, output_row, output_col, filters)</code></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>参考文献</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/1506.04214v1">Convolutional LSTM Network：A Machine Learning Approach for Precipitation Nowcasting</a></li>
</ul>
</li>
</ul>
<h2 id="SimpleRNNCell"><a href="#SimpleRNNCell" class="headerlink" title="SimpleRNNCell"></a>SimpleRNNCell</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNNCell(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&#x27;tanh&#x27;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><em>SimpleRNN</em>的单元类</p>
<h2 id="GRUCell"><a href="#GRUCell" class="headerlink" title="GRUCell"></a>GRUCell</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.GRUCell(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&#x27;tanh&#x27;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&#x27;hard_sigmoid&#x27;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	implementation=<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><em>GRU</em>层的单元类</p>
<h2 id="LSTMCell"><a href="#LSTMCell" class="headerlink" title="LSTMCell"></a>LSTMCell</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.LSTMCell(</span><br><span class="line">	units,</span><br><span class="line">	activation=<span class="string">&#x27;tanh&#x27;</span>,</span><br><span class="line">	recurrent_activation=<span class="string">&#x27;hard_sigmoid&#x27;</span>,</span><br><span class="line">	use_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	dropout=<span class="number">0.0</span>,</span><br><span class="line">	recurrent_dropout=<span class="number">0.0</span>,</span><br><span class="line">	implementation=<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>LSTM层的单元类</p>
<h2 id="StackedRNNCells"><a href="#StackedRNNCells" class="headerlink" title="StackedRNNCells"></a>StackedRNNCells</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.StackedRNNCells(cells)</span><br></pre></td></tr></table></figure>
<p>将一堆RNN单元表现为一个单元的封装器</p>
<ul>
<li><p>说明</p>
<ul>
<li>用于实现高效堆叠的 RNN。</li>
</ul>
</li>
<li><p>参数</p>
<ul>
<li><code>cells</code>：RNN 单元实例的列表</li>
</ul>
</li>
</ul>
<p>例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cells = [</span><br><span class="line">    keras.layers.LSTMCell(output_dim),</span><br><span class="line">    keras.layers.LSTMCell(output_dim),</span><br><span class="line">    keras.layers.LSTMCell(output_dim),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">inputs = keras.Input((timesteps, input_dim))</span><br><span class="line">x = keras.layers.RNN(cells)(inputs)</span><br></pre></td></tr></table></figure>
<h2 id="CuDNNGRU"><a href="#CuDNNGRU" class="headerlink" title="CuDNNGRU"></a>CuDNNGRU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.CuDNNGRU(</span><br><span class="line">	units,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>由 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">CuDNN</a> 
支持的快速<em>GRU</em>实现</p>
<ul>
<li><p>说明</p>
<ul>
<li>只能以<em>TensorFlow</em>后端运行在<em>GPU</em>上</li>
</ul>
</li>
</ul>
<h2 id="CuDNNLSTM"><a href="#CuDNNLSTM" class="headerlink" title="CuDNNLSTM"></a>CuDNNLSTM</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.CuDNNLSTM(</span><br><span class="line">	units,</span><br><span class="line">	kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">	recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">	bias_initializer=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">	unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">	kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">	recurrent_regularizer=<span class="literal">None</span>,</span><br><span class="line">	bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">	activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">	kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">	recurrent_constraint=<span class="literal">None</span>,</span><br><span class="line">	bias_constraint=<span class="literal">None</span>,</span><br><span class="line">	return_sequences=<span class="literal">False</span>,</span><br><span class="line">	return_state=<span class="literal">False</span>,</span><br><span class="line">	stateful=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>由 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">CuDNN</a>
支持的快速<em>LSTM</em>实现</p>
<ul>
<li><p>说明</p>
<ul>
<li>只能以<em>TensorFlow</em>后端运行在<em>GPU</em>上</li>
</ul>
</li>
</ul>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/WWxwMVBsOUtoNTdMSlJ4Z1lWVnRISERsd2t6ek9MeVpEUWs0YldlZGxUdXlKdDNmMEZVV1hWaFZFYWFSNmFKL25penZodWx3UzRiaVkxcnREWCtOYUJhZWhNbWpzaUdyc1hPangycUh5RTVjRXFnZnFGdVdSTzZvVzJBcTJHKzl8aXpDK1ROWWl4N080YkFEK3QvbEVWNEJuQjFqdWdxODZQcGNoM1NqbERXST0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>