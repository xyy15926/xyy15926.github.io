<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: Entropy - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Entropy</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-07-12T01:55:27.000Z" title="7/12/2021, 9:55:27 AM">2021-07-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-12T01:55:27.000Z" title="7/12/2021, 9:55:27 AM">2021-07-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a><span> / </span><a class="link-muted" href="/categories/Math-Mixin/Statistics/">Statistics</a></span><span class="level-item">16 minutes read (About 2441 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/Statistics/stat_entropy.html">统计量 - 熵</a></h1><div class="content"><h2 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a><em>Entropy</em></h2><blockquote>
<ul>
<li>（信息）熵：在概率分布上对复杂程度/多样性/不确定性/混乱程度的度量</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\begin{align*}
HOD(X) & = -E_P log P(x) \\
& = \sum_d^D P(x_d) log \frac 1 {P(x_d)} \\
& = - \sum_d^D p_d log p_d \\
\end{align*}</script><blockquote>
<ul>
<li>$p_d$：随机变量各取值对应概率</li>
<li>事件 $i$ 发生概率 $p_d=0$：约定 $p_d log(p_d)$ 为 0</li>
<li>其中 $log$ 以 2 为底，单位为 <em>bit</em>，以 $e$ 为底，单位为 <em>nat</em></li>
</ul>
</blockquote>
<ul>
<li><p>信息论中，熵越高能传输越多信息</p>
<ul>
<li>可携带的信息量 = 单位消息熵 * 消息长度</li>
<li>熵衡量系统复杂程度，提高系统确定性即削弱系统多样性，降低熵</li>
</ul>
</li>
<li><p>概率分布包含的信息即其复杂程度（可能取值数量）</p>
<ul>
<li>考虑按照 $(p_1,\cdots,p_D)$ 分布、长度为 $N$ 的随机变量序列，其可能排列数为 $\frac {N!} {\prod_d^D (p_d N)!}$</li>
<li><p>则根据 <em>Stirling</em> 公式有</p>
<script type="math/tex; mode=display">\begin{align*}
log (\frac {N!} {\prod_d^D (p_d N)!}) & = log(N!)
  - \sum_d^D log((p_d N)!) \\
& \overset {\lim_{N \rightarrow \infty}} = log(\sqrt {2\pi N}
  ({\frac N e})^N) + \sum_d^D log(\sqrt {2\pi p_dN}
  ({\frac {p_dN} e})^{p_dN}) \\
& = log(\sqrt {2\pi N}) + N(logN-1) - \sum_d^D log(\sqrt {2\pi p_dN})
  - \sum_d^D p_dN (log(p_dN) - 1) \\
& = log(\sqrt {2\pi N} + \sum_d^D log(\sqrt {2\pi p_dN}))
  + N \sum_d^D p_d log p_d \\
& \approx N \sum_d^D p_d log p_d
\end{align*}</script></li>
<li><p>则长度为 $N$ 的随机变量串的多样性、信息量为 $H * N$，其中 $H=\sum_d^D p_d log p_d$ 概率分布的信息熵</p>
</li>
</ul>
</li>
<li><p>某个事件包含的信息可以用编码长度理解</p>
<ul>
<li>对概率 $p$ 事件，编码 $1/p$ 个需编码（2进制编码）长度 $log_2 \frac 1 p$</li>
<li>则概率 $p$ 事件包含信息量可以定义为 $log \frac 1 p$，即事件包含的信息量可用表示事件需要编码的长度表示
（底数则取决于编码元，只影响系数）</li>
<li>则整个随机变量的信息为各事件信息量加权和</li>
</ul>
</li>
<li><p>熵可以视为变量取值概率的加权和</p>
<ul>
<li>只依赖随机变量 $X$ 的分布，与其取值无关，可将其记为 $H(P)$</li>
<li>由定义 $0 \leq H(P) \leq log_2 k$<ul>
<li>$H(p) = 0$：$\exists j, p_j=1$，随机变量只能取一个值，无不确定性</li>
<li>$H(p) = log k$：$\forall j, p_j=1/k$，随机变量在任意取值概率相等，不确定性最大</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>empirical entropy</em>：经验熵，熵中的概率由数据估计时（尤极大似然估计）</li>
<li>参考链接<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)">https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27876027">https://zhuanlan.zhihu.com/p/27876027</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73710585">https://zhuanlan.zhihu.com/p/73710585</a></li>
</ul>
</blockquote>
</li>
<li><em>Stirling</em> 公式即用积分近似计算 $\sum logn$：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143992660">https://zhuanlan.zhihu.com/p/143992660</a></li>
</ul>
</blockquote>
<h3 id="熵的性质"><a href="#熵的性质" class="headerlink" title="熵的性质"></a>熵的性质</h3><ul>
<li><p>对称性：事件取值不影响熵</p>
</li>
<li><p>极值性</p>
<ul>
<li><p>所有符号有同等机会出现的情况下，熵达到极大（琴生不等式）</p>
<script type="math/tex; mode=display">\begin{align*}
H(X) & = E[log(\frac 1 {P(X)})] \leq log(E[\frac 1 {P(x)}])
  & = log(n)
\end{align*}</script></li>
<li><p>仅有一个符号确定出现的情况下，熵达到极小 0</p>
</li>
</ul>
</li>
<li><p><em>Continuity</em>连续性：度量连续，概率微小变化只能引起熵微小变化</p>
</li>
<li><p><em>Normalization</em>规范化：$H_2(\frac 1 2, \frac 1 2) = 1$</p>
</li>
<li><p><em>Grouping</em>组合法则/可加和性：熵与过程如何划分无关
（此即要求熵形式为对数）</p>
<ul>
<li><p>若子系统间相互作用已知，则可以通过子系统熵值计算系统整体熵</p>
<script type="math/tex; mode=display">
H(X) = H(X_1,\cdots,X_K) + \sum_{k=1}^K
  \frac {|X_k|} {|X|} H(X_k)</script><blockquote>
<ul>
<li>$X_1,\cdots,X_K$：$K$ 个子系统，可以理解为将随机变量 $X$ 划分为 $K$ 种情况</li>
<li>$H(X_1,\cdots,X_K)$：子系统相互作用熵</li>
</ul>
</blockquote>
<ul>
<li>子系统相互作用熵可以认为是，通过已知信息消除的多样性（即信息增益）</li>
<li>子系统熵之和则是利用已知信息消除多样性之后，系统剩余混乱程度</li>
</ul>
</li>
<li><p>一般的，两个事件 $X,Y$ 熵满足以下计算关系</p>
<script type="math/tex; mode=display">\begin{align*}
H(X, Y) & = H(X) + H(Y|X) \\
& = H(Y) + H(X|Y) \\
& \leqslant H(X) + H(Y) \\
H(X|Y) & \leqslant H(X) \\
\end{align*}</script></li>
<li><p>特别的，若事件 $X, Y$ 相互独立</p>
<script type="math/tex; mode=display">\begin{align*}
H(X|Y) &= H(X) \\
H(X, Y) &= H(X) + H(Y)
\end{align*}</script></li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>满足以上特性的熵定义必然为如下形式</li>
</ul>
</blockquote>
<pre><code>$$
-K \sum P(x)log(P(x))
$$
</code></pre><blockquote>
<ul>
<li>在热力学、信息论等领域，熵有多种不同定义，满足熵性质的测度泛函，只能具有（<em>Shannon</em> 熵和 <em>Hartley</em> 熵）或（<em>von Neumann</em> 熵和 <em>Shannon</em> 熵）线性组合的函数形式，若不要求满足组合法则，还有 <em>Tsallis</em> 熵等</li>
</ul>
</blockquote>
<h3 id="Conditinal-Entropy"><a href="#Conditinal-Entropy" class="headerlink" title="Conditinal Entropy"></a><em>Conditinal Entropy</em></h3><p>条件熵：随机变量 $X$ 给定条件下，随机变量 $Y$ 的<strong>条件概率分布的熵</strong>对 $X$ 的数学期望</p>
<script type="math/tex; mode=display">\begin{align*}
H(Y|X) & = \sum_{i=1}^N p_i H(Y|X=x_i) \\
H(Y|x=x_i) & = - \sum_j P(y_j|x_i) log P(y_j|x_i)
\end{align*}</script><blockquote>
<ul>
<li>$P(X=x<em>i, Y=y_j)=p</em>{i,j}$：随机变量 $(X,Y)$ 联合概率分布</li>
<li>$p_i=P(X=x_i)$</li>
<li>$H(Y|X=x_i)$：后验熵</li>
</ul>
</blockquote>
<ul>
<li><p>特别的，考虑数据集 $D$ 被分为 $D_1,\cdots,D_m$，条件经验熵可计算如下</p>
<script type="math/tex; mode=display">\begin{align*}
H(D|A) & = \sum_{m=1}^M \frac {|D_m|} {|D|} H(D_m) \\
& = -\sum_{m=1}^M \frac {|D_m|} {|D|}
   \sum_{k=1}^K \frac {|D_{m,k}|} {|D_m|}
   log_2 \frac {|D_{m,k}|} {|D_m|}
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li><em>postorior entropy</em>：后验熵，随机变量 $X$ 给定条件下，随机变量 $Y$ 的<strong>条件概率分布的熵</strong></li>
<li><em>empirical conditional entropy</em>：经验条件熵，概率由数据估计</li>
</ul>
</blockquote>
<h3 id="Infomation-Gain-Mutual-Infomation"><a href="#Infomation-Gain-Mutual-Infomation" class="headerlink" title="Infomation Gain/Mutual Infomation"></a><em>Infomation Gain</em>/<em>Mutual Infomation</em></h3><p>互信息/信息增益：（经验）熵与（经验）条件熵之差</p>
<script type="math/tex; mode=display">\begin{align*}
g(Y|X) & = H(Y) - H(Y|X) \\
& = \sum_{x \in X} \sum_{y \in Y} P(x,y) log
    \frac {P(x,y)} {P(x)P(y)}
\end{align*}</script><ul>
<li><p>与数据集具体分布有关、与具体取值无关</p>
<ul>
<li>绝对大小同易受熵影响，（经验）熵较大时，互信息也相对较大</li>
<li>由于误差存在，分类取值数目较多者信息增益较大</li>
</ul>
</li>
<li><p>可衡量变量 $X$ 对 $Y$ 预测能力、减少不确定性的能力</p>
<ul>
<li>信息增益越大，变量之间相关性越强，自变量预测因变量能力越强</li>
<li>只能考察特征对整个系统的贡献，无法具体到特征某个取值</li>
<li>只适合作全局特征选择，即所有类使用相同的特征集合</li>
</ul>
</li>
</ul>
<h3 id="Infomation-Gain-Ratio"><a href="#Infomation-Gain-Ratio" class="headerlink" title="Infomation Gain Ratio"></a><em>Infomation Gain Ratio</em></h3><p>信息增益比：信息增益对原始信息熵的比值</p>
<script type="math/tex; mode=display">\begin{align*}
g_R(Y|X) & = \frac {g(Y|X)} {H(X)}
\end{align*}</script><ul>
<li>考虑熵大小，减弱熵绝对大小的影响</li>
</ul>
<h3 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a><em>Cross Entropy</em></h3><blockquote>
<ul>
<li>信息论：基于相同事件测度的两个概率分布 $P, Q$，基于非自然（相较于真实分布 $P$）概率分布 $Q$ 进行编码，在事件集合中唯一标识事件所需 <em>bit</em></li>
<li>概率论：概率分布 $P, Q$ 之间差异</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">\begin{align*}
H(P, Q) & = E_P[-log Q] = \left \{ \begin{array}{l}
    -\sum_{X} P(x) logQ(x), & 离散分布 \\
    -\int_X P(x) log(Q(x)) d(r(x)), & 连续分布
\end{array} \right. \\
& = H(P) + D_{KL}(P||Q)
\end{align*}</script><blockquote>
<ul>
<li>$P(x), Q(x)$：概率分布（密度）函数</li>
<li>$r(x)$：测度，通常是 $Borel \sigma$ 代数上的勒贝格测度</li>
<li>$D_{KL}(P||Q)$：$P$ 到 $Q$ 的 <em>KL</em> 散度（$P$ 相对于 $Q$ 的相对熵）</li>
</ul>
</blockquote>
<ul>
<li>信息论中，交叉熵可以看作是信息片段在错误分布 $Q$ 分布下的期望编码长度<ul>
<li>信息实际分布实际为 $P$，所以期望基于 $P$</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>交叉熵是常用的损失函数：效果等价于 <em>KL</em> 散度，但计算方便</li>
<li><em>sigmoid</em> 激活函数时：相较于二次损失，收敛速度更快</li>
</ul>
</blockquote>
<h2 id="Entropy-衍生指标"><a href="#Entropy-衍生指标" class="headerlink" title="Entropy 衍生指标"></a><em>Entropy</em> 衍生指标</h2><h3 id="Kullback-Leibler-Divergence"><a href="#Kullback-Leibler-Divergence" class="headerlink" title="Kullback-Leibler Divergence"></a><em>Kullback-Leibler Divergence</em></h3><p><em>KL</em> 散度/相对熵：衡量概率分布 $P, Q$ 之间差异的量化指标</p>
<script type="math/tex; mode=display">\begin{align*}
D_{KL}(P||Q) & = E_P[(-log Q(x)) - (-log P(x))] \\
& = E_P[log P(x) - log Q(x)] \\
& = \sum_{d=1}^D P(x_d) (log P(x_d) - log Q(x_d)) \\
& = \sum_{d=1} P(x_d) log \frac {P(x_d)} {Q(x_d)}
\end{align*}</script><ul>
<li><p><em>KL</em> 散度含义</p>
<ul>
<li>原始分布 $P$、近似分布 $Q$ 之间对数差值期望</li>
<li>若使用观察分布 $Q$ 描述真实分布 $P$，还需的额外信息量</li>
</ul>
</li>
<li><p><em>KL</em> 散度不对称，分布 $P$ 度量 $Q$、$Q$ 度量 $P$ 损失信息不同</p>
<ul>
<li>从计算公式也可以看出</li>
<li>KL散度不能作为不同分布之间距离的度量</li>
</ul>
</li>
</ul>
<h3 id="Population-Stability-Index"><a href="#Population-Stability-Index" class="headerlink" title="Population Stability Index"></a><em>Population Stability Index</em></h3><p><em>PSI</em>：衡量分布 $P, Q$ 之间的差异程度</p>
<script type="math/tex; mode=display">\begin{align*}
PSI &= \sum_d^D (P_d - Q_d) * log \frac {P_d} {Q_d} \\
&= \sum_d^D P_d log \frac {P_d} {Q_d} +
    \sum_d^D Q_d log \frac {Q_d} {P_d} \\
&= D_{KL}(P||Q) + D_{KL}(Q||P)
\end{align*}</script><ul>
<li>是 <em>KL</em> 散度的对称操作<ul>
<li>更全面的描述两个分布的差异</li>
</ul>
</li>
</ul>
<h2 id="Gini-指数"><a href="#Gini-指数" class="headerlink" title="Gini 指数"></a><em>Gini</em> 指数</h2><p>基尼指数：可视为信息熵的近似替代</p>
<script type="math/tex; mode=display">\begin{align*}
Gini(p) & = \sum_{k=1}^K p_k(1-p_k) \\
    & = 1 - \sum_{k=1}^K p_k^2
\end{align*}</script><blockquote>
<ul>
<li>$p$：概率分布</li>
<li>异质性最小：<em>Gini</em> 系数为 0</li>
<li>异质性最大：<em>Gini</em> 系数为 $1 - \frac 1 k$</li>
</ul>
</blockquote>
<ul>
<li><em>Gini</em> 指数度量分布的不纯度<ul>
<li>包含类别越多，<em>Gini</em> 指数越大</li>
<li>分布越均匀，<em>Gini</em> 指数越大</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>熵较 <em>Gini</em> 指数对不纯度判罚更重</li>
</ul>
</blockquote>
<p><img src="/imgs/gini_entropy_error_rate_in_binary_classification.png" alt="gini_entropy_error_rate_in_binary_classification"></p>
<blockquote>
<ul>
<li>经济学领域的 <em>Gini</em> 系数更类似 <em>AUC</em> 值</li>
</ul>
</blockquote>
<h3 id="与-Entropy-关系"><a href="#与-Entropy-关系" class="headerlink" title="与 Entropy 关系"></a>与 <em>Entropy</em> 关系</h3><script type="math/tex; mode=display">\begin{align*}
H(X) & = -E_P log P(x) \\
& = - \sum_i^N p_i log p_i \\
& = - \sum_i^N p_i (log (1 + (p_i-1))) \\
& = - \sum_i^N p_i (p_i - 1 + \xi(p_i^{'}-1)) \\
& \approx 1 - \sum_i^N p_i^2
\end{align*}</script><ul>
<li><em>Gini</em> 指数可以视为是熵在 1 附近的一阶泰勒展开近似</li>
</ul>
<h3 id="条件-Gini-指数"><a href="#条件-Gini-指数" class="headerlink" title="条件 Gini 指数"></a>条件 <em>Gini</em> 指数</h3><script type="math/tex; mode=display">
Gini(Y|X) = \sum_{k=1}^K P(X=x_k)Gini(Y|X=x_k)</script><blockquote>
<ul>
<li>性质类似信息增益</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-31T18:10:08.000Z" title="8/1/2019, 2:10:08 AM">2019-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:52:53.000Z" title="7/16/2021, 2:52:53 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">25 minutes read (About 3688 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/maximum_entropy.html">最大熵模型</a></h1><div class="content"><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><h3 id="逻辑斯蒂分布"><a href="#逻辑斯蒂分布" class="headerlink" title="逻辑斯蒂分布"></a>逻辑斯蒂分布</h3><script type="math/tex; mode=display">\begin{align*}
F(x) & = P(X \leq x) = \frac 1 {1 + e^{-(x-\mu)/\gamma}} \\
f(x) & = F^{'}(x) = \frac {e^{-(x-\mu)/\gamma}}
    {\gamma(1+e^{-(x-\mu)/\gamma})^2}
\end{align*}</script><blockquote>
<ul>
<li>$\mu$：位置参数</li>
<li>$\gamma$：形状参数</li>
</ul>
</blockquote>
<ul>
<li>分布函数属于逻辑斯蒂函数</li>
<li><p>分布函数图像为sigmoid curve</p>
<ul>
<li>关于的$(\mu, \frac 1 2)$中心对称<script type="math/tex; mode=display">
F(-x+\mu) - \frac 1 2 = -F(x+\mu) + \frac 1 2</script></li>
<li>曲线在靠近$\mu$中心附近增长速度快，两端速度增长慢</li>
<li>形状参数$\gamma$越小，曲线在中心附近增加越快</li>
</ul>
</li>
<li><p>模型优点</p>
<ul>
<li>模型输出值位于0、1之间，天然具有概率意义，方便观测
样本概率分数</li>
<li>可以结合$l-norm$正则化解决过拟合、共线性问题</li>
<li>实现简单，广泛用于工业问题</li>
<li>分类时计算量比较小、速度快、消耗资源少</li>
</ul>
</li>
<li><p>模型缺点</p>
<ul>
<li>特征空间很大时，性能不是很好，容易欠拟合，准确率一般</li>
<li>对非线性特征需要进行转换</li>
</ul>
</li>
</ul>
<h3 id="Binomial-Logistic-Regression-Model"><a href="#Binomial-Logistic-Regression-Model" class="headerlink" title="Binomial Logistic Regression Model"></a><em>Binomial Logistic Regression Model</em></h3><p>二项逻辑斯蒂回归模型：形式为参数化逻辑斯蒂分布的二分类
生成模型</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y=1|x) & = \frac {exp(wx + b)} {1 + exp (wx + b)} \\
P(Y=0|x) & = \frac 1 {1 + exp(wx + b)} \\
P(Y=1|\hat x) & = \frac {exp(\hat w \hat x)}
    {1 + exp (\hat w \hat x)} \\
P(Y=0|\hat x) & = \frac 1 {1+exp(\hat w \hat x)}
\end{align*}</script><blockquote>
<ul>
<li>$w, b$：权值向量、偏置</li>
<li>$\hat x = (x^T|1)^T$</li>
<li>$\hat w = (w^T|b)^T$</li>
</ul>
</blockquote>
<ul>
<li><p>逻辑回归比较两个条件概率值，将实例$x$归于条件概率较大类</p>
</li>
<li><p>通过逻辑回归模型，可以将线性函数$wx$转换为概率</p>
<ul>
<li>线性函数值越接近正无穷，概率值越接近1</li>
<li>线性函数值越接近负无穷，概率值越接近0</li>
</ul>
</li>
</ul>
<h4 id="Odds-Odds-Ratio"><a href="#Odds-Odds-Ratio" class="headerlink" title="Odds/Odds Ratio"></a>Odds/Odds Ratio</h4><ul>
<li><p>在逻辑回归模型中，输出$Y=1$的对数几率是输入x的线性函数</p>
<script type="math/tex; mode=display">
log \frac {P(Y=1|x)} {1-P(Y=1|x)} = \hat w \hat x</script></li>
<li><p>OR在逻辑回归中意义：$x_i$每增加一个单位，odds将变为原来
的$e^{w_i}$倍</p>
<script type="math/tex; mode=display">\begin{align*}
odd &= \frac {P(Y=1|x)} {1-P(Y=1|x)} = e^{\hat w \hat x} \\
OR_{x_i+1 / x_i} &= e^{w_i}
\end{align*}</script><ul>
<li><p>对数值型变量</p>
<ul>
<li>多元LR中，变量对应的系数可以计算相应
<em>Conditional OR</em></li>
<li>可以建立单变量LR，得到变量系数及相应
<em>Marginal OR</em></li>
</ul>
</li>
<li><p>对分类型变量</p>
<ul>
<li>可以直接计算变量各取值间对应的OR</li>
<li>变量数值化编码建立模型，得到变量对应OR</li>
</ul>
<blockquote>
<ul>
<li>根据变量编码方式不同，变量对应OR的含义不同，其中
符合数值变量变动模式的是WOE线性编码</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>极大似然：极小对数损失（交叉熵损失）</p>
<script type="math/tex; mode=display">\begin{align*}
L(w) & = log \prod_{i=1}^N [\pi(x_i)]^{y_i}
    [1-\pi(x_i)]^{1-y_i} \\
& = \sum_{i=1}^N [y_i log \pi(x_i) + (1-y_i)log(1-\pi(x_i))] \\
& = \sum_{i=1}^N [y_i log \frac {\pi(x_i)}
    {1-\pi(x_i)} log(1-\pi(x_i))] \\
& = \sum_{i=1}^N [y_i(\hat w \hat x_i) -
    log(1+exp(\hat w \hat x_i))]
\end{align*}</script><blockquote>
<ul>
<li>$\pi(x) = P(Y=1|x)$</li>
</ul>
</blockquote>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ul>
<li>通常采用梯度下降、拟牛顿法求解有以上最优化问题</li>
</ul>
<h3 id="Multi-Nominal-Logistic-Regression-Model"><a href="#Multi-Nominal-Logistic-Regression-Model" class="headerlink" title="Multi-Nominal Logistic Regression Model"></a><em>Multi-Nominal Logistic Regression Model</em></h3><p>多项逻辑斯蒂回归：二项逻辑回归模型推广</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y=j|x) & = \frac {exp(\hat w_j \hat x)} {1+\sum_{k=1}^{K-1}
    exp(\hat w_k \hat x)}, k=1,2,\cdots,K-1 \\
P(Y=K|x) & = \frac 1 {1+\sum_{k=1}^{K-1}
    exp(\hat w_k \hat x)}
\end{align*}</script><ul>
<li>策略、算法类似二项逻辑回归模型</li>
</ul>
<h2 id="Generalized-Linear-Model"><a href="#Generalized-Linear-Model" class="headerlink" title="Generalized Linear Model"></a><em>Generalized Linear Model</em></h2><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><h2 id="Maximum-Entropy-Model"><a href="#Maximum-Entropy-Model" class="headerlink" title="Maximum Entropy Model"></a><em>Maximum Entropy Model</em></h2><h3 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h3><p>最大熵原理：学习概率模型时，在所有可能的概率模型（分布）中，
<strong>熵最大的模型是最好的模型</strong></p>
<ul>
<li><p>使用约束条件确定概率模型的集合，则最大熵原理也可以表述为
<strong>在满足约束条件的模型中选取熵最大的模型</strong></p>
</li>
<li><p>直观的，最大熵原理认为</p>
<ul>
<li>概率模型要满足已有事实（约束条件）</li>
<li>没有更多信息的情况下，不确定部分是等可能的</li>
<li>等可能不容易操作，所有考虑使用<strong>可优化</strong>的熵最大化
表示等可能性</li>
</ul>
</li>
</ul>
<h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><p>最大熵模型为生成模型</p>
<ul>
<li><p>对给定数据集$T={(x_1,y_1),\cdots,(x_N,y_N)}$，联合分布
P(X,Y)、边缘分布P(X)的经验分布如下</p>
<script type="math/tex; mode=display">\begin{align*}
\tilde P(X=x, Y=y) & = \frac {v(X=x, Y=y)} N \\
\tilde P(X=x) & = \frac {v(X=x)} N
\end{align*}</script><blockquote>
<ul>
<li>$v(X=x,Y=y)$：训练集中样本$(x,y)$出频数</li>
</ul>
</blockquote>
</li>
<li><p>用如下<em>feature function</em> $f(x, y)$描述输入x、输出y之间
某个事实</p>
<script type="math/tex; mode=display">f(x, y) = \left \{ \begin{array}{l}
1, & x、y满足某一事实 \\
0, & 否则
\end{array} \right.</script><ul>
<li><p>特征函数关于经验分布$\tilde P(X, Y)$的期望</p>
<script type="math/tex; mode=display">
E_{\tilde P} = \sum_{x,y} \tilde P(x,y)f(x,y)</script></li>
<li><p>特征函数关于生成模型$P(Y|X)$、经验分布$\tilde P(X)$
期望</p>
<script type="math/tex; mode=display">
E_P(f(x)) = \sum_{x,y} \tilde P(x)P(y|x)f(x,y)</script></li>
</ul>
</li>
<li><p>期望模型$P(Y|X)$能够获取数据中信息，则两个期望值应该相等</p>
<script type="math/tex; mode=display">\begin{align*}
E_P(f) & = E_{\tilde P}(f) \\
\sum_{x,y} \tilde P(x)P(y|x)f(x,y) & =
   \sum_{x,y} \tilde P(x,y)f(x,y)
\end{align*}</script><p>此即作为模型学习的约束条件</p>
<ul>
<li><p>此约束是纯粹的关于$P(Y|X)$的约束，只是约束形式特殊，
需要通过期望关联熵</p>
</li>
<li><p>若有其他表述形式、可以直接带入的、关于$P(Y|X)$约束，
可以直接使用</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>满足所有约束条件的模型集合为<script type="math/tex; mode=display">
  \mathcal{C} = \{P | E_{P(f_i)} = E_{\tilde P (f_i)},
      i=1,2,\cdots,n \}</script>  定义在条件概率分布$P(Y|X)$上的条件熵为<script type="math/tex; mode=display">
  H(P) = -\sum_{x,y} \tilde P(x) P(y|x) logP(y|x)</script>  则模型集合$\mathcal{C}$中条件熵最大者即为最大是模型</li>
</ul>
</blockquote>
<h3 id="策略-1"><a href="#策略-1" class="headerlink" title="策略"></a>策略</h3><p>最大熵模型的策略为以下约束最优化问题</p>
<script type="math/tex; mode=display">\begin{array}{l}
\max_{P \in \mathcal{C}} & -H(P)=\sum_{x,y} \tilde P(x)
    P(y|x) logP(y|x) \\
s.t. & E_P(f_i) - E_{\tilde P}(f_i) = 0, i=1,2,\cdots,M \\
& \sum_{y} P(y|x)  = 1
\end{array}</script><ul>
<li><p>引入拉格朗日函数</p>
<script type="math/tex; mode=display">\begin{align*}
L(P, w) & = -H(P) - w_0(1-\sum_y P(y|x)) + \sum_{m=1}^M
   w_m(E_{\tilde P}(f_i) - E_P(f_i)) \\
& = \sum_{x,y} \tilde P(x) P(y|x) logP(y|x) + w_0
   (1-\sum_y P(y|x)) + \sum_{m=1}^M w_m (\sum_{x,y}
   \tilde P(x,y)f_i(x, y) - \tilde P(x)P(y|x)f_i(x,y))
\end{align*}</script><ul>
<li><p>原始问题为</p>
<script type="math/tex; mode=display">
\min_{P \in \mathcal{C}} \max_{w} L(P, w)</script></li>
<li><p>对偶问题为</p>
<script type="math/tex; mode=display">
\max_{w} \min_{P \in \mathcal{C}} L(P, w)</script></li>
<li><p>考虑拉格朗日函数$L(P, w)$是P的凸函数，则原始问题、
对偶问题解相同</p>
</li>
</ul>
</li>
<li><p>记</p>
<script type="math/tex; mode=display">\begin{align*}
\Psi(w) & = \min_{P \in \mathcal{C}} L(P, w)
   = L(P_w, w) \\
P_w & = \arg\min_{P \in \mathcal{C}} L(P, w) = P_w(Y|X)
\end{align*}</script></li>
<li><p>求$L(P, w)$对$P(Y|X)$偏导</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial L(P, w)} {\partial P(Y|X)} & =
   \sum_{x,y} \tilde P(x)(logP(y|x)+1) - \sum_y w_0 -
   \sum_{x,y}(\tilde P(x) \sum_{i=1}^N w_i f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x)(log P(y|x) + 1 - w_0 -
   \sum_{i=1}^N w_i f_i(x, y))
\end{align*}</script><p>偏导置0，考虑到$\tilde P(x) &gt; 0$，其系数必始终为0，有</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y|X) & = \exp(\sum_{i=1}^N w_i f_i(x,y) + w_0 - 1) \\
& = \frac {exp(\sum_{i=1}^N w_i f_i(x,y))} {exp(1-w_0)}
\end{align*}</script></li>
<li><p>考虑到约束$\sum_y P(y|x) = 1$，有</p>
<script type="math/tex; mode=display">\begin{align*}
P_w(y|x) & = \frac 1 {Z_w(x)} exp(\sum_{i=1}^N w_i
   f_i(x,y)) \\
Z_w(x) & = \sum_y exp(\sum_{i=1}^N w_i f_i(x,y)) \\
& = exp(1 - w_0)
\end{align*}</script><blockquote>
<ul>
<li>$Z_w(x)$：规范化因子</li>
<li>$f(x, y)$：特征</li>
<li>$w_i$：特征权值</li>
</ul>
</blockquote>
</li>
<li><p>原最优化问题等价于求解偶问题极大化问题$\max_w \Psi(w)$</p>
<script type="math/tex; mode=display">\begin{align*}
\Psi(w) & = \sum_{x,y} \tilde P(x) P_w(y|x) logP_w(y|x)
   + \sum_{i=1}^N w_i(\sum_{x,y} \tilde P(x,y) f_i(x,y)
   - \sum_{x,y} \tilde P(x) P_w(y|x) f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) +
   \sum_{x,y} \tilde P(x,y) P_w(y|x)(log P_w(y|x) -
   \sum_{i=1}^N w_i f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_{x,y} \tilde P(x,y) P_w(y|x) log Z_w(x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_x \tilde P(x) log Z_w(x)
\end{align*}</script><p>记其解为</p>
<script type="math/tex; mode=display">w^{*} = \arg\max_w \Psi(w)</script><p>带入即可得到最优（最大熵）模型$P_{w^{*}}(Y|X)$</p>
</li>
</ul>
<h4 id="策略性质"><a href="#策略性质" class="headerlink" title="策略性质"></a>策略性质</h4><ul>
<li><p>已知训练数据的经验概率分布为$\tilde P(X,Y)$，则条件概率
分布$P(Y|X)$的对数似然函数为</p>
<script type="math/tex; mode=display">\begin{align*}
L_{\tilde P}(P_w) & = N log \prod_{x,y}
   P(y|x)^{\tilde P(x,y)} \\
& = \sum_{x,y} N * \tilde P(x,y) log P(y|x)
\end{align*}</script><blockquote>
<ul>
<li>这里省略了系数样本数量$N$</li>
</ul>
</blockquote>
</li>
<li><p>将最大熵模型带入，可得</p>
<script type="math/tex; mode=display">\begin{align*}
L_{\tilde P_w} & = \sum_{x,y} \tilde P(y|x) logP(y|x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_{x,y} \tilde P(x,y)log Z_w(x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_x \tilde P(x) log Z_w(x) \\
& = \Psi(w)
\end{align*}</script><p>对偶函数$\Psi(w)$等价于对数似然函数$L_{\tilde P}(P_w)$，
即最大熵模型中，<strong>对偶函数极大等价于模型极大似然估计</strong></p>
</li>
</ul>
<h3 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h3><ul>
<li><p>思想</p>
<ul>
<li>假设最大熵模型当前参数向量$w=(w_1,w_2,\cdots,w_M)^T$</li>
<li>希望能找到新的参数向量（参数向量更新）
$w+\sigma=(w_1+\sigma_1,\cdots,w_M+\sigma_M)$
使得模型对数似然函数/对偶函数值增加</li>
<li>不断对似然函数值进行更新，直到找到对数似然函数极大值</li>
</ul>
</li>
<li><p>对给定经验分布$\tilde P(x,y)$，参数向量更新至$w+\sigma$
时，对数似然函数值变化为</p>
<script type="math/tex; mode=display">\begin{align*}
L(w+\sigma) - L(w) & = \sum_{x,y} \tilde P(x,y)
   log P_{w+\sigma}(y|x) - \sum_{x,y} \tilde P(x,y)
   log P_w(y|x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) - \sum_x \tilde P(x) log \frac
   {Z_{w+\sigma}(x)} {Z_w(x)} \\
& \geq \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) + 1 - \sum_x \tilde P(x) \frac
   {Z_{w+\sigma}(x)} {Z_w(x)} \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) + 1 - \sum_x \tilde P(x) \sum_y P_y(y|x)
   exp(\sum_{i=1}^M \sigma_i f_i(x,y))
\end{align*}</script><ul>
<li><p>不等式步利用$a - 1 \geq log a, a \geq 1$</p>
</li>
<li><p>最后一步利用</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {Z_{w+\sigma}(x)} {Z_w(x)} & = \frac 1 {Z_w(x)}
  \sum_y exp(\sum_{i=1}^M (w_i + \sigma_i)
  f_i(x, y)) \\
& = \frac 1 {Z_w(x)} \sum_y exp(\sum_{i=1}^M w_i
  f_i(x,y) + \sigma_i f_i(x,y)) \\
& = \sum_y P_w(y|x) exp(\sum_{i=1}^n \sigma_i
  f_i(x,y))
\end{align*}</script></li>
</ul>
</li>
<li><p>记上式右端为$A(\sigma|w)$，则其为对数似然函数改变量的
一个下界</p>
<script type="math/tex; mode=display">
L(w+\sigma) - L(w) \geq A(\sigma|w)</script><ul>
<li>若适当的$\sigma$能增加其值，则对数似然函数值也应该
增加</li>
<li>函数$A(\sigma|w)$中因变量$\sigma$为向量，难以同时
优化，尝试每次只优化一个变量$\sigma_i$，固定其他变量
$\sigma_j$</li>
</ul>
</li>
<li><p>记</p>
<script type="math/tex; mode=display">f^{**} (x,y) = \sum_i f_i(x,y)</script><p>考虑到$f_i(x,y)$为二值函数，则$f^{**}(x,y)$表示所有特征
在$(x,y)$出现的次数，且有</p>
<script type="math/tex; mode=display">
A(\sigma|w) = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M
   \sigma_i f_i(x,y) + 1 - \sum_x \tilde P(x)
   \sum_y P_w(y|x) exp(f^{**}(x,y) \sum_{i=1}^M
   \frac {\sigma_i f_i(x,y)} {f^{**}(x,y)})</script></li>
<li><p>考虑到$\sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)} = 1$，
由指数函数凸性、Jensen不等式有</p>
<script type="math/tex; mode=display">
exp(\sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)} \sigma_i
   f^{**}(x,y)) \leq \sum_{i=1}^M \frac {f_i(x,y)}
   {f^{**}(x,y)} exp(\sigma_i f^{**}(x,y))</script><p>则</p>
<script type="math/tex; mode=display">
A(\sigma|w) \geq \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M
   \sigma_i f_i(x,y) + 1 - \sum_x \tilde P(x) \sum_y
   P_w(y|x) \sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)}
   exp(\sigma_i f^{**}(x,y))</script></li>
<li><p>记上述不等式右端为$B(\sigma|w)$，则有</p>
<script type="math/tex; mode=display">
L(w+\sigma) - L(w) \geq B(\sigma|w)</script><p>其为对数似然函数改变量的一个新、相对不紧的下界</p>
</li>
<li><p>求$B(\sigma|w)$对$\sigma_i$的偏导</p>
<script type="math/tex; mode=display">
\frac {\partial B(\sigma|w)} {\partial \sigma_i} =
   \sum_{x,y} \tilde P(x,y) f_i(x,y) -
   \sum_x \tilde P(x) \sum_y P_w(y|x) f_i(x,y)
   exp(\sigma_i f^{**}(x,y))</script><p>置偏导为0，可得</p>
<script type="math/tex; mode=display">
\sum_x \tilde P(x) \sum_y P_w(y|x) f_i(x,y) exp(\sigma_i
   f^{**}(x,y)) = \sum_{x,y} \tilde P(x,y) f_i(x,y) =
   E_{\tilde P}(f_i)</script><p>其中仅含变量$\sigma_i$，则依次求解以上方程即可得到
$\sigma$</p>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><blockquote>
<ul>
<li>输入：特征函数$f_1, f_2, \cdots, f_M$、经验分布
  $\tilde P(x)$、最大熵模型$P_w(x)$</li>
<li>输出：最优参数值$w<em>i^{*}$、最优模型$P</em>{w^{*}}$</li>
</ul>
</blockquote>
<ol>
<li><p>对所有$i \in {1,2,\cdots,M}$，取初值$w_i = 0$</p>
</li>
<li><p>对每个$i \in {1,2,\cdots,M}$，求解以上方程得$\sigma_i$</p>
<ul>
<li><p>若$f^{**}(x,y)=C$为常数，则$\sigma_i$有解析解</p>
<script type="math/tex; mode=display">
\sigma_i = \frac 1 C log \frac {E_{\tilde P}(f_i)}
 {E_P(f_i)}</script></li>
<li><p>若$f^{**}(x,y)$不是常数，则可以通过牛顿法迭代求解</p>
<script type="math/tex; mode=display">
\sigma_i^{(k+1)} = \sigma_i^{(k)} - \frac
 {g(\sigma_i^{(k)})} {g^{'}(\sigma_i^{(k)})}</script><blockquote>
<ul>
<li>$g(\sigma_i)$：上述方程对应函数</li>
</ul>
</blockquote>
<ul>
<li>上述方程有单根，选择适当初值则牛顿法恒收敛</li>
</ul>
</li>
</ul>
</li>
<li><p>更新$w_i$，$w_i \leftarrow w_i + \sigma_i$，若不是所有
$w_i$均收敛，重复2</p>
</li>
</ol>
<h3 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h3><p>对最大熵模型</p>
<ul>
<li><p>为方便，目标函数改为求极小</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min_{w \in R^M} f(w) = \sum_x \tilde P(x) log \sum_{y}
   exp(\sum_{i=1}^M w_i f_i(x,y)) - \sum_{x,y}
   \tilde P(x,y) \sum_{i=1}^M w_i f_i(x,y)
\end{array}</script></li>
<li><p>梯度为</p>
<script type="math/tex; mode=display">\begin{align*}
g(w) & = (\frac {\partial f(w)} {\partial w_i}, \cdots,
   \frac {\partial f(w)} {\partial w_M})^T \\
\frac {\partial f(w)} {\partial w_M} & = \sum_{x,y}
   \tilde P(x) P_w(y|x) f_i(x,y) - E_{\tilde P}(f_i)
\end{align*}</script></li>
</ul>
<h4 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h4><p>将目标函数带入BFGS算法即可</p>
<blockquote>
<ul>
<li>输入：特征函数$f_1, f_2, \cdots, f_M$、经验分布
  $\tilde P(x)$、最大熵模型$P_w(x)$</li>
<li>输出：最优参数值$w<em>i^{*}$、最优模型$P</em>{w^{*}}$</li>
</ul>
</blockquote>
<ol>
<li><p>取初值$w^{(0)}$、正定对称矩阵$B^{(0)}$，置k=0</p>
</li>
<li><p>计算$g^{(k)} = g(w^{(k)})$，若$|g^{(k)}| &lt; \epsilon$，
停止计算，得到解$w^{*} = w^{(k)}$</p>
</li>
<li><p>由拟牛顿公式$B^{(k)}p^{(k)} = -g^{(k)}$求解$p^{(k)}$</p>
</li>
<li><p>一维搜索，求解</p>
<script type="math/tex; mode=display">
\lambda^{(k)} = \arg\min_{\lambda} f(w^{(k)} +
  \lambda p_k)</script></li>
<li><p>置$w^{(k+1)} = w^{(k)} + \lambda^{(k)} p_k$</p>
</li>
<li><p>计算$g^{(k+1)} = g(w^{(k+1)})$，若
$|g^{(k+1)}| &lt; \epsilon$，停止计算，得到解
$w^{*} = w^{(k+1)}$，否则求</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} - \frac {B^{(k)} s^{(k)}
  (s^{(k)})^T B^{(k)}} {(s^{(k)})^T B^{(k)} s^{(k)}}
  + \frac {y^{(k)} (y^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script><blockquote>
<ul>
<li>$s^{(k)} = w^{(k+1)} - w^{(k)}$</li>
<li>$y^{(k)} = g^{(k+1)} - g^{(k)}$</li>
</ul>
</blockquote>
</li>
<li><p>置k=k+1，转3</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-13T15:26:35.000Z" title="7/13/2019, 11:26:35 PM">2019-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T08:09:43.000Z" title="7/16/2021, 4:09:43 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Unsupervised-Model/">Unsupervised Model</a></span><span class="level-item">25 minutes read (About 3774 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Unsupervised-Model/expectation_maximization.html">EM算法</a></h1><div class="content"><h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p><em>expectation maximization algorithm</em>：含有隐变量的概率模型
参数的极大似然估计法、极大后验概率估计法</p>
<ul>
<li><p>模型含有<em>latent variable</em>（潜在变量）、<em>hidden variable</em>
（隐变量）似然函数将没有解析解</p>
</li>
<li><p>所以EM算法需要迭代求解，每次迭代由两步组成</p>
<ul>
<li>E步：求期望expectation</li>
<li>M步：求极大maximization</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>模型变量都是<em>observable variable</em>、给定数据情况下，可以
  直接使用极大似然估计、贝叶斯估计</li>
</ul>
</blockquote>
<h2 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h2><p>对含有隐变量的概率模型，目标是极大化观测数据（不完全数据）
$Y$关于参数$\theta$的对数似然函数，即极大化</p>
<script type="math/tex; mode=display">\begin{align*}
L(\theta) & = log P(Y|\theta) \\
& = log \sum_Z P(Y, Z|\theta) \\
& = log \left(\sum_Z P(Y|Z,\theta) P(Z|\theta) \right)
\end{align*}</script><blockquote>
<ul>
<li>$Y$：观测变量数据</li>
<li>$Z$：隐随机变量数据（未知）</li>
<li>$Y,Z$合在一起称为完全数据</li>
<li>$P(Y,Z|\theta)$：联合分布</li>
<li>$P(Z|Y,\theta)$：条件分布</li>
</ul>
</blockquote>
<ul>
<li>但是极大化目标函数中包括未观测数据$Z$、求和（积分）的
对数，直接求极大化非常困难</li>
<li>EM算法通过<strong>迭代</strong>逐步近似极大化$L(\theta)$</li>
</ul>
<h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><ul>
<li><p>假设第i次迭代后$\theta$的估计值是$\theta^{(i)}$，希望
新估计值$\theta$能使$L(\theta)$增加，并逐步增加到极大值
，考虑两者之差</p>
<script type="math/tex; mode=display">
L(\theta) - L(\theta^{(i)}) = log (\sum_Z P(Y|Z,\theta)
   P(Z|\theta)) - log P(Y|\theta^{(i)})</script></li>
<li><p>利用Jensen不等式有</p>
<script type="math/tex; mode=display">\begin{align*}
L(\theta) - L(|\theta^{(i)}) & = log(\sum_Z P(Y|Z,
   \theta^{(i)}) \frac {P(Y|Z,\theta) P(Z|\theta)}
   {P(Y|Z,\theta^{(i)})}) - log P(Y|\theta^{(i)}) \\
& \geq \sum_Z P(Z|Y,\theta^{(i)}) log \frac
   {P(Y|Z,\theta) P(Z|\theta)} {P(Z|Y,\theta^{(i)})}
   - log P(Y|\theta^{(i)}) \\
& = \sum_z P(Z|Y,\theta^{(i)}) log \frac
   {P(Y|Z,\theta) P(Z|\theta)}
   {P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})}
\end{align*}</script></li>
<li><p>令</p>
<script type="math/tex; mode=display">
B(\theta, \theta^{(i)}) = L(\theta^{(i)}) + \sum_Z
   P(Z|Y,\theta^{(i)}) log \frac
   {P(Y|Z,\theta) P(Z|\theta)}
   {P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})}</script><p>则$B(\theta, \theta^{(i)})$是$L(\theta)$的一个下界，即</p>
<script type="math/tex; mode=display">\begin{align*}
L(\theta) & \geq B(\theta, \theta^{(i)}) \\
\end{align*}</script><p>并根据$B(\theta, \theta^{(i)})$定义有</p>
<script type="math/tex; mode=display">\begin{align*}
L(\theta^{(i)}) = B(\theta^{(i)}, \theta^{(i)})
\end{align*}</script></li>
<li><p>则任意$\theta$满足
$B(\theta,\theta^{(i)}) &gt; B(\theta^{(i)},\theta^{(i)})$
，将满足$L(\theta) &gt; L(\theta^{(i)})$，应选择
$\theta^{(i+1)}$使得$B(\theta,\theta^{(i)})$达到极大</p>
<script type="math/tex; mode=display">\begin{align*}
\theta^{(i+1)} & = \arg\max_{\theta}
   B(\theta,\theta^{(i)}) \\
& = \arg\max_{\theta} L(\theta^{(i)}) + \sum_Z
   P(Z|Y,\theta^{(i)}) log \frac
   {P(Y|Z,\theta) P(Z|\theta)}
   {P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})} \\
& = \arg\max_{\theta} (\sum_Z P(Z|Y,\theta^{(i)})
   log(P(Y|Z,\theta)P(Z|\theta))) \\
& = \arg\max_{\theta} (\sum_Z P(Z|Y,\theta^{(i)})
   log P(Y,Z|\theta)) \\
& = \arg\max_{\theta} Q(\theta, \theta^{(i)})
\end{align*}</script><blockquote>
<ul>
<li>和$\theta$无关的常数项全部舍去</li>
</ul>
</blockquote>
</li>
</ul>
<blockquote>
<ul>
<li>$Q(\theta, \theta^{(i)})$：Q函数，完全数据的对数似然函数
  $logP(Y,Z|\theta)$，关于在给定观测$Y$和当前参数
  $\theta^{(i)}$下，对未观测数据Z的条件概率分布
  $P(Z|Y,\theta^{(i)})$<script type="math/tex; mode=display">
  Q(\theta, \theta^{(i)}) = E_z
      [logP(Y,Z|\theta)|Y,\theta^{(i)}]</script></li>
</ul>
</blockquote>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ol>
<li><p>选择参数初值$\theta^{0}$，开始迭代</p>
</li>
<li><p>E步：记$\theta^{(i)}$为第$i$迭代时，参数$\theta$的估计值
，在第$i+1$步迭代的E步时，计算Q函数
$Q(\theta, \theta^{(i)})$</p>
</li>
<li><p>M步：求使得Q函数极大化$\theta$作为第$i+1$次估计值
$\theta^{(i+1)}$</p>
<script type="math/tex; mode=display">
\theta^{(i+1)} = \arg\max_{\theta} Q(\theta, \theta^{(i)})</script></li>
<li><p>重复E步、M步直到待估参数收敛</p>
</li>
</ol>
<blockquote>
<ul>
<li><p>算法初值可以任意选择，但EM算法对初值敏感</p>
</li>
<li><p>E步：参数值估计缺失值分布，计算Q函数（似然函数）</p>
</li>
<li><p>M步：Q函数取极大得新参数估计值</p>
</li>
<li><p>收敛条件一般是对较小正数$\epsilon$，满足
  $|\theta^{(i+1)} - \theta^{(i)}| &lt; \epsilon$或
  $|Q(\theta^{(i+1)},\theta^{(i)}) - Q(\theta^{(i)},\theta^{(i)})| &lt; \epsilon$</p>
</li>
</ul>
</blockquote>
<h3 id="EM算法特点"><a href="#EM算法特点" class="headerlink" title="EM算法特点"></a>EM算法特点</h3><h4 id="EM算法优点"><a href="#EM算法优点" class="headerlink" title="EM算法优点"></a>EM算法优点</h4><ul>
<li>EM算法可以用于估计含有隐变量的模型参数</li>
<li>非常简单，稳定上升的步骤能非常可靠的找到最优估计值</li>
<li>应用广泛，能应用在多个领域中<ul>
<li>生成模型的非监督学习</li>
</ul>
</li>
</ul>
<h4 id="EM算法缺点"><a href="#EM算法缺点" class="headerlink" title="EM算法缺点"></a>EM算法缺点</h4><ul>
<li>EM算法计算复杂、受外较慢，不适合高维数据、大规模数据集</li>
<li>参数估计结果依赖初值，不够稳定，不能保证找到全局最优解</li>
</ul>
<h3 id="算法收敛性"><a href="#算法收敛性" class="headerlink" title="算法收敛性"></a>算法收敛性</h3><h4 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h4><blockquote>
<ul>
<li>设$P(Y|\theta)$为观测数据的似然函数，$\theta^{(i)}$为
  EM算法得到的参数估计序列，$P(Y|\theta^{(i)}),i=1,2,…$
  为对应的似然函数序列，则$P(Y|\theta^{(i)})$是单调递增的<script type="math/tex; mode=display">
  P(Y|\theta^{(i+1)}) \geq P(Y|\theta^{(i)})</script></li>
</ul>
</blockquote>
<ul>
<li><p>由条件概率</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y|\theta) & = \frac {P(Y,Z|\theta)} {P(Z|Y,\theta)} \\
logP(Y|\theta) & = logP(Y,Z|\theta) - logP(Z|Y,\theta)
\end{align*}</script></li>
</ul>
<ul>
<li><p>则对数似然函数有</p>
<script type="math/tex; mode=display">
logP(Y|\theta) = Q(\theta, \theta^{(i)}) -
   H(\theta, \theta^{(i)})</script><blockquote>
<ul>
<li>$H(\theta, \theta^{(i)}) = \sum_Z log P(Z|Y,\theta) P(Z|Y,\theta)$</li>
<li>$Q(\theta, \theta^{(i)})$：前述Q函数</li>
<li>$logP(Y|\theta)$和$Z$无关，可以直接提出</li>
</ul>
</blockquote>
</li>
<li><p>分别取$\theta^{(i+1)}, \theta^{(i)}$带入，做差</p>
<script type="math/tex; mode=display">
logP(Y|\theta^{(i+1)}) - logP(Y|\theta^{(i)}) =
   [Q(\theta^{(i+1)}, \theta^{(i)}) - 
   Q(\theta^{(i)}, \theta^{(i)}] -
   [H(\theta^{(i+1)}, \theta^{(i)}) -
   H(\theta^{(i)}, \theta^{(i)})]</script><ul>
<li><p>$\theta^{(i+1)}$使得$Q(\theta, \theta^{(i)})$取极大</p>
</li>
<li><p>又有</p>
<script type="math/tex; mode=display">\begin{align*}
& H(\theta^{(i+1)}, \theta^{(i)}) -
  H(\theta^{(i)}, \theta^{(i)}) \\
= & \sum_Z (log \frac {P(Z|Y,\theta^{(i+1)})}
  {P(Z|Y,\theta^{(I)})}) P(Z|Y,\theta^{(i)}) \\
\leq & log (\sum_Z \frac {P(Z|Y,\theta^{(i+1)})}
  {P(Z|Y,\theta^{(I)})} P(Z|Y,\theta^{(i)})) \\
= & log \sum_Z P(Z|Y,\theta^{(i+1)}) = 0
\end{align*}</script></li>
</ul>
</li>
</ul>
<h4 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h4><blockquote>
<ul>
<li>设$L(\theta)=log P(Y|\theta)$为观测数据的对数似然函数，
  $\theta^{(i)},i=1,2,…$为EM算法得到的参数估计序列，
  $L(\theta^{(i)}),i=1,2,…$为对应的对数似然函数序列<blockquote>
<ul>
<li>若$P(Y|\theta)$有上界，则$L(\theta^{(i)})$收敛到某
 定值$L^{*}$</li>
<li>Q函数$Q(\theta, \theta^{‘})$与$L(\theta)$满足一定
 条件的情况下，由EM算法得到的参数估计序列
 $\theta^{(i)}$的收敛值$\theta^{*}$是$L(\theta)$的
 稳定点</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li>结论1由序列单调、有界显然</li>
</ul>
<blockquote>
<ul>
<li>Q函数$Q(\theta, \theta^{‘})$与$L(\theta)$的条件在大多数
  情况下是满足的</li>
<li>EM算法收敛性包含对数似然序列$L(\theta^{(i)})$、参数估计
  序列$\theta^{(i)}$的收敛性，前者不蕴含后者</li>
<li>此定理只能保证参数估计序列收敛到对数似然序列的稳定点，
  不能保证收敛到极大点，可选取多个不同初值迭代，从多个结果
  中选择最好的</li>
</ul>
</blockquote>
<h2 id="Gaussion-Mixture-Model"><a href="#Gaussion-Mixture-Model" class="headerlink" title="Gaussion Mixture Model"></a><em>Gaussion Mixture Model</em></h2><blockquote>
<ul>
<li>高斯混合模型是指具有如下概率分布模型<script type="math/tex; mode=display">
  P(y|\theta) = \sum_{k=1}^K \alpha_k \phi(y|\theta_k)</script><blockquote>
<ul>
<li>$\alpha<em>k \geq 0, \sum</em>{k=1}^K \alpha_k=1$：系数</li>
<li>$\phi(y|\theta_k)$：高斯分布密度函数</li>
<li>$\theta_k=(\mu_k, \sigma_k)$：第k个分模型参数</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li>用EM算法估计高斯混合模型参数
$\theta=(\alpha_1,…,\alpha_2,\theta_1,…,\theta_K)$</li>
</ul>
<h3 id="推导-1"><a href="#推导-1" class="headerlink" title="推导"></a>推导</h3><h4 id="明确隐变量"><a href="#明确隐变量" class="headerlink" title="明确隐变量"></a>明确隐变量</h4><p>明确隐变量，写出完全数据对数似然函数</p>
<ul>
<li><p>反映观测数据$y_j$来自第k个分模型的数据是未知的</p>
<script type="math/tex; mode=display">\gamma_{j,k} = \left \{ \begin{array}{l}
1, & 第j个观测来自第k个分模型 \\
0, & 否则
\end{array} \right.</script><blockquote>
<ul>
<li>$j=1,2,\cdots,N$：观测编号</li>
<li>$k=1,2,\cdots,K$：模型编号</li>
</ul>
</blockquote>
</li>
<li><p>则完全数据为</p>
<script type="math/tex; mode=display">(y_j,\gamma_{j,1},\cdots,\gamma_{j,K}), j=1,2,...,N</script></li>
<li><p>完全数据似然函数为</p>
<script type="math/tex; mode=display">\begin{align*}
P(y,\gamma|\theta) & = \prod_{j=1}^N
   P(y_j,\gamma_{j,1},\cdots,\gamma_{j,N}|\theta) \\
& = \prod_{k=1}^{K} \prod_{j=1}^N
   [\alpha_k \phi(y_j|\theta_k)]^{\gamma _{j,k}} \\
& = \prod_{k=1}^{K} \alpha_k^{n_k} \prod_{j=1}^N
   [\phi(y_j|\theta_k)]^{\gamma _{j,k}} \\
\end{align*}</script><blockquote>
<ul>
<li>$n<em>k = \sum</em>{j=1}^{N} \gamma_{j,k}$</li>
<li>$\sum_{k=1}^K n_k = N$</li>
</ul>
</blockquote>
</li>
<li><p>完全数据的对数似然函数为</p>
<script type="math/tex; mode=display">
logP(y, \gamma|\theta) = \sum_{k=1}^K \left \{
   n_k log \alpha_k + \sum_{j=1}^N \gamma_{j,k}
   [log \frac 1 {\sqrt {2\pi}} - log \sigma_k -
   \frac 1 {2\sigma_k}(y_j - \mu_k)^2] \right \}</script></li>
</ul>
<h4 id="E步：确定Q函数"><a href="#E步：确定Q函数" class="headerlink" title="E步：确定Q函数"></a>E步：确定Q函数</h4><script type="math/tex; mode=display">\begin{align*}
Q(\theta, \theta^{(i)}) & =
    E_z[logP(y,\gamma|\theta)|Y,\theta^{(i)}] \\
& = E \sum_{k=1}^K \left \{ n_k log\alpha_k + \sum_{j=1}^N
    \gamma_{j,k} [log \frac 1 {\sqrt {2\pi}} - log \sigma_k
    - \frac 1 {2\sigma_k}(y_j - \mu_k)^2] \right \} \\
& = \sum_{k=1}^K \left \{ \sum_{k=1}^K (E\gamma_{j,k})
    log\alpha_k + \sum_{j=1}^N (E\gamma_{j,k})
    [log \frac 1 {\sqrt {2\pi}} - log \sigma_k
    - \frac 1 {2\sigma_k}(y_j - \mu_k)^2] \right \}
\end{align*}</script><blockquote>
<ul>
<li>$E\gamma<em>{j,k} = E(\gamma</em>{j,k}|y,\theta)$：记为
  $\hat \gamma_{j,k}$</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">\begin{align*}
\hat \gamma_{j,k} & = E(\gamma_{j,k}|y,\theta) =
    P(\gamma_{j,k}|y,\theta) \\
& = \frac {P(\gamma_{j,k}=1, y_j|\theta)}
    {\sum_{k=1}^K P(\gamma_{j,k}=1,y_j|\theta)} \\
& = \frac {P(y_j|\gamma_{j,k}=1,\theta)
    P(\gamma_{j,k}=1|\theta)} {\sum_{k=1}^K
    P(y_j|\gamma_{j,k}=1,\theta) P(\gamma_{j,k}|\theta)} \\
& = \frac {\alpha_k \phi(y_j|\theta _k)}
    {\sum_{k=1}^K \alpha_k \phi(y_j|\theta_k)}
\end{align*}</script><p>带入可得</p>
<script type="math/tex; mode=display">
Q(\theta, \theta^{(i)}) = \sum_{k=1}^K \left\{
    n_k log\alpha_k + \sum_{k=1}^N \hat \gamma_{j,k}
    [log \frac 1 {\sqrt{2\pi}} - log \sigma_k -
    \frac 1 {2\sigma^2}(y_j - \mu_k)^2] \right \}</script><h4 id="M步"><a href="#M步" class="headerlink" title="M步"></a>M步</h4><p>求新一轮模型参数
$\theta^{(i+1)}=(\hat \alpha_1,…,\hat \alpha_2,\hat \theta_1,…,\hat \theta_K)$</p>
<script type="math/tex; mode=display">\begin{align*}
\theta^{(i+1)} & = \arg\max_{\theta} Q(\theta,\theta^{(i)}) \\
\hat \mu_k & = \frac {\sum_{j=1}^N \hat \gamma_{j,k} y_j}
    {\sum_{j=1}^N \hat \gamma_{j,k}} \\
\hat \sigma_k^2 & = \frac {\sum_{j=1}^N \hat \gamma_{j,k}
    (y_j - \mu_p)^2} {\sum_{j=1}^N \hat \gamma_{j,k}} \\
\hat \alpha_k & = \frac {n_k} N = \frac {\sum_{j=1}^N
    \hat \gamma_{j,k}} N
\end{align*}</script><blockquote>
<ul>
<li>$\hat \theta_k = (\hat \mu_k, \hat \sigma_k^2)$：直接求
  偏导置0即可得</li>
<li>$\hat \alpha<em>k$：在$\sum</em>{k=1}^K \alpha_k = 1$条件下求
  偏导置0求得</li>
</ul>
</blockquote>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>输入：观测数据$y_1, y_2,\cdots, y_N$，N个高斯混合模型</li>
<li>输出：高斯混合模型参数</li>
</ul>
</blockquote>
<ol>
<li><p>取参数初始值开始迭代</p>
</li>
<li><p>E步：依据当前模型参数，计算分模型k对观测数据$y_j$响应度</p>
<script type="math/tex; mode=display">
\hat \gamma_{j,k} = \frac {\alpha \phi(y_k|\theta_k)}
  {\sum_{k=1}^N \alpha_k \phi(y_j|\theta)}</script></li>
<li><p>M步：计算新一轮迭代的模型参数
$\hat mu_k, \hat \sigma_k^2, \hat \alpha_k$</p>
</li>
<li><p>重复2、3直到收敛</p>
</li>
</ol>
<blockquote>
<ul>
<li>GMM模型的参数估计的EM算法非常类似K-Means算法<blockquote>
<ul>
<li>E步类似于K-Means中计算各点和各聚类中心之间距离，不过
 K-Means将点归类为离其最近类，而EM算法则是算期望</li>
<li>M步根据聚类结果更新聚类中心</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h2 id="GEM"><a href="#GEM" class="headerlink" title="GEM"></a>GEM</h2><h3 id="Maximization-Maximization-Algorithm"><a href="#Maximization-Maximization-Algorithm" class="headerlink" title="Maximization-Maximization Algorithm"></a><em>Maximization-Maximization Algorithm</em></h3><h4 id="Free-Energy函数"><a href="#Free-Energy函数" class="headerlink" title="Free Energy函数"></a><em>Free Energy</em>函数</h4><blockquote>
<ul>
<li>假设隐变量数据Z的概率分布为$\tilde P(Z)$，定义分布
  $\tilde P$与参数$\theta$的函数$F(\tilde P, \theta)$如下<script type="math/tex; mode=display">
  F(\tilde P, \theta) = E_{\tilde P}
      [log P(Y,Z|\theta)] + H(\tilde P)</script></li>
</ul>
<blockquote>
<ul>
<li>$H(\tilde P)=-E_{\tilde P} log \tilde P(Z)$：分布
   $\tilde P(Z)$的熵</li>
<li>通常假设$P(Y,Z|\theta)$是$\theta$的连续函数，则函数
   $F(\tilde P,\theta)$是$\tilde P, \theta$的连续函数</li>
</ul>
</blockquote>
</blockquote>
<h4 id="定理1-1"><a href="#定理1-1" class="headerlink" title="定理1"></a>定理1</h4><blockquote>
<ul>
<li>对于固定$\theta$，存在唯一分布$\tilde P<em>\theta$，极大化
  $F(\tilde P, \theta)$，这时$\tilde P</em>\theta$由下式给出<script type="math/tex; mode=display">
  \tilde P_\theta(Z) = P(Z|Y,\theta)</script>  并且$\tilde P_{\theta}$随$\theta$连续变化</li>
</ul>
</blockquote>
<ul>
<li><p>对于固定的$\theta$，求使得$F(\tilde P, \theta)$的极大，
构造Lagrange函数</p>
<script type="math/tex; mode=display">
L(\tilde P, \lambda, \mu) = F(\tilde P, \theta) +
   \lambda(1 - \sum_Z \tilde P(Z)) - \mu \tilde P(Z)</script><p>因为$\tilde P(Z)$是概率密度，自然包含两个约束</p>
<script type="math/tex; mode=display">\left \{ \begin{array}{l}
\sum_Z \tilde P(Z) = 1 \\
\tilde P(Z) \geq 0
\end{array} \right.</script><p>即Lagrange方程中后两项</p>
</li>
<li><p>对$\tilde P(Z)$求偏导，得</p>
<script type="math/tex; mode=display">
\frac {\partial L} {\partial \tilde P(Z)} =
   log P(Y,Z|\theta) - log \tilde P(Z) - \lambda - \mu</script><p>令偏导为0，有</p>
<script type="math/tex; mode=display">\begin{align*}
log P(Y,Z|\theta) - log \tilde P(Z) & = \lambda + \mu \\
\frac {P(Y,Z|\theta)} {\tilde P(Z)} & = e^{\lambda + \mu}
\end{align*}</script></li>
<li><p>则使得$F(\tilde P, \theta)$极大的$\tilde P_\theta(Z)$
应该和$P(Y,Z|\theta)$成比例，由概率密度自然约束有</p>
<script type="math/tex; mode=display">\tilde P_\theta(Z) = P(Y,Z|\theta)</script><p>而由假设条件，$P(Y,Z|\theta)$是$\theta$的连续函数</p>
</li>
</ul>
<blockquote>
<ul>
<li><p>这里概率密度函数$\tilde P(Z)$是作为自变量出现</p>
</li>
<li><p>理论上对$\tilde P(Z)$和一般的<strong>复合函数求导</strong>没有区别，
  但$E_{\tilde P}, \sum_Z$使得整体看起来非常不和谐</p>
<script type="math/tex; mode=display">\begin{align*}
  E_{\tilde P} f(Z) & = \sum_Z f(Z) \tilde P(Z) \\
  & = \int f(Z) d(\tilde P(Z))
  \end{align*}</script></li>
</ul>
</blockquote>
<h4 id="定理2-1"><a href="#定理2-1" class="headerlink" title="定理2"></a>定理2</h4><blockquote>
<ul>
<li>若$\tilde P_\theta(Z) = P(Z|Y, \theta)$，则<script type="math/tex; mode=display">
  F(\tilde P, \theta) = log P(Y|\theta)</script></li>
</ul>
</blockquote>
<h4 id="定理3"><a href="#定理3" class="headerlink" title="定理3"></a>定理3</h4><blockquote>
<ul>
<li>设$L(\theta)=log P(Y|\theta)$为观测数据的对数似然函数，
  $\theta^{(i)}, i=1,2,\cdots$为EM算法得到的参数估计序列，
  函数$F(\tilde P,\theta)$如上定义<blockquote>
<ul>
<li>若$F(\tilde P,\theta)$在$\tilde P^{<em>}, \theta^{</em>}$
 上有局部极大值，则$L(\theta)$在$\theta^{*}$也有局部
 最大值</li>
<li>若$F(\tilde P,\theta)$在$\tilde P^{<em>}, \theta^{</em>}$
 达到全局最大，则$L(\theta)$在$\theta^{*}$也达到全局
 最大</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li><p>由定理1、定理2有</p>
<script type="math/tex; mode=display">
L(\theta) = logP(Y|\theta) = F(\tilde P_\theta, \theta)</script><p>特别的，对于使$F(\tilde P,\theta)$极大$\theta^{8}$有</p>
<script type="math/tex; mode=display">
L(\theta^{*}) = logP(Y|\theta^{*}) =
   F(\tilde P_\theta^{*}, \theta{*})</script></li>
<li><p>由$\tilde P_\theta$关于$\theta$连续，局部点域内不存在点
$\theta^{<strong>}$使得$L(\theta^{</strong>}) &gt; L(\theta^{<em>})$，否则
与$F(\tilde P, \theta^{</em>})$矛盾</p>
</li>
</ul>
<h4 id="定理4"><a href="#定理4" class="headerlink" title="定理4"></a>定理4</h4><blockquote>
<ul>
<li>EM算法的依次迭代可由F函数的极大-极大算法实现</li>
<li>设$\theta^{(i)}$为第i次迭代参数$\theta$的估计，
  $\tilde P^{(i)}$为第i次迭代参数$\tilde P$的估计，在第
  i+1次迭代的两步为<blockquote>
<ul>
<li>对固定的$\theta^{(i)}$，求$\tilde P^{(i)}$使得
 $F(\tilde P, \theta^{(i)})$极大</li>
<li>对固定的$\tilde P^{(i+1)}$，求$\theta^{(i+1)}$使
 $F(\tilde P^{(t+1)}, \theta)$极大化</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li><p>固定$\theta^{(i)}$</p>
<script type="math/tex; mode=display">\begin{align*}
F(\tilde P^{(i+1)}, \theta^{(i)} & = E_{\tilde P^{(t+1)}}
   [log P(Y,Z|\theta)] + H(\tilde P^{(i+1)}) \\
& = \sum_Z log P(Y,Z|\theta) P(Z|Y,\theta^{(i)}) +
   H(\tilde P^{(i+1)}) \\
& = Q(\theta, \theta^{(i)}) + H(\tilde P^{(i+1)})
\end{align*}</script></li>
<li><p>则固定$\tilde P^{(i+1)}$求极大同EM算法M步</p>
</li>
</ul>
<h3 id="GEM算法"><a href="#GEM算法" class="headerlink" title="GEM算法"></a>GEM算法</h3><blockquote>
<ul>
<li>输入：观测数据，F函数</li>
<li>输出：模型参数</li>
</ul>
</blockquote>
<ol>
<li><p>初始化$\theta^{(0)}$，开始迭代</p>
</li>
<li><p>第i+1次迭代：记$\theta^{(i)}$为参数$\theta$的估计值，
$\tilde P^{(i)}$为函数$\tilde P$的估计，求
$\tilde P^{(t+1)}$使$\tilde P$极大化$F(\tilde P,\theta)$</p>
</li>
<li><p>求$\theta^{(t+1)}$使$F(\tilde P^{(t+1)l}, \theta)$极大化</p>
</li>
<li><p>重复2、3直到收敛</p>
</li>
</ol>
<h3 id="次优解代替最优解"><a href="#次优解代替最优解" class="headerlink" title="次优解代替最优解"></a>次优解代替最优解</h3><blockquote>
<ul>
<li>输入：观测数据，Q函数</li>
<li>输出：模型参数</li>
</ul>
</blockquote>
<ol>
<li><p>初始化参数$\theta^{(0)}$，开始迭代</p>
</li>
<li><p>第i+1次迭代，记$\theta^{(i)}$为参数$\theta$的估计值，
计算</p>
<script type="math/tex; mode=display">\begin{align*}
Q(\theta, \theta^{(i)}) & = E_Z [
  log P(Y,Z|\theta)|Y,\theta^{(i)}] \\
& = \sum_Z P(Z|Y, \theta^{(i)}) log P(Y,Z|\theta)
\end{align*}</script></li>
<li><p>求$\theta^{(i+1)}$使</p>
<script type="math/tex; mode=display">
Q(\theta^{(i+1)}, \theta^{(i)}) >
  Q(\theta^{(i)}, \theta^{(i)})</script></li>
<li><p>重复2、3直到收敛</p>
</li>
</ol>
<blockquote>
<ul>
<li>有时候极大化$Q(\theta, \theta^{(i)})$非常困难，此算法
  仅寻找使目标函数值上升方向</li>
</ul>
</blockquote>
<h3 id="ADMM求次优解"><a href="#ADMM求次优解" class="headerlink" title="ADMM求次优解"></a>ADMM求次优解</h3><blockquote>
<ul>
<li>输入：观测数据，Q函数</li>
<li>输出：函数模型</li>
</ul>
</blockquote>
<ol>
<li><p>初始化参数
$\theta^{(0)} = (\theta_1^{(0)},…,\theta_d^{(0)})$，
开始迭代</p>
</li>
<li><p>第i次迭代，记
$\theta^{(i)} = (\theta_1^{(i)},…,\theta_d^{(i)})$，
为参数$\theta = (\theta_1,…,\theta_d)$的估计值，计算</p>
<script type="math/tex; mode=display">\begin{align*}
Q(\theta, \theta^{(i)}) & = E_Z [
  log P(Y,Z|\theta)|Y,\theta^{(i)}] \\
& = \sum_Z P(Z|Y, \theta^{(i)}) log P(Y,Z|\theta)
\end{align*}</script></li>
<li><p>进行d次条件极大化</p>
<ol>
<li><p>在$\theta<em>1^{(i)},…,\theta</em>{j-1}^{(i)},\theta_{j+1}^{(i)},…,\theta_d^{(i)}$
保持不变条件下
，求使$Q(\theta, \theta^{(i)})$达到极大的
$\theta_j^{(i+1)}$</p>
</li>
<li><p>j从1到d，进行d次条件极大化的，得到
$\theta^{(i+1)} = (\theta_1^{(i+1)},…,\theta_d^{(i+1)})$
使得</p>
<script type="math/tex; mode=display">
Q(\theta^{(i+1)}, \theta^{(i)}) >
Q(\theta^{(i)}, \theta^{(i)})</script></li>
</ol>
</li>
<li><p>重复2、3直到收敛</p>
</li>
</ol>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>