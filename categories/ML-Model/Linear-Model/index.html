<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: Linear Model - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/ML-Model/">ML Model</a></li><li class="is-active"><a href="#" aria-current="page">Linear Model</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-31T18:10:08.000Z" title="8/1/2019, 2:10:08 AM">2019-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:52:53.000Z" title="7/16/2021, 2:52:53 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">25 minutes read (About 3688 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/maximum_entropy.html">最大熵模型</a></h1><div class="content"><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><h3 id="逻辑斯蒂分布"><a href="#逻辑斯蒂分布" class="headerlink" title="逻辑斯蒂分布"></a>逻辑斯蒂分布</h3><script type="math/tex; mode=display">\begin{align*}
F(x) & = P(X \leq x) = \frac 1 {1 + e^{-(x-\mu)/\gamma}} \\
f(x) & = F^{'}(x) = \frac {e^{-(x-\mu)/\gamma}}
    {\gamma(1+e^{-(x-\mu)/\gamma})^2}
\end{align*}</script><blockquote>
<ul>
<li>$\mu$：位置参数</li>
<li>$\gamma$：形状参数</li>
</ul>
</blockquote>
<ul>
<li>分布函数属于逻辑斯蒂函数</li>
<li><p>分布函数图像为sigmoid curve</p>
<ul>
<li>关于的$(\mu, \frac 1 2)$中心对称<script type="math/tex; mode=display">
F(-x+\mu) - \frac 1 2 = -F(x+\mu) + \frac 1 2</script></li>
<li>曲线在靠近$\mu$中心附近增长速度快，两端速度增长慢</li>
<li>形状参数$\gamma$越小，曲线在中心附近增加越快</li>
</ul>
</li>
<li><p>模型优点</p>
<ul>
<li>模型输出值位于0、1之间，天然具有概率意义，方便观测
样本概率分数</li>
<li>可以结合$l-norm$正则化解决过拟合、共线性问题</li>
<li>实现简单，广泛用于工业问题</li>
<li>分类时计算量比较小、速度快、消耗资源少</li>
</ul>
</li>
<li><p>模型缺点</p>
<ul>
<li>特征空间很大时，性能不是很好，容易欠拟合，准确率一般</li>
<li>对非线性特征需要进行转换</li>
</ul>
</li>
</ul>
<h3 id="Binomial-Logistic-Regression-Model"><a href="#Binomial-Logistic-Regression-Model" class="headerlink" title="Binomial Logistic Regression Model"></a><em>Binomial Logistic Regression Model</em></h3><p>二项逻辑斯蒂回归模型：形式为参数化逻辑斯蒂分布的二分类
生成模型</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y=1|x) & = \frac {exp(wx + b)} {1 + exp (wx + b)} \\
P(Y=0|x) & = \frac 1 {1 + exp(wx + b)} \\
P(Y=1|\hat x) & = \frac {exp(\hat w \hat x)}
    {1 + exp (\hat w \hat x)} \\
P(Y=0|\hat x) & = \frac 1 {1+exp(\hat w \hat x)}
\end{align*}</script><blockquote>
<ul>
<li>$w, b$：权值向量、偏置</li>
<li>$\hat x = (x^T|1)^T$</li>
<li>$\hat w = (w^T|b)^T$</li>
</ul>
</blockquote>
<ul>
<li><p>逻辑回归比较两个条件概率值，将实例$x$归于条件概率较大类</p>
</li>
<li><p>通过逻辑回归模型，可以将线性函数$wx$转换为概率</p>
<ul>
<li>线性函数值越接近正无穷，概率值越接近1</li>
<li>线性函数值越接近负无穷，概率值越接近0</li>
</ul>
</li>
</ul>
<h4 id="Odds-Odds-Ratio"><a href="#Odds-Odds-Ratio" class="headerlink" title="Odds/Odds Ratio"></a>Odds/Odds Ratio</h4><ul>
<li><p>在逻辑回归模型中，输出$Y=1$的对数几率是输入x的线性函数</p>
<script type="math/tex; mode=display">
log \frac {P(Y=1|x)} {1-P(Y=1|x)} = \hat w \hat x</script></li>
<li><p>OR在逻辑回归中意义：$x_i$每增加一个单位，odds将变为原来
的$e^{w_i}$倍</p>
<script type="math/tex; mode=display">\begin{align*}
odd &= \frac {P(Y=1|x)} {1-P(Y=1|x)} = e^{\hat w \hat x} \\
OR_{x_i+1 / x_i} &= e^{w_i}
\end{align*}</script><ul>
<li><p>对数值型变量</p>
<ul>
<li>多元LR中，变量对应的系数可以计算相应
<em>Conditional OR</em></li>
<li>可以建立单变量LR，得到变量系数及相应
<em>Marginal OR</em></li>
</ul>
</li>
<li><p>对分类型变量</p>
<ul>
<li>可以直接计算变量各取值间对应的OR</li>
<li>变量数值化编码建立模型，得到变量对应OR</li>
</ul>
<blockquote>
<ul>
<li>根据变量编码方式不同，变量对应OR的含义不同，其中
符合数值变量变动模式的是WOE线性编码</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>极大似然：极小对数损失（交叉熵损失）</p>
<script type="math/tex; mode=display">\begin{align*}
L(w) & = log \prod_{i=1}^N [\pi(x_i)]^{y_i}
    [1-\pi(x_i)]^{1-y_i} \\
& = \sum_{i=1}^N [y_i log \pi(x_i) + (1-y_i)log(1-\pi(x_i))] \\
& = \sum_{i=1}^N [y_i log \frac {\pi(x_i)}
    {1-\pi(x_i)} log(1-\pi(x_i))] \\
& = \sum_{i=1}^N [y_i(\hat w \hat x_i) -
    log(1+exp(\hat w \hat x_i))]
\end{align*}</script><blockquote>
<ul>
<li>$\pi(x) = P(Y=1|x)$</li>
</ul>
</blockquote>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ul>
<li>通常采用梯度下降、拟牛顿法求解有以上最优化问题</li>
</ul>
<h3 id="Multi-Nominal-Logistic-Regression-Model"><a href="#Multi-Nominal-Logistic-Regression-Model" class="headerlink" title="Multi-Nominal Logistic Regression Model"></a><em>Multi-Nominal Logistic Regression Model</em></h3><p>多项逻辑斯蒂回归：二项逻辑回归模型推广</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y=j|x) & = \frac {exp(\hat w_j \hat x)} {1+\sum_{k=1}^{K-1}
    exp(\hat w_k \hat x)}, k=1,2,\cdots,K-1 \\
P(Y=K|x) & = \frac 1 {1+\sum_{k=1}^{K-1}
    exp(\hat w_k \hat x)}
\end{align*}</script><ul>
<li>策略、算法类似二项逻辑回归模型</li>
</ul>
<h2 id="Generalized-Linear-Model"><a href="#Generalized-Linear-Model" class="headerlink" title="Generalized Linear Model"></a><em>Generalized Linear Model</em></h2><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><h2 id="Maximum-Entropy-Model"><a href="#Maximum-Entropy-Model" class="headerlink" title="Maximum Entropy Model"></a><em>Maximum Entropy Model</em></h2><h3 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h3><p>最大熵原理：学习概率模型时，在所有可能的概率模型（分布）中，
<strong>熵最大的模型是最好的模型</strong></p>
<ul>
<li><p>使用约束条件确定概率模型的集合，则最大熵原理也可以表述为
<strong>在满足约束条件的模型中选取熵最大的模型</strong></p>
</li>
<li><p>直观的，最大熵原理认为</p>
<ul>
<li>概率模型要满足已有事实（约束条件）</li>
<li>没有更多信息的情况下，不确定部分是等可能的</li>
<li>等可能不容易操作，所有考虑使用<strong>可优化</strong>的熵最大化
表示等可能性</li>
</ul>
</li>
</ul>
<h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><p>最大熵模型为生成模型</p>
<ul>
<li><p>对给定数据集$T={(x_1,y_1),\cdots,(x_N,y_N)}$，联合分布
P(X,Y)、边缘分布P(X)的经验分布如下</p>
<script type="math/tex; mode=display">\begin{align*}
\tilde P(X=x, Y=y) & = \frac {v(X=x, Y=y)} N \\
\tilde P(X=x) & = \frac {v(X=x)} N
\end{align*}</script><blockquote>
<ul>
<li>$v(X=x,Y=y)$：训练集中样本$(x,y)$出频数</li>
</ul>
</blockquote>
</li>
<li><p>用如下<em>feature function</em> $f(x, y)$描述输入x、输出y之间
某个事实</p>
<script type="math/tex; mode=display">f(x, y) = \left \{ \begin{array}{l}
1, & x、y满足某一事实 \\
0, & 否则
\end{array} \right.</script><ul>
<li><p>特征函数关于经验分布$\tilde P(X, Y)$的期望</p>
<script type="math/tex; mode=display">
E_{\tilde P} = \sum_{x,y} \tilde P(x,y)f(x,y)</script></li>
<li><p>特征函数关于生成模型$P(Y|X)$、经验分布$\tilde P(X)$
期望</p>
<script type="math/tex; mode=display">
E_P(f(x)) = \sum_{x,y} \tilde P(x)P(y|x)f(x,y)</script></li>
</ul>
</li>
<li><p>期望模型$P(Y|X)$能够获取数据中信息，则两个期望值应该相等</p>
<script type="math/tex; mode=display">\begin{align*}
E_P(f) & = E_{\tilde P}(f) \\
\sum_{x,y} \tilde P(x)P(y|x)f(x,y) & =
   \sum_{x,y} \tilde P(x,y)f(x,y)
\end{align*}</script><p>此即作为模型学习的约束条件</p>
<ul>
<li><p>此约束是纯粹的关于$P(Y|X)$的约束，只是约束形式特殊，
需要通过期望关联熵</p>
</li>
<li><p>若有其他表述形式、可以直接带入的、关于$P(Y|X)$约束，
可以直接使用</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>满足所有约束条件的模型集合为<script type="math/tex; mode=display">
  \mathcal{C} = \{P | E_{P(f_i)} = E_{\tilde P (f_i)},
      i=1,2,\cdots,n \}</script>  定义在条件概率分布$P(Y|X)$上的条件熵为<script type="math/tex; mode=display">
  H(P) = -\sum_{x,y} \tilde P(x) P(y|x) logP(y|x)</script>  则模型集合$\mathcal{C}$中条件熵最大者即为最大是模型</li>
</ul>
</blockquote>
<h3 id="策略-1"><a href="#策略-1" class="headerlink" title="策略"></a>策略</h3><p>最大熵模型的策略为以下约束最优化问题</p>
<script type="math/tex; mode=display">\begin{array}{l}
\max_{P \in \mathcal{C}} & -H(P)=\sum_{x,y} \tilde P(x)
    P(y|x) logP(y|x) \\
s.t. & E_P(f_i) - E_{\tilde P}(f_i) = 0, i=1,2,\cdots,M \\
& \sum_{y} P(y|x)  = 1
\end{array}</script><ul>
<li><p>引入拉格朗日函数</p>
<script type="math/tex; mode=display">\begin{align*}
L(P, w) & = -H(P) - w_0(1-\sum_y P(y|x)) + \sum_{m=1}^M
   w_m(E_{\tilde P}(f_i) - E_P(f_i)) \\
& = \sum_{x,y} \tilde P(x) P(y|x) logP(y|x) + w_0
   (1-\sum_y P(y|x)) + \sum_{m=1}^M w_m (\sum_{x,y}
   \tilde P(x,y)f_i(x, y) - \tilde P(x)P(y|x)f_i(x,y))
\end{align*}</script><ul>
<li><p>原始问题为</p>
<script type="math/tex; mode=display">
\min_{P \in \mathcal{C}} \max_{w} L(P, w)</script></li>
<li><p>对偶问题为</p>
<script type="math/tex; mode=display">
\max_{w} \min_{P \in \mathcal{C}} L(P, w)</script></li>
<li><p>考虑拉格朗日函数$L(P, w)$是P的凸函数，则原始问题、
对偶问题解相同</p>
</li>
</ul>
</li>
<li><p>记</p>
<script type="math/tex; mode=display">\begin{align*}
\Psi(w) & = \min_{P \in \mathcal{C}} L(P, w)
   = L(P_w, w) \\
P_w & = \arg\min_{P \in \mathcal{C}} L(P, w) = P_w(Y|X)
\end{align*}</script></li>
<li><p>求$L(P, w)$对$P(Y|X)$偏导</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial L(P, w)} {\partial P(Y|X)} & =
   \sum_{x,y} \tilde P(x)(logP(y|x)+1) - \sum_y w_0 -
   \sum_{x,y}(\tilde P(x) \sum_{i=1}^N w_i f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x)(log P(y|x) + 1 - w_0 -
   \sum_{i=1}^N w_i f_i(x, y))
\end{align*}</script><p>偏导置0，考虑到$\tilde P(x) &gt; 0$，其系数必始终为0，有</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y|X) & = \exp(\sum_{i=1}^N w_i f_i(x,y) + w_0 - 1) \\
& = \frac {exp(\sum_{i=1}^N w_i f_i(x,y))} {exp(1-w_0)}
\end{align*}</script></li>
<li><p>考虑到约束$\sum_y P(y|x) = 1$，有</p>
<script type="math/tex; mode=display">\begin{align*}
P_w(y|x) & = \frac 1 {Z_w(x)} exp(\sum_{i=1}^N w_i
   f_i(x,y)) \\
Z_w(x) & = \sum_y exp(\sum_{i=1}^N w_i f_i(x,y)) \\
& = exp(1 - w_0)
\end{align*}</script><blockquote>
<ul>
<li>$Z_w(x)$：规范化因子</li>
<li>$f(x, y)$：特征</li>
<li>$w_i$：特征权值</li>
</ul>
</blockquote>
</li>
<li><p>原最优化问题等价于求解偶问题极大化问题$\max_w \Psi(w)$</p>
<script type="math/tex; mode=display">\begin{align*}
\Psi(w) & = \sum_{x,y} \tilde P(x) P_w(y|x) logP_w(y|x)
   + \sum_{i=1}^N w_i(\sum_{x,y} \tilde P(x,y) f_i(x,y)
   - \sum_{x,y} \tilde P(x) P_w(y|x) f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) +
   \sum_{x,y} \tilde P(x,y) P_w(y|x)(log P_w(y|x) -
   \sum_{i=1}^N w_i f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_{x,y} \tilde P(x,y) P_w(y|x) log Z_w(x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_x \tilde P(x) log Z_w(x)
\end{align*}</script><p>记其解为</p>
<script type="math/tex; mode=display">w^{*} = \arg\max_w \Psi(w)</script><p>带入即可得到最优（最大熵）模型$P_{w^{*}}(Y|X)$</p>
</li>
</ul>
<h4 id="策略性质"><a href="#策略性质" class="headerlink" title="策略性质"></a>策略性质</h4><ul>
<li><p>已知训练数据的经验概率分布为$\tilde P(X,Y)$，则条件概率
分布$P(Y|X)$的对数似然函数为</p>
<script type="math/tex; mode=display">\begin{align*}
L_{\tilde P}(P_w) & = N log \prod_{x,y}
   P(y|x)^{\tilde P(x,y)} \\
& = \sum_{x,y} N * \tilde P(x,y) log P(y|x)
\end{align*}</script><blockquote>
<ul>
<li>这里省略了系数样本数量$N$</li>
</ul>
</blockquote>
</li>
<li><p>将最大熵模型带入，可得</p>
<script type="math/tex; mode=display">\begin{align*}
L_{\tilde P_w} & = \sum_{x,y} \tilde P(y|x) logP(y|x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_{x,y} \tilde P(x,y)log Z_w(x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_x \tilde P(x) log Z_w(x) \\
& = \Psi(w)
\end{align*}</script><p>对偶函数$\Psi(w)$等价于对数似然函数$L_{\tilde P}(P_w)$，
即最大熵模型中，<strong>对偶函数极大等价于模型极大似然估计</strong></p>
</li>
</ul>
<h3 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h3><ul>
<li><p>思想</p>
<ul>
<li>假设最大熵模型当前参数向量$w=(w_1,w_2,\cdots,w_M)^T$</li>
<li>希望能找到新的参数向量（参数向量更新）
$w+\sigma=(w_1+\sigma_1,\cdots,w_M+\sigma_M)$
使得模型对数似然函数/对偶函数值增加</li>
<li>不断对似然函数值进行更新，直到找到对数似然函数极大值</li>
</ul>
</li>
<li><p>对给定经验分布$\tilde P(x,y)$，参数向量更新至$w+\sigma$
时，对数似然函数值变化为</p>
<script type="math/tex; mode=display">\begin{align*}
L(w+\sigma) - L(w) & = \sum_{x,y} \tilde P(x,y)
   log P_{w+\sigma}(y|x) - \sum_{x,y} \tilde P(x,y)
   log P_w(y|x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) - \sum_x \tilde P(x) log \frac
   {Z_{w+\sigma}(x)} {Z_w(x)} \\
& \geq \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) + 1 - \sum_x \tilde P(x) \frac
   {Z_{w+\sigma}(x)} {Z_w(x)} \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) + 1 - \sum_x \tilde P(x) \sum_y P_y(y|x)
   exp(\sum_{i=1}^M \sigma_i f_i(x,y))
\end{align*}</script><ul>
<li><p>不等式步利用$a - 1 \geq log a, a \geq 1$</p>
</li>
<li><p>最后一步利用</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {Z_{w+\sigma}(x)} {Z_w(x)} & = \frac 1 {Z_w(x)}
  \sum_y exp(\sum_{i=1}^M (w_i + \sigma_i)
  f_i(x, y)) \\
& = \frac 1 {Z_w(x)} \sum_y exp(\sum_{i=1}^M w_i
  f_i(x,y) + \sigma_i f_i(x,y)) \\
& = \sum_y P_w(y|x) exp(\sum_{i=1}^n \sigma_i
  f_i(x,y))
\end{align*}</script></li>
</ul>
</li>
<li><p>记上式右端为$A(\sigma|w)$，则其为对数似然函数改变量的
一个下界</p>
<script type="math/tex; mode=display">
L(w+\sigma) - L(w) \geq A(\sigma|w)</script><ul>
<li>若适当的$\sigma$能增加其值，则对数似然函数值也应该
增加</li>
<li>函数$A(\sigma|w)$中因变量$\sigma$为向量，难以同时
优化，尝试每次只优化一个变量$\sigma_i$，固定其他变量
$\sigma_j$</li>
</ul>
</li>
<li><p>记</p>
<script type="math/tex; mode=display">f^{**} (x,y) = \sum_i f_i(x,y)</script><p>考虑到$f_i(x,y)$为二值函数，则$f^{**}(x,y)$表示所有特征
在$(x,y)$出现的次数，且有</p>
<script type="math/tex; mode=display">
A(\sigma|w) = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M
   \sigma_i f_i(x,y) + 1 - \sum_x \tilde P(x)
   \sum_y P_w(y|x) exp(f^{**}(x,y) \sum_{i=1}^M
   \frac {\sigma_i f_i(x,y)} {f^{**}(x,y)})</script></li>
<li><p>考虑到$\sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)} = 1$，
由指数函数凸性、Jensen不等式有</p>
<script type="math/tex; mode=display">
exp(\sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)} \sigma_i
   f^{**}(x,y)) \leq \sum_{i=1}^M \frac {f_i(x,y)}
   {f^{**}(x,y)} exp(\sigma_i f^{**}(x,y))</script><p>则</p>
<script type="math/tex; mode=display">
A(\sigma|w) \geq \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M
   \sigma_i f_i(x,y) + 1 - \sum_x \tilde P(x) \sum_y
   P_w(y|x) \sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)}
   exp(\sigma_i f^{**}(x,y))</script></li>
<li><p>记上述不等式右端为$B(\sigma|w)$，则有</p>
<script type="math/tex; mode=display">
L(w+\sigma) - L(w) \geq B(\sigma|w)</script><p>其为对数似然函数改变量的一个新、相对不紧的下界</p>
</li>
<li><p>求$B(\sigma|w)$对$\sigma_i$的偏导</p>
<script type="math/tex; mode=display">
\frac {\partial B(\sigma|w)} {\partial \sigma_i} =
   \sum_{x,y} \tilde P(x,y) f_i(x,y) -
   \sum_x \tilde P(x) \sum_y P_w(y|x) f_i(x,y)
   exp(\sigma_i f^{**}(x,y))</script><p>置偏导为0，可得</p>
<script type="math/tex; mode=display">
\sum_x \tilde P(x) \sum_y P_w(y|x) f_i(x,y) exp(\sigma_i
   f^{**}(x,y)) = \sum_{x,y} \tilde P(x,y) f_i(x,y) =
   E_{\tilde P}(f_i)</script><p>其中仅含变量$\sigma_i$，则依次求解以上方程即可得到
$\sigma$</p>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><blockquote>
<ul>
<li>输入：特征函数$f_1, f_2, \cdots, f_M$、经验分布
  $\tilde P(x)$、最大熵模型$P_w(x)$</li>
<li>输出：最优参数值$w<em>i^{*}$、最优模型$P</em>{w^{*}}$</li>
</ul>
</blockquote>
<ol>
<li><p>对所有$i \in {1,2,\cdots,M}$，取初值$w_i = 0$</p>
</li>
<li><p>对每个$i \in {1,2,\cdots,M}$，求解以上方程得$\sigma_i$</p>
<ul>
<li><p>若$f^{**}(x,y)=C$为常数，则$\sigma_i$有解析解</p>
<script type="math/tex; mode=display">
\sigma_i = \frac 1 C log \frac {E_{\tilde P}(f_i)}
 {E_P(f_i)}</script></li>
<li><p>若$f^{**}(x,y)$不是常数，则可以通过牛顿法迭代求解</p>
<script type="math/tex; mode=display">
\sigma_i^{(k+1)} = \sigma_i^{(k)} - \frac
 {g(\sigma_i^{(k)})} {g^{'}(\sigma_i^{(k)})}</script><blockquote>
<ul>
<li>$g(\sigma_i)$：上述方程对应函数</li>
</ul>
</blockquote>
<ul>
<li>上述方程有单根，选择适当初值则牛顿法恒收敛</li>
</ul>
</li>
</ul>
</li>
<li><p>更新$w_i$，$w_i \leftarrow w_i + \sigma_i$，若不是所有
$w_i$均收敛，重复2</p>
</li>
</ol>
<h3 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h3><p>对最大熵模型</p>
<ul>
<li><p>为方便，目标函数改为求极小</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min_{w \in R^M} f(w) = \sum_x \tilde P(x) log \sum_{y}
   exp(\sum_{i=1}^M w_i f_i(x,y)) - \sum_{x,y}
   \tilde P(x,y) \sum_{i=1}^M w_i f_i(x,y)
\end{array}</script></li>
<li><p>梯度为</p>
<script type="math/tex; mode=display">\begin{align*}
g(w) & = (\frac {\partial f(w)} {\partial w_i}, \cdots,
   \frac {\partial f(w)} {\partial w_M})^T \\
\frac {\partial f(w)} {\partial w_M} & = \sum_{x,y}
   \tilde P(x) P_w(y|x) f_i(x,y) - E_{\tilde P}(f_i)
\end{align*}</script></li>
</ul>
<h4 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h4><p>将目标函数带入BFGS算法即可</p>
<blockquote>
<ul>
<li>输入：特征函数$f_1, f_2, \cdots, f_M$、经验分布
  $\tilde P(x)$、最大熵模型$P_w(x)$</li>
<li>输出：最优参数值$w<em>i^{*}$、最优模型$P</em>{w^{*}}$</li>
</ul>
</blockquote>
<ol>
<li><p>取初值$w^{(0)}$、正定对称矩阵$B^{(0)}$，置k=0</p>
</li>
<li><p>计算$g^{(k)} = g(w^{(k)})$，若$|g^{(k)}| &lt; \epsilon$，
停止计算，得到解$w^{*} = w^{(k)}$</p>
</li>
<li><p>由拟牛顿公式$B^{(k)}p^{(k)} = -g^{(k)}$求解$p^{(k)}$</p>
</li>
<li><p>一维搜索，求解</p>
<script type="math/tex; mode=display">
\lambda^{(k)} = \arg\min_{\lambda} f(w^{(k)} +
  \lambda p_k)</script></li>
<li><p>置$w^{(k+1)} = w^{(k)} + \lambda^{(k)} p_k$</p>
</li>
<li><p>计算$g^{(k+1)} = g(w^{(k+1)})$，若
$|g^{(k+1)}| &lt; \epsilon$，停止计算，得到解
$w^{*} = w^{(k+1)}$，否则求</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} - \frac {B^{(k)} s^{(k)}
  (s^{(k)})^T B^{(k)}} {(s^{(k)})^T B^{(k)} s^{(k)}}
  + \frac {y^{(k)} (y^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script><blockquote>
<ul>
<li>$s^{(k)} = w^{(k+1)} - w^{(k)}$</li>
<li>$y^{(k)} = g^{(k+1)} - g^{(k)}$</li>
</ul>
</blockquote>
</li>
<li><p>置k=k+1，转3</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:50:26.000Z" title="7/16/2021, 2:50:26 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">8 minutes read (About 1192 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/factorization_machine.html">Factorization Machine</a></h1><div class="content"><h2 id="因子分解机"><a href="#因子分解机" class="headerlink" title="因子分解机"></a>因子分解机</h2><p>因子分解机：将变量交互影响因子化
（每个变量用隐向量代表、衡量其交叉影响）</p>
<script type="math/tex; mode=display">
\hat y(x) := w_0 + \sum_{i=1}^m w_i x_i + \sum_{i=1}^m
    \sum_{j=i+1}^m <v_i, v_j> x_i x_j</script><blockquote>
<ul>
<li>$w_0$：全局偏置</li>
<li>$w_i$：变量$i$权重</li>
<li>$w_{i,j} := <v_i, v_j>$：变量$i$、$j$之间交互项权重</li>
<li>$v_i$：$k$维向量，变量交叉影响因子</li>
</ul>
</blockquote>
<ul>
<li><p>FM通过<strong>因子化交互影响解耦交互项参数</strong></p>
<ul>
<li><p>即使没有足够数据也能较好估计高维稀疏特征交互影响参数</p>
<ul>
<li>无需大量有交互影响（交互特征取值同时非0）样本</li>
<li>包含某交互影响数据也能帮助估计相关的交互影响</li>
<li><strong>可以学习数据不存在的模式</strong></li>
</ul>
</li>
<li><p>可以视为embedding，特征之间关联性用embedding向量
（隐向量）內积表示</p>
</li>
</ul>
</li>
<li><p>参数数量、模型复杂度均为线性</p>
<ul>
<li>可以方便使用SGD等算法对各种损失函数进行优化</li>
<li>无需像SVM需要支持向量，可以扩展到大量数据集</li>
</ul>
</li>
<li><p>适合任何实值特征向量，对某些输入特征向量即类似
<em>biased MF</em>、<em>SVD++</em>、<em>PITF</em>、<em>FPMC</em></p>
</li>
</ul>
<blockquote>
<ul>
<li>另外还有d-way因子分解机，交互作用以PARAFAC模型因子化<script type="math/tex; mode=display">
  \hat y(x) := w_0 + \sum_{i=1}^n w_i x_i + \sum_{l=2}^d \sum_{i_1=1}
      \cdots \sum_{i_l=i_{l-1}+1}(\prod_{j=1}^l x_{i_j})
      (\sum_{f=1} \prod_{j=1}^l v_{i_j,f}^{(l)}) \\</script><blockquote>
<ul>
<li>$V^{(l)} \in R^{n * k_l}, k_l \in N_0^{+}$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h3 id="模型表达能力"><a href="#模型表达能力" class="headerlink" title="模型表达能力"></a>模型表达能力</h3><ul>
<li><p>考虑任何正定矩阵$W$总可以被分解为$W=V V^T$，则$k$足够大
时，FM总可以表达（还原）交叉项权重矩阵$W$</p>
<ul>
<li>FM是MF降维的推广，在用户-物品评分矩阵基础上集成其他
特征</li>
<li>特征组合发生所有变量之间</li>
</ul>
</li>
<li><p>实际应该选取较小的$k$</p>
<ul>
<li>对较大$k$，稀疏特征没有足够数据估计复杂交叉项权重
矩阵$W$</li>
<li>限制FM的表达能力，模型有更好的泛化能力、交互权重矩阵</li>
</ul>
</li>
</ul>
<h3 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h3><script type="math/tex; mode=display">\begin{align*}
\sum_{i=1}^m \sum_{j=i+1}^m <v_i, v_j> x_i x_j & = 
    \frac 1 2 \sum_{i=1}^m \sum_{j=i}^m <v_i, v_j> x_i x_j -
    \frac 1 2 \sum_{i=1}^m <v_i, v_i> x_i^2 \\
& = \frac 1 2 (x^T V^T V x - x^T diag(V^T V) x) \\
& = \frac 1 2 (\|Vx\|_2^2 - x^T diag(V^T V) x) \\
& = \frac 1 2 \sum_{f=1}^k ((\sum_{i=1}^m v_{i,f} x_i)^ 2
    - \sum_{i=1}^m v_{i,f}^2 x_i^2) \\
\end{align*}</script><blockquote>
<ul>
<li>$V = (v_1, v_2, \cdots, v_m)$</li>
<li>$x = (x_1, x_2, \cdots, x_m)^T$</li>
</ul>
</blockquote>
<ul>
<li><p>模型计算复杂度为线性$\in O(kn)$</p>
</li>
<li><p>模型可以使用梯度下降类方法高效学习</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial \hat y(x)} {\partial \theta} & = \left \{
   \begin{array}{l}
       1, & \theta := w_0 \\
       x_i, & \theta := w_i \\
       x_i Vx - v_i x_i^2& \theta := v_i
   \end{array} \right. \\
& = \left \{ \begin{array}{l}
       1, & \theta := w_0 \\
       x_i, & \theta := w_i \\
       x_i \sum_{j=1}^m v_{j,f} x_j - v_{i,f} x_i^2,
           & \theta := v_{i,f}
   \end{array} \right.
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li>考虑到稀疏特征，內积只需计算非零值</li>
</ul>
</blockquote>
<h3 id="模型适用"><a href="#模型适用" class="headerlink" title="模型适用"></a>模型适用</h3><ul>
<li>回归：直接用$\hat y(x)$作为回归预测值</li>
<li>二分类：结合logit损失、hinge损失优化</li>
<li>ranking：$\hat y(x)$作为得分排序，使用成对分类损失优化</li>
</ul>
<h2 id="Field-aware-Factorization-Machines"><a href="#Field-aware-Factorization-Machines" class="headerlink" title="Field-aware Factorization Machines"></a>Field-aware Factorization Machines</h2><p>域感知因子分解机：在FM基础上考虑对特征分类，特征对其他类别
特征训练分别训练隐向量</p>
<script type="math/tex; mode=display">\begin{align*}
\hat y(x) & = w_0 + \sum_{i=0}^m w_i x_i + \sum_{a=1}^m
    \sum_{b=a+1}^m <V_{a, f_b}, V_{b, f_a}> x_a x_b \\
& = w_0 + \sum_{i=1}^M \sum_{j=1}^{M_i} w_{i,j} x_{i,j} +
    \sum_{i=1}^M \sum_{j=1}^{M_i} \sum_{a=i}^M \sum_{b=1}^{M_i}
    <V_{i,j,a}, V_{a,b,i}> x_{i,j} x_{a,b}
\end{align*}</script><blockquote>
<ul>
<li>$m$：特征数量</li>
<li>$M, M_i$：特征域数量、各特征域中特征数量</li>
<li>$V_{i,j,a}$：特征域$i$中$j$特征对特征与$a$的隐向量</li>
<li>$V_{a, f_b}$：特征$x_a$对特征$b$所属域$f_b$的隐向量</li>
</ul>
</blockquote>
<ul>
<li><p>FFM中特征都属于特定域，相同特征域中特征性质应该相同，
一般的</p>
<ul>
<li>连续特征自己单独成域</li>
<li>离散0/1特征按照性质划分，归于不同特征域</li>
</ul>
</li>
<li><p>特征对其他域分别有隐向量表示<strong>和其他域的隐含关系</strong></p>
<ul>
<li>考虑交互作用时，对不同域使用不同隐向量计算交互作用</li>
<li>FFM中隐变量维度也远远小于FM中隐向量维度</li>
</ul>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/imgs/ffm_steps.png" alt="ffm_steps"></p>
<h3 id="模型特点"><a href="#模型特点" class="headerlink" title="模型特点"></a>模型特点</h3><ul>
<li>模型总体类似FM，仅通过多样化隐向量实现细化因子分解</li>
<li>模型总体较FM复杂度大、参数数量多<ul>
<li>无法抽取公因子化简为线性</li>
<li>数据量较小时可能无法有效训练隐向量</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-13T15:24:10.000Z" title="7/13/2019, 11:24:10 PM">2019-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:56:09.000Z" title="7/16/2021, 2:56:09 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">12 minutes read (About 1859 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/naive_bayes.html">Naive Bayes</a></h1><div class="content"><h2 id="Naive-Bayes-Classifier"><a href="#Naive-Bayes-Classifier" class="headerlink" title="Naive Bayes Classifier"></a><em>Naive Bayes Classifier</em></h2><p>朴素贝叶斯：在训练数据集上学习联合概率分布$P(X,Y)$，利用后验
分布作为结果</p>
<ul>
<li>朴素：条件概率分布有<strong>条件独立性假设</strong>，即特征在类别确定
下条件独立</li>
</ul>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ul>
<li><p>输出<strong>Y的先验概率分布</strong>为</p>
<script type="math/tex; mode=display">
P(Y = c_k), k = 1,2,\cdots,K</script><blockquote>
<ul>
<li>先验概率是指输出变量，即待预测变量的先验概率分布，
 反映其在无条件下的各取值可能性</li>
<li>同理所有的条件概率中也是以输出变量取值作为条件</li>
</ul>
</blockquote>
</li>
<li><p>条件概率分布为</p>
<script type="math/tex; mode=display">
P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},\cdots,X^{(D)}=x^{(D)}|
   Y=c_k)</script><blockquote>
<ul>
<li>$D$：用于分类特征数量</li>
</ul>
</blockquote>
<p>其中有指数数量级的参数（每个参数的每个取值都需要参数）</p>
</li>
<li><p>因此对条件概率分布做<strong>条件独立性假设</strong>，即分类特征在类别
确定条件下是独立的</p>
<script type="math/tex; mode=display">\begin{align*}
P(X=x|Y=c_k) & = P(X^{(1)}=x^{(1)},\cdots,X^{(D)}=x^{(D)}
   |Y=c_k) \\
& = \prod_{j=1}^D P(X^{(j)}=x^{(j)}|Y=c_k)
\end{align*}</script><ul>
<li>条件独立性假设是比较强的假设，也是<strong>朴素</strong>的由来</li>
<li><p>其使得朴素贝叶斯方法变得简单，但有时也会牺牲准确率</p>
</li>
<li><p>以上即可得到联合概率分布$P(X,Y)$</p>
</li>
<li><p>朴素贝叶斯学习到的联合概率分布$P(X,Y)$是数据生成的
机制，即其为生成模型</p>
</li>
</ul>
</li>
</ul>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>策略：选择使得后验概率最大化的类$c_k$作为最终分类结果</p>
<script type="math/tex; mode=display">
P(Y=c_k|X=x) = \frac {P(Y=c_k, X=x)} {\sum_{i=1}^K
    P(Y=c_k, X=x)}</script><blockquote>
<ul>
<li>$K$：输出类别数量</li>
</ul>
</blockquote>
<ul>
<li><p>后验概率根计算根据贝叶斯定理计算</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y=c_k|X=x) & = \frac {P(X=x|Y=c_k)P(Y=c_k)}
   {\sum_{k=1}^K P(X=x|Y=c_k) P(Y=c_k)} \\
& = \frac {P(Y=c_k) \prod_{j=1}^D P(X^{(j)}|Y=c_k)}
   {\sum_{k=1}^K P(Y=c_k) \prod_{j=1}^D P(X^{(j)}|Y=c_k)}
\end{align*}</script></li>
<li><p>考虑上式中分母对所有$c_k$取值均相等，则最终分类器为</p>
<script type="math/tex; mode=display">
y = \arg\max_{c_k} P(Y=c_k) \prod_{j=1}^D
   P(X^{(j)} = x^{(j)}|Y=c_k)</script><ul>
<li>即分类时，对给定输入$x$，将其归类为后验概率最大的类</li>
</ul>
</li>
</ul>
<h4 id="策略性质"><a href="#策略性质" class="headerlink" title="策略性质"></a>策略性质</h4><p>后验概率最大化等价于0-1损失的经验风险最小化</p>
<ul>
<li><p>经验风险为</p>
<script type="math/tex; mode=display">
\begin{align*}
R_{emp}(f) & = E[L(Y, f(X))] \\
& = E_x \sum_{k=1}^K L(y, c_k) P(c_k | X)
\end{align*}</script></li>
<li><p>为使经验风险最小化，对训练集中每个$X=x$取极小化，对每个
个体$(x,y)$有</p>
<script type="math/tex; mode=display">\begin{align*}
f(x) & = \arg\min_{c_k} \sum_{k=1}^K L(y, c_k)
   P(c_k|X=x) \\
& = \arg\min_{c_k} \sum_{k=1}^K P(y \neq c_k|X=x) \\
& = \arg\min_{c_k} (1-P(y=c_k|X=x)) \\
& = \arg\max_{c_k} P(y=c_k|X=x)
\end{align*}</script><p>即后验概率最大化</p>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><ul>
<li><p>先验概率的极大似然估计为</p>
<script type="math/tex; mode=display">
P(Y=c_k) = \frac {\sum_{i=1}^N I(y_i = c_k)} N,
   k=1,2,\cdots,K</script></li>
<li><p>条件概率的极大似然估计为</p>
<script type="math/tex; mode=display">
P(X^{(j)}=a_{j,l}|Y=c_k) = \frac {\sum_{i=1}^N
   I(x_i^{(j)}=a_{j,l}, y_i=c_k)}
   {\sum_{i=1}^N I(y_i=c_k)} \\
   j=1,2,\cdots,N;l=1,2,\cdots,S_j;k=1,2,\cdots,K</script><blockquote>
<ul>
<li>$a_{j,l}$；第j个特征的第l个可能取值</li>
<li>$S_j$：第j个特征的可能取值数量</li>
<li>$I$：特征函数，满足条件取1、否则取0</li>
</ul>
</blockquote>
</li>
</ul>
<h5 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h5><blockquote>
<ul>
<li>输入：训练数据T</li>
<li>输出：朴素贝叶斯分类器</li>
</ul>
</blockquote>
<ol>
<li><p>依据以上公式计算先验概率、条件概率</p>
</li>
<li><p>将先验概率、条件概率带入，得到朴素贝叶斯分类器</p>
<script type="math/tex; mode=display">
y = \arg\max_{c_k} P(Y=c_k) \prod_{j=1}^D
  P(X^{(j)} = x^{(j)}|Y=c_k)</script></li>
</ol>
<h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><ul>
<li><p>条件概率贝叶斯估计</p>
<script type="math/tex; mode=display">
P(X^{(j)}=a_{j,l}|Y=c_k) = \frac {\sum_{i=1}^N
   I(x_i^{(j)}=a_{j,l}, y_i=c_k) + \lambda}
   {\sum_{i=1}^N I(y_i=c_k) + S_j \lambda} \\
   j=1,2,\cdots,N;l=1,2,\cdots,S_j;k=1,2,\cdots,K</script><blockquote>
<ul>
<li>$\lambda \geq 0$</li>
</ul>
</blockquote>
<ul>
<li>$\lambda=0$时就是极大似然估计</li>
<li>常取$\lambda=1$，此时称为<em>Laplace Smoothing</em></li>
<li>以上设计满足概率分布性质<script type="math/tex; mode=display">\begin{align*}
P_{\lambda}(X^{(j)}=a_{j,l}|Y=c_k) \geq 0 \\
\sum_{l=1}^{S_j} P_{\lambda}(X^{(j)}=a_{j,l}|Y=c_k)
  = 1
\end{align*}</script></li>
</ul>
</li>
<li><p>先验概率贝叶斯估计</p>
<script type="math/tex; mode=display">
P_{\lambda}(Y=c_k) = \frac {\sum_{i=1}^N I(y_i = c_i)
   + \lambda} {N + K\lambda}</script></li>
</ul>
<blockquote>
<ul>
<li>极大似然估计可能出现所需估计概率值为0，影响后验概率计算
  结果，贝叶斯估计能够避免这点</li>
</ul>
</blockquote>
<h2 id="Semi-Naive-Bayes-Classifier"><a href="#Semi-Naive-Bayes-Classifier" class="headerlink" title="Semi-Naive Bayes Classifier"></a><em>Semi-Naive Bayes Classifier</em></h2><p>半朴素贝叶斯分类器：适当考虑部分特征之间的相互依赖信息</p>
<ul>
<li><p><em>Semi-Naive Bayes</em>可以视为是<strong>利用规则对变量加权</strong>，以
此来体现相关变量的协同影响</p>
<script type="math/tex; mode=display">
y = \arg\max_{c_k} P(Y=c_k) \prod_{j=1}^D
   \beta_j P(X^{(j)} = x^{(j)}|Y=c_k)</script><ul>
<li>特别的：权值为0/1即为变量筛选</li>
</ul>
</li>
</ul>
<h3 id="One-Depentdent-Estimator"><a href="#One-Depentdent-Estimator" class="headerlink" title="One-Depentdent Estimator"></a><em>One-Depentdent Estimator</em></h3><p>独依赖估计：假设特征在类别之外最多依赖一个其他特征，这是半
朴素贝叶斯分类器中最常用的一种策略</p>
<script type="math/tex; mode=display">
P(X=x|Y=c_k) = \prod_{j=1}^D P(X^{(j)}=x^{(j)} | Y=c_k, pa_j)</script><blockquote>
<ul>
<li>$pa_j$：特征$X^{(j)}$依赖的父特征</li>
</ul>
</blockquote>
<ul>
<li><p>若父特征已知，同样可以使用条件概率计算
$P(X^{(j)}=x^{(j)} | Y=c_k, pa_j)$</p>
<script type="math/tex; mode=display">
P(X^{(j)}=x^{(j)} | Y=c_k, pa_j) = \frac 
{P(X^{(j)}=x^{(j)}, Y=c_k, pa_j)} {P(Y=c_k, pa_j)}</script></li>
<li><p>ODE形式半朴素贝叶斯分类器相应的策略为</p>
<script type="math/tex; mode=display">
y = \arg\max_{c_k} P(Y=c_k) \prod_{j=1}^D
   P(X^{(j)} = x^{(j)}|Y=c_k, pa_j)</script></li>
<li><p>根据确定各特征父特征的不同做法，可以分为不同类型的独依赖
分类器</p>
<ul>
<li><em>Super-Parent ODE</em>：假设所有特征都依赖同一父特征</li>
<li><em>Averaged ODE</em>：类似随机森林方法，尝试将每个属性作为
超父特征构建<em>SPODE</em></li>
<li><em>Tree Augmented Naive Bayes</em>：基于最大带权生成树发展</li>
</ul>
</li>
</ul>
<h4 id="SPODE"><a href="#SPODE" class="headerlink" title="SPODE"></a>SPODE</h4><p>SPODE：每个特征只与其他唯一一个特征有依赖关系</p>
<script type="math/tex; mode=display">
y = \arg\max_{c_k} P(Y=c_k, pa) \prod_{j=1}^D
    P(X^{(j)} = x^{(j)}|Y=c_k, pa)</script><blockquote>
<ul>
<li>$pa$：所有特征共有的依赖父特征</li>
</ul>
</blockquote>
<h4 id="AODE"><a href="#AODE" class="headerlink" title="AODE"></a>AODE</h4><p>AODE：以所有特征依次作为超父特征构建SPODE，以具有足够训练
数据支撑的SPODE集群起来作为最终结果</p>
<script type="math/tex; mode=display">
y = \arg\max_{c_k} (\sum_{i=1}^D P(Y=c_k, X^{(i)})
    \prod_{j=1}^D P(X^{(j)} = x^{(j)}|Y=c_k, X^{(i)}))</script><ul>
<li>这里只选取训练数据足够，即取特征$X^{(i)}$某个取值的样本
数量大于某阈值的SPODE加入结果</li>
</ul>
<h4 id="TAN"><a href="#TAN" class="headerlink" title="TAN"></a>TAN</h4><h5 id="TAN步骤"><a href="#TAN步骤" class="headerlink" title="TAN步骤"></a>TAN步骤</h5><ul>
<li><p>计算任意特征之间的互信息</p>
<script type="math/tex; mode=display">
g(X^{(i)}, X^{(j)}| Y) = \sum P(X^{(i)}, X^{(j)} | Y=c_k)
   log \frac {P(X^{(i)}, X^{(j)} | Y=c_k)}
   {P(X^{(i)} | Y=c_k) P(X^{(j)} | Y=c_k)}</script></li>
<li><p>以特征为节点构建完全图，节点边权重设为相应互信息</p>
</li>
<li><p>构建此完全图的最大带权生成树</p>
<ul>
<li>挑选根变量</li>
<li>将边设置为有向</li>
</ul>
</li>
<li><p>加入预测节点$Y$，增加从$Y$到每个属性的有向边</p>
</li>
</ul>
<h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><ul>
<li><p>条件互信息$g(X^{(i)}, X^{(j)}| Y)$刻画了特征在已知类别
情况下的相关性</p>
</li>
<li><p>通过最大生成树算法，TAN仅保留了强相关属性之间的依赖性</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-13T15:24:07.000Z" title="7/13/2019, 11:24:07 PM">2019-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:50:43.000Z" title="7/16/2021, 2:50:43 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">7 minutes read (About 1080 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/linear_regression.html">回归变量选择</a></h1><div class="content"><h2 id="子集回归"><a href="#子集回归" class="headerlink" title="子集回归"></a>子集回归</h2><blockquote>
<ul>
<li>特征子集选择独立于回归模型拟合，属于封装器特征选择</li>
</ul>
</blockquote>
<h3 id="最优子集"><a href="#最优子集" class="headerlink" title="最优子集"></a>最优子集</h3><ul>
<li>特点<ul>
<li>可以得到稀疏的模型</li>
<li>但搜索空间离散，可变性大，稳定性差</li>
</ul>
</li>
</ul>
<h3 id="Forward-Feature-Elimination"><a href="#Forward-Feature-Elimination" class="headerlink" title="Forward Feature Elimination"></a><em>Forward Feature Elimination</em></h3><p>前向变量选择</p>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ul>
<li>初始变量集合$S_0 = \varnothing$</li>
<li>选择具有某种最优特性的变量进入变量集合，得到$S_1$</li>
<li>第j步时，从剩余变量中选择最优变量进入集合，得到$S_{j+1}$</li>
<li>若满足终止条件，则结束，否则重复上步添加变量<ul>
<li>j达到上限</li>
<li>添加剩余变量均无法满足要求</li>
</ul>
</li>
</ul>
<h3 id="Backward-Feature-Elimination"><a href="#Backward-Feature-Elimination" class="headerlink" title="Backward Feature Elimination"></a><em>Backward Feature Elimination</em></h3><p>后向变量选择</p>
<h4 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h4><ul>
<li>初始变量集合$S_0$包含全部变量</li>
<li>从变量集合中剔除具有某种最差特性变量，得到$S_1$</li>
<li>第j步时，从剩余变量中剔除最差变量，得到$S_{j+1}$</li>
<li>若满足终止条件，则结束，否则重复上步添加变量<ul>
<li>j达到上限</li>
<li>剔除剩余变量均无法满足要求</li>
</ul>
</li>
</ul>
<h2 id="范数正则化约束"><a href="#范数正则化约束" class="headerlink" title="范数正则化约束"></a>范数正则化约束</h2><blockquote>
<ul>
<li>回归过程中自动选择特征，属于集成特征选择</li>
</ul>
</blockquote>
<h3 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a><em>Ridge Regression</em></h3><script type="math/tex; mode=display">
\min_{\beta \in R^n} \left\{ ||y - X\beta||_2^2 +
    \lambda ||\beta||_2^2 \right\}</script><ul>
<li>在L2范数约束下最小化残差平方</li>
<li>作为连续收缩方法<ul>
<li>通过<em>bias-variance trade-off</em>，岭回归较普通最小二乘
预测表现更好</li>
<li>倾向于保留所有特征，无法产生疏系数模型</li>
</ul>
</li>
</ul>
<h3 id="LASSO"><a href="#LASSO" class="headerlink" title="LASSO"></a>LASSO</h3><script type="math/tex; mode=display">
\min_{\beta \in R^n} \left\{ ||y - X\beta||_2^2 +
    \lambda||\beta||_1 \right\}</script><p>能够选择部分特征，产生疏系数模型</p>
<ul>
<li>p &gt; n时，即使所有特征都有用，LASSO也只能从中挑选n个</li>
<li>如果存在相关性非常高的特征，LASSO倾向于只从该组中选择
一个特征，而且是随便挑选的<ul>
<li>极端条件下，两个完全相同的特征函数，严格凸的罚函数
（如Ridge）可以保证最优解在两个特征的系数相等，而
LASSO的最优解甚至不唯一</li>
</ul>
</li>
</ul>
<h3 id="Elastic-Net"><a href="#Elastic-Net" class="headerlink" title="Elastic Net"></a>Elastic Net</h3><h4 id="Naive-Elastic-Net"><a href="#Naive-Elastic-Net" class="headerlink" title="Naive Elastic Net"></a>Naive Elastic Net</h4><script type="math/tex; mode=display">
\begin{align*}
& \min_{\beta \in R^n} \left\{ ||y - X\beta||_2^2 +
    \lambda_1||\beta||_1 + \lambda_2||\beta||_2^2 \right\} \\

\Rightarrow &
\min_{\beta^* \in R^p} \left\{ ||y - X^*\beta^*||_2^2 +
    \lambda^*||\beta^*||_1 \right\} \\

where: & y^* = \begin{pmatrix}
        y \\ \vec 0_p
    \end{pmatrix}    \\
& X^* = \frac 1 {\sqrt {1+\lambda^2}}
    \begin{pmatrix}
        X \\ \sqrt {\lambda_2} I_p
    \end{pmatrix} \\
& \beta^* = \sqrt {1+\lambda_2} \beta \\
& \lambda^* = \frac {\lambda_1} {1+\lambda_2} \\
\end{align*}</script><ul>
<li><p>弹性网在Lasso的基础上添加系数的二阶范数</p>
<ul>
<li>能同时做变量选择和连续收缩</li>
<li>并且可以选择一组变量</li>
</ul>
</li>
<li><p>传统的估计方法通过二阶段估计找到参数</p>
<ul>
<li>首先设置ridge系数$\lambda_2$求出待估参数$\beta$，
然后做lasso的收缩</li>
<li>这种方法有两次收缩，会导致估计偏差过大，估计不准</li>
</ul>
</li>
<li><p>弹性网可以变换为LASSO，因而lasso的求解方法都可以用于
elastic net</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf">elastic_net</a></p>
<h2 id="Least-Angle-Regression"><a href="#Least-Angle-Regression" class="headerlink" title="Least Angle Regression"></a><em>Least Angle Regression</em></h2><ul>
<li>线性回归即找的一组系数能够用自变量的线性组合表示
因变量</li>
</ul>
<h3 id="Forward-Selection-Forward-Stepwise-Regression"><a href="#Forward-Selection-Forward-Stepwise-Regression" class="headerlink" title="Forward Selection/Forward Stepwise Regression"></a>Forward Selection/Forward Stepwise Regression</h3><ul>
<li><p>从所有给定predictors中选择和y相关系数绝对值最大的变量
$x_{j1}$，做线性回归</p>
<ul>
<li>对于标准化后的变量，相关系数即为变量之间的内积</li>
<li>变量之间相关性越大，变量的之间的夹角越小，单个变量
能解释得效果越好</li>
<li>此时残差同解释变量正交</li>
</ul>
</li>
<li><p>将上一步剩余的残差作为reponse，将剩余变量投影到残差上
重复选择步骤</p>
<ul>
<li>k步之后即可选出一组变量，然后用于建立普通线性模型</li>
</ul>
</li>
<li><p>前向选择算法非常贪心，可能会漏掉一些有效的解释变量，只是
因为同之前选出向量相关</p>
</li>
</ul>
<h3 id="Forward-Stagewise"><a href="#Forward-Stagewise" class="headerlink" title="Forward Stagewise"></a>Forward Stagewise</h3><p>前向选择的catious版本</p>
<ul>
<li><p>和前向选择一样选择和y夹角最小的变量，但是每次只更新较小
步长，每次更新完确认和y夹角最小的变量，使用新变量进行
更新</p>
<ul>
<li>同一个变量可能会被多次更新，即系数会逐渐增加</li>
<li>每次更新一小步，避免了前向选择的可能会忽略关键变量</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-13T15:23:48.000Z" title="7/13/2019, 11:23:48 PM">2019-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:58:22.000Z" title="7/16/2021, 2:58:22 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">9 minutes read (About 1347 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/perceptron.html">Perceptron</a></h1><div class="content"><ul>
<li>输入：实例的特征向量</li>
<li>输出：实例类别+1、-1</li>
</ul>
<h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><p>感知机：线性二分类模型（判别模型）</p>
<script type="math/tex; mode=display">
f(x) = sign(wx + b)</script><blockquote>
<ul>
<li>$x \in \chi \subseteq R^n$：输入空间</li>
<li>$y \in \gamma \subseteq R^n$：输出空间</li>
<li>$w \in R^n, b \in R$：<em>weight vector</em>、<em>bias</em></li>
<li>也常有$\hat w = (w^T, b^T)^T, \hat x = (x^T + 1)^T$，
  则有$\hat w \hat x = wx + b$</li>
</ul>
</blockquote>
<ul>
<li><p>感知机模型的假设空间是定义在特征空间的所有
<em>linear classification model/linear classifier</em>，即函数
集合${f|f(x)=wx+b}$</p>
</li>
<li><p>线性方程$wx+b=0$：对应特征空间$R^n$中一个<em>hyperplane</em></p>
<blockquote>
<ul>
<li>$w$：超平面法向量</li>
<li>$b$：超平面截距</li>
</ul>
</blockquote>
<ul>
<li><p>超平面将特征空间划分为两个部分，其中分别被分为正、负
两类</p>
</li>
<li><p>也被称为<em>separating hyperplane</em></p>
</li>
</ul>
</li>
</ul>
<h3 id="Linearly-Separable-Data-Set"><a href="#Linearly-Separable-Data-Set" class="headerlink" title="Linearly Separable Data Set"></a><em>Linearly Separable Data Set</em></h3><blockquote>
<ul>
<li>对数据集$T={(x_1,y_1),\cdots,(x_N,y_N)}$，若存在超平面
  $S: wx + b=0$能够将正、负实例点，完全正确划分到超平面
  两侧，即<script type="math/tex; mode=display">\begin{align*}
  wx_i + b > 0, & \forall y_i > 0 \\
  wx_i + b < 0, & \forall y_i < 0
  \end{align*}</script>  则称数据集T为线性可分数据集</li>
</ul>
</blockquote>
<h2 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h2><p>感知机学习策略：定义适当损失函数，并将经验风险极小化，确定
参数$w, b$</p>
<h3 id="0-1损失"><a href="#0-1损失" class="headerlink" title="0-1损失"></a>0-1损失</h3><p>经验风险：误分率（误分点总数）</p>
<ul>
<li>不是参数$w, b$的连续可导函数，不易优化</li>
</ul>
<h3 id="绝对值损失"><a href="#绝对值损失" class="headerlink" title="绝对值损失"></a>绝对值损失</h3><p>经验风险：误分类点到超平面距离</p>
<ul>
<li><p>对误分类数据$(x_i, y_i)$，有$-y_i(wx_i + b) &gt; 0$</p>
</li>
<li><p>则误分类点$(x_i, y_i)$到超平面S距离</p>
<script type="math/tex; mode=display">\begin{align*}
d_i & = \frac 1 {\|w\|} |wx_i + b| \\
   & =-\frac 1 {\|w\|} y_i(wx_i + b)
\end{align*}</script></li>
<li><p>则感知机损失函数可定义为
$L(w,b) = -\sum_{x_i \in M} y_i(wx_i + b)$</p>
<blockquote>
<ul>
<li>$M$：误分类点集合</li>
<li>损失函数是$w, b$的连续可导函数：使用$y_i$替代绝对值</li>
</ul>
</blockquote>
</li>
<li><p>损失函数$L(w,b)$梯度有</p>
<script type="math/tex; mode=display">\begin{align*}
\bigtriangledown_wL(w, b) & = -\sum_{x_i \in M} y_ix_i \\
\bigtriangledown_bL(w, b) & = -\sum_{x_i \in M} y_i
\end{align*}</script></li>
</ul>
<h2 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h2><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a><em>Stochastic Gradient Descent</em></h3><p>随机梯度下降法</p>
<blockquote>
<ul>
<li>输入：数据集$T$、学习率$\eta, 0 \leq \eta \leq 1$</li>
<li>输出：$w,b$、感知模型$f(x)=sgn(wx+b)$</li>
</ul>
</blockquote>
<ol>
<li><p>选取初值$w_0, b_0$</p>
</li>
<li><p>随机选取一个误分类点$(x_i, y_i)$，即$y_i(wx_i+b) \leq 0$
，对$w, b$进行更新</p>
<script type="math/tex; mode=display">\begin{align*}
w^{(n+1)} & \leftarrow w^{(n)} + \eta y_ix_i \\
b^{(n+1)} & \leftarrow b^{(n)} + \eta y_i
\end{align*}</script><blockquote>
<ul>
<li>$0 &lt; \eta \leq 1$：<em>learning rate</em>，学习率，步长</li>
</ul>
</blockquote>
</li>
<li><p>转2，直至训练集中无误分类点</p>
</li>
</ol>
<blockquote>
<ul>
<li>不同初值、随机取点顺序可能得到不同的解</li>
<li>训练数据线性可分时，算法迭代是收敛的</li>
<li>训练数据不线性可分时，学习算法不收敛，迭代结果发生震荡</li>
<li>直观解释：当实例点被误分类，应该调整$w, b$值，使得分离
  超平面向<strong>误分类点方向</strong>移动，减少误分类点与超平面距离，
  直至被正确分类</li>
</ul>
</blockquote>
<h3 id="学习算法对偶形式"><a href="#学习算法对偶形式" class="headerlink" title="学习算法对偶形式"></a>学习算法对偶形式</h3><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><h3 id="算法收敛性"><a href="#算法收敛性" class="headerlink" title="算法收敛性"></a>算法收敛性</h3><p>为方便做如下记号</p>
<blockquote>
<ul>
<li>$\hat w = (w^T, b^T)^T, \hat w \in R^{n+1}$</li>
<li>$\hat x = (x^T, 1)^T, \hat x \in R^{n+1}$</li>
</ul>
</blockquote>
<p>此时，感知模型可以表示为</p>
<script type="math/tex; mode=display">
xw + b = \hat w \hat x = 0</script><blockquote>
<ul>
<li>数据集$T={(x_1, y_1), (x_2, y_2),…}$线性可分，其中：
$x_i \in \mathcal{X = R^n}$，
$y_i \in \mathcal{Y = {-1, +1}}$，则</li>
</ul>
<blockquote>
<ul>
<li><p>存在满足条件$|\hat w<em>{opt}|=1$超平面
   $\hat w</em>{opt} \hat x = 0$将训练数据完全正确分开，且
   $\exists \gamma &gt; 0, y<em>i(\hat w</em>{opt} x_i) \geq \gamma$</p>
</li>
<li><p>令$R = \arg\max_{1\leq i \leq N} |\hat x_i|$，则
   随机梯度感知机误分类次数$k \leq (\frac R \gamma)^2$</p>
</li>
</ul>
</blockquote>
</blockquote>
<h4 id="超平面存在性"><a href="#超平面存在性" class="headerlink" title="超平面存在性"></a>超平面存在性</h4><ul>
<li><p>训练集线性可分，存在超平面将训练数据集完全正确分开，可以
取超平面为$\hat w_{opt} \hat x = 0$</p>
</li>
<li><p>令$|\hat w_{opt}| = 1$，有</p>
<script type="math/tex; mode=display">\forall i, y_i(\hat w_{opt} \hat x_i) > 0</script><p>可取</p>
<script type="math/tex; mode=display">\gamma = \min_i \{ y_i (\hat w_{opt} \hat x) \}</script><p>满足条件</p>
</li>
</ul>
<h4 id="感知机算法收敛性"><a href="#感知机算法收敛性" class="headerlink" title="感知机算法收敛性"></a>感知机算法收敛性</h4><ul>
<li><p>给定学习率$\eta$，随机梯度下降法第k步更新为
$\hat w<em>k = \hat w</em>{k-1} + \eta y_i \hat x_i$</p>
</li>
<li><p>可以证明</p>
<ul>
<li><p>$\hat w<em>k \hat w</em>{opt} \geq k\eta\gamma$</p>
<script type="math/tex; mode=display">\begin{align*}
\hat w_k \hat w_{opt} & =
  \hat w_{k-1} \hat w_{opt} +
      \eta y_i \hat w_{opt} \hat x_i \\ 
  & \geq \hat w_{k-1} \hat w_{opt} +
      \eta\gamma \\
  & \geq k\eta\gamma
\end{align*}</script></li>
<li><p>$|\hat w_k|^2 \leq k \eta^2 R^2$</p>
<script type="math/tex; mode=display">\begin{align*}
\|\hat w_k\|^2 & = \|\hat w_{k-1} +
  \eta y_i x_i \|^2 \\
& = \|\hat w_{k-1}\|^2 + 2\eta y_i \hat w_{k-1}
  \hat x_i + \eta^2 \|\hat x_i\|^2 \\
& \leq \|w_{k-1}\|^2 + \eta^2 \|\hat x_i\|^2 \\
& \leq \|w_{k-1}\|^2 + \eta^2 R^2 \\
& \leq k\eta^2 R^2
\end{align*}</script></li>
</ul>
</li>
<li><p>则有</p>
<script type="math/tex; mode=display">\begin{align*}
k \eta \gamma & \leq \hat w_k \hat w_{opt} \leq
   \|\hat w\| \|\hat w_{opt}\| = \|\hat w\|
   \leq \sqrt k \eta R \\
k^2 \gamma^2 & \leq k R^2
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li><p>直观理解就是超平面<strong>最大移动次数</strong>不大于<strong>最大移动距离</strong>
  除以<strong>最小移动步长</strong></p>
<blockquote>
<ul>
<li>$\eta \gamma^2$：超平面法向量最小增加量（移动步长）</li>
<li>$\eta R^2$：超平面法向最大增加量（移动距离）</li>
<li>但是超平面不可能将所有点都归为同一侧</li>
</ul>
</blockquote>
</li>
<li><p>误分类次数有上界，经过有限次搜索可以找到将训练数据完全
  正确分开的分离超平面，即训练数据集线性可分时，算法的迭代
  形式是收敛的</p>
</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-13T15:23:47.000Z" title="7/13/2019, 11:23:47 PM">2019-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:59:11.000Z" title="7/16/2021, 2:59:11 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">38 minutes read (About 5636 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/support_vector_machine.html">Support Vector Machine</a></h1><div class="content"><h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p>支持向量机是二分类模型</p>
<h3 id="学习要素"><a href="#学习要素" class="headerlink" title="学习要素"></a>学习要素</h3><ul>
<li><p>基本模型：定义在特征空间上的间隔最大线性分类器</p>
</li>
<li><p>学习策略：间隔最大化</p>
<ul>
<li><p>可形式化为求解凸二次规划问题，也等价于正则化的合页
损失函数最小化问题</p>
</li>
<li><p>间隔最大使其有别于感知机，训练数据线性可分时分离
超平面唯一</p>
<ul>
<li>误分类最小策略（0-1损失）得到分离超平面的解无穷
多个</li>
<li>距离和最小策略（平方损失）得到分离超平面唯一，
但与此不同</li>
</ul>
</li>
</ul>
</li>
<li><p>学习算法：求解凸二次规划的最优化算法</p>
</li>
</ul>
<h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>在给定特征空间上的训练数据集
$T={(x_1,y_1), (x_2,y_2),\cdots,(x_N,y_N)}$，其中
$x_i \in \mathcal{X} = R^n, y_i \in {-1, +1}, i=1,2,…,N$</p>
<blockquote>
<ul>
<li>输入空间：欧氏空间或离散集合</li>
<li>特征空间：希尔伯特空间</li>
<li>输出空间：离散集合</li>
</ul>
</blockquote>
<ul>
<li><p>SVM假设输入空间、特征空间为两个不同空间，SVM在特征空间
上进行学习</p>
</li>
<li><p>线性（可分）支持向量机假设两个空间元素<strong>一一对应</strong>，并将
输入空间中的输入映射为特征空间中特征向量</p>
</li>
<li><p>非线性支持向量机利用，输入空间到特征空间的非线性映射
（核函数）将输入映射为特征向量</p>
</li>
</ul>
<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><h2 id="Linear-Support-Vector-Machine-in-Linearly-Separable-Case"><a href="#Linear-Support-Vector-Machine-in-Linearly-Separable-Case" class="headerlink" title="Linear Support Vector Machine in Linearly Separable Case"></a><em>Linear Support Vector Machine in Linearly Separable Case</em></h2><p>线性可分支持向量机：硬间隔支持向量机，训练数据线性可分时，
通过硬间隔最大化策略学习</p>
<ul>
<li>直观含义：不仅将正负实例分开，而且对于最难分的实例点
（离超平面最近的点），也有足够大的确信度将其分开，这样的
超平面对新实例预测能力较好</li>
</ul>
<blockquote>
<ul>
<li><em>Hard-margin Maximization</em>：硬间隔最大化，最大化超平面
  $(w,b)$关于线性可分训练数据集的两类样本集几何间隔</li>
</ul>
</blockquote>
<h3 id="原始问题策略"><a href="#原始问题策略" class="headerlink" title="原始问题策略"></a>原始问题策略</h3><ul>
<li><p>约束最优化问题表述</p>
<script type="math/tex; mode=display">\begin{align}
\max_{w,b} & \gamma & \\
s.t. & \frac {y_i} {\|w\|} (wx + b) \geq \gamma,
   i=1,2,\cdots,N
\end{align}</script></li>
<li><p>考虑函数间隔、几何间隔关系得到问题</p>
<ul>
<li>目标、约束中使用函数间隔表示几何间隔</li>
<li>就是普通<strong>等比缩放</strong>、<strong>分母移位</strong>，不用考虑太多</li>
</ul>
<script type="math/tex; mode=display">\begin{align*}
\max_{w,b} & \frac {\hat{\gamma}} {\|w\|} \\
s.t. & y_i(wx_i + b) \geq \hat{\gamma}, i=1,2,\cdots,N
\end{align*}</script></li>
<li><p>而函数间隔$\hat{\gamma}$大小会随着超平面参数变化
成比例变化，其取值对问题求解无影响，所以可取
$\hat{\gamma}=1$带入，得到最优化问题</p>
<script type="math/tex; mode=display">\begin{align*}
\min_{w,b} & \frac 1 2 {\|w\|}^2 \\
s.t. & y_i(wx_i + b) - 1 \geq 0, i=1,2,\cdots,N
\end{align*}</script><blockquote>
<ul>
<li>$\max \frac 1 {|w|}$和
 $\min \frac 1 2 {|w|}^2$等价</li>
</ul>
</blockquote>
<ul>
<li><p>这里确实没有通用变换技巧，因为这里的$\hat \gamma$
是特殊的值，其取值与$w,b$相关，这是<strong>问题自然蕴含</strong>
，可以视为还有以下一个依赖</p>
</li>
<li><p>当然也可以直接证明两个问题等价：先证明最优解在等式
成立时取到，然后目标函数中1替换为等式左边</p>
</li>
</ul>
</li>
</ul>
<h3 id="最大间隔分离平面存在性"><a href="#最大间隔分离平面存在性" class="headerlink" title="最大间隔分离平面存在性"></a>最大间隔分离平面存在性</h3><blockquote>
<ul>
<li>若训练数据集T线性可分，则可将训练数据集中样本点完全
  正确分开的最大间隔分离超平面存在且唯一</li>
</ul>
</blockquote>
<h4 id="存在性"><a href="#存在性" class="headerlink" title="存在性"></a>存在性</h4><ul>
<li>训练数据集线性可分，所以以上中最优化问题一定存在可行解</li>
<li>又目标函数又下界，则最优化问题必有解<h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1></li>
<li>又训练数据集中正、负类点都有，所以$(0,b)$必不是最优化
可行解</li>
</ul>
<h4 id="唯一性"><a href="#唯一性" class="headerlink" title="唯一性"></a>唯一性</h4><ul>
<li><p>若以上最优化问题存在两个最优解$(w_1^{<em>},b_1^{</em>})$、
$w_2^{<em>},b_2^{</em>}$</p>
</li>
<li><p>显然$|w_1^{<em>}| = |w_2^{</em>}| = c$，
$(w=\frac {w_1^{<em>}+w_2^{</em>}} 2,b=\frac {b_1^{<em>}+b_2^{</em>}} 2)$
使最优化问题的一个可行解，则有</p>
<script type="math/tex; mode=display">
c \leq \|w\| \leq \frac 1 2 \|w_1^{*}\| + \frac 1 2
   \|w_2^{*} = c</script></li>
<li><p>则有$|w|=\frac 1 2 |w_1^{<em>}|+\frac 1 2 |w_2^{</em>}|$
，有$w_1^{<em>} = \lambda w_2^{</em>}, |\lambda|=1$</p>
<ul>
<li>$\lambda = -1$，不是可行解，矛盾</li>
<li>$\lambda = 1$，则$w_1^{(<em>)} = w_2^{</em>}$，两个最优解
写为$(w^{<em>}, b_1^{</em>})$、$(w^{<em>}, b_2^{</em>})$</li>
</ul>
</li>
<li><p>设$x_1^{+}, x_1^{-}, x_2^{+}, x_2^{-}$分别为对应以上两组
超平面，使得约束取等号、正/负类别的样本点，则有</p>
<script type="math/tex; mode=display">\begin{align*}
b_1^{*} & = -\frac 1 2 (w^{*} x_1^{+} + w^{*} x_1^{-}) \\
b_2^{*} & = -\frac 1 2 (w^{*} x_2^{+} + w^{*} x_2^{-}) \\
\end{align*}</script><p>则有</p>
<script type="math/tex; mode=display">
b_1^{*} - b_2^{*} = -\frac 1 2 [w^{*}(x_1^{+} - x_2^{+})
   + w^{*} (x_1^{-} - x_2^{-})]</script></li>
<li><p>又因为以上支持向量的性质可得</p>
<script type="math/tex; mode=display">\begin{align*}
w^{*}x_2^{+} + b_1^{*} & \geq 1 = w^{*}x_1^{+} + b_1^{*} \\
w^{*}x_1^{+} + b_2^{*} & \geq 1 = w^{*}x_2^{+} + b_2^{*}
\end{align*}</script><p>则有$w^{<em>}(x_1^{+} - x_2^{+})=0$，同理有
$w^{</em>}(x_1^{-} - x_2^{-})=0$</p>
</li>
<li><p>则$b_1^{<em>} - b_2^{</em>} = 0$</p>
</li>
</ul>
<h3 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h3><h4 id="Support-Vector"><a href="#Support-Vector" class="headerlink" title="Support Vector"></a><em>Support Vector</em></h4><p>支持向量：训练数据集中与超平面距离最近的样本点实例</p>
<ul>
<li>在线性可分支持向量机中即为使得约束条件取等号成立的点</li>
<li>在决定分离超平面时，只有支持向量起作用，其他实例点不起
作用</li>
<li>支持向量一般很少，所以支持向量机由很少的“重要”训练样本
决定</li>
</ul>
<h4 id="间隔边界"><a href="#间隔边界" class="headerlink" title="间隔边界"></a>间隔边界</h4><p>间隔边界：超平面$wx + b = +/-1$</p>
<ul>
<li>支持向量位于其上</li>
<li>两个间隔边界之间距离称为间隔$=\frac 2 {|w|}$</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>输入：线性可分训练数据集T</li>
<li>输出：最大间隔分离超平面、分类决策函数</li>
</ul>
</blockquote>
<ol>
<li><p>构造并求解约束最优化问题</p>
<script type="math/tex; mode=display">\begin{align*}
\min_{w,b} &  \frac 1 2 {\|w\|}^2 \\
s.t. & y_i(wx_i + b) - 1 \geq 0, i=1,2,\cdots,N
\end{align*}</script><p>得到最优解$w^{<em>}, b^{</em>}$</p>
</li>
<li><p>得到分离超平面</p>
<script type="math/tex; mode=display">
w^{*}x + b^{*} = 0</script><p>分类决策函数</p>
<script type="math/tex; mode=display">
f(x) = sign(w^{*}x + b^{*})</script></li>
</ol>
<h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><ul>
<li><p><em>1 vs n-1</em>：对类$k=1,…,n$分别训练当前类对剩余类分类器</p>
<ul>
<li>分类器数据量有偏，可以在负类样本中进行抽样</li>
<li>训练n个分类器</li>
</ul>
</li>
<li><p><em>1 vs 1</em>：对$k=1,…,n$类别两两训练分类器，预测时取各
分类器投票多数</p>
<ul>
<li>需要训练$\frac {n(n-1)} 2$给分类器</li>
</ul>
</li>
<li><p><em>DAG</em>：对$k=1,…,n$类别两两训练分类器，根据预先设计的、
可以使用DAG表示的分类器预测顺序依次预测</p>
<ul>
<li>即排除法排除较为不可能类别</li>
<li>一旦某次预测失误，之后分类器无法弥补<ul>
<li>但是错误率可控</li>
<li>设计DAG时可以每次选择相差最大的类别优先判别</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Dual-Algorithm"><a href="#Dual-Algorithm" class="headerlink" title="Dual Algorithm"></a><em>Dual Algorithm</em></h2><p>对偶算法：求解对偶问题得到原始问题的最优解</p>
<ul>
<li>对偶问题往往更容易求解</li>
<li>自然引入核函数，进而推广到非线性分类问题</li>
</ul>
<h3 id="对偶问题策略"><a href="#对偶问题策略" class="headerlink" title="对偶问题策略"></a>对偶问题策略</h3><ul>
<li><p>Lagrange函数如下</p>
<script type="math/tex; mode=display">
L(w,b,\alpha) = \frac 1 2 \|w\|^2 - \sum_{i=1}^N
   \alpha_i y_i (wx_i + b) + \sum_{i=1}^N \alpha_i</script><blockquote>
<ul>
<li>$\alpha_i &gt; 0$：<em>Lagrange multiplier</em></li>
</ul>
</blockquote>
</li>
<li><p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题</p>
<script type="math/tex; mode=display">
\max_{\alpha} \min_{w,b} L(w,b,\alpha)</script></li>
</ul>
<ul>
<li><p>求$\min_{w,b} L(w,b,\alpha)$，对拉格朗日函数求偏导置0</p>
<script type="math/tex; mode=display">\begin{align*}
\triangledown_w L(w,b,\alpha) & = w - \sum_{i=1}^N
   \alpha_i y_i x_i = 0 \\
\triangledown_b L(w,b,\alpha) & = -\sum_{i=1}^N
   \alpha_i y_i = 0
\end{align*}</script><p>解得</p>
<script type="math/tex; mode=display">\begin{align*}
w = \sum_{i=1}^N \alpha_i y_i x_i \\
\sum_{i=1}^N \alpha_i y_i = 0
\end{align*}</script></li>
<li><p>将以上结果代理拉格朗日函数可得</p>
<script type="math/tex; mode=display">\begin{align*}
L(w,b,\alpha) & = \frac 1 2 \sum_{i=1}^N \sum_{j=1}^N
   \alpha_i \alpha_j y_i y_j (x_i x_j) - \sum_{i=1}^N
   \alpha_i y_i ((\sum_{j=1}^N \alpha_j y_j x_j)x_i + b)
   + \sum_{i=1}^N \alpha_i \\
& = -\frac 1 2 \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j
   y_i y_i (x_i x_j) + \sum_{i=1}^N \alpha_i
\end{align*}</script></li>
<li><p>以上函数对$\alpha$极大即为对偶问题，为方便改成极小</p>
<script type="math/tex; mode=display">\begin{align*}
\min_{\alpha} & \frac 1 2 \sum_{i=1}^N \sum_{j=1}^N
   \alpha_i \alpha_j y_i y_j (x_i x_j) -
   \sum_{i=1}^N \alpha_i \\
s.t. & \sum_{i=1}^N \alpha_i y_i = 0, \\
& \alpha_i > 0, i = 1,2,\cdots,N
\end{align*}</script></li>
</ul>
<h3 id="原始问题解"><a href="#原始问题解" class="headerlink" title="原始问题解"></a>原始问题解</h3><blockquote>
<ul>
<li>设$\alpha^{<em>} = (\alpha_1^{</em>}, \cdots, \alpha_N^{<em>})$是
  上述对偶优化问题的解，则存在$j \in [1, N]$使得
  $\alpha_j^{</em>} &gt; 0$，且原始最优化问题解如下<script type="math/tex; mode=display">\begin{align*}
  w^{*} & = \sum_{i=1}^N \alpha_i^{*} y_i x_i \\
  b^{*} & = y_j - \sum_{i=1}^N \alpha_i^{*} y_i (x_i x_j)
  \end{align*}</script></li>
</ul>
</blockquote>
<ul>
<li><p>由KKT条件成立有</p>
<script type="math/tex; mode=display">\begin{align*}
& \triangledown_w L(w^{*}, b^{*}, \alpha^{*}) = w^{*} -
   \sum_{i=1}^N \alpha_i{*} y_i x_i = 0 \\
& \triangledown_b L(w^{*}, b^{*}, \alpha^{*}) =
   -\sum_{i=1}^N \alpha_i^{*} y_i = 0 \\
& \alpha^{*}(y_i (w^{*} x_i + b^{*}) - 1) = 0,
   i = 1,2,\cdots,N \\
& y_i(w^{*} x_i + b) - 1 \geq 0, i = 1,2,\cdots,N \\
& \alpha_i^{*} \geq 0, i = 1,2,\cdots,N \\
\end{align*}</script></li>
<li><p>可得</p>
<script type="math/tex; mode=display">
w^{*} = \sum_i \alpha_i^{*} y_i x_i</script></li>
<li><p>其中至少有一个$\alpha_j &gt; 0$，否则$w^{*}=0$不是原问题解
，且有</p>
<script type="math/tex; mode=display">\begin{align*}
y_j(w^{*} x_j + b^{*}) - 1 = 0
\end{align*}</script><p>注意到$y_j \in {-1, +1}$，则有</p>
<script type="math/tex; mode=display">
b^{*} = y_j - \sum_{i=1}^N \alpha_i^{*} y_i (x_i x_j)</script></li>
</ul>
<h4 id="分离超平面"><a href="#分离超平面" class="headerlink" title="分离超平面"></a>分离超平面</h4><ul>
<li><p>则分离超平面为</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N \alpha_i^{*} y_i (x x_i) + b^{*} = 0</script></li>
<li><p>分类决策函数为</p>
<script type="math/tex; mode=display">
f(x) = sgn(\sum_{i=1}^N \alpha_i^{*} y_i (x x_i) + b^{*})</script><p>即分类决策函数只依赖输入$x$和训练样本的内积</p>
</li>
</ul>
<h4 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h4><blockquote>
<ul>
<li>将对偶最优化问题中，训练数据集中对应$\alpha_i^{*} &gt; 0$
  的样本点$(x_i, y_i)$称为支持向量</li>
</ul>
</blockquote>
<ul>
<li>由KKT互补条件可知，对应$\alpha_i^{*} &gt; 0$的实例$x_i$有<script type="math/tex; mode=display">
y_i(w^{*} x_i + b^{*}) - 1 = 0</script>即$(x_i, y_i)$在间隔边界上，同原始问题中支持向量定义一致</li>
</ul>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>输入：线性可分数据集$T$</li>
<li>输出：分离超平面、分类决策函数</li>
</ul>
</blockquote>
<ol>
<li><p>构造并求解以上对偶约束最优化问题，求得最优解
$\alpha^{<em>} = (\alpha_1^{</em>},…,\alpha_N^{*})^T$</p>
</li>
<li><p>依据以上公式求$w^{<em>}$，选取$\alpha^{</em>}$正分量
$\alpha_j^{<em>} &gt; 0$计算$b^{</em>}$</p>
</li>
<li><p>求出分类超平面、分类决策函数</p>
</li>
</ol>
<h2 id="Linear-Support-Vector-Machine"><a href="#Linear-Support-Vector-Machine" class="headerlink" title="Linear Support Vector Machine"></a><em>Linear Support Vector Machine</em></h2><p>线性支持向量机：训练数据线性不可分时，通过软间隔最大化策略
学习</p>
<ul>
<li><p>训练集线性不可分通常是由于存在一些<em>outlier</em></p>
<ul>
<li>这些特异点不能满足函数间隔大于等于1的约束条件</li>
<li>将这些特异点除去后，剩下大部分样本点组成的集合是
线性可分的</li>
</ul>
</li>
<li><p>对每个样本点$(x_i,y_i)$引入松弛变量$\xi_i \geq 0$</p>
<ul>
<li>使得函数间隔加上松弛变量大于等于1</li>
<li>对每个松弛变量$\xi_i$，支付一个代价$\xi_i$</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>soft-margin maximization</em>：软间隔最大化，最大化样本点
  几何间隔时，尽量减少误分类点数量</li>
</ul>
</blockquote>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>线性不可分的线性支持向量机的学习变成如下凸二次规划，即
软间隔最大化</p>
<script type="math/tex; mode=display">\begin{align*}
\min_{w,b,\xi} & \frac 1 2 \|w\|^2 + C \sum_{i=1}^N \xi_i \\
s.t. & y_i(w x_i + b) \geq 1 - \xi_i, i=1,2,\cdots,N \\
& \xi_i \geq 0, i=1,2,\cdots,N
\end{align*}</script><blockquote>
<ul>
<li>$\xi_i$：松弛变量</li>
<li>$C &gt; 0$：惩罚参数，由应用问题决定，C越大对误分类惩罚越大</li>
</ul>
</blockquote>
<ul>
<li><p>最小化目标函数包含两层含义</p>
<ul>
<li>$\frac 1 2 |w|^2$尽量小，间隔尽量大</li>
<li>误分类点个数尽量小</li>
</ul>
</li>
<li><p>以上问题是凸二次规划问题，所以$(w,b,\xi)$的解是存在的</p>
<ul>
<li>$w$解唯一</li>
<li>$b$解可能不唯一，存在一个区间</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>对给定的线性不可分训练数据集，通过求解以上凸二次规划问题
  得到的分类超平面<script type="math/tex; mode=display">w^{*} x + b^{*} = 0</script>  以及相应的分类决策函数<script type="math/tex; mode=display">f(x) = sgn(w^{*} x + b^{*})</script>  称为线性支持向量机</li>
</ul>
</blockquote>
<ul>
<li>线性支持向量包括线性可分支持向量机</li>
<li>现实中训练数据集往往线性不可分，线性支持向量机适用性更广</li>
</ul>
<h3 id="对偶问题策略-1"><a href="#对偶问题策略-1" class="headerlink" title="对偶问题策略"></a>对偶问题策略</h3><p>原始问题的对偶问题</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_\alpha & \frac 1 2 \sum_{i=1}^N \sum_{j=1}^N \alpha_i
    \alpha_j y_i y_j (x_i x_j) - \sum_{i=1}^N \alpha_i \\
s.t. & \sum_{i=1}^N \alpha_i y_i = 0 \\
& 0 \leq \alpha_i \leq C, i=1,2,\cdots,N
\end{align*}</script><ul>
<li>类似线性可分支持向量机利用Lagrange对偶即可得</li>
</ul>
<h3 id="原始问题解-1"><a href="#原始问题解-1" class="headerlink" title="原始问题解"></a>原始问题解</h3><blockquote>
<ul>
<li>设$\alpha^{<em>} = (\alpha_1^{</em>}, \cdots, \alpha_N^{<em>})$是
  上述对偶优化问题的解，则存在$j \in [1, N]$使得
  $0 &lt; \alpha_j^{</em>} &lt; C$，且原始最优化问题解如下<script type="math/tex; mode=display">\begin{align*}
  w^{*} & = \sum_{i=1}^N \alpha_i^{*} y_i x_i \\
  b^{*} & = y_j - \sum_{i=1}^N \alpha_i^{*} y_i (x_i x_j)
  \end{align*}</script></li>
</ul>
</blockquote>
<ul>
<li>类似线性可分支持向量机利用KKT条件即可得</li>
</ul>
<h4 id="分离超平面-1"><a href="#分离超平面-1" class="headerlink" title="分离超平面"></a>分离超平面</h4><ul>
<li><p>则分离超平面为</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N \alpha_i^{*} y_i (x x_i) + b^{*} = 0</script></li>
<li><p>分类决策函数/线性支持向量机对偶形式</p>
<script type="math/tex; mode=display">
f(x) = sgn(\sum_{i=1}^N \alpha_i^{*} y_i (x x_i) + b^{*})</script><p>即分类决策函数只依赖输入$x$和训练样本的内积</p>
</li>
</ul>
<h4 id="支持向量-1"><a href="#支持向量-1" class="headerlink" title="支持向量"></a>支持向量</h4><blockquote>
<ul>
<li>将对偶最优化问题中，训练数据集中对应$\alpha_i^{*} &gt; 0$
  的样本点$x_i$称为（软间隔）支持向量</li>
</ul>
</blockquote>
<ul>
<li>$\alpha_i^{*} &lt; C$：则$\xi_i = 0$，恰好落在间隔边界上</li>
<li>$\alpha_i^{*} = C, 0 &lt; \xi_i &lt; 1$：间隔边界与分离超平面
之间</li>
<li>$\alpha_i^{*} = C, \xi=1$：分离超平面上</li>
<li>$\alpha_i^{*} = C, \xi&gt;1$：分离超平面误分一侧</li>
</ul>
<h3 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a><em>Hinge Loss</em></h3><p>线性支持向量机策略还可以视为最小化以下目标函数</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N [1-y_i(w x_i + b)]_{+} + \lambda \|w\|^2</script><blockquote>
<ul>
<li>第一项：经验风险，合页损失函数</li>
<li>第二项：正则化项</li>
</ul>
</blockquote>
<ul>
<li>$w$模越大，间隔越小，合页损失越小，所以用这个作为
正则化项是合理的</li>
</ul>
<blockquote>
<ul>
<li>参见<em>data_science/loss</em></li>
</ul>
</blockquote>
<h4 id="等价证明"><a href="#等价证明" class="headerlink" title="等价证明"></a>等价证明</h4><p>令</p>
<script type="math/tex; mode=display">
\xi_i = [1 - y_i(w x_i + b)]_{+}</script><ul>
<li><p>则有$\xi_i \geq 0$</p>
</li>
<li><p>且</p>
<script type="math/tex; mode=display">\left \{ \begin{align*}
& y_i(w x_i + b) = 1 - \xi_i, & y_i(w x_i + b) \leq 1 \\
& y_i(w x_i + b) > 1 - \xi_i = 1, & y_i(w x_i + b) > 1
\end{align*} \right.</script><p>所以有</p>
<script type="math/tex; mode=display">
y_i(w x_i + b) \geq 1 - \xi_i</script></li>
<li><p>则原问题两个约束条件均得到满足，此问题可写成</p>
<script type="math/tex; mode=display">
\min_{w,b} \sum_{i=1}^N \xi_i + \lambda \|w\|^2</script><p>取$\lambda = \frac 1 {2C}$，即同原问题</p>
</li>
</ul>
<h2 id="Non-Linear-Support-Vector-Machine"><a href="#Non-Linear-Support-Vector-Machine" class="headerlink" title="Non-Linear Support Vector Machine"></a><em>Non-Linear Support Vector Machine</em></h2><p>非线性支持向量机</p>
<ul>
<li><p>非线性问题：通过非线性模型才能很好进行分类的问题</p>
<ul>
<li>通过<strong>非线性变换</strong>$\phi(x)$，将输入空间映射到特征
空间（维数可能非常高）</li>
<li>原空间中非线性可分问题在特征空间可能变得线性可分，
在特征空间中学习分类模型</li>
</ul>
</li>
<li><p>SVM求解非线性问题时</p>
<ul>
<li><em>kernel trick</em>：通过非线性变换将输入空间对应一个特征
空间，使得输入空间中超曲面对应于特征空间的超平面模型</li>
<li>软间隔最大化策略：在特征空间中求解线性支持向量机</li>
</ul>
</li>
</ul>
<h3 id="Kernal-Trick"><a href="#Kernal-Trick" class="headerlink" title="Kernal Trick"></a><em>Kernal Trick</em></h3><p>核技巧：线性SVM的对偶问题中，目标函数、决策函数均只涉及输入
实例、实例之间的内积</p>
<ul>
<li><p>将内积使用核函数代替，等价进行复杂的非线性变换</p>
</li>
<li><p>映射函数是非线性函数时，学习的含有核函数的支持向量机
是非线性模型</p>
</li>
<li><ul>
<li>学习是隐式地在特征空间中进行的，不需要显式定义特征
空间、映射函数</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>参见<em>data_science/ref/functions</em></li>
</ul>
</blockquote>
<h4 id="对偶问题目标函数"><a href="#对偶问题目标函数" class="headerlink" title="对偶问题目标函数"></a>对偶问题目标函数</h4><script type="math/tex; mode=display">
W(\alpha) = \frac 1 2 \sum_{i=1}^N \sum_{j=1}^N
    \alpha_i \alpha_j y_i y_j K(x_i, x_j) -
    \sum_{i=1}^N \alpha_i</script><h3 id="原始问题解-2"><a href="#原始问题解-2" class="headerlink" title="原始问题解"></a>原始问题解</h3><h4 id="分类决策函数"><a href="#分类决策函数" class="headerlink" title="分类决策函数"></a>分类决策函数</h4><script type="math/tex; mode=display">\begin{align*}
f(x) & = sgn(\sum_{i=1}^{N_s} \alpha_i^{*} y_i \phi(x_i)
    \phi(x) + b^{*}) \\
& = sgn(\sum_{i=1}^{N_s} \alpha_i^{*} y_i K(x_i, x) + b^{*})
\end{align*}</script><h3 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>输入：训练数据集$T$</li>
<li>输出：分类决策函数</li>
</ul>
</blockquote>
<ol>
<li><p>选取适当的核函数$K(x,z)$、适当参数C，构造求解最优化问题</p>
<script type="math/tex; mode=display">\begin{align*}
\min_\alpha & \frac 1 2 \sum_{i=1}^N \sum_{j=1}^N
  \alpha_1 \alpha_j y_i y_j K(x_i, x_j) -
  \sum_{i=1}^N \alpha_i \\
s.t. & \sum_{i=1}^N \alpha_i y_i = 0 \\
& 0 \leq \alpha_i \leq C, i=1,2,...,N
\end{align*}</script><p>求得最优解
$\alpha^{<em>} = (\alpha_1^{</em>},\alpha_2^{<em>},\cdots,\alpha_N^{</em>})$</p>
</li>
<li><p>选择$\alpha^{<em>}$的一个正分量$0 &lt; \alpha_j^{</em>} &lt; C$，计算</p>
<script type="math/tex; mode=display">
b^{*} = y_j - \sum_{i=1}^N \alpha_i^{*} y_i K(x_i x_j)</script></li>
<li><p>构造决策函数</p>
</li>
</ol>
<blockquote>
<ul>
<li>$K(x,z)$是正定核函数时，最优化问题是凸二次规划，有解</li>
</ul>
</blockquote>
<h2 id="Sequential-Minimal-Optimization"><a href="#Sequential-Minimal-Optimization" class="headerlink" title="Sequential Minimal Optimization"></a><em>Sequential Minimal Optimization</em></h2><p>序列最小最优化算法，主要解如下凸二次规划的对偶问题</p>
<script type="math/tex; mode=display">\begin{align*}
\min_\alpha & \frac 1 2 \sum_{i=1}^N \sum_{j=1}^N
    \alpha_1 \alpha_j y_i y_j K(x_i, x_j) -
    \sum_{i=1}^N \alpha_i \\
s.t. & \sum_{i=1}^N \alpha_i y_i = 0 \\
& 0 \leq \alpha_i \leq C, i=1,2,...,N
\end{align*}</script><blockquote>
<ul>
<li>凸二次规划有很多算法可以求得全局最优解，但是在训练样本
  容量较大时，算法会很低效</li>
</ul>
</blockquote>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><p>将原问题不断分解为子问题求解，进而求解原问题</p>
<ul>
<li><p>如果所有变量的解都满足此优化问题的KKT条件，得到此最优化
问题的一个可能解</p>
<ul>
<li>对凸二次规划就是最优解，因为凸二次规划只有一个稳定点</li>
</ul>
</li>
<li><p>否则选择两个变量，固定其他变量，构建一个二次规划</p>
<ul>
<li><p>目标是使得解符合KKT条件，但是因为等式约束的存在，
不可能<strong>单独改变一个变量</strong>而保持等式约束</p>
</li>
<li><p>子问题有解析解，求解速度快</p>
</li>
<li><p>二次规划问题关于两个变量的解会使得原始二次规划的目标
函数值变得更小，更接近原始二次规划的解
（这里SMO原始论文有证明，违反KKT条件的变量可以做到）</p>
</li>
</ul>
</li>
<li><p>不失一般性，假设选择两个变量$\alpha_1, \alpha_2$，其他
变量$\alpha_i(i=3,4,…,N)$是固定的，则SMO最优化子问题</p>
<script type="math/tex; mode=display">\begin{align*}
\min_{\alpha_1, \alpha_2} & W(\alpha_1, \alpha_2) =
   \frac 1 2 K_{11} \alpha_1^2 + \frac 1 2
   K_{22} \alpha_2^2 + y_1 y_2 K_{12} \alpha_1 \alpha_2
   - (\alpha_1 + \alpha_2) + y_1 \alpha_1 \sum_{i=3}^N
   y_i \alpha_i K_{i1} + y_2 \alpha_2 \sum_{i=1}^N
   y_i \alpha_i K{i2} \\
s.t. & \alpha_1 y_1 + \alpha_2 y_2 = -\sum_{i=3}^N
   y_i \alpha_i = \zeta \\
& 0 \leq \alpha_i \leq C, i=1,2
\end{align*}</script><blockquote>
<ul>
<li>$K_{ij} = K(x_i, x_j), i,j=1,2,\cdots,N$</li>
<li>$\zeta$：常数</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="两变量二次规划取值范围"><a href="#两变量二次规划取值范围" class="headerlink" title="两变量二次规划取值范围"></a>两变量二次规划取值范围</h3><ul>
<li><p>由<strong>等式约束</strong>，$\alpha_1, \alpha_2$中仅一个自由变量，
不妨设为$\alpha_2$</p>
<ul>
<li><p>设初始可行解为$\alpha_1, \alpha_2$</p>
</li>
<li><p>设最优解为$\alpha_1^{<em>}, \alpha_2^{</em>}$</p>
</li>
<li><p>未经剪辑（不考虑约束条件而来得取值范围）最优解为
$\alpha_2^{**}$</p>
</li>
</ul>
</li>
<li><p>由<strong>不等式</strong>约束，可以得到$\alpha_2$取值范围$[L, H]$</p>
<ul>
<li><p>$y_1 = y_2 = +/-1$时</p>
<script type="math/tex; mode=display">\begin{align*}
H & = \min \{C, \alpha_1 + \alpha_2 \} \\
L & = \max \{0, \alpha_2 + \alpha_1 - C \}
\end{align*}</script></li>
<li><p>$y_1 \neq y_2$时</p>
<script type="math/tex; mode=display">\begin{align*}
L & = \max \{0, \alpha_2 - \alpha_1 \} \\
L & = \min \{C, C + \alpha_2 - \alpha_1 \}
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li>以上取值范围第二项都是应用等式约束情况下，考虑
不等式约束</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="两变量二次规划求解"><a href="#两变量二次规划求解" class="headerlink" title="两变量二次规划求解"></a>两变量二次规划求解</h3><p>为叙述，记</p>
<script type="math/tex; mode=display">\begin{align*}
g(x) & = \sum_{i=1}^N \alpha_i y_i K(x_i, x) + b \\
E_j & = g(x_j) - y_j \\
& = (\sum_{i=1}^N \alpha_i y_i K(x_i, x_j)) - y_j
\end{align*}</script><ul>
<li>$g(x)$：SVM预测函数（比分类器少了符号函数）（函数间隔）</li>
<li>$E_j$：SVM对样本预测与真实值偏差</li>
</ul>
<blockquote>
<ul>
<li>以上两变量二次规划问题，沿约束方向未经剪辑解是<script type="math/tex; mode=display">\begin{align*}
  \alpha_2^{**} & = \alpha_2 + \frac {y_2 (E_1 - E_2)}
      \eta \\
  \eta & = K_{11} + K_{22} - 2K_{12} \\
  & = \|\phi(x_1) - \phi_(x_2)\|^2
  \end{align*}</script>  剪辑后的最优解是<script type="math/tex; mode=display">\begin{align*}
  \alpha_2^{*} & = \left \{ \begin{array}{l}
      H, & \alpha_2^{**} > H \\
      \alpha_2^{**}, & L \leq \alpha_2^{**} \leq H \\
      L, & \alpha_2^{*} < L
  \end{array} \right. \\
  \alpha_1^{*} & = \alpha_1 + y_1 y_2 (\alpha_2 -
      \alpha_2^{*})
  \end{align*}</script></li>
</ul>
</blockquote>
<ul>
<li><p>记</p>
<script type="math/tex; mode=display">\begin{align*}
v_i & = \sum_{j=3}^N \alpha_j y_j K(x_i, x_j) \\
& = g(x_i) - \sum_{j=1}^2 \alpha_j y_j K(x_i, x_j) - b
   i=1,2
\end{align*}</script><p>由等式约束$\alpha_1 = (\zeta - y_2 \alpha_2) y_1$，均
带入目标函数有</p>
<script type="math/tex; mode=display">\begin{align*}
W(\alpha_1, \alpha_2) = & \frac 1 2 K_{11} (\zeta -
   y_2 \alpha_2)^2 + \frac 1 2 K_{22} \alpha_2^2 +
   y_2 K_{12} (\zeta - y_2 \alpha_2) \alpha_2 - \\
& y_1 (\zeta - \alpha_2 y_2) - \alpha_2 +
   v_1 (\zeta - \alpha_2 y_2) + y_2 v_2 \alpha_2
\end{align*}</script></li>
<li><p>对$\alpha_2$求导置0可得</p>
<script type="math/tex; mode=display">\begin{align*}
(K_{11} + K_{22} - 2K_{12}) \alpha_2 & = y_2 (y_2 -
   y_1 + \zeta K_{11} - \zeta K_{12} + v_1 + v_2) \\
& = y_2 [y_2 - y_1 + \zeta K_{11} - \zeta K_{12} +
   (g(x_1) - \sum_{j=1}^2 y_j \alpha_j K_{1j} - b) -
   (g(x_2) - \sum_{j=1}^2 y_j \alpha_j K_{2j} -b )]
\end{align*}</script><p>带入$\zeta = \alpha_1 y_1 + \alpha_2 y2$，可得</p>
<script type="math/tex; mode=display">\begin{align*}
(K_11 + K_22 - 2K_{12}) \alpha_2^{**} & = y_2((K_{11} +
   K_{22} - 2K_{12}) \alpha_2 y_2 + y_2 - y_1 +
   g(x_1) - g(x_1)) \\
& = (K_11 + K_{22} - 2K_{12}) \alpha_2 + y_2 (E_1 - E_2)
\end{align*}</script></li>
</ul>
<h3 id="变量选择"><a href="#变量选择" class="headerlink" title="变量选择"></a>变量选择</h3><p>SMO算法每个子问题的两个优化变量，其中至少一个违反KKT条件</p>
<h4 id="外层循环—首个变量选择"><a href="#外层循环—首个变量选择" class="headerlink" title="外层循环—首个变量选择"></a>外层循环—首个变量选择</h4><p>在训练样本中选择违反KKT条件最严重的样本点，将对应的变量作为
第一个变量$\alpha_1$</p>
<ul>
<li><p>检查样本点是否满足KKT条件</p>
<script type="math/tex; mode=display">\begin{align*}
\alpha_i = 0 \Leftrightarrow y_i g(x_i) \geq 1 \\
0 < \alpha_i < C \Leftrightarrow y_i g(x_i) = 1 \\
\alpha_i = C \Leftrightarrow y_i g(x_i) \leq 1
\end{align*}</script></li>
<li><p>检验过程中</p>
<ul>
<li><p>首先遍历所有满足条件的$0 &lt; \alpha_i &lt; C$样本点，即
在间隔边界上的支持向量点</p>
</li>
<li><p>若没有找到违背KKT条件的样本点，则遍历整个训练集</p>
</li>
</ul>
</li>
</ul>
<h4 id="内层循环"><a href="#内层循环" class="headerlink" title="内层循环"></a>内层循环</h4><p>第二个变量$\alpha_2$选择标准是其自身有足够大的变化，以加快
计算速度</p>
<ul>
<li><p>由以上推导知，$\alpha_2^{*}$取值依赖于$|E_1 - E_2|$，
所以可以选择$\alpha_2$使其对应的$|E_1 - E_2|$最大</p>
<ul>
<li>$\alpha_1$已经确定，$E_1$已经确定</li>
<li>$E_1 &gt; 0$：选最小的$E_2$</li>
<li>$E_1 &lt; 0$：选最大的$E_2$</li>
</ul>
</li>
<li><p>但以上方法可能不能使得目标函数值有足够下降，采用以下
<strong>启发式</strong>规则继续选择$\alpha_2$</p>
<ul>
<li>遍历间隔边界上的点</li>
<li>遍历整个训练数据集</li>
<li>放弃$\alpha_1$</li>
</ul>
</li>
</ul>
<h4 id="更新阈值"><a href="#更新阈值" class="headerlink" title="更新阈值"></a>更新阈值</h4><ul>
<li><p>$0&lt; \alpha_1^{*} &lt; C$时，由KKT条件可知</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N \alpha_i y_i K_{i1} + b = y_1</script><ul>
<li><p>则有</p>
<script type="math/tex; mode=display">
b_1^{*} = y_1 - \sum_{i=3}^N \alpha_i y_i K_{i1} -
  \alpha_1^{*} y_1 K_{11} - \alpha_2^{*} y_2 K_{21}</script></li>
<li><p>将$E_1$定义式带入有</p>
<script type="math/tex; mode=display">
b_1^{*} = -E_1 - y_1 K_{11} (\alpha_1^{*} - \alpha_1)
  - y_2 K_{21} (\alpha_2^{*} - \alpha_2) + b</script></li>
</ul>
</li>
<li><p>类似的$0 &lt; \alpha_2^{*} &lt; C$时</p>
<script type="math/tex; mode=display">
b_2^{*} = -E_2 - y_2 K_{22} (\alpha_2^{*} - \alpha_2)
   - y_1 K_{12} (\alpha_1^{*} - \alpha_1) + b</script></li>
<li><p>若</p>
<ul>
<li><p>$0 &lt; \alpha_1^{<em>}, \alpha_2^{</em>} &lt; C$，则
$b_1{<em>} = b_2^{</em>}$</p>
</li>
<li><p>$\alpha_1^{<em>}, $\alpha_2^{</em>}$均为0、C，则
$[b_1^{<em>}, b_2^{</em>}]$中均为符合KKT条件的阈值，选择
中点作为新阈值$b^{*}$</p>
</li>
</ul>
</li>
<li><p>同时使用新阈值更新所有的$E_i$值</p>
<script type="math/tex; mode=display">
E_i^{*} = \sum_S y_j \alpha_j K(x_i, x_j) + b^{*} - y_i</script><blockquote>
<ul>
<li>$S$：所有支持向量的集合</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>输入：训练数据集$T$，精度$\epsilon$</li>
<li>输出：近似解$\hat \alpha$</li>
</ul>
</blockquote>
<ol>
<li><p>初值$\alpha^{(0)}$，置k=0</p>
</li>
<li><p>选取优化变量$\alpha_1^{(k)}, \alpha_2^{(k)}$，解析求解
两变量最优化问题得$\alpha_1^{(k+1)}, \alpha_2^{(k+2)}$，
更新$\alpha^{(k+1)}$</p>
</li>
<li><p>若在精度范围内满足停机条件（KKT条件）</p>
<script type="math/tex; mode=display">\begin{align*}
& \sum_{i=1}^N \alpha_i y_i = 0 \\
& 0 \leq \alpha_i \leq C, i=1,2,\cdots,N \\
& y_i g(x_i) = \left \{ \begin{array}{l}
  \geq 1, & \{x_i | \alpha_i = 0\} \\
  = 1, & \{x_i | 0 < \alpha_i < C\} \\
  \leq 1, & \{x_i | \alpha_i = C\}
\end{array} \right. \\
& g(x_i) = \sum_{j=1}^N \alpha_j y_j K(x_j, x_i) + b
\end{align*}</script><p>则转4，否则置k=k+1，转2</p>
</li>
<li><p>取$\hat \alpha = \alpha^{(k+1)}$</p>
</li>
</ol>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>