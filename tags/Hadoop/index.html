<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: Hadoop - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="follow_it-verification-code" content="SVBypAPPHxjjr7Y4hHfn"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Hadoop</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-09T16:48:32.000Z" title="7/10/2019, 12:48:32 AM">2019-07-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T09:16:06.000Z" title="7/16/2021, 5:16:06 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">11 minutes read (About 1679 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/hive.html">Hive</a></h1><div class="content"><h2 id="Hive简介"><a href="#Hive简介" class="headerlink" title="Hive简介"></a>Hive简介</h2><p>Hive是Hadoop平台上的数据仓库，面向结构化数据分析</p>
<ul>
<li><p>将<strong>结构化</strong>数据文件映射为一张<strong>数据库表</strong></p>
</li>
<li><p>提供完整的SQL查询功能，所用语言称为HiveQL</p>
<ul>
<li>Hive将HiveQL转换为MapReduce作业，在hadoop平台运行</li>
<li>Hive相当于一个在hadoop平台上的SQL Shell</li>
<li>方便用户使用HiveQL快速实现简单数据分析、统计，而不必
开发专用MapReduce程序，学习成本低</li>
</ul>
</li>
<li><p>相较于传统关系数据库，Hive具有如下特点</p>
<p>||Hive|传统关系型数据库|
|———|———|———-|
|数据存储|HDFS分布式文件系统|服务器本地文件系统|
|查询处理|MapReduce计算模型|自行设计的查询处理模型|
|应用场景|海量数据分析处理|高性能查询，实时性好|
|数据更新|不支持对具体数据行修改，只能覆盖、追加|支持|
|事务处理|不支持|支持|
|索引支持|不支持，一般需要对数据进行全部扫描|支持，多种索引|
|扩展能力|基于Hadoop平台，存储、计算强大的扩展能力|扩展性较差|
|数据加载|Writing Time Schema：数据加载时无需进行模式检查，在读取数据时对数据以一定模式进行解释|Reading Time Schema：要求数据必须符合数据库表结构|</p>
</li>
</ul>
<h3 id="Hive服务端组件"><a href="#Hive服务端组件" class="headerlink" title="Hive服务端组件"></a>Hive服务端组件</h3><h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>负责将用户的编写的HiveQL查询语句进行解析、编译、优化、生成
执行计划，然后调用底层MapReduce计算模型执行，包括</p>
<ul>
<li>Compiler：编译器</li>
<li>Optimizer：优化器</li>
<li>Executor：执行器</li>
</ul>
<h4 id="MetaStore"><a href="#MetaStore" class="headerlink" title="MetaStore"></a>MetaStore</h4><p>元信息管理器，对Hive正确运行举足轻重</p>
<ul>
<li><p>MetaStore实际上就是Thrift服务</p>
<ul>
<li>MetaStore客户端（hive、spark shell等）和服务端通过
thrift协议进行通信</li>
<li>客户端通过连接metastore服务，实现对元数据的存取</li>
<li>通过Thrift获取元数据，屏蔽了访问MetaStore Database
所需的驱动、url、用户名、密码等细节</li>
</ul>
</li>
<li><p>负责存储元数据在关系型数据库（称为MetaStore Database）</p>
<ul>
<li>元数据包括Hive创建的database、table等元信息</li>
<li>支持的关系型数据库<ul>
<li>Derby：Apache旗下Java数据库</li>
<li>MySQL</li>
</ul>
</li>
</ul>
</li>
<li><p>MetaStore服务可以独立运行，可以让多个客户端同时连接、
甚至安装到远程服务器集群，保持Hive运行的健壮性</p>
</li>
</ul>
<h5 id="Embedded-Metastore-Server-Database-Derby"><a href="#Embedded-Metastore-Server-Database-Derby" class="headerlink" title="Embedded Metastore Server(Database Derby)"></a>Embedded Metastore Server(Database Derby)</h5><p>内嵌模式：使用内嵌的Derby数据库存储元数据</p>
<ul>
<li>不需要额外起Metastore服务</li>
<li>一次只能一个客户端连接，使用做实验，不适合生产环境</li>
<li>Derby默认会在调用hive命令所在目录的<code>metastore_db</code>文件中
持久化元数据</li>
</ul>
<p><img src="/imgs/embeded_metastore_database.png" alt="embeded_metastore_database"></p>
<h5 id="Local-Metastore-Server"><a href="#Local-Metastore-Server" class="headerlink" title="Local Metastore Server"></a>Local Metastore Server</h5><p>本地元存储</p>
<ul>
<li><p>采用外部数据库，支持</p>
<ul>
<li>MySQL</li>
<li>Postgres</li>
<li>Orcale</li>
<li>MSSQL</li>
</ul>
</li>
<li><p>数据库独立于hive部署，hive服务使用JDBC访问元数据，多个
服务可以同时进行</p>
</li>
<li><p>本地元存储不需要单独起metastore服务，用的是跟hive在同一
进程metastore服务</p>
</li>
</ul>
<p><img src="/imgs/local_metastore_server.png" alt="local_metastore_server"></p>
<h5 id="Remote-Metastore-Server"><a href="#Remote-Metastore-Server" class="headerlink" title="Remote Metastore Server"></a>Remote Metastore Server</h5><p>远程元存储</p>
<ul>
<li><p>类似于本地元存储，只是需要单独启动metastore服务，和hive
运行在不同的进程（甚至主机）中</p>
</li>
<li><p>需要在每个客户端配置文件配置连接到该metastore服务</p>
<ul>
<li>hive通过thrift访问metastore</li>
</ul>
</li>
<li><p>此模式可以控制到数据库的连接</p>
</li>
</ul>
<p><img src="/imgs/remote_metastore_server.png" alt="remote_metastore_server"></p>
<h4 id="hiveserver2"><a href="#hiveserver2" class="headerlink" title="hiveserver2"></a>hiveserver2</h4><p>基于的Thrift RPC实现</p>
<ul>
<li><p>远程客户端可以通过hiveserver2执行对hive的查询并返回结果</p>
<ul>
<li>支持多客户端并发、身份验证</li>
</ul>
</li>
<li><p>可以使用JDBC、ODBC、Thrift连接hiveserver2（Thrift Server
特性）</p>
</li>
<li><p>hiveserver2也能访问元数据，不依赖于metastore服务</p>
</li>
</ul>
<h3 id="Hive客户端组件"><a href="#Hive客户端组件" class="headerlink" title="Hive客户端组件"></a>Hive客户端组件</h3><h4 id="CLI"><a href="#CLI" class="headerlink" title="CLI"></a>CLI</h4><p>Command Line Interface</p>
<ul>
<li>允许用户交互式的使用Hive</li>
</ul>
<h4 id="THrift-Client-beeline"><a href="#THrift-Client-beeline" class="headerlink" title="THrift Client/beeline"></a>THrift Client/beeline</h4><p>基于Thrift的JDBC Client</p>
<ul>
<li>包括JDBC/ODBC驱动程序</li>
</ul>
<h4 id="WEB-GUI"><a href="#WEB-GUI" class="headerlink" title="WEB GUI"></a>WEB GUI</h4><p>允许用户通过WEB GUI图形界面访问Hive</p>
<ul>
<li>需要首先启动Hive Web Interface服务</li>
</ul>
<h2 id="Hive查询处理"><a href="#Hive查询处理" class="headerlink" title="Hive查询处理"></a>Hive查询处理</h2><h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ol>
<li>用户提交HQL至Driver</li>
<li>Driver把查询交给Compiler，Compiler使用MetaStore中元信息
检查、编译</li>
<li>查询经过Optimizer优化交由Executor Engine执行，转换为
MapReduce作业后调用MapReduce执行</li>
<li>MapReduce存取HDFS，对数据进行处理，查询结果返回Driver</li>
</ol>
<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><ul>
<li><p>基础数据类型</p>
<ul>
<li>Integer</li>
<li>Float</li>
<li>Double</li>
<li>String</li>
</ul>
</li>
<li><p>复杂数据类型：通过嵌套表达复杂类型</p>
<ul>
<li>Map</li>
<li>List</li>
<li>Struct</li>
</ul>
</li>
<li><p>还允许用户自定以类型、函数扩展系统</p>
</li>
</ul>
<h3 id="数据存储模型"><a href="#数据存储模型" class="headerlink" title="数据存储模型"></a>数据存储模型</h3><p>使用传统数据库：Table、Row、Column、Partition等概念，易于
理解</p>
<h4 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h4><p>相当于关系型数据库中的Namespace</p>
<ul>
<li>将不同用户数据隔离到不同的数据库、模式中</li>
</ul>
<h4 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h4><p>表格</p>
<ul>
<li><p>逻辑上由存储的数据、描述数据格式的相关元数据组成</p>
<ul>
<li>表格数据存放在分布式文件系统（HDFS）中</li>
<li>元数据存储在MetaStore服务指定关系型数据库中</li>
</ul>
</li>
<li><p>创建表格、加载数据之前，表格在HDFS中就是一个目录，
表格分为两种类型</p>
<ul>
<li>托管表：数据文件存放在Hive数据仓库中，即HDFS中的一个
目录，是Hive数据文件默认存放路径</li>
<li>外部表：数据文件<strong>可以</strong>存放在其他文件系统中</li>
</ul>
</li>
</ul>
<h4 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h4><p>根据“分区列”的值，对表格数据进行粗略划分的极值</p>
<ul>
<li><p>存储上：是Hive中表格主目录的子目录，名字即为定义的分区列
名字</p>
</li>
<li><p>逻辑上：分区不是表中的实际字段，是虚拟列</p>
<ul>
<li>根据虚拟列（可能包含多个实际字段）划分、存储表格数据</li>
<li>同一虚拟列中字段通常应该经常一起被查询，这样在需要
存取部分数据字段时，可以只扫描部分表</li>
</ul>
</li>
</ul>
<h4 id="Bucket"><a href="#Bucket" class="headerlink" title="Bucket"></a>Bucket</h4><p>Table、Partition都是目录级别的数据拆分，指定Bucket的表格，
数据文件将按照规律拆分成多个文件</p>
<ul>
<li><p>每个桶就是table、partition目录中的文件</p>
</li>
<li><p>一般使用Hash函数实现数据分桶，创建表时，需要指定桶数量、
分桶操作依据的列</p>
</li>
<li><p>用户执行Sample查询时，Hive可以使用分桶信息，有效的Prune
Data，如：对每个目录下单个桶文件进行查询</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T09:10:43.000Z" title="7/16/2021, 5:10:43 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">5 minutes read (About 724 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/computing_schema.html">Hadoop 计算模型</a></h1><div class="content"><h2 id="分布式计算模型"><a href="#分布式计算模型" class="headerlink" title="分布式计算模型"></a>分布式计算模型</h2><p>分布式系统，不像一般的数据库、文件系统，无法从上至下、从头
到尾进行求和等操作，需要由分散的节点不断向一个点聚拢计算过程
，考虑将分布式计算模型可以考虑分为两部分</p>
<ul>
<li>分布：操作原语</li>
<li>聚合：处理流程</li>
</ul>
<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce计算模型主要描述是<strong>分布：操作原语</strong>部分，
但是此也包含了<strong>聚合：处理流程</strong></p>
<h4 id="操作原语"><a href="#操作原语" class="headerlink" title="操作原语"></a>操作原语</h4><ul>
<li><em>map</em>：映射<ul>
<li>以键值对二元组为主要处理对象</li>
<li>map过程相互独立、各mapper相互不通信的</li>
<li>所以mapper比较适合<strong>可</strong>独立计算任务</li>
</ul>
</li>
<li><em>reduce</em>：规约<ul>
<li>直接处理map输出，reduce中二元组键位map过程中输出值</li>
</ul>
</li>
</ul>
<h4 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h4><p>以hadoop中MapReduce为例</p>
<ul>
<li><p>需要把任何计算任务转换为一系列MapReduce作业，然后依次
执行这些作业</p>
</li>
<li><p>计算过程的各个步骤之间，各个作业输出的中间结果需要存盘，
然后才能被下个步骤使用（因为各个步骤之间没有明确流程）</p>
</li>
</ul>
<h4 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h4><h5 id="操作原语部分"><a href="#操作原语部分" class="headerlink" title="操作原语部分"></a>操作原语部分</h5><ul>
<li><p>提供原语少：需要用户处理更多逻辑，易用性差</p>
</li>
<li><p>所以抽象层次低：数据处理逻辑隐藏在用户代码中，可读性差</p>
</li>
<li><p>所以其表达能力有限：</p>
<ul>
<li>复杂数据处理任务，如：机器学习算法、SQL连接查询很难
表示用MapReduce计算默认表达</li>
</ul>
</li>
</ul>
<h5 id="处理流程部分"><a href="#处理流程部分" class="headerlink" title="处理流程部分"></a>处理流程部分</h5><ul>
<li><p>MapReduce需要通过把中间结果存盘实现同步，I/O延迟高</p>
</li>
<li><p>reduce任务需要等待map任务全部完成才能继续，同步Barrier
大</p>
</li>
</ul>
<h4 id="适合场景"><a href="#适合场景" class="headerlink" title="适合场景"></a>适合场景</h4><ul>
<li><p>One Pass Computation：只需要一遍扫描处理的计算任务
MapReduce计算模型非常有效</p>
<ul>
<li>批数据处理</li>
</ul>
</li>
<li><p>Multi Pass Computation：需要在数据上进行多遍扫描、处理
的计算任务，需要执行多个MapReduce作业计算任务，因为
多副本复制、磁盘存取，其效率不高</p>
</li>
</ul>
<h3 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h3><p>Directed Acyclic Graph：只是表示数据处理流程的有向无环图</p>
<ul>
<li>顶点：数据处理任务，反映一定的业务逻辑，即如何对数据进行
转换和分析</li>
<li>边：数据在不同的顶点间的传递</li>
</ul>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><ul>
<li><p>DAG本身并不涉及如何处理数据，只是对数据数据流程的规划</p>
</li>
<li><p>所以DAG并不能改进MapReduce计算模型中原语部分的缺陷，只能
优化数据处理流程</p>
<ul>
<li>减少MapReduce作业中间结果存盘，减少磁盘I/O</li>
<li>优化map、reduce任务执行，减少同步Barrier</li>
</ul>
</li>
</ul>
<blockquote>
<p>   这也正是Tez所做的事情</p>
</blockquote>
<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T09:12:40.000Z" title="7/16/2021, 5:12:40 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">12 minutes read (About 1790 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/hbase.html">HBase</a></h1><div class="content"><h2 id="HBase简介"><a href="#HBase简介" class="headerlink" title="HBase简介"></a>HBase简介</h2><p>HBase是高可靠、高性能、面向列、可伸缩的分布式数据库系统</p>
<ul>
<li><p>利用HBase技术可以在廉价硬件上搭建大规模非结构化数据管理
集群</p>
</li>
<li><p>HBase借鉴Google Bigtable技术实现的开源软件</p>
<p>||HBase|Bigtable|
|——-|——-|——-|
|存储系统|HDFS|GFS|
|数据处理|Hadoop MapReduce|MapReduce|
|协同服务|Zookeeper|Chubby|
|RDBMS数据导入|Sqoop|-|</p>
</li>
<li><p>HBase访问接口</p>
<ul>
<li>Native Java API：常用、高效的访问方式</li>
<li>HBase Shell：HBase命令行工具，适合用于管理HBase</li>
<li>Thrift Gateway：利用Thrift序列化技术，支持C++、PHP、
Python多种语言异构系统访问HBase表数据</li>
<li>REST Gateway：支持REST风格的Http API访问HBase</li>
<li>Pig：支持Pig Latin语言操作HBase中数据<ul>
<li>最终被转换为MapReduce Job处理HBase表数据</li>
<li>适合做数据统计</li>
</ul>
</li>
<li>Hive：支持用户使用HiveQL访问HBase</li>
</ul>
</li>
<li><p>可以在HBase系统上运行MapReduce作业，实现数据批处理
<img src="/imgs/hbase_mapreduce.png" alt="hbase_mapreduce"></p>
</li>
</ul>
<h2 id="HBase数据结构"><a href="#HBase数据结构" class="headerlink" title="HBase数据结构"></a>HBase数据结构</h2><p><img src="/imgs/hbase_storage_structure.png" alt="hbase_storage_structure"></p>
<h3 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h3><p>HBase的表格，类似关系型数据库中的表格，但有所不同</p>
<h4 id="特殊Table"><a href="#特殊Table" class="headerlink" title="特殊Table"></a>特殊Table</h4><p>HBase中有两张特殊的Table</p>
<ul>
<li><code>.META.</code>：记录用户表Region信息，自身可以有多个region</li>
<li><code>-ROOT-</code>：记录<code>.META.</code>表Region信息的，自身只能有一个
region</li>
</ul>
<h3 id="Row-Key"><a href="#Row-Key" class="headerlink" title="Row Key"></a>Row Key</h3><p>行键，Table行主键，Table记录按照此排序</p>
<h3 id="Column、Column-Family"><a href="#Column、Column-Family" class="headerlink" title="Column、Column Family"></a>Column、Column Family</h3><ul>
<li>Table在水平方向由一个或多个列簇组成</li>
<li>一个列簇可以由任意多个Column组成</li>
<li>列簇支持动态扩展，无需预先定义列数量、类型</li>
<li>所有列均义二进制格式存储，用户需要自行进行类型转换</li>
</ul>
<h3 id="Timestamp"><a href="#Timestamp" class="headerlink" title="Timestamp"></a>Timestamp</h3><p>时间戳：每次数据操作对应的时间戳，可视为是数据的版本号</p>
<h3 id="Region"><a href="#Region" class="headerlink" title="Region"></a>Region</h3><p>Table记录数不断增加而变大后，逐渐分裂出的多个split</p>
<ul>
<li>每个region由<code>[startkey, endkey)</code>表示</li>
<li>不同region被Master分配给相应RegionServer进行管理（存储）</li>
</ul>
<h2 id="HBase系统架构"><a href="#HBase系统架构" class="headerlink" title="HBase系统架构"></a>HBase系统架构</h2><p><img src="/imgs/hbase_structure.png" alt="hbase_structure"></p>
<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><ul>
<li>HBase Client使用HBase RPC机制同HMaster、HRegionServer
进行通信<ul>
<li>对于管理类操作，通过RPC机制访问HMaster</li>
<li>对于读写操作，通过RPC机制访问HRegionServer</li>
</ul>
</li>
</ul>
<h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><ul>
<li><p>Zookeeper Quorum中记录<code>-ROOT</code>表的位置</p>
<ul>
<li>客户端访问数据之前首先访问zookeeper</li>
<li>然访问<code>-ROOT-</code>表</li>
<li>然后访问<code>.META.</code>表</li>
<li>最后根据用户数据位置，访问具体数据</li>
</ul>
</li>
<li><p>Zookeeper Quorum中存储有HMaster地址</p>
</li>
<li><p>HRegionServer把自己义Ephemeral方式注册到Zookeeper中，
使得HMaster可以随时感知各个HRegionServer健康状态</p>
</li>
<li><p>引入Zookeeper，避免了HMaster单点失败问题</p>
<ul>
<li>HBase中可以启动多个HMaster</li>
<li>通过Zookeeper的Master Election机制保证总有一个Master
运行</li>
</ul>
</li>
</ul>
<h3 id="HMaster"><a href="#HMaster" class="headerlink" title="HMaster"></a>HMaster</h3><p>HMaster在功能上主要负责Table、Region管理工作</p>
<ul>
<li>管理用户对Table增、删、查、找操作？？？</li>
<li>管理HRegionServer负载均衡，调整Region分布</li>
<li>在Region分裂后，负责新Region分配</li>
<li>在HRegionServer停机后，负责失效HRegionServer上region迁移</li>
</ul>
<h3 id="HRegionServer"><a href="#HRegionServer" class="headerlink" title="HRegionServer"></a>HRegionServer</h3><p>HRegionServer负责响应用户I/O请求，向HDFS文件系统写数据，是
HBase中最核心的模块</p>
<p><img src="/imgs/hbase_hregion_server_structure.png" alt="hbase_hregion_server_structure"></p>
<h4 id="HRegion"><a href="#HRegion" class="headerlink" title="HRegion"></a>HRegion</h4><p>HRegionServer内部管理一系列HRegion对象</p>
<ul>
<li>HRegion对象对应Table中一个Region</li>
<li>HRegion由多个HStore组成</li>
</ul>
<h4 id="HStore"><a href="#HStore" class="headerlink" title="HStore"></a>HStore</h4><p>每个HStore对应Table中一个列簇的存储，是HBase存储核心模块</p>
<ul>
<li><p>由此可以看出列簇就是一个集中存储单元</p>
<ul>
<li>因此最好将具备共同IO特性的列放在同一个列簇中，可以
提高IO效率</li>
</ul>
</li>
<li><p>HStore由两部分构成</p>
<ul>
<li>MemStore</li>
<li>StoreFile：底层实现是HFile，是对HFile的轻量级包装</li>
</ul>
</li>
</ul>
<h5 id="MemStore"><a href="#MemStore" class="headerlink" title="MemStore"></a>MemStore</h5><p>Sorted memory buffer，用户写入数据首先放入MemStore中，满了
后Flush成一个StoreFile</p>
<h5 id="StoreFile"><a href="#StoreFile" class="headerlink" title="StoreFile"></a>StoreFile</h5><ul>
<li><p>文件数量增长到一定阈值时会触发Compact合并操作，将多个
StoreFile合并成一个StoreFile</p>
<ul>
<li>合并过程中会进行版本合并、数据删除</li>
<li>即HBase其实只有增加数据，所有更新、删除操作都是后续
Compact过程中进行的</li>
<li>这使得用户写操作只要进入内存就可以立即返回，保证
HBase IO高性能</li>
</ul>
</li>
<li><p>Compact操作会逐步形成越来越大的StoreFile，超过阈值之后
会触发Split操作</p>
<ul>
<li>当前Region分裂成2个Region</li>
<li>父Region下线</li>
<li>新分裂出的2个子Region会被HMaster分配到相应的
HRegionServer上，实现负载均衡</li>
</ul>
</li>
</ul>
<h4 id="HLog"><a href="#HLog" class="headerlink" title="HLog"></a>HLog</h4><p>每个<strong>HRegionServer</strong>中都有一个HLog对象，避免因为分布式系统
中节点宕机导致的MemStore中内存数据丢失</p>
<ul>
<li><p>HLog是实现<code>WriteAheadLog</code>的类</p>
</li>
<li><p>HLog作用</p>
<ul>
<li>每次用户写入MemStore时，也会写入一份数据至HLog文件中</li>
<li>HLog定时删除已持久化到StoreFile中的数据</li>
</ul>
</li>
<li><p>HRegion意外终止后，HMaster会通过zookeeper感知到</p>
<ul>
<li>HMaster首先处理遗留的HLog文件，将其中不同Region的Log
数据进行拆分，分别放到相应Region目录下</li>
<li>然后将失效Region重新分配</li>
<li>领取到Region的HRegionServer在Load Region过程中，会
发现有历史HLog需要处理，会Replay HLog中的数据到
MemStore中，然后flush到StoreFile中，完成数据恢复</li>
</ul>
</li>
</ul>
<h2 id="HBase存储"><a href="#HBase存储" class="headerlink" title="HBase存储"></a>HBase存储</h2><p>HBase中所有数据存储在HDFS中</p>
<h3 id="HFile"><a href="#HFile" class="headerlink" title="HFile"></a>HFile</h3><p>HFile是Hadoop二进制格式文件，实现HBase中<em>Key-Value</em>数据存储</p>
<ul>
<li>HFile是不定长的，长度固定的只有：Trailer、FileInfo</li>
</ul>
<p><img src="/imgs/hbase_hfile_storage_formation.png" alt="hbase_hfile_storage_formation"></p>
<h4 id="Trailer"><a href="#Trailer" class="headerlink" title="Trailer"></a>Trailer</h4><p>含有指针指向其他数据块起点</p>
<h4 id="FileInfo"><a href="#FileInfo" class="headerlink" title="FileInfo"></a>FileInfo</h4><p>记录文件的一些元信息，如</p>
<ul>
<li>AVG_KEY_LEN</li>
<li>AVG_VALUE_LEN</li>
<li>LAST_KEY</li>
<li>COMPARATOR</li>
<li>MAX_SEQ_ID_KEY</li>
</ul>
<h4 id="Data-Index"><a href="#Data-Index" class="headerlink" title="Data Index"></a>Data Index</h4><p>记录每个Data块起始点</p>
<h4 id="Meta-Index"><a href="#Meta-Index" class="headerlink" title="Meta Index"></a>Meta Index</h4><p>记录每个Meta块起始点</p>
<h4 id="Data-Block"><a href="#Data-Block" class="headerlink" title="Data Block"></a>Data Block</h4><p>Data Block是HBase IO基本单元</p>
<ul>
<li><p>为了提高效率，HRegionServer中实现了基于LRU的Block Cache
机制</p>
</li>
<li><p>Data块大小可以在创建Table时通过参数指定</p>
<ul>
<li>较大的块有利于顺序Scan</li>
<li>较小的块有利于随机查询</li>
</ul>
</li>
<li><p>Data块除了开头的Magic信息外，就是一个个<code>&lt;key, value&gt;</code>
键值对拼接而成</p>
</li>
<li><p>Magic内容就是一些随机数字，防止数据损坏</p>
</li>
<li><p>每个键值对就是简单的byte array，但是包含很多项，且有固定
结构
<img src="/imgs/hbase_hfile_datablock_kv.png" alt="hbase_hfile_datablock_kv"></p>
<ul>
<li>开头是两个固定长度的数值，分别表示key、value长度</li>
<li>然后是key部分<ul>
<li>固定长度数值表示RowKey长度</li>
<li>RowKey</li>
<li>固定长度数值表示Column Family的长度</li>
<li>Column Family</li>
<li>Qualifier</li>
<li>固定长度数值表示：Timestamp、KeyType（Put/Delete）</li>
</ul>
</li>
<li>Value部分就是二进制数据</li>
</ul>
</li>
</ul>
<h3 id="HLogFile"><a href="#HLogFile" class="headerlink" title="HLogFile"></a>HLogFile</h3><p>HBase中Write Ahead Log存储格式，本质上是Hadoop Sequence File</p>
<ul>
<li><p>Sequence File的Key是HLogKey对象，记录了写入数据的归属信息</p>
<ul>
<li>table</li>
<li>region</li>
<li>squecence number：起始值为0，或最近一次存入文件系统
中的squence number</li>
<li>timestamp：写入时间</li>
</ul>
</li>
<li><p>Squence File的Value是KeyValue对象，即对应HFile中KeyValue</p>
</li>
</ul>
<p>e</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T09:15:35.000Z" title="7/16/2021, 5:15:35 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">27 minutes read (About 4104 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/hdp_tools.html">Zookeeper</a></h1><div class="content"><p>Zookeeper是一个协调软件服务，用于构建可靠的、分布式群组</p>
<ul>
<li><p>提供群组成员维护、领导人选举、工作流协同、分布式系统同步
、命名、配置信息维护等服务</p>
</li>
<li><p>提供广义的分布式数据结构：锁、队列、屏障、锁存器</p>
</li>
<li><p>Zookeeper促进户端间的松耦合，提供最终一致的、类似传统
文件系统中文件、目录的Znode视图，提供基本操作，如：创建
、删除、检查Znode是否存在</p>
</li>
<li><p>提供事件驱动模型，客户端能观察到Znode的变化</p>
</li>
<li><p>Zookeeper运行多个Zookeeper Ensemble以获得高可用性，每个
服务器上的Ensemble都持有分布式系统内存副本，为客户端读取
请求提供服务</p>
</li>
</ul>
<p><img src="/imgs/zookeeper_structure.png" alt="zookeeper_structure"></p>
<h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><p>Flume是分布式日志收集系统，收集日志、事件等数据资源，并集中
存储</p>
<h2 id="Flume组件、结构"><a href="#Flume组件、结构" class="headerlink" title="Flume组件、结构"></a>Flume组件、结构</h2><ul>
<li><p>旧版本组件、结构
<img src="/imgs/flume_structure_old_version.png" alt="flume_struture_old_version"></p>
</li>
<li><p>新版本组件、结构：每个Flume整体称为Agent
<img src="/imgs/flume_dataflow.png" alt="flume_dataflow"></p>
<ul>
<li>两个版本的组件功能、数据流结构都有区别</li>
<li>但是3大组件基本可以一一对应（功能略有差异）</li>
</ul>
</li>
<li><p>Agent是一组独立的JVM守护进程，从客户端、其他Agent接收
数据、迅速传递给下个目的节点</p>
</li>
<li><p>支持多路径流量、多管道接入流量、多管道接出流量、上下文
路由</p>
</li>
</ul>
<h3 id="Source（Agent）"><a href="#Source（Agent）" class="headerlink" title="Source（Agent）"></a>Source（Agent）</h3><p>采集数据，是Flume产生数据流的地方</p>
<ul>
<li><p>运行在数据发生器所在的服务器上，接收数据发生器接受数据，
将数据以event格式传递给一个或多个Channel</p>
</li>
<li><p>支持多种数据接收方式</p>
<ul>
<li>Avro Source：支持Avro RPC协议，内置支持</li>
<li>Thrift Source：支持Thrift协议</li>
<li>Exec Source：支持Unix标准输出</li>
<li>JMS Source：从JMS（消息、主题）读取数据</li>
<li>Spooling Directory Source：监控指定目录内数据变更</li>
<li>Twitter 1% firehose Source：通过API持续下载Twitter
数据</li>
<li>Netcat Source：监控端口，将流经端口的每个文本行数据
作为Event输入</li>
<li>Sequence Generator Source：序列生成器数据源</li>
<li>HTTP Source：基于POST、GET方式数据源，支持JSON、BLOB
格式</li>
</ul>
</li>
<li><p>收集数据模式</p>
<ul>
<li>Push Source：外部系统主动将数据推送到Flume中，如
RPC、syslog</li>
<li>Polling Source：Flume主动从外部系统获取数据，如
text、exec</li>
</ul>
</li>
</ul>
<h3 id="Channel-（Collector）"><a href="#Channel-（Collector）" class="headerlink" title="Channel （Collector）"></a>Channel （Collector）</h3><p>暂时的存储容器，缓存接收到的event格式的数据，直到被sink消费</p>
<ul>
<li><p>在source、sink间起桥梁作用</p>
</li>
<li><p>Channel基于事务传递Event，保证数据在收发时的一致性</p>
</li>
<li><p>Channel可以和任意数量source、sink连接</p>
</li>
<li><p>主要Channel类型有</p>
<ul>
<li>JDBC channel：数据持久化在数据库中，内置支持Derby</li>
<li>File Channel：数据存储在磁盘文件中</li>
<li>Memory Channel：数据存储在内存中</li>
<li>Spillable Meemory Channel：优先存在内存中，内存队列
满则持久到磁盘中</li>
<li>Custom Channel：自定义Channel实现</li>
</ul>
</li>
</ul>
<h3 id="Sink（Storage-Tier）"><a href="#Sink（Storage-Tier）" class="headerlink" title="Sink（Storage Tier）"></a>Sink（Storage Tier）</h3><p>将从Channel接收数据存储到集中存储器中（HDFS、HBase）</p>
<h2 id="Flume-Event"><a href="#Flume-Event" class="headerlink" title="Flume Event"></a>Flume Event</h2><p>Flume事件是内部数据传输最小基本单元、事务处理基本单位</p>
<ul>
<li>由一个装载数据的byte array、可选header构成<ul>
<li>数据对Flume是不透明的</li>
<li>header是容纳键值对字符串的无需集合，键在集合内唯一</li>
<li>header可以在上下文路由中使用扩展，如：数据清洗</li>
</ul>
</li>
<li>Event将传输数据进行封装</li>
</ul>
<h2 id="Flume架构特性（旧版）"><a href="#Flume架构特性（旧版）" class="headerlink" title="Flume架构特性（旧版）"></a>Flume架构特性（旧版）</h2><h3 id="Reliablity"><a href="#Reliablity" class="headerlink" title="Reliablity"></a>Reliablity</h3><p>Flume提供了3种数据可靠性选项</p>
<ul>
<li><p>End-to-End：使用磁盘日志、接受端Ack的方式，保证Flume接收
数据最终到导致目的地</p>
</li>
<li><p>Store on Failure：目的地不可用时，将数据保存在本地硬盘，
但进程如果出问题，可能丢失部分数据（发送后目的地不可用）</p>
</li>
<li><p>Best Effort：不做任何QoS保证</p>
</li>
</ul>
<h3 id="Scalability"><a href="#Scalability" class="headerlink" title="Scalability"></a>Scalability</h3><p>易扩展性</p>
<ul>
<li>Flume三大组件都是可伸缩的</li>
<li>Flume对事件的处理不需要带状态，Scalability容易实现</li>
</ul>
<h3 id="Avaliablity"><a href="#Avaliablity" class="headerlink" title="Avaliablity"></a>Avaliablity</h3><p>高可用性：Flume引入Zookeeper用于保存配置数据</p>
<ul>
<li>Zookeeper本身可以保证配置数据一致性、高可用</li>
<li>在配置数据发生变化时，Zookeeper通知Flume Master节点
Flume Master节点通过gossip协议同步数据</li>
</ul>
<p><img src="/imgs/flume_distributed_deployment.png" alt="flume_distributed_deployment"></p>
<h3 id="Manageablity"><a href="#Manageablity" class="headerlink" title="Manageablity"></a>Manageablity</h3><p>易管理性</p>
<ul>
<li>多个Master，保证可以管理大量节点</li>
</ul>
<h3 id="Extensibility"><a href="#Extensibility" class="headerlink" title="Extensibility"></a>Extensibility</h3><p>可开发性：可以基于Java为Flume添加各种新功能</p>
<ul>
<li>实现<code>Source</code>子类，自定义数据接入方式</li>
<li>实现<code>Sink</code>子类，将数据写入特定目标</li>
<li>实现<code>SinkDecorator</code>子类，对数据进行一定的预处理</li>
</ul>
<h2 id="适合场景"><a href="#适合场景" class="headerlink" title="适合场景"></a>适合场景</h2><ul>
<li>高效率的从多个网站服务器收集日志信息存储在HDFS上</li>
<li>将从多个服务器获取的数据迅速移交给Hadoop</li>
<li>可以收集社交网络站点事件数据，如：facebook、amazon</li>
</ul>
<h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><p>分布式、分区的、可复制的Message System（提交日志服务），
得益于特有的设计，Kafka具有高性能、高可扩展的特点</p>
<ul>
<li>完全分布式系统，易于横向扩展、处理极大规模数据</li>
<li>同时为发布、订阅提供极高吞吐能力</li>
<li>支持多订阅，出现失败状态时，可以自动平衡消费者</li>
<li>将消息持久化到磁盘，保证消息系统可靠性，可用于消息
批量消费应用（ETL系统）、实时应用</li>
</ul>
<h2 id="Kafka组件"><a href="#Kafka组件" class="headerlink" title="Kafka组件"></a>Kafka组件</h2><p><img src="/imgs/kafka_structure.png" alt="kafka_structure"></p>
<h3 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h3><p>话题：特定类型的消息流</p>
<ul>
<li><p>话题是消息的分类机制</p>
<ul>
<li>消息产生器向Kafka发布消息必须指定话题</li>
</ul>
</li>
<li><p>Kafka安照Topic维护接收到的消息</p>
<ul>
<li>话题被划分为一系列分区</li>
<li>Kafka集群为每个Topic维护一个分区日志文件存储消息</li>
</ul>
</li>
</ul>
<blockquote>
<p>消息是字节的Payload（有效载荷）</p>
</blockquote>
<h3 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h3><p>生产者：向Kafka发布消息的进程</p>
<ul>
<li>生产者需要指定消息分配至哪个分区<ul>
<li>采用Round-Robin方式方便均衡负载</li>
<li>根据应用的语义要求，设置专用Partition Function进行
消息分区</li>
</ul>
</li>
</ul>
<h3 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h3><p>代理：AMQP客户端，保存已经发布消息的服务器进程</p>
<blockquote>
<p>   AMQP：the Advanced Message Queuing Protocal，标准开放
    的应用层消息中间件协议。AMQP定义了通过网络发送的字节流
    的数据格式，兼容性非常好，任何实现AMQP协议的程序可以和
    兼容AMQP协议兼容的其他应用程序交互，容易做到跨语言、
    跨平台。</p>
</blockquote>
<ul>
<li><p>一组代理服务器构成Kafka集群</p>
</li>
<li><p>Kafka代理是无状态的，消费者需要自行维护已消费状态信息</p>
<ul>
<li><p>因此Kafka无法知晓信息是否已经被消费、应该删除，因此
代理使用简单的、基于时间的Serice Level Agreement应用
于保留策略，消息在代理中超过一定时间自动删除</p>
</li>
<li><p>这种设计允许消费者可以重复消费已消费数据</p>
<ul>
<li>虽然违反队列常见约定</li>
<li>但是实际应用中很多消费者有这种特征</li>
</ul>
</li>
</ul>
</li>
<li><p>消息代理将紧密耦合的系统设计解耦，可以对未及时处理的消息
进行缓存</p>
<ul>
<li>提高了吞吐能力</li>
<li>提供了分区、复制、容错支持</li>
</ul>
</li>
<li><p>Kafka代理通过Zookeeper与其他Kafka代理协同
<img src="/imgs/kafka_with_zookeeper.png" alt="kafka_with_zookeeper"></p>
<ul>
<li>系统中新增代理或代理故障失效时，Zookeeper通知生产者
、消费者</li>
<li>生产者、消费者据此开始同其他代理协同工作</li>
</ul>
</li>
</ul>
<h3 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h3><p>消费者：向Kafka subscribe话题，以处理Kafka消息的进程</p>
<ul>
<li><p>消费者可以订阅一个或多个话题，从代理拉取数据，消费已经
发布的消息</p>
</li>
<li><p>消费者获取消息系统一般采用两种模型</p>
<ul>
<li><p>Queuing：队列模型，一组消费者从一个服务器读取信息，
每个消息仅可被其中一个消费者消费</p>
</li>
<li><p>Publish Subscribe：发布订阅模型，消息被广播给所有
消费者</p>
</li>
</ul>
</li>
<li><p>Kafka采用一种抽象方法：消费者组Consumer Group提供对上述
两种消息系统模型的支持
<img src="/imgs/kafka_comsumer_group.png" alt="kafka_comsumer_group"></p>
<ul>
<li><p>给每个消费者打上属于某个消费者组的标签（这里组只是
表示同组内消费者只能有一个消费信息）</p>
</li>
<li><p>每个发布到话题的消息分发给消费者组的其中一个消费者</p>
</li>
<li><p>一般情况下每个话题下有多个消费者组，每个组中有多个
消费者实例，以达到扩展处理能力、容错</p>
</li>
<li><p>极端情况：如果所有消费者实例都隶属于同一个消费者组，
Kafka工作模式类似于队列模型；所有消费者实例隶属于
不同的消费者组，Kafka工作模式类似于发布-订阅模型</p>
</li>
</ul>
</li>
</ul>
<h2 id="消息分区、存储、分发"><a href="#消息分区、存储、分发" class="headerlink" title="消息分区、存储、分发"></a>消息分区、存储、分发</h2><h3 id="分区日志"><a href="#分区日志" class="headerlink" title="分区日志"></a>分区日志</h3><p>每个分区是<strong>有序的</strong>、<strong>不可更改</strong>、<strong>可在末尾不断追加</strong>的
消息序列</p>
<h4 id="分区优势"><a href="#分区优势" class="headerlink" title="分区优势"></a>分区优势</h4><ul>
<li><p>允许Kafka处理超过一台服务器容量的日志规模</p>
</li>
<li><p>分区作为并行处理基本单元，允许Kafka进行并行处理</p>
</li>
<li><p>通过保证每个分区仅仅<strong>由一个消费者消费</strong>，可以保证同一
分区内消息消费的有序</p>
<ul>
<li>由于可以设置很多分区，仍然可以保证在不同消费者之间
实现负载均衡</li>
<li>分区内外保证消息有序、数据分区处理对大部分实际应用
已经足够</li>
</ul>
</li>
</ul>
<h4 id="分区管理"><a href="#分区管理" class="headerlink" title="分区管理"></a>分区管理</h4><p>每个分区由单独的（一组）服务器处理，负责该分区数据管理、消息
请求，支持多个副本以支持容错</p>
<ul>
<li><p>每个分区中有一台服务器作为leader、若干服务器作为follower</p>
</li>
<li><p>领导者负责分区读、写请求，跟随者以被动的方式领导者数据
进行复制</p>
</li>
<li><p>领导者失败，则追随者之一在Zookeeper协调下成为新领导者</p>
</li>
<li><p>为保证负载均衡，每个服务器担任部分分区领导者、其他分区
追随者</p>
</li>
</ul>
<h3 id="存储布局"><a href="#存储布局" class="headerlink" title="存储布局"></a>存储布局</h3><p>Kafka存储布局非常简单</p>
<p><img src="/imgs/kafka_storage.png" alt="kafka_storage"></p>
<h4 id="分区存储"><a href="#分区存储" class="headerlink" title="分区存储"></a>分区存储</h4><ul>
<li><p>话题每个分区对应一个逻辑日志</p>
</li>
<li><p>每个日志为相同的大小的一组分段文件</p>
</li>
<li><p>生产者发布的消息被代理追加到对应分区最后一个段文件中</p>
</li>
<li><p>发布消息数量达到设定值、经过一段时间后，段文件真正写入
磁盘，然后公开给消费者</p>
</li>
</ul>
<h4 id="Offset"><a href="#Offset" class="headerlink" title="Offset"></a>Offset</h4><p>分区中每个消息的Sequential ID Number（Offset），唯一标识
分区中消息，并没有明确的消息ID</p>
<ul>
<li><p>偏移量是增量的但不连续，下个消息ID通过在其偏移量加上
消息长度得到</p>
</li>
<li><p>偏移量标识每个消费者目前处理到某分区消息队列的位置，
对分区消息队列处理依赖于其（消息通过日志偏移量公开）</p>
</li>
<li><p>偏移量由消费者控制，所以消费者可以以任何顺序消费消息</p>
<ul>
<li>可以回推偏移量重复消费消息</li>
<li>设计消费者仅仅查看分区末尾若干消息，不改变消息，
其他消费者可以正常的消费</li>
</ul>
</li>
</ul>
<p>从消息分区机制、消费者基于偏移量消费机制，可以看出Kafka消息
消费机制不会对集群、其他消费者造成影响</p>
<h2 id="适合场景-1"><a href="#适合场景-1" class="headerlink" title="适合场景"></a>适合场景</h2><ul>
<li><p>Messaging：消息传递，作为传递消息队列（ActiveMQ、
RabbitMQ等）替代品，提供高吞吐能力、高容错、低延迟</p>
</li>
<li><p>Website Activity Tracking：网站活动跟踪，要求系统必须
快速处理产生消息</p>
</li>
<li><p>Metric：度量，把分布式各个应用程序的运营数据集中，进行
汇总统计</p>
</li>
<li><p>Streaming Processing：流数据处理</p>
</li>
<li><p>Event Sourcing：事件溯源，把应用程序状态变化以时间顺序
存储，需要支持大量数据</p>
</li>
<li><p>Commit Log：日志提交，作为分布式系统提交日志的外部存储
服务</p>
</li>
</ul>
<h1 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h1><p>Storm是分布式、高容错的实时流数据处理的开源系统</p>
<ul>
<li>Storm为流数据处理设计，具有很高的容错性</li>
<li>Storm保证每个消息只能得到一次完整处理，任务失败时会负责
从消息源重试消息，从而支持可靠的消息处理</li>
<li>可以通过实现Storm通讯协议，提供其他语言支持</li>
</ul>
<h2 id="Storm架构"><a href="#Storm架构" class="headerlink" title="Storm架构"></a>Storm架构</h2><p><img src="/imgs/storm_structure.png" alt="storm_structure"></p>
<ul>
<li><p>主节点的运行Nimbus守护进程</p>
<ul>
<li>分配代码</li>
<li>布置任务</li>
<li>故障检测</li>
</ul>
</li>
<li><p>工作节点运行Supervisor守护进程</p>
<ul>
<li>监听、开始、终止工作进程</li>
</ul>
</li>
<li><p>Nimbus、Supervisor都是无状态的（不负责维护客户端两次调用
之间状态维护）</p>
<ul>
<li>这使得两者十分健壮</li>
<li>两者之间的协调由Zookeeper完成</li>
</ul>
</li>
<li><p>Storm在ZeorMQ内部传递消息</p>
</li>
</ul>
<h3 id="Nimbus"><a href="#Nimbus" class="headerlink" title="Nimbus"></a>Nimbus</h3><h3 id="Supervisor"><a href="#Supervisor" class="headerlink" title="Supervisor"></a>Supervisor</h3><h3 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h3><h2 id="Storm编程模型"><a href="#Storm编程模型" class="headerlink" title="Storm编程模型"></a>Storm编程模型</h2><h3 id="Stream"><a href="#Stream" class="headerlink" title="Stream"></a>Stream</h3><p>数据流：没有边界的tuple序列</p>
<ul>
<li>这些tuple以分布式的方式，并行的创建、处理</li>
</ul>
<h3 id="Topology"><a href="#Topology" class="headerlink" title="Topology"></a>Topology</h3><p>计算拓扑：实时计算应用程序处理逻辑封装成的Topology对象</p>
<ul>
<li>相当于Mapreduce作业，但是MapReduce作业最终会结束、而
Topology会一直运行直到被杀死</li>
<li>Topology由Spout、Bolt组成</li>
</ul>
<h4 id="Spout"><a href="#Spout" class="headerlink" title="Spout"></a>Spout</h4><p>消息源：消息tuple生产者</p>
<ul>
<li>消息源可以是可靠的、不可靠的</li>
<li>可靠的消息源可在tuple没有被storm成功处理时，可以重新发送</li>
<li>不可靠的消息源则在发送tuple之后彻底丢弃</li>
</ul>
<h4 id="Bolt"><a href="#Bolt" class="headerlink" title="Bolt"></a>Bolt</h4><p>消息处理者：封装所有的消息处理逻辑</p>
<ul>
<li>Bolt可以做很多事情，包括过滤、聚集</li>
<li>Bolt一般数据处理流程<ul>
<li>处理一个输入tuple，发送0个、多个tuple</li>
<li>调用ack接口，通知storm子集已经处理过了</li>
</ul>
</li>
</ul>
<h4 id="Task、Executor"><a href="#Task、Executor" class="headerlink" title="Task、Executor"></a>Task、Executor</h4><p>Topology每个Spout、Bolt转换为若干个任务在整个集群里执行</p>
<ul>
<li>默认情况下，每个Task对应一个线程Executor，线程用于执行
task</li>
<li>同一个Spout/Bolt里的Task共享一个物理线程</li>
</ul>
<h4 id="Stream-Grouping"><a href="#Stream-Grouping" class="headerlink" title="Stream Grouping"></a>Stream Grouping</h4><p>数据分发策略：定义Spout、Bolt间Tasks的数据分发</p>
<ul>
<li><p>Shuffle Grouping：洗牌式分组，上游Spout数据流tuples随机
分发到下游Bolt的Task</p>
</li>
<li><p>Fields Grouping：按指定字段进行分组</p>
</li>
<li><p>All Grouping：Spout数据tuple分发给所有下Bolt</p>
</li>
<li><p>Global Grouping：Spout数据tuple分发给最小id的task</p>
</li>
<li><p>Non-Grouping：类似shuffle Grouping，把具有Non-Grouping
设置Bolt推到其订阅的上游Spout、Bolt</p>
</li>
<li><p>Direct Grouping：tuple生产者决定接收tuple下游bolt中的task</p>
</li>
<li><p>Local or Shuffle Grouping：如果目标bolt中由一个或多个
task工作在同一进程中，tuple分配给这些task，否则同洗牌式
分组</p>
</li>
<li><p>Partial Key Grouping：类似Fields Grouping，但是在下游
Bolt中做负载均衡，提高资源利用率</p>
</li>
</ul>
<h3 id="消息处理保证"><a href="#消息处理保证" class="headerlink" title="消息处理保证"></a>消息处理保证</h3><p>Storm追踪由每个SpoutTuple产生的Tuple树</p>
<ul>
<li><p>每个从Spout发出tuple，可能会生成成千上万个tuple</p>
<ul>
<li>根据血缘关系形成一棵tuple树</li>
<li>当tuple树中所有节点都被成功处理了，才说明tuple被完全
处理</li>
</ul>
</li>
<li><p>每个Topology都有一个消息超时设置，如果Storm在时间内无法
检验tuple树是否完全执行，该tuple标记为执行失败，之后重发</p>
</li>
</ul>
<p>重发</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T08:18:27.000Z" title="8/4/2021, 4:18:27 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">15 minutes read (About 2247 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/hdp_family.html">Hadoop概述</a></h1><div class="content"><ul>
<li><em>Hadoop</em>（核心）：<em>HDFS</em>和<em>MapReduce/YARN</em></li>
<li><em>Hadoop家族</em>：建立在<em>Hadoop</em>基础上的一系列开源工具</li>
</ul>
<p><img src="hadoop_relations.jpg" alt="hadoop_relations"></p>
<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><p><em>Hadoop</em>时<em>Apache</em>的一个分布式计算、java语言实现的开源框架，
实现在大量计算机组成的集群中对海量数据进行分布式计算。相比于
依赖硬件的可靠性，<em>Hadoop</em>被设计为可以检测、处理应用层面的
failures，能够提供构建于电脑集群上的可靠服务。</p>
<p><em>Hadoop</em>：<em>Apache</em>的分布式计算开源框架，提供分布式文件系统
<em>HDFS</em>、<em>MapReduce/YARN</em>分布式计算的软件架构</p>
<h3 id="Hadoop-Common"><a href="#Hadoop-Common" class="headerlink" title="Hadoop Common"></a>Hadoop Common</h3><p>支持其它<em>Hadoop</em>模块的公用组件</p>
<h3 id="Hadoop-Distributed-File-System-HDFS"><a href="#Hadoop-Distributed-File-System-HDFS" class="headerlink" title="Hadoop Distributed File System(HDFS)"></a>Hadoop Distributed File System(HDFS)</h3><p>虚拟文件系统，让整个系统表面上看起来是一个空间，实际上是很多
服务器的磁盘构成的</p>
<h3 id="Hadoop-YARN"><a href="#Hadoop-YARN" class="headerlink" title="Hadoop YARN"></a>Hadoop YARN</h3><p>Yet Another Resource Negotiator，通用任务、集群资源分配框架
，面向<em>Hadoop</em>的编程模型</p>
<ul>
<li><p>YARN将classic/MapReduce1中Jobtracker职能划分为多个独立
实体，改善了其面临的扩展瓶颈问题</p>
</li>
<li><p>YARN比MapReduce更具一般性，MapReduce只是YARN应用的一种
形式，可以运行Spark、Storm等其他通用计算框架</p>
</li>
<li><p>YARN精妙的设计可以让不同的YARN应用在同一个集群上共存，
如一个MapReduce应用可以同时作为MPI应用运行，提高可管理性
和集群利用率</p>
</li>
</ul>
<h3 id="Hadoop-MapReduce"><a href="#Hadoop-MapReduce" class="headerlink" title="Hadoop MapReduce"></a>Hadoop MapReduce</h3><p><em>YARN</em>基础上的大数据集并行处理系统（框架）</p>
<ul>
<li><p>包括两个阶段</p>
<ul>
<li><em>Map</em>：映射</li>
<li><em>Reduce</em>：归一</li>
</ul>
</li>
<li><p>在分布式系统上进行计算操作基本都是由Map、Reduce概念步骤
组成</p>
<ul>
<li>分布式系统，不像一般的数据库、文件系统，无法从上至下
、从头到尾进行求和等操作</li>
<li>需要由分散的节点不断向一个点聚拢的计算过程</li>
</ul>
</li>
<li><p>不适合实时性要求的应用，只适合大数据离线处理</p>
</li>
</ul>
<h2 id="Apache下Hadoop相关项目"><a href="#Apache下Hadoop相关项目" class="headerlink" title="Apache下Hadoop相关项目"></a>Apache下<em>Hadoop</em>相关项目</h2><h3 id="高频"><a href="#高频" class="headerlink" title="高频"></a>高频</h3><h4 id="Ambari"><a href="#Ambari" class="headerlink" title="Ambari"></a>Ambari</h4><p>用于部署（供应）、管理、监控<em>Hadoop</em>集群的Web工具</p>
<ul>
<li><p>支持<em>HDFS</em>、<em>MapReduce</em>、<em>Hive</em>、<em>HCatalog</em>、<em>HBase</em>、
<em>Oozie</em>、<em>ZooKeeper</em>、<em>Pig</em>、<em>Sqoop</em></p>
</li>
<li><p>提供dashboard用于查看集群健康程度，如：热度图</p>
</li>
<li><p>能够直观的查看<em>MapReduce</em>、<em>Pig</em>、<em>Hive</em>应用特点，提供
易用的方式考察其执行情况</p>
</li>
</ul>
<h4 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a><em>HBase</em></h4><p><em>Hadoop</em>项目子项目，高可靠、高性能、面向列、可伸缩的分布式
存储系统</p>
<ul>
<li><p>该技术源于<em>Fay Chang</em>撰写的<em>Google</em>论文《Bigtable：一个
结构化数据的分布式存储系统》，类似于<em>Bigtable</em>在Google
文件系统上提供的分布式数据存储一样，<em>HBase</em>在<em>Hadoop</em>的
基础上提供了类似于<em>Bigtable</em>的能力</p>
</li>
<li><p>适合非结构化数据存储</p>
</li>
<li><p>可用于在廉价PC Server上搭建大规模结构化存储集群，是
<em>NoSQL</em>数据库的两个首选项目（<em>MongoDB</em>）</p>
</li>
</ul>
<h4 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a><em>Hive</em></h4><p>基于<em>Hadoop</em>的数据仓库工具</p>
<ul>
<li><p>在<em>Hive</em>中建立表，将表映射为结构化数据文件</p>
</li>
<li><p>可以通过类SQL语句直接查询数据实现简单的<em>MapReduce</em>统计，
而不必开发专门的<em>MapReduce</em>应用</p>
<ul>
<li><em>Hive</em>会将SQL语句转换为<em>MapReduce</em>任务查询<em>Hadoop</em></li>
<li>速度很慢</li>
<li>适合数据仓库的统计分析</li>
<li>支持SQL语法有限</li>
</ul>
</li>
</ul>
<h4 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a><em>Pig</em></h4><p>基于<em>Hadoop</em>的大规模数据<strong>高层</strong>分析工具（类似于<em>Hive</em>）</p>
<ul>
<li><p>提供SQL-Like语言<code>PigLatin</code></p>
<ul>
<li><p>其编译器会把类SQL的数据分析请求，转换为一系列经过
优化处理的<em>MapReduce</em>运算</p>
</li>
<li><p>是一种过程语言，和<em>Hive</em>中的类SQL语句相比，更适合写
脚本，而<em>Hive</em>的类SQL语句适合直接在命令行执行</p>
</li>
</ul>
</li>
</ul>
<h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a><em>Zookeeper</em></h4><p><em>Hadoop</em>正式子项目，针对大型分布式应用设计的分布式、开源协调
系统</p>
<ul>
<li><p>提供功能：配置维护、名字服务、分布式同步、组服务</p>
</li>
<li><p>封装好复杂、易出错的关键服务，提供简单易用、功能稳定、
性能高效的接口（系统），解决分布式应用中经常遇到的数据
管理问题，简化分布式应用协调及管理难度，提供高性能分布式
服务</p>
</li>
<li><p>通常为<em>HBase</em>提供节点间的协调，部署<em>HDFS</em>的<em>HA</em>模式时是
必须的</p>
</li>
</ul>
<h4 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a><em>Spark</em></h4><p>基于内存计算的开源集群计算系统，目的是让数据分析更加快速</p>
<h3 id="低频"><a href="#低频" class="headerlink" title="低频"></a>低频</h3><h4 id="Mahout"><a href="#Mahout" class="headerlink" title="Mahout"></a><em>Mahout</em></h4><p>基于<em>Hadoop</em>的机器学习、数据挖掘的分布式框架</p>
<ul>
<li><p>使用<em>MapReduce</em>实现了部分数据挖掘算法，解决了并行挖掘问题</p>
<ul>
<li>包括聚类、分类、推荐过滤、频繁子项挖掘</li>
</ul>
</li>
<li><p>通过使用<em>Hadoop</em>库，<em>Mahout</em>可以有效扩展至云端</p>
</li>
</ul>
<h4 id="Cassandra"><a href="#Cassandra" class="headerlink" title="Cassandra"></a><em>Cassandra</em></h4><p>开源分布式<em>NoSQL</em>数据库系统，最初由<em>Facebook</em>开发，用于存储
简单格式数据，集<em>Google BigTable</em>数据模型和<em>Amazon Dynamo</em>
的完全分布式架构于一身</p>
<h4 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a><em>Avro</em></h4><p>数据序列化系统，设计用于支持数据密集型、大批量数据交换应用，
是新的数据序列化格式、传输工具，将逐步取代<em>Hadoop</em>原有的
<code>IPC</code>机制</p>
<h4 id="Chukwa"><a href="#Chukwa" class="headerlink" title="Chukwa"></a><em>Chukwa</em></h4><p>用于监控大型分布式系统的开源数据收集系统，可以将各种类型的
数据收集成适合Hadoop处理的文件，保存在<em>HDFS</em>中供<em>MapReduce</em>
操作</p>
<h4 id="Tez"><a href="#Tez" class="headerlink" title="Tez"></a><em>Tez</em></h4><p>基于<em>YARN</em>的泛用数据流编程平台</p>
<ul>
<li>提供强力、灵活的引擎用于执行任何<em>DAG</em>任务，为批处理和
交互用例处理数据</li>
</ul>
<p><em>Tez</em>正逐渐被<em>Hive</em>、<em>Pig</em>等<em>Hadoop</em>生态框架采用，甚至被一些
商业公司用于替代<em>MapReduce</em>作为底层执行引擎</p>
<h2 id="其他Hadoop相关项目"><a href="#其他Hadoop相关项目" class="headerlink" title="其他Hadoop相关项目"></a>其他Hadoop相关项目</h2><h3 id="高频-1"><a href="#高频-1" class="headerlink" title="高频"></a>高频</h3><h4 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a><em>Sqoop</em></h4><p>用于将<em>Hadoop</em>和关系型数据库中数据相互转移的开源工具</p>
<ul>
<li><p>可以将关系型数据库（<em>MySQL</em>、<em>Oracle</em>、<em>Postgres</em>）中
数据转移至<em>Hadoop</em>的<em>HDFS</em>中</p>
</li>
<li><p>也可以将<em>HDFS</em>的数据转移进关系型数据库中 </p>
</li>
</ul>
<h4 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a><em>Impala</em></h4><p>由<em>Cloudera</em>发布的实时查询开源项目</p>
<ul>
<li><p>模仿<em>Google Dremel</em></p>
</li>
<li><p>称比基于<em>MapReduce</em>的<em>Hive SQL</em>查询速度提升3~30倍，更加
灵活易用</p>
</li>
</ul>
<h4 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a><em>Phoenix</em></h4><p><em>apache</em>顶级项目，在<em>HBase</em>上构建了一层关系型数据库，可以用
SQL查询<em>HBase</em>数据库，且速度比<em>Impala</em>更快，还支持包括
二级索引在内的丰富特性，借鉴了很多关系型数据库优化查询方法</p>
<h4 id="Oozie"><a href="#Oozie" class="headerlink" title="Oozie"></a><em>Oozie</em></h4><p>工作流引擎服务器，用于管理、协调运行在<em>Hadoop</em>平台
（<em>HDFS</em>、<em>Pig</em>、<em>MapReduce</em>）的任务</p>
<h4 id="Cloudera-Hue"><a href="#Cloudera-Hue" class="headerlink" title="Cloudera Hue"></a><em>Cloudera Hue</em></h4><p>基于<em>Web</em>的监控、管理系统，实现对<em>HDFS</em>、<em>MapReduce/YARN</em>、
<em>HBase</em>、<em>Hive</em>、<em>Pig</em>的<em>Web</em>化操作和管理</p>
<h3 id="低频-1"><a href="#低频-1" class="headerlink" title="低频"></a>低频</h3><h4 id="Hama"><a href="#Hama" class="headerlink" title="Hama"></a><em>Hama</em></h4><p>基于<em>HDFS</em>的<em>BSP(Bulk Synchronous Parallel)</em>并行
计算框架，可以用包括图、矩阵、网络算法在内的大规模、
大数据计算</p>
<h4 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a><em>Flume</em></h4><p>分布的、可靠的、高可用的海量日志聚合系统，可用于日志数据
收集、处理、传输</p>
<h4 id="Giraph"><a href="#Giraph" class="headerlink" title="Giraph"></a><em>Giraph</em></h4><p>基于Hadoop的可伸缩的分布式迭代图处理系统，灵感来自于<em>BSP</em>和
<em>Google Pregel</em></p>
<h4 id="Crunch"><a href="#Crunch" class="headerlink" title="Crunch"></a><em>Crunch</em></h4><p>基于<em>Google FlumeJava</em>库编写的<em>Java</em>库，用于创建<em>MapReduce</em>
流水线（程序）</p>
<ul>
<li><p>类似于<em>Hive</em>、<em>Pig</em>，提供了用于实现如连接数据、执行聚合
、排序记录等常见任务的模式库</p>
<ul>
<li><p>但是<em>Crunch</em>不强制所有输入遵循同一数据类型</p>
</li>
<li><p>其使用一种定制的类型系统，非常灵活，能直接处理复杂
数据类型，如：时间序列、<em>HDF5</em>文件、<em>HBase</em>、序列化
对象（<em>protocol buffer</em>、<em>Avro</em>记录）</p>
</li>
</ul>
</li>
<li><p>尝试简化<em>MapReduce</em>的思考方式</p>
<ul>
<li><p><em>MapReduce</em>有很多优点，但是对很多问题，并不是合适的
抽象级别</p>
</li>
<li><p>出于性能考虑，需要将逻辑上独立的操作（数据过滤、投影
、变换）组合为一个物理上的<em>MapReduce</em>操作</p>
</li>
</ul>
</li>
</ul>
<h4 id="Whirr"><a href="#Whirr" class="headerlink" title="Whirr"></a><em>Whirr</em></h4><p>运行于云服务的类库（包括<em>Hadoop</em>），提供高度互补性</p>
<ul>
<li>相对中立</li>
<li>支持<em>AmazonEC2</em>和<em>Rackspace</em>的服务</li>
</ul>
<h4 id="Bigtop"><a href="#Bigtop" class="headerlink" title="Bigtop"></a><em>Bigtop</em></h4><p>对<em>Hadoop</em>及其周边生态打包、分发、测试的工具</p>
<h4 id="HCatalog"><a href="#HCatalog" class="headerlink" title="HCatalog"></a><em>HCatalog</em></h4><p>基于<em>Hadoop</em>的数据表、存储管理，实现中央的元数据、模式管理，
跨越<em>Hadoop</em>和<em>RDBMS</em>，利用<em>Pig</em>、<em>Hive</em>提供关系视图</p>
<h4 id="Llama"><a href="#Llama" class="headerlink" title="Llama"></a><em>Llama</em></h4><p>让外部服务器从<em>YARN</em>获取资源的框架</p>
<h3 id="非CDH组件"><a href="#非CDH组件" class="headerlink" title="非CDH组件"></a>非<em>CDH</em>组件</h3><h4 id="Fuse"><a href="#Fuse" class="headerlink" title="Fuse"></a><em>Fuse</em></h4><p>让<em>HDFS</em>系统看起来像普通文件系统</p>
<h4 id="Hadoop-Streamin"><a href="#Hadoop-Streamin" class="headerlink" title="Hadoop Streamin"></a><em>Hadoop Streamin</em></h4><p><em>MapReduce</em>代码其他语言支持，包括：<em>C/C++</em>、<em>Perl</em>、<em>Python</em>
、<em>Bash</em>等</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:08.000Z" title="2/17/2019, 11:57:08 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">20 minutes read (About 3030 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/mapred_yarn.html">MapReduce YARN</a></h1><div class="content"><h2 id="MapReduce1-0组件"><a href="#MapReduce1-0组件" class="headerlink" title="MapReduce1.0组件"></a>MapReduce1.0组件</h2><p>MapReduce1.0是指Hadoop1.0中组件，不是指MapReduce计算模型</p>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ul>
<li><p>方便扩展：能够运行在普通服务器构成的超大规模集群上</p>
</li>
<li><p>IO瓶颈：通过将IO分散在大规模集群的各个节点上，可以提高
数据装载速度（大规模数据时只有部分数据可以状态在内存中）</p>
</li>
</ul>
<h3 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h3><ul>
<li><p>MapReduce计算模型问题（参见<em>MapReduce计算模型</em>）</p>
</li>
<li><p>数据处理延迟大</p>
<ul>
<li>MapReduce作业在Map阶段、Reduce阶段执行过程中，需要
把中间结果存盘</li>
<li>在MR作业间也需要通过磁盘实现作业间的数据交换</li>
</ul>
</li>
<li><p>资源利用率低</p>
<ul>
<li>任务调度方法远未达到优化资源利用率的效果，给每个
TaskTracker分配任务的过程比较简单</li>
</ul>
</li>
</ul>
<h3 id="资源分配"><a href="#资源分配" class="headerlink" title="资源分配"></a>资源分配</h3><ul>
<li><p>每个TaskTracker拥有一定数量的slots，每个活动的Map、
Reduce任务占用一个slot</p>
</li>
<li><p>JobTracker把任务分配给最靠近数据、有slot空闲TT</p>
</li>
<li><p>不考虑Task运算量大小，所有Task视为相同，如果有某个TT
当前负载过高，会影响整体的执行</p>
</li>
<li><p>也可以通过Speculative Execution模式，在多个slave上
启动同一个任务，只要其中有一个任务完成即可</p>
</li>
</ul>
<h3 id="执行引擎"><a href="#执行引擎" class="headerlink" title="执行引擎"></a>执行引擎</h3><p>MapReduce执行引擎运行在HDFS上</p>
<ul>
<li><p>JobTracker：运行在NameNode上</p>
<ul>
<li>分解客户端提交的job为数据处理tasks，分发给集群里相关
节点上的TaskTacker运行</li>
<li>发送任务原则：尽量把任务推送到离数据最近的节点上，
甚至推送到数据所在的节点上运行</li>
</ul>
</li>
<li><p>TaskTracker：运行在DataNode上</p>
<ul>
<li>在节点上执行数据处理map、reduce tasks</li>
<li>可能需要从其他DataNode中获取需要数据</li>
</ul>
</li>
</ul>
<h2 id="MapRedudce2-0"><a href="#MapRedudce2-0" class="headerlink" title="MapRedudce2.0"></a>MapRedudce2.0</h2><h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h3><p>Shuffle：系统执行排序的过程</p>
<ul>
<li>为了确保MapReduce的输入是按键排序的</li>
</ul>
<h4 id="Map端"><a href="#Map端" class="headerlink" title="Map端"></a>Map端</h4><p>每个Map Task都有一个内存缓冲区用于存储map输出结果，缓冲区
快满时需要将缓冲区数据以临时文件方式存放到磁盘中，整个Task
结束后再对此Map Task产生所有临时作合并，生成最终正式输出文件
，等待Reduce Task拉数据</p>
<h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><p>Yet Another Resource Negotiator，通用任务、集群资源分配框架
，面向<em>Hadoop</em>的编程模型</p>
<h3 id="YARN优势"><a href="#YARN优势" class="headerlink" title="YARN优势"></a>YARN优势</h3><h4 id="扩展性"><a href="#扩展性" class="headerlink" title="扩展性"></a>扩展性</h4><ul>
<li><p>YARN将classic/MapReduce1中Jobtracker职能划分为多个独立
实体，改善了其面临的扩展瓶颈问题</p>
</li>
<li><p>MapReduce现在只是批数据处理框架，是YARN支持的数据处理
框架的一种，独立于资源管理层，单独演化、改进</p>
</li>
<li><p>YARN精妙的设计可以让不同的YARN应用在同一个集群上共存，
如一个MapReduce应用可以同时作为MPI应用运行，提高可管理性
、集群利用率</p>
</li>
</ul>
<h4 id="高效率"><a href="#高效率" class="headerlink" title="高效率"></a>高效率</h4><ul>
<li><p>ResourceManager是单独的资源管理器</p>
</li>
<li><p>Job Scheduler指负责作业调度</p>
</li>
<li><p>根据资源预留要求、公平性、Service Level Agreement等标准
，优化整个集群的资源利用</p>
</li>
</ul>
<h4 id="一般性"><a href="#一般性" class="headerlink" title="一般性"></a>一般性</h4><p>YARN是通用资源管理框架，在其上可以搭建多种数据处理框架</p>
<ul>
<li>批处理：MapReduce</li>
<li>交互式处理：Tez</li>
<li>迭代处理：Spark</li>
<li>实时流处理：Storm</li>
<li>图数据处理：GraphLab/Giraph</li>
</ul>
<h3 id="YARN中实体"><a href="#YARN中实体" class="headerlink" title="YARN中实体"></a>YARN中实体</h3><h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h4><p>RM物理上对应主节点，逻辑上管理集群上的资源使用，其功能由
Scheduler、ApplicationManager协调完成</p>
<ul>
<li><p>AppplicatonManager：接受、监控任务</p>
<ul>
<li><p>接受客户端提交的job</p>
</li>
<li><p>判断启动该job的ApplicationMaster所需的资源</p>
</li>
<li><p>监控ApplicationMaster的状态，并在失败时重启其</p>
</li>
</ul>
</li>
<li><p>Scheduler：分配资源、调度</p>
<ul>
<li><p>Schedular计算启动ApplicationManager提交的job的AM所需
资源，将资源封装成Container</p>
</li>
<li><p>然后根据调度算法调度，在某个NM上启动job的AM</p>
</li>
<li><p>不提供失败重启、监控功能</p>
</li>
<li><p>Scheduler收到AM任务完成汇报之后，回收资源、向RM返回
执行结果</p>
</li>
<li><p>调度算法可自定以，YARN根据不同场景提供</p>
<ul>
<li>FIFO Scheduler</li>
<li>Capacity Scheduler</li>
<li>Fair Scheduler</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h4><p>NM物理上对应计算节点，逻辑上<strong>监控、管理</strong>当前节点资源</p>
<ul>
<li><p>仅仅抽象本节点资源（cpu、内存、磁盘、网络等），并且定时
像RM的Scheduler汇报</p>
</li>
<li><p>接受并处理AM的tasks启动、停止等请求</p>
</li>
</ul>
<h4 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h4><p>AM管理集群上运行任务生命周期</p>
<ul>
<li><p>每个job都有一个专用的AM</p>
</li>
<li><p>AM启动后会计算job所需资源，并向Scheduler申请资源</p>
</li>
<li><p>AM运行在job运行期间，负责整个job执行过程的监控</p>
<ul>
<li>NM分配完任务container后，AM开始监控这些containers
、tasks状态</li>
<li>任务失败则回收资源重新生成</li>
<li>成功则释放资源</li>
<li>任务执行完毕后回报Scheduler</li>
</ul>
</li>
</ul>
<h4 id="Containers"><a href="#Containers" class="headerlink" title="Containers"></a>Containers</h4><p>YARN为将来的资源隔离提出的框架，是一组资源的集合，每个task
对应一个container，只能在container中运行</p>
<ul>
<li><p>容器有特定的内存分配范围</p>
<ul>
<li><p>容器内存最小值即为内存分配单位，内存最大值也应该是
内存分配单位整数倍</p>
</li>
<li><p>根据任务所需资源多少分配给容器整数倍内存单位，但是
如果任务所需内存大于容器内存最大值，运行时可能会报错</p>
</li>
</ul>
</li>
<li><p>由NM确保task使用的资源不会超过分配的资源</p>
</li>
<li><p>注意</p>
<ul>
<li>AM并不运行于container中，真正的task才运行在container</li>
</ul>
</li>
</ul>
<p><img src="/imgs/yarn_procedure.png" alt="yarn_procedure"></p>
<h3 id="Job运行过程"><a href="#Job运行过程" class="headerlink" title="Job运行过程"></a>Job运行过程</h3><h4 id="作业提交"><a href="#作业提交" class="headerlink" title="作业提交"></a>作业提交</h4><ul>
<li>从RM获取新的作业ID</li>
<li>作业客户端检查作业输出说明，计算输入分片（也可以配置
在集群中产生分片）</li>
<li>将作业资源复制到HDFS</li>
<li>调用RM上的<code>submitApplication</code>方法提交作业</li>
</ul>
<h4 id="作业初始化"><a href="#作业初始化" class="headerlink" title="作业初始化"></a>作业初始化</h4><ol>
<li>RM收到调用<code>submitApplication</code>消息后，将请求传递给
内部scheduler，scheduler分配一个container</li>
<li>NM在RM的管理下在容器中启动应用程序的master进程AM，
其对作业进行初始化</li>
<li>AM创建多个簿记对象用于接受任务进度、完成报告，保持
对作业进度的跟踪</li>
<li>AM接受来自共享文件系统的在客户端计算的输入分片，对
每个分片创建一个map对象，及由<code>mapreduce.job.reduces</code>
属性确定的多个reduce任务对象</li>
<li>AM根据任务大小决定如何运行job，如果在新容器中分配、
运行任务的开销大于并行运行时的开销，AM会在单个节点
上运行，这样的作业称为<em>uberized</em></li>
<li>AM在任何tasks执行之前通过job的<code>setup</code>方法设置job的
<code>OutputCommiter</code>，建立作业输出目录</li>
</ol>
<h4 id="任务分配"><a href="#任务分配" class="headerlink" title="任务分配"></a>任务分配</h4><ol>
<li>若作业不适合作为<em>uber</em>任务运行，AM为该作业中所有map
、reduce任务向RM请求容器</li>
<li>请求附着heart beat，包括每个map任务的数据本地化信息
，特别是输入分片所在的主机、机架信息，scheduler据此
做调度决策<ul>
<li>理想化情况下任务分配到数据本地化节点</li>
<li>否则优先使用机架本地化</li>
</ul>
</li>
<li>请求同时指定了任务内存需求，YARN中的资源分为更细粒度
，task可以请求最小到最大限制范围、任意最小值倍数的
内存容量</li>
</ol>
<h4 id="任务执行"><a href="#任务执行" class="headerlink" title="任务执行"></a>任务执行</h4><ol>
<li>当NM的scheduler为task分配了container，AM就可以通过
与NM通信启动容器</li>
<li>任务由<code>YarnChild</code>执行，在执行任务之前，需要将任务
所需资源本地化，包括作业的配置、JAR文件、所有来自
分布式缓存的文件，然后运行map、reduce任务</li>
<li>对于Streaming、Pipes程序，<code>YarnChild</code>启动Streaming、
Pipes进程，使用标准输入输出、socket与其通信（以
MapReduce1方式运行）</li>
</ol>
<h4 id="进度和状态更新"><a href="#进度和状态更新" class="headerlink" title="进度和状态更新"></a>进度和状态更新</h4><ol>
<li>task每3s通过<code>umbilical</code>接口向AM汇报进度、状态（包括
计数器），作为job的aggregate view</li>
<li>客户端则默认没1s查询AM接受进度更新</li>
</ol>
<h4 id="作业完成"><a href="#作业完成" class="headerlink" title="作业完成"></a>作业完成</h4><ol>
<li>客户端每5s通过调用job的<code>waitForCompletion</code>检查作业
是否完成，也可以通过HTTP callback完成作业</li>
<li>作业完成后AM和task容器清理工作状态，OutputCommiter
作业清理方法被调用</li>
</ol>
<h1 id="todo：这里逻辑有问题，要删"><a href="#todo：这里逻辑有问题，要删" class="headerlink" title="todo：这里逻辑有问题，要删"></a>todo：这里逻辑有问题，要删</h1><h2 id="MapReduce计算模型"><a href="#MapReduce计算模型" class="headerlink" title="MapReduce计算模型"></a>MapReduce计算模型</h2><ul>
<li>分布式系统，不像一般的数据库、文件系统，无法从上至下
、从头到尾进行求和等操作</li>
<li>需要由分散的节点不断向一个点聚拢的计算过程，即分布式系统
上计算模型基本都是由map、reduce步骤组成</li>
</ul>
<h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce每步数据处理流程包括两个阶段</p>
<ul>
<li><p><em>Map</em>：映射</p>
<ul>
<li>map过程相互独立、各mapper见不通信，所以mapreduce
只适合处理<strong>独立计算</strong>的任务</li>
</ul>
</li>
<li><p><em>Reduce</em>：归一</p>
<ul>
<li>reduce直接处理map的输出，reduce的<strong>键</strong>为map输出
<strong>值</strong></li>
</ul>
</li>
</ul>
<h4 id="数据处理过程"><a href="#数据处理过程" class="headerlink" title="数据处理过程"></a>数据处理过程</h4><ul>
<li><p>需要把任何计算任务转换为一系列MapReduce作业，然后依次
执行这些作业</p>
</li>
<li><p>计算过程的各个步骤之间，各个作业输出的中间结果需要存盘，
然后才能被下个步骤使用（因为各个步骤之间没有明确流程）</p>
</li>
<li><p>One Pass Computation：只需要一遍扫描处理的计算任务
MapReduce计算模型非常有效</p>
</li>
<li><p>Multi Pass Computation：需要在数据上进行多遍扫描、处理
的计算任务，需要执行多个MapReduce作业计算任务，因为
多副本复制、磁盘存取，其效率不高</p>
</li>
</ul>
<h3 id="Mapred-on-DAG"><a href="#Mapred-on-DAG" class="headerlink" title="Mapred on DAG"></a>Mapred on DAG</h3><p>Directed Acyclic Graph；表示数据处理流程的有向无环图</p>
<ul>
<li>顶点：数据处理任务，反映一定的业务逻辑，即如何对数据进行
转换和分析</li>
<li>边：数据在不同的顶点间的传递</li>
</ul>
<h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><div class="table-container">
<table>
<thead>
<tr>
<th>比较方面</th>
<th>MapReduce</th>
<th>DAG</th>
</tr>
</thead>
<tbody>
<tr>
<td>操作原语</td>
<td>map、reduce</td>
<td>较多</td>
</tr>
<tr>
<td>抽象层次</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>表达能力</td>
<td>差</td>
<td>强</td>
</tr>
<tr>
<td>易用性</td>
<td>要手动处理job之间依赖关系，易用性差</td>
<td>DAG本身体现数据处理流程</td>
</tr>
<tr>
<td>可读性</td>
<td>处理逻辑隐藏在代码中，没有整体逻辑</td>
<td>较好</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>正是MapReduce提供操作原语少、抽象层次低，所以其表达能力
差，同时需要用户处理更多的逻辑，易用性、可读性差</p>
<ul>
<li><p>复杂数据处理任务，如：机器学习算法、SQL连接查询很难
表示用MapReduce计算默认表达</p>
</li>
<li><p>操作原语多并不是DAG本身的要求，DAG本身只是有向无环图，
只是使用DAG计算模型可以提供更多的操作原语</p>
</li>
</ul>
</li>
<li><p>由于DAG的表达能力强于MapReduce，对某些处理逻辑，DAG
   所需作业数目小于MapReduce，消除不必要的任务</p>
<ul>
<li><p>DAG显著提高了数据处理效率，对小规模、低延迟和大规模、
高吞吐量的负载均有效</p>
</li>
<li><p>MapReduce需要通过把中间结果存盘实现同步，而DAG整合
部分MapReduce作业，减少磁盘I/O</p>
</li>
<li><p>reduce任务需要等待map任务全部完成才能继续，DAG优化
数据处理流程，减少同步Barrier</p>
</li>
</ul>
</li>
<li><p>DAG部分计算模型也由map、reduce任务构成，只是不像传统
MapReduce计算模型中map、reduce必须成对出现</p>
<ul>
<li><p>或者说DAG只有一次map任务（扫描数据时），其余都是
reduce任务？</p>
</li>
<li><p>从MapReduce配置也可以看出，MapReduce可以选择基于
<code>yarn</code>或<code>yarn-tez</code></p>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:08.000Z" title="2/17/2019, 11:57:08 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">3 minutes read (About 506 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/tez.html">Tez</a></h1><div class="content"><h2 id="Tez简介"><a href="#Tez简介" class="headerlink" title="Tez简介"></a>Tez简介</h2><p>Tezm目标就是建立执行框架，支持大数据上DAG表达的作业处理</p>
<ul>
<li><p>YARN将资源管理功能从数据处理模型中独立出来，使得在Hadoop
执行DAG表达的作业处理成为可能，Tez成为可扩展、高效的执行
引擎</p>
<ul>
<li>Tez在YARN和Hive、Pig之间提供一种通用数据处理模型DAG</li>
<li>Hive、Pig、Cascading作业在Tez上执行更快，提供交互式
查询响应</li>
</ul>
</li>
<li><p>Tez把DAG的每个顶点建模为Input、Processer、Output模块的
组合</p>
<ul>
<li>Input、Output决定数据格式、输入、输出</li>
<li>Processor包装了数据转换、处理逻辑</li>
<li>Processor通过Input从其他顶点、管道获取数据输入，通过
Output向其他顶点、管道传送生成数据</li>
<li>通过把不同的Input、Processor、Output模块组合成顶点，
建立DAG数据处理工作流，执行特定数据处理逻辑</li>
</ul>
</li>
<li><p>Tez自动把DAG映射到物理资源，将其逻辑表示扩展为物理表示，
并执行其中的业务逻辑</p>
<ul>
<li><p>Tez能为每个节点增加并行性，即使用多个任务执行节点
的计算任务</p>
</li>
<li><p>Tez能动态地优化DAG执行过程，能够根据执行过程中获得地
信息，如：数据采样，优化DAG执行计划，优化资源使用、
提高性能</p>
<ul>
<li>去除了连续作业之间的写屏障</li>
<li>去除了工作流中多余的Map阶段</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Tez执行过程"><a href="#Tez执行过程" class="headerlink" title="Tez执行过程"></a>Tez执行过程</h3><ul>
<li><p>初始化例程，提供context/configuration信息给Tez runtime</p>
</li>
<li><p>对每个顶点的每个任务（任务数量根据并行度创建）进行初始化</p>
</li>
<li><p>执行任务Processor直到所有任务完成，则节点完成</p>
</li>
<li><p>Output把从Processor接收到的数据，通过管道传递给下游顶点
的Input</p>
</li>
<li><p>直到整个DAG所有节点任务执行完毕</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T09:13:17.000Z" title="7/16/2021, 5:13:17 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">39 minutes read (About 5813 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/hdfs.html">Hadoop HDFS</a></h1><div class="content"><h2 id="HDFS设计模式"><a href="#HDFS设计模式" class="headerlink" title="HDFS设计模式"></a>HDFS设计模式</h2><ul>
<li><p>数据读取、写入</p>
<ul>
<li>HDFS一般存储不可更新的文件，只能对文件进行数据追加
，也不支持多个写入者的操作</li>
<li>认为一次写入、多次读取是最高效的访问模式</li>
<li>namenode将metadata存储在内存中，所以文件系统所能
存储的文件总数受限于NameNode的内存</li>
</ul>
</li>
<li><p>适合模式</p>
<ul>
<li>每次分析都涉及数据集大部分数据甚至全部，因此读取整个
数据集的大部分数据、甚至全部，因此读取整个数据集的
时间比读取第一条记录时间延迟更重要</li>
<li>HDFS不适合要求地时间延迟的数据访问应用，HDFS是为
高数据吞吐量应用优化的，可能会提高时间延迟</li>
</ul>
</li>
<li><p>硬件：HDFS无需高可靠硬件，HDFS被设计为即使节点故障也能
继续运行，且不让用户察觉</p>
</li>
</ul>
<h3 id="数据块"><a href="#数据块" class="headerlink" title="数据块"></a>数据块</h3><p>和普通文件系统一样，HDFS同样也有块的概念，默认为64MB</p>
<ul>
<li><p>HDFS上的文件也被划分为块大小的多个chunk，作为独立的存储
单元，但是HDFS中小于块大小的文件不会占据整个块空间</p>
</li>
<li><p>对分布式文件系统块进行抽象的好处</p>
<ul>
<li>文件大小可以大于网络中任意一个磁盘的容量</li>
<li>使用抽象块而非整个文件作为存储单元，简化了存储子系统
的设计<ul>
<li>简化存储管理，块大小固定，计算磁盘能存储块数目
相对容易</li>
<li>消除了对元素见的顾虑，文件的元信息（权限等），
不需要和块一起存储，可由其他系统单独管理</li>
</ul>
</li>
</ul>
</li>
<li><p>块非常适合用于数据备份，进而提供数据容错能力、提高可用性</p>
</li>
</ul>
<h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>HDFS系统中的管理者</p>
<ul>
<li><p>集中存储了HDFS的元信息Metadata</p>
<ul>
<li>维护文件系统的文件树、全部的文件和文件夹的元数据</li>
<li>管理文件系统的Namespace：创建、删除、修改、列出所有
文件、目录</li>
</ul>
</li>
<li><p>执行数据块的管理操作</p>
<ul>
<li>把文件映射到所有的数据块</li>
<li>创建、删除数据块</li>
<li>管理副本的Placement、Replication</li>
</ul>
</li>
<li><p>负责DataNode的成员管理</p>
<ul>
<li>接受DataNode的Registration</li>
<li>接受DataNode周期性的Heart Beat</li>
</ul>
</li>
</ul>
<p>Hadoop上层模块，根据NameNode上的元信息，就可以知道每个数据块
有多少副本、存放在哪些节点上，据此分配计算任务，通过
<strong>Move Computation to Data</strong>，而不是移动数据本身，减少数据
移动开销，加快计算过程</p>
<h4 id="Metadata的保存"><a href="#Metadata的保存" class="headerlink" title="Metadata的保存"></a>Metadata的保存</h4><p>为了支持高效的存取操作，NameNode把所有的元信息保存在主内存，
包括文件和数据块的命名空间、文件到数据块的映射、数据块副本
的位置信息。文件命名空间、文件到数据块的映射信息也会持久化
到NameNode本地文件系统</p>
<ul>
<li>FsImage：命名空间镜像文件，保存整个文件系统命名空间、
文件到数据块的映射信息</li>
<li>EditLog：编辑日志文件，是一个Transaction Log文件，记录了
对文件系统元信息的所有更新操作：创建文件、改变文件的
Replication Factor</li>
</ul>
<p>NameNode启动时，读取FsImage、EditLog文件，把EditLog的所有
事务日志应用到从FsImage文件中装载的旧版本元信息上，生成新的
FsImage并保存，然后截短EditLog</p>
<h4 id="NameNode可恢复性"><a href="#NameNode可恢复性" class="headerlink" title="NameNode可恢复性"></a>NameNode可恢复性</h4><h5 id="多个文件系统备份"><a href="#多个文件系统备份" class="headerlink" title="多个文件系统备份"></a>多个文件系统备份</h5><p>备份文件系统元信息的持久化版本</p>
<ul>
<li>在NameNode写入元信息的持久化版本时，同步、atomic写入多个
文件系统（一般是本地磁盘、mount为本地目录的NFS）</li>
</ul>
<h5 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h5><p>运行Secondary NameNode：负责周期性的使用EditLog更新
FsImage，保持EditLog在一定规模内</p>
<ul>
<li><p>Seconadary NameNode保存FsImage、EditLog文件副本，
每个一段时间从NameNode拷贝FsImage，和EditLog文件进行
合并，然后把更新后的FsImage复制回NameNode</p>
</li>
<li><p>若NameNode宕机，可以启动其他机器，从Secondary
NameNode获得FsImage、EditLog，恢复宕机之前的最新的
元信息，当作新的NameNode，也可以直接作为主NameNode</p>
</li>
<li><p>Secondary NameNode保存的出状态总是滞后于主节点，需要
从NFS获取NameNode部分丢失的metadata</p>
</li>
<li><p>Secondary NameNode需要运行在另一台机器，需要和主
NameNode一样规模的CPU计算能力、内存，以便完成元信息
管理</p>
</li>
</ul>
<p>想要从失效的NameNode恢复，需要启动一个拥有文件系统数据副本的
新NameNode，并配置DataNode和客户端以便使用新的NameNode</p>
<ul>
<li>将namespace的映像导入内存中</li>
<li>重做编辑日志</li>
<li>接收到足够多的来自DataNode的数据块报告，并退出安全模式</li>
</ul>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>HDFS中保存数据的节点</p>
<ul>
<li><p>数据被切割为多个数据块，以冗余备份的形式存储在多个
DataNode中，因此不需要再每个节点上安装RAID存储获得硬件上
可靠存储支持。DataNode之间可以拷贝数据副本，从而可以重新
平衡每个节点存储数据量、保证数据可靠性（保证副本数量）</p>
</li>
<li><p>DotaNode定期向NameNode报告其存储的数据块列表，以备使用者
通过直接访问DataNode获得相应数据</p>
</li>
<li><p>所有NameNode和DataNode之间的通讯，包括DataNode的注册、
心跳信息、报告数据块元信息，都是由DataNode发起请求，由
NameNode被动应答和完成管理</p>
</li>
</ul>
<h3 id="HDFS高可用性"><a href="#HDFS高可用性" class="headerlink" title="HDFS高可用性"></a>HDFS高可用性</h3><p>对于大型集群，NN冷启动需要30min甚至更长，因此Hadoop2.x中添加
对高可用性HA（high-availability）的支持</p>
<ul>
<li><p>配置Active-Standby NameNode</p>
<ul>
<li>ANN失效后，SNN就会接管任务并开始服务，没有明显中断</li>
<li>ANN、SNN应该具有相同的硬件配置</li>
</ul>
</li>
<li><p>NN之间需要通过<strong>高可用的共享存储（JounalNode）</strong>实现
Editlog共享</p>
<ul>
<li>JN进程轻量，可以和其他节点部署在同一台机器</li>
<li>JN至少为3个，最好为奇数个，这样JN失效$(n-1)/2$个时
  仍然可以正常工作</li>
<li>SNN接管工作后，将通读共享编辑日志直到末尾，实现与ANN
状态同步</li>
</ul>
</li>
<li><p>DN需要同时向两个NN发送数据块处理报告，因为数据块映射信息
存储在NN内存中</p>
</li>
<li><p>客户端需要使用特定机制处理NN失效问题，且机制对用户透明</p>
</li>
<li><p>如果两个namenode同时失效，同样可以冷启动其他namenode，
此时情况就和<em>no-HA</em>模式冷启动类似</p>
</li>
</ul>
<p>注意：HA模式下，不应该再次配置Secondary NameNode</p>
<blockquote>
<p>   Note that, in an HA cluster, the Standby NameNode also 
    performs checkpoints of the namespace state, and thus it
    is not necessary to run a Secondary NameNode, 
    CheckpointNode, or BackupNode in an HA cluster. In fact,
    to do so would be an error. This also allows one who is
    reconfiguring a non-HA-enabled HDFS cluster to be
    HA-enabled to reuse the hardware which they had
    previously dedicated to the Secondary NameNode.</p>
</blockquote>
<h4 id="Failover-Controller"><a href="#Failover-Controller" class="headerlink" title="Failover Controller"></a>Failover Controller</h4><p>故障转移控制器系统中有一个新实体管理者管理namenode之间切换，</p>
<ul>
<li><p>Failover Controller最初实现基于Zookeeper，可插拔</p>
</li>
<li><p>每个namenode运行着一个Failover Controller，用于监视宿主
namenode是否失效（heart beat机制）， 并在失效时进行故障
切换</p>
<ul>
<li>管理员也可以手动发起故障切换，称为<em>平稳故障转移</em></li>
</ul>
</li>
<li><p>在非平稳故障切换时，无法确切知道失效namenode是否已经停止
运行，如网速慢、网络切割均可能激发故障转移，引入fencing
机制</p>
<ul>
<li>杀死namenode进程</li>
<li>收回对共享存储目录权限</li>
<li>屏蔽相应网络端口</li>
<li>STONITH：shoot the other node in the head，断电</li>
</ul>
</li>
</ul>
<h3 id="联邦HDFS"><a href="#联邦HDFS" class="headerlink" title="联邦HDFS"></a>联邦HDFS</h3><p>NameNode在内存中保存文件系统中每个文件、数据块的引用关系，
所以对于拥有大量文件的超大集群，内存将成为系统扩展的瓶颈，
2.x中引入的联邦HDFS可以添加NameNode实现扩展</p>
<ul>
<li>每个NameNode维护一个namespace volume，包括命名空间的
元数据、命令空间下的文件的所有数据块、数据块池</li>
<li>namespace volume之间相互独立、不通信，其中一个NameNode
失效也不会影响其他NameNode维护的命名空间的可用性</li>
<li>数据块池不再切分，因此集群中的DataNode需要注册到每个
NameNode，并且存储来自多个数据块池的数据块</li>
</ul>
<h2 id="Hadoop文件系统"><a href="#Hadoop文件系统" class="headerlink" title="Hadoop文件系统"></a>Hadoop文件系统</h2><p>Hadoop有一个抽象问的文件系统概念，HDFS只是其中的一个实现，
Java抽象类<code>org.apche.hadoop.fs.FileSystem</code>定义了Hadoop中的
一个文件系统接口，包括以下具体实现</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>文件系统</th>
<th>URI方案</th>
<th>Java实现</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Local</td>
<td><code>file</code></td>
<td><code>fs.LocalFileSystem</code></td>
<td>使用客户端校验和本地磁盘文件系统，没有使用校验和文件系统<code>RawLocalFileSystem</code></td>
</tr>
<tr>
<td>HDFS</td>
<td><code>hdfs</code></td>
<td><code>hdfs.DistributedFileSystem</code></td>
<td>HDFS设计为与MapReduce结合使用实现高性能</td>
</tr>
<tr>
<td>HFTP</td>
<td><code>hftp</code></td>
<td><code>hdfs.HftpFileSystem</code></td>
<td>在HTTP上提供对HDFS只读访问的文件系统，通常与distcp结合使用，以实现在运行不同版本HDFS集群之间复制数据</td>
</tr>
<tr>
<td>HSFTP</td>
<td><code>hsftp</code></td>
<td><code>hdfs.HsftpFileSystem</code></td>
<td>在HTTPS上同以上</td>
</tr>
<tr>
<td>WebHDFS</td>
<td><code>Webhdfs</code></td>
<td><code>hdfs.web.WebHdfsFileSystem</code></td>
<td>基于HTTP，对HDFS提供安全读写访问的文件系统，为了替代HFTP、HFSTP而构建</td>
</tr>
<tr>
<td>HAR</td>
<td><code>har</code></td>
<td><code>fs.HarFileSystem</code></td>
<td>构建于其他文件系统之上，用于文件存档的文件系统，通常用于需要将HDFS中的文件进行存档时，以减少对NN内存的使用</td>
</tr>
<tr>
<td>hfs</td>
<td><code>kfs</code></td>
<td><code>fs.kfs.kosmosFileSystem</code></td>
<td>CloudStore（前身为Kosmos文件系统）类似于HDFS（GFS），<em>C++</em>编写</td>
</tr>
<tr>
<td>FTP</td>
<td><code>ftp</code></td>
<td><code>fs.ftp.FTPFileSystem</code></td>
<td>由FTP服务器支持的文件系统</td>
</tr>
<tr>
<td>S3（原生）</td>
<td><code>S3n</code></td>
<td><code>fs.s3native.NativeS3FileSystem</code></td>
<td>由Amazon S3支持的文件系统</td>
</tr>
<tr>
<td>S3（基于块）</td>
<td><code>S3</code></td>
<td><code>fs.sa.S3FileSystem</code></td>
<td>由Amazon S3支持的文件系统，以块格式存储文件（类似于HDFS），以解决S3Native 5GB文件大小限制</td>
</tr>
<tr>
<td>分布式RAID</td>
<td><code>hdfs</code></td>
<td><code>hdfs.DistributedRaidFileSystem</code></td>
<td>RAID版本的HDFS是为了存档而设计的。针对HDFS中每个文件，创建一个更小的检验文件，并允许数据副本变为2，同时数据丢失概率保持不变。需要在集群中运行一个RaidNode后台进程</td>
</tr>
<tr>
<td>View</td>
<td><code>viewfs</code></td>
<td><code>viewfs.ViewFileSystem</code></td>
<td>针对其他Hadoop文件系统挂载的客户端表，通常用于联邦NN创建挂载点</td>
</tr>
</tbody>
</table>
</div>
<h3 id="文件系统接口"><a href="#文件系统接口" class="headerlink" title="文件系统接口"></a>文件系统接口</h3><p>Hadoop对文件系统提供了许多接口，一般使用<strong>URI方案</strong>选择合适的
文件系统实例进行交互</p>
<h4 id="命令行接口"><a href="#命令行接口" class="headerlink" title="命令行接口"></a>命令行接口</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -copyFromLocal file hdfs://localhost/user/xyy15926/file</span><br><span class="line">	<span class="comment"># 调用Hadoop文件系统的shell命令`fs`</span></span><br><span class="line">	<span class="comment"># `-copyFromLocalFile`则是`fs`子命令</span></span><br><span class="line">	<span class="comment"># 事实上`hfds://localhost`可以省略，使用URI默认设置，即</span></span><br><span class="line">		<span class="comment"># 在`core-site.xml`中的默认设置</span></span><br><span class="line">	<span class="comment"># 类似的默认复制文件路径为HDFS中的`$HOME`</span></span><br><span class="line"></span><br><span class="line">$ hadoop fs -copyToLocal file file</span><br></pre></td></tr></table></figure>
<h4 id="HTTP"><a href="#HTTP" class="headerlink" title="HTTP"></a>HTTP</h4><ul>
<li><p>直接访问：HDFS后台进程直接服务来自于客户端的请求</p>
<ul>
<li>由NN内嵌的web服务器提供目录服务（默认50070端口）</li>
<li>DN的web服务器提供文件数据（默认50075端口）</li>
</ul>
</li>
<li><p>代理访问：依靠独立代理服务器通过HTTP访问HDFS</p>
<ul>
<li>代理服务器可以使用更严格的防火墙策略、贷款限制策略</li>
</ul>
</li>
</ul>
<h4 id="C"><a href="#C" class="headerlink" title="C"></a>C</h4><p>Hadoop提供<code>libhdfs</code>的C语言库，是Java <code>FileSystem</code>接口类的
镜像</p>
<ul>
<li>被写成访问HDFS的C语言库，但其实可以访问全部的Hadoop文件
系统</li>
<li>使用Java原生接口（JNI）调用Java文件系统客户端</li>
</ul>
<h4 id="FUSE"><a href="#FUSE" class="headerlink" title="FUSE"></a>FUSE</h4><p>Filesystem in Userspace允许把按照用户空间实现的文件系统整合
成一个Unix文件系统</p>
<ul>
<li>使用Hadoop Fuse-DFS功能模块，任何一个Hadoop文件系统可以
作为一个标准文件系统进行挂载<ul>
<li>Fuse_DFS使用C语言实现，调用libhdfs作为访问HDFS的接口</li>
</ul>
</li>
<li>然后可以使用Unix工具（<code>ls</code>、<code>cat</code>等）与文件系统交互</li>
<li>还可以使用任何编程语言调用POSIX库访问文件系统</li>
</ul>
<h3 id="读文件"><a href="#读文件" class="headerlink" title="读文件"></a>读文件</h3><ol>
<li><p>客户端程序使用要读取的文件名、Read Range的开始偏移量、
读取范围的程度等信息，询问NameNode</p>
</li>
<li><p>NameNode返回落在读取范围内的数据块的Location信息，根据
与客户端的临近性（Proximity）进行排序，客户端一般选择
最临近的DataNode发送读取请求</p>
</li>
</ol>
<p>具体实现如下</p>
<ol>
<li><p>客户端调用<code>FileSystem</code>对象<code>open</code>方法，打开文件，获得
<code>DistributedFileSystem</code>类的一个实例</p>
</li>
<li><p><code>DistributedFileSystem</code>返回<code>FSDataInputStream</code>类的实例，
支持文件的定位、数据读取</p>
<ul>
<li><code>DistributedFileSystem</code>通过<strong>RPC</strong>调用NameNode，获得
文件首批若干数据块的位置信息（Locations of Blocks）</li>
<li>对每个数据块，NameNode会返回拥有其副本的所有DataNode
地址</li>
<li>其包含一个<code>DFSInputStream</code>对象，负责管理客户端对HDFS
中DataNode、NameNode存取</li>
</ul>
</li>
<li><p>客户端从输入流调用函数<code>read</code>，读取文件第一个数据块，不断
调用<code>read</code>方法从DataNode获取数据</p>
<ul>
<li><code>DFSInputStream</code>保存了文件首批若干数据块所在的
DataNode地址，连接到closest DataNode</li>
<li>当达到数据块末尾时，<code>DFSInputStream</code>关闭到DataNode
的连接，创建到保存其他数据块DataNode的连接</li>
<li>首批数据块读取完毕之后，<code>DFSInputStream</code>向NameNode
询问、提取下一批数据块的DataNode的位置信息</li>
</ul>
</li>
<li><p>客户端完成文件的读取，调用<code>FSDataInputStream</code>实例<code>close</code>
方法关闭文件</p>
</li>
</ol>
<h3 id="写文件"><a href="#写文件" class="headerlink" title="写文件"></a>写文件</h3><ul>
<li><p>客户端询问NameNode，了解应该存取哪些DataNode，然后客户端
直接和DataNode进行通讯，使用Data Transfer协议传输数据，
这个流式数据传输协议可以提高数据传输效率</p>
</li>
<li><p>创建文件时，客户端把文件数据缓存在一个临时的本地文件上，
当本地文件累计超过一个数据块大小时，客户端程序联系
NameNode，NameNode更新文件系统的NameSpace，返回Newly
Allocated数据块的位置信息，客户端根据此信息本文件数据块
从临时文件Flush到DataNode进行保存</p>
</li>
</ul>
<p>具体实现如下：</p>
<ol>
<li><p>客户端调用<code>DistributedFileSystem</code>的<code>create</code>方法</p>
<ul>
<li><code>DistributedFileSystem</code>通过发起RPC告诉NameNode在
其NameSpace创建一个新文件，此时新文件没有任何数据块</li>
<li>NameNode检查：文件是否存在、客户端权限等，检查通过
NameNode为新文件创建新记录、保存其信息，否则文件创建
失败</li>
</ul>
</li>
<li><p><code>DistributedFileSystem</code>返回<code>FSDataOutputStream</code>给客户端</p>
<ul>
<li>其包括一个<code>DFSOutputStream</code>对象，负责和NameNode、
DataNode的通讯</li>
</ul>
</li>
<li><p>客户端调用<code>FSDataOutputStream</code>对象<code>write</code>方法写入数据</p>
<ul>
<li><code>DFSOutputStream</code>把数据分解为数据包Packet，写入内部
Data Queue</li>
<li><code>DataSteamer</code>消费这个队列，写入本地临时文件中</li>
<li>当写入数据超过一个数据块大小时，<code>DataStreamer</code>请求
NameNode为新的数据块分配空间，即选择一系列合适的
DataNode存放各个数据块各个副本</li>
<li>存放各个副本的DataNode形成一个Pipeline，流水线上的
Replica Factor数量的DataNode接收到数据包之后转发给
下个DataNode</li>
<li><code>DFSOutputStream</code>同时维护数据包内部Ack Queue，用于
等待接受DataNode应答信息，只有某个数据包已经被流水线
上所有DataNode应答后，才会从Ack Queue上删除</li>
</ul>
</li>
<li><p>客户端完成数据写入，调用<code>FSDataOutputStream</code>的<code>close</code>
方法</p>
<ul>
<li><code>DFSOutputStream</code>把所有的剩余的数据包发送到DataNode
流水线上，等待应答信息</li>
<li>最后通知NameNode文件结束</li>
<li>NameNode自身知道文件由哪些数据块构成，其等待数据块
复制完成，然后返回文件创建成功</li>
</ul>
</li>
</ol>
<h2 id="Hadoop平台上的列存储"><a href="#Hadoop平台上的列存储" class="headerlink" title="Hadoop平台上的列存储"></a>Hadoop平台上的列存储</h2><p>列存储的优势</p>
<ul>
<li><p>更少的IO操作：读取数据的时候，支持Prject Pushdown，甚至
是Predicate Pushdown，可大大减少IO操作</p>
</li>
<li><p>更大的压缩比：每列中数据类型相同，可以针对性的编码、压缩</p>
</li>
<li><p>更少缓存失效：每列数据相同，可以使用更适合的Cpu Pipline
编码方式，减少CPU cache miss</p>
</li>
</ul>
<h3 id="RCFile"><a href="#RCFile" class="headerlink" title="RCFile"></a>RCFile</h3><p>Record Columnar File Format：FB、Ohio州立、中科院计算所合作
研发的列存储文件格式，首次在Hadoop中引入列存储格式</p>
<ul>
<li><p><em>允许按行查询，同时提供列存储的压缩效率</em>的列存储格式</p>
<ul>
<li>具备相当于行存储的数据加载速度、负载适应能力</li>
<li>读优化可以在扫描表格时，避免不必要的数据列读取</li>
<li>使用列维度压缩，有效提升存储空间利用率</li>
</ul>
</li>
<li><p>具体存储格式</p>
<ul>
<li>首先横向分割表格，生成多个Row Group，大小可以由用户
指定</li>
<li>在RowGroup内部，按照列存储一般做法，按列把数据分开，
分别连续保存<ul>
<li>写盘时，RCFile针对每列数据，使用Zlib/LZO算法进行
压缩，减少空间占用</li>
<li>读盘时，RCFile采用Lazy Decompression策略，即用户
查询只涉及表中部分列时，会跳过不需要列的解压缩、
反序列化的过程</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="ORC存储格式"><a href="#ORC存储格式" class="headerlink" title="ORC存储格式"></a>ORC存储格式</h3><p>Optimized Row Columnar File：对RCFile优化的存储格式</p>
<ul>
<li><p>支持更加丰富的数据类型</p>
<ul>
<li>包括Date Time、Decimal</li>
<li>Hive的各种Complex Type，包括：Struct、List、Map、
Union</li>
</ul>
</li>
<li><p>Self Describing的列存储文件格式</p>
<ul>
<li>为Streaming Read操作进行了优化</li>
<li>支持快速查找少数数据行</li>
</ul>
</li>
<li><p>Type Aware的列存储文件格式</p>
<ul>
<li>文件写入时，针对不同的列的数据类型，使用不同的编码器
进行编码，提高压缩比<ul>
<li>整数类型：Variable Length Compression</li>
<li>字符串类型：Dictionary Encoding</li>
</ul>
</li>
</ul>
</li>
<li><p>引入轻量级索引、基本统计信息</p>
<ul>
<li>包括各数据列的最大/小值、总和、记录数等信息</li>
<li>在查询过程中，通过谓词下推，可以忽略大量不符合查询
条件的记录</li>
</ul>
</li>
</ul>
<h4 id="文件结构"><a href="#文件结构" class="headerlink" title="文件结构"></a>文件结构</h4><p>一个ORC文件由多个Stripe、一个包含辅助信息的FileFooter、以及
Postscript构成</p>
<h5 id="Stripe"><a href="#Stripe" class="headerlink" title="Stripe"></a>Stripe</h5><p>每个stripe包含index data、row data、stripe footer</p>
<ul>
<li><p>stripe就是ORC File中划分的row group</p>
<ul>
<li>默认大小为256MB，可扩展的长度只受HDFS约束</li>
<li>大尺寸的strip、对串行IO的优化，能提高数据吞吐量、
读取更少的文件，同时能把减轻NN负担</li>
</ul>
</li>
<li><p>Index Data部分</p>
<ul>
<li>包含每个列的极值</li>
<li>一系列Row Index Entry记录压缩模块的偏移量，用于跳转
到正确的压缩块的位置，实现数据的快速读取，缺省可以
跳过10000行</li>
</ul>
</li>
</ul>
<ul>
<li><p>Row Data部分；包含每个列的数据，每列由若干Data Stream
构成</p>
</li>
<li><p>Stripe Footer部分</p>
<ul>
<li>Data Stream位置信息</li>
<li>每列数据的编码方式</li>
</ul>
</li>
</ul>
<h5 id="File-Footer"><a href="#File-Footer" class="headerlink" title="File Footer"></a>File Footer</h5><p>包含该ORCFile文件中所有stripe的元信息</p>
<ul>
<li>每个Stripe的位置</li>
<li>每个Stripe的行数</li>
<li>每列的数据类型</li>
<li>还有一些列级别的聚集结果，如：记录数、极值、总和</li>
</ul>
<h5 id="Postscript"><a href="#Postscript" class="headerlink" title="Postscript"></a>Postscript</h5><ul>
<li>用来存储压缩参数</li>
<li>压缩过后的Footer的长度</li>
</ul>
<h3 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h3><p>灵感来自于Google关于Drenel系统的论文，其介绍了一种支持嵌套
结构的列存储格式，以提升查询性能</p>
<h4 id="支持"><a href="#支持" class="headerlink" title="支持"></a>支持</h4><p>Parquet为hadoop生态系统中的所有项目，提供支持高压缩率的
列存储格式</p>
<ul>
<li><p>兼容各种数据处理框架</p>
<ul>
<li>MapReduce</li>
<li>Spark</li>
<li>Cascading</li>
<li>Crunch</li>
<li>Scalding</li>
<li>Kite</li>
</ul>
</li>
<li><p>支持多种对象模型</p>
<ul>
<li>Avro</li>
<li>Thrift</li>
<li>Protocol Buffers</li>
</ul>
</li>
<li><p>支持各种查询引擎</p>
<ul>
<li>Hive</li>
<li>Impala</li>
<li>Presto</li>
<li>Drill</li>
<li>Tajo</li>
<li>HAWQ</li>
<li>IBM Big SQL</li>
</ul>
</li>
</ul>
<h4 id="Parquet组件"><a href="#Parquet组件" class="headerlink" title="Parquet组件"></a>Parquet组件</h4><ul>
<li><p>Storage Format：存储格式，定义了Parquet内部的数据类型、
存储格式</p>
</li>
<li><p>Object Model Converter：对象转换器，由Parquet-mr实现，
完成对象模型与Parquet数据类型的映射</p>
<ul>
<li>如Parquet-pig子项目，负责把内存中的Pig Tuple序列化
并按存储格式保存为Parquet格式的文件，已经反过来，
把Parquet格式文件的数据反序列化为Pig Tuple</li>
</ul>
</li>
<li><p>Object Model：对象模型，可以理解为内存中的数据表示，包括
Avro、Thrift、Protocal Buffer、Hive Serde、Pig Tuple、
SparkSQL Internal Row等对象模型</p>
</li>
</ul>
<h4 id="Parquet数据schema"><a href="#Parquet数据schema" class="headerlink" title="Parquet数据schema"></a>Parquet数据schema</h4><p>数据schema（结构）可以用一个棵树表达</p>
<ul>
<li><p>有一个根节点，根节点包含多个Feild（子节点），子节点可以
包含子节点</p>
</li>
<li><p>每个field包含三个属性</p>
<ul>
<li><p>repetition：field出现的次数</p>
<ul>
<li><code>required</code>：必须出现1次</li>
<li><code>optional</code>：出现0次或1次</li>
<li><code>repeated</code>：出现0次或多次</li>
</ul>
</li>
<li><p>type：数据类型</p>
<ul>
<li>primitive：原生类惬</li>
<li>group：衍生类型</li>
</ul>
</li>
<li><p>name：field名称</p>
</li>
</ul>
</li>
<li><p>Parquet通过把多个schema结构按树结构组合，提供对复杂类型
支持</p>
<ul>
<li>List、Set：repeated field</li>
<li>Map：包含键值对的Repeated Group（key为Required）</li>
</ul>
</li>
<li><p>schema中有多少<strong>叶子节点</strong>，Parquet格式实际存储多少列，
父节点则是在表头组合成schema的结构</p>
</li>
</ul>
<h4 id="Parquet文件结构"><a href="#Parquet文件结构" class="headerlink" title="Parquet文件结构"></a>Parquet文件结构</h4><ul>
<li>HDFS文件：包含数据和元数据，数据存储在多个block中</li>
<li>HDFS Block：HDFS上最小的存储单位</li>
<li>Row Group：按照行将数据表格划分多个单元，每个行组包含
一定行数，行组包含该行数据各列对应的列块<ul>
<li>一般建议采用更大的行组（512M-1G），此意味着更大的
列块，有毅力在磁盘上串行IO</li>
<li>由于可能依次需要读取整个行组，所以一般让一个行组刚好
在一个HDFS数据块中，HDFS Block需要设置得大于等于行组
大小</li>
</ul>
</li>
<li>Column Chunk：每个行组中每列保存在一个列块中<ul>
<li>行组中所有列连续的存储在行组中</li>
<li>不同列块使用不同压缩算法压缩</li>
<li>列块存储时保存相应统计信息，极值、空值数量，用于加快
查询处理</li>
<li>列块由多个页组成</li>
</ul>
</li>
<li>Page：每个列块划分为多个Page<ul>
<li>Page是压缩、编码的单元</li>
<li>列块的不同页可以使用不同的编码方式</li>
</ul>
</li>
</ul>
<h2 id="HDFS命令"><a href="#HDFS命令" class="headerlink" title="HDFS命令"></a>HDFS命令</h2><h3 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h3><ul>
<li>HDFS的用户就是当前linux登陆的用户</li>
</ul>
<h2 id="Hadoop组件"><a href="#Hadoop组件" class="headerlink" title="Hadoop组件"></a>Hadoop组件</h2><h3 id="Hadoop-Streaming"><a href="#Hadoop-Streaming" class="headerlink" title="Hadoop Streaming"></a>Hadoop Streaming</h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-02-17T03:57:08.000Z" title="2/17/2019, 11:57:08 AM">2019-02-17</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Hadoop/">Hadoop</a></span><span class="level-item">an hour read (About 7085 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Hadoop/hadoop_inset.html">Hadoop安装配置</a></h1><div class="content"><h2 id="Hadoop安装"><a href="#Hadoop安装" class="headerlink" title="Hadoop安装"></a>Hadoop安装</h2><h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li><p>Java</p>
<ul>
<li>具体版本<a target="_blank" rel="noopener" href="http://wiki.apache.org/hadoop/HadoopJavaVersions">http://wiki.apache.org/hadoop/HadoopJavaVersions</a></li>
<li>需要配置好java环境（<code>~/.bashrc</code>）</li>
</ul>
</li>
<li><p>ssh：必须安装且保证sshd一直运行，以便使用hadoop脚本管理
远端hadoop守护进程</p>
<ul>
<li>pdsh：建议安装获得更好的ssh资源管理</li>
<li>要设置免密登陆</li>
</ul>
</li>
</ul>
<h3 id="机器环境配置"><a href="#机器环境配置" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc"><a href="#bashrc" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><p>这里所有的设置都只是设置环境变量</p>
<ul>
<li><p>所以这里所有环境变量都可以放在<code>hadoop-env.sh</code>中</p>
</li>
<li><p>放在<code>.bashrc</code>中不是基于用户隔离的考虑</p>
<ul>
<li>因为hadoop中配置信息大部分放在<code>.xml</code>，放在这里无法
实现用户隔离</li>
<li>更多的考虑是给hive等依赖hadoop的应用提供hadoop配置</li>
</ul>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_PREFIX=/opt/hadoop</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 自定义部分</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 此处是直接解压放在`/opt`目录下</span></span><br><span class="line">export HADOOP_HOME=$HADOOP_PREFIX</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_PREFIX</span><br><span class="line"><span class="meta">	#</span><span class="bash"> hadoop common</span></span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_PREFIX</span><br><span class="line"><span class="meta">	#</span><span class="bash"> hdfs</span></span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_PREFIX</span><br><span class="line"><span class="meta">	#</span><span class="bash"> mapreduce</span></span><br><span class="line">export HADOOP_YARN_HOME=$HADOOP_PREFIX</span><br><span class="line"><span class="meta">	#</span><span class="bash"> YARN</span></span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop</span><br><span class="line"></span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export HADOOP_OPTS=&quot;$HADOOP_OPTS -Djava.library.path=$HADOOP_COMMON_LIB_NATIVE_DIR&quot;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里`-Djava`间不能有空格</span></span><br><span class="line"></span><br><span class="line">export CLASSPATH=$CLASS_PATH:$HADOOP_PREFIX/lib/*</span><br><span class="line">export PATH=$PATH:$HADOOP_PREFIX/sbin:$HADOOP_PREFIX/bin</span><br></pre></td></tr></table></figure>
<h4 id="etc-hosts"><a href="#etc-hosts" class="headerlink" title="/etc/hosts"></a><code>/etc/hosts</code></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.31.129 hd-master</span><br><span class="line">192.168.31.130 hd-slave1</span><br><span class="line">192.168.31.131 hd-slave2</span><br><span class="line">127.0.0.1 localhost</span><br></pre></td></tr></table></figure>
<ul>
<li>这里配置的ip地址是各个主机的ip，需要自行配置</li>
<li><code>hd-master</code>、<code>hd-slave1</code>等就是主机ip-主机名映射</li>
<li><h1 id="todo-一定需要在-etc-hostname中设置各个主机名称"><a href="#todo-一定需要在-etc-hostname中设置各个主机名称" class="headerlink" title="todo?一定需要在/etc/hostname中设置各个主机名称"></a>todo?一定需要在<code>/etc/hostname</code>中设置各个主机名称</h1></li>
</ul>
<h4 id="firewalld"><a href="#firewalld" class="headerlink" title="firewalld"></a><code>firewalld</code></h4><p>必须关闭所有节点的防火墙</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo systemctl stop firewalld.service</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo systemctl <span class="built_in">disable</span> firewalld.service</span></span><br></pre></td></tr></table></figure>
<h4 id="文件夹建立"><a href="#文件夹建立" class="headerlink" title="文件夹建立"></a>文件夹建立</h4><ul>
<li>所有节点都需要建立</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir tmp</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p hdfs/data hdfs/name</span></span><br></pre></td></tr></table></figure>
<h3 id="Hadoop配置"><a href="#Hadoop配置" class="headerlink" title="Hadoop配置"></a>Hadoop配置</h3><p>Hadoop<strong>全系列</strong>（包括hive、tez等）配置取决于以下两类配置文件</p>
<ul>
<li><p>只读默认配置文件</p>
<ul>
<li><code>core-defualt.xml</code></li>
<li><code>hdfs-default.xml</code></li>
<li><code>mapred-default.xml</code></li>
</ul>
</li>
<li><p>随站点变化的配置文件</p>
<ul>
<li><code>etc/hadoop/core-site.xml</code></li>
<li><code>etc/hadoop/hdfs-site.xml</code></li>
<li><code>etc/hadoop/mapred-site.xml</code></li>
<li><code>etc/hadoop/yarn-env.xml</code></li>
</ul>
</li>
<li><p>环境设置文件：设置随站点变化的值，从而控制<code>bin/</code>中的
hadoop脚本行为</p>
<ul>
<li><code>etc/hadoop/hadoop-env.sh</code>、</li>
<li><code>etc/hadoop/yarn-env.sh</code></li>
<li><code>etc/hadoop/mapred-env.sh</code></li>
</ul>
<p>中一般是环境变量配置，<strong>补充</strong>在shell中未设置的环境变量</p>
</li>
<li><p>注意</p>
<ul>
<li><p><code>.xml</code>配置信息可在不同应用的配置文件中<strong>继承</strong>使用，
如在tez的配置中可以使用<code>core-site.xml</code>中
<code>$&#123;fs.defaultFS&#125;</code>变量</p>
</li>
<li><p>应用会读取/执行相应的<code>*_CONF_DIR</code>目录下所有
<code>.xml</code>/<code>.sh</code>文件，所以理论上可以在<code>etc/hadoop</code>中存放
所以配置文件，因为hadoop是最底层应用，在其他所有应用
启动前把环境均已设置完毕？？？</p>
</li>
</ul>
</li>
</ul>
<p>Hadoop集群有三种运行模式</p>
<ul>
<li>Standalone Operation</li>
<li>Pseudo-Distributed Operation</li>
<li>Fully-Distributed Operation</li>
</ul>
<p>针对不同的运行模式有，hadoop有三种不同的配置方式</p>
<h4 id="Standalone-Operation"><a href="#Standalone-Operation" class="headerlink" title="Standalone Operation"></a>Standalone Operation</h4><p>hadoop被配置为以非分布模式运行的一个独立Java进程，对调试有
帮助</p>
<ul>
<li>默认为单机模式，无需配置</li>
</ul>
<h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /path/to/hadoop</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cp etc/hadoop/*.xml input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.1.jar grep input output <span class="string">&#x27;dfs[a-z.]+&#x27;</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat output/*</span></span><br></pre></td></tr></table></figure>
<h4 id="Pseudo-Distributed-Operation"><a href="#Pseudo-Distributed-Operation" class="headerlink" title="Pseudo-Distributed Operation"></a>Pseudo-Distributed Operation</h4><p>在单节点（服务器）上以所谓的伪分布式模式运行，此时每个Hadoop
守护进程作为独立的Java进程运行</p>
<h5 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a><code>core-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a><code>hdfs-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a><code>mapred-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configruration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">preperty</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configruation</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a><code>yarn-site.xml</code></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<h4 id="Fully-Distributed-Operation"><a href="#Fully-Distributed-Operation" class="headerlink" title="Fully-Distributed Operation"></a>Fully-Distributed Operation</h4><ul>
<li><strong>单节点配置完hadoop之后，需要将其同步到其余节点</strong></li>
</ul>
<h5 id="core-site-xml-1"><a href="#core-site-xml-1" class="headerlink" title="core-site.xml"></a><code>core-site.xml</code></h5><p>模板：<code>core-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hd-master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode address<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///opt/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>131702<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 为将用户`root`设置为超级代理，代理所有用户，如果是其他用户需要相应的将root修改为其用户名 --&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 是为hive的JDBCServer远程访问而设置，应该有其他情况也需要 --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="hdfs-site-xml-1"><a href="#hdfs-site-xml-1" class="headerlink" title="hdfs-site.xml"></a><code>hdfs-site.xml</code></h5><p>模板：<code>hdfs-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///opt/hadoop/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>namenode data directory<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///opt/hadoop/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>datanode data directory<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>replication number<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.directoryscan.throttle.limit.ms.per.sec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!--bug--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="yarn-site-xml-1"><a href="#yarn-site-xml-1" class="headerlink" title="yarn-site.xml"></a><code>yarn-site.xml</code></h5><ul>
<li>模板：<code>yarn-site.xml</code></li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:9099<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- container --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>maximum memory allocation per container<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>256<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>minimum memory allocation per container<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- container --&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- node --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>maximium memory allocation per node<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>virtual memmory ratio<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- node --&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>384<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.command-opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xms128m -Xmx256m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="mapred-site-xml-1"><a href="#mapred-site-xml-1" class="headerlink" title="mapred-site.xml"></a><code>mapred-site.xml</code></h5><ul>
<li>模板：<code>mapred-site.xml.template</code></li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">		&lt;value&gt;yarn-tez&lt;/value&gt;</span></span><br><span class="line"><span class="comment">		设置整个hadoop运行在Tez上，需要配置好Tez</span></span><br><span class="line"><span class="comment">		--&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- mapreduce --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>256<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>memory allocation for map task, which should between minimum container and maximum<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>256<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>memory allocation for reduce task, which should between minimum container and maximum<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- mapreduce --&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!-- java heap size options --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xms128m -Xmx256m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xms128m -Xmx256m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- java heap size options --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h5><ul>
<li><p><code>yarn.scheduler.minimum-allocation-mb</code>：container内存
单位，也是container分配的内存最小值</p>
</li>
<li><p><code>yarn.scheduler.maximum-allocation-mb</code>：container内存
最大值，应该为最小值整数倍</p>
</li>
<li><p><code>mapreduce.map.memeory.mb</code>：map task的内存分配</p>
<ul>
<li>hadoop2x中mapreduce构建于YARN之上，资源由YARN统一管理</li>
<li>所以maptask任务的内存应设置container最小值、最大值间</li>
<li>否则分配一个单位，即最小值container</li>
</ul>
</li>
<li><p><code>mapreduce.reduce.memeory.mb</code>：reduce task的内存分配</p>
<ul>
<li>设置一般为map task的两倍</li>
</ul>
</li>
<li><p><code>*.java.opts</code>：JVM进程参数设置</p>
<ul>
<li>每个container（其中执行task）中都会运行JVM进程</li>
<li><code>-Xmx...m</code>：heap size最大值设置，所以此参数应该小于
task（map、reduce）对应的container分配内存的最大值，
如果超出会出现physical memory溢出</li>
<li><code>-Xms...m</code>：heap size最小值？#todo</li>
</ul>
</li>
<li><p><code>yarn.nodemanager.vmem-pmem-ratio</code>：虚拟内存比例</p>
<ul>
<li>以上所有配置都按照此参数放缩</li>
<li>所以在信息中会有physical memory、virtual memory区分</li>
</ul>
</li>
<li><p><code>yarn.nodemanager.resource.memory-mb</code>：节点内存设置</p>
<ul>
<li>整个节点被设置的最大内存，剩余内存共操作系统使用</li>
</ul>
</li>
<li><p><code>yarn.app.mapreduce.am.resource.mb</code>：每个Application
Manager分配的内存大小</p>
</li>
</ul>
<h4 id="主从文件"><a href="#主从文件" class="headerlink" title="主从文件"></a>主从文件</h4><h5 id="masters"><a href="#masters" class="headerlink" title="masters"></a><code>masters</code></h5><ul>
<li>设置主节点地址，根据需要设置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hd-master</span><br></pre></td></tr></table></figure>
<h5 id="slaves"><a href="#slaves" class="headerlink" title="slaves"></a><code>slaves</code></h5><ul>
<li>设置从节点地址，根据需要设置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hd-slave1</span><br><span class="line">hd-slave2</span><br></pre></td></tr></table></figure>
<h4 id="环境设置文件"><a href="#环境设置文件" class="headerlink" title="环境设置文件"></a>环境设置文件</h4><ul>
<li>这里环境设置只是起补充作用，在<code>~/.bashrc</code>已经设置的
环境变量可以不设置</li>
<li>但是在这里设置环境变量，然后把整个目录同步到其他节点，
可以保证在其余节点也能同样的设置环境变量</li>
</ul>
<h5 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a><code>hadoop-env.sh</code></h5><p>设置<code>JAVA_HOME</code>为Java安装根路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/opt/java/jdk</span><br></pre></td></tr></table></figure>
<h5 id="hdfs-env-sh"><a href="#hdfs-env-sh" class="headerlink" title="hdfs-env.sh"></a><code>hdfs-env.sh</code></h5><p>设置<code>JAVA_HOME</code>为Java安装根路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/opt/java/jdk</span><br></pre></td></tr></table></figure>
<h5 id="yarn-env-sh"><a href="#yarn-env-sh" class="headerlink" title="yarn-env.sh"></a><code>yarn-env.sh</code></h5><p>设置<code>JAVA_HOME</code>为Java安装根路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/opt/java/jdk</span><br><span class="line">JAVA_HEAP_MAX=Xmx3072m</span><br></pre></td></tr></table></figure>
<h4 id="初始化、启动、测试"><a href="#初始化、启动、测试" class="headerlink" title="初始化、启动、测试"></a>初始化、启动、测试</h4><h5 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h5><ul>
<li><p>格式化、启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs namenode -format</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 格式化文件系统</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-dfs.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 启动NameNode和DataNode</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 此时已可访问NameNode，默认http://localhost:9870/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> stop-dfs.sh</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfsadmin -report</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 应该输出3个节点的情况</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -mkdir /user</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -mkdir /user/&lt;username&gt;</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 创建执行MapReduce任务所需的HDFS文件夹</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -mkdir input</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -put etc/hadoop/*.xml input</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 复制文件至分布式文件系统</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar grep input output <span class="string">&#x27;dfs[a-z]+&#x27;</span></span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 执行自带样例</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 样例名称取决于版本</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -get output outut</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat output/*</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 检查输出文件：将所有的输出文件从分布式文件系统复制</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 至本地文件系统，并检查</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -cat output/*</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 或者之间查看分布式文件系统上的输出文件</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.7.jar \</span></span><br><span class="line"><span class="bash">	-input /path/to/hdfs_file \</span></span><br><span class="line"><span class="bash">	-output /path/to/hdfs_dir \</span></span><br><span class="line"><span class="bash">	-mapper <span class="string">&quot;/bin/cat&quot;</span> \</span></span><br><span class="line"><span class="bash">	-reducer <span class="string">&quot;/user/bin/wc&quot;</span> \</span></span><br><span class="line"><span class="bash">	-file /path/to/local_file \</span></span><br><span class="line"><span class="bash">	-numReduceTasks 1</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sbin/start-yarn.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 启动ResourceManger守护进程、NodeManager守护进程</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 即可访问ResourceManager的web接口，默认：http://localhost:8088/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sbin/stop-yarn.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 关闭守护进程</span></span><br></pre></td></tr></table></figure>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ul>
<li><p><code>hdfs namenode -format</code>甚至可以在datanode节点没有java时
成功格式化</p>
</li>
<li><p>没有关闭防火墙时，整个集群可以正常启动，甚至可以在hdfs里
正常建立文件夹，但是<strong>无法写入文件</strong>，尝试写入文件时报错</p>
</li>
</ul>
<h4 id="可能错误"><a href="#可能错误" class="headerlink" title="可能错误"></a>可能错误</h4><h5 id="节点启动不全"><a href="#节点启动不全" class="headerlink" title="节点启动不全"></a>节点启动不全</h5><ul>
<li><p>原因</p>
<ul>
<li>服务未正常关闭，节点状态不一致</li>
</ul>
</li>
<li><p>关闭服务、删除存储数据的文件夹<code>dfs/data</code>、格式化namenode</p>
</li>
</ul>
<h5 id="文件无法写入"><a href="#文件无法写入" class="headerlink" title="文件无法写入"></a>文件无法写入</h5><blockquote>
<p>   could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and 2 node(s) are excluded in this operation.</p>
</blockquote>
<ul>
<li><p>原因</p>
<ul>
<li>未关闭防火墙</li>
<li>存储空间不够</li>
<li>节点状态不一致、启动不全</li>
<li>在log里面甚至可能会出现一个连接超时1000ms的ERROR</li>
</ul>
</li>
<li><p>处理</p>
<ul>
<li>关闭服务、删除存储数据的文件夹<code>dfs/data</code>、格式化
namenode<ul>
<li>这样处理会丢失数据，不能用于生产环境</li>
</ul>
</li>
<li>尝试修改节点状态信息文件<code>VERSION</code>一致<ul>
<li><code>$&#123;hadoop.tmp.dir&#125;</code></li>
<li><code>$&#123;dfs.namenode.name.dir&#125;</code></li>
<li><code>$&#123;dfs.datanode.data.dir&#125;</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="Unhealthy-Node"><a href="#Unhealthy-Node" class="headerlink" title="Unhealthy Node"></a>Unhealthy Node</h5><blockquote>
<p>   1/1 local-dirs are bad: /opt/hadoop/tmp/nm-local-dir; 1/1 log-dirs are bad: /opt/hadoop/logs/userlogs</p>
</blockquote>
<ul>
<li>原因：磁盘占用超过90%</li>
</ul>
<h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/hadoop/etc/hadoop centos2:/opt/hadoop/etc</span><br><span class="line">scp -r /opt/hadoop/etc/hadoop centos3:/opt/hadoop/etc</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 同步配置</span></span><br><span class="line"></span><br><span class="line">scp /root/.bashrc centos2:/root</span><br><span class="line">scp /root/.bashrc centos3:/root</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 同步环境</span></span><br><span class="line"></span><br><span class="line">rm -r /opt/hadoop/tmp /opt/hadoop/hdfs</span><br><span class="line">mkdir -p /opt/hadoop/tmp /opt/hadoop/hdfs</span><br><span class="line">ssh centos2 rm -r /opt/hadoop/tmp /opt/hadoop/hdfs</span><br><span class="line">ssh centos2 mkdir -p /opt/hadoop/tmp /opt/hadoop/hdfs/name /opt/hadoop/hdfs/data</span><br><span class="line">ssh centos3 rm -r /opt/hadoop/tmp /opt/hadoop/hdfs/name /opt/hadoop/data</span><br><span class="line">ssh centos3 mkdir -p /opt/hadoop/tmp /opt/hadoop/hdfs/name /opt/hadoop/hdfs/data</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 同步清除数据</span></span><br><span class="line"></span><br><span class="line">rm -r /opt/hadoop/logs/*</span><br><span class="line">ssh centos2 rm -r /opt/hadoop/logs/*</span><br><span class="line">ssh centos3 rm -r /opt/hadoop/logs/*</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 同步清除<span class="built_in">log</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="依赖-1"><a href="#依赖-1" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>hadoop：配置完成hadoop，则相应java等也配置完成</li>
<li>关系型数据库：mysql、derby等</li>
</ul>
<h3 id="机器环境配置-1"><a href="#机器环境配置-1" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-1"><a href="#bashrc-1" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/opt/hive</span><br><span class="line"><span class="meta">	#</span><span class="bash"> self designed</span></span><br><span class="line">export HIVE_CONF_DIR=$HIVE_HOME/conf</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">export CLASSPATH=$CLASS_PATH:$HIVE_HOME/lib/*</span><br></pre></td></tr></table></figure>
<h4 id="文件夹建立-1"><a href="#文件夹建立-1" class="headerlink" title="文件夹建立"></a>文件夹建立</h4><h5 id="HDFS-1"><a href="#HDFS-1" class="headerlink" title="HDFS"></a>HDFS</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -rm -r /user/hive</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -mkdir -p /user/hive/warehouse /user/hive/tmp /user/hive/logs</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这三个目录与配置文件中对应</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hdfs dfs -chmod 777 /user/hive/warehouse /user/hive/tmp /user/hive/logs</span></span><br></pre></td></tr></table></figure>
<h5 id="FS"><a href="#FS" class="headerlink" title="FS"></a>FS</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir data</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> chmod 777 data</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> hive数据存储文件夹</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir logs</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> chmod 777 logs</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="built_in">log</span>目录</span></span><br></pre></td></tr></table></figure>
<h3 id="Hive配置"><a href="#Hive配置" class="headerlink" title="Hive配置"></a>Hive配置</h3><h4 id="XML参数"><a href="#XML参数" class="headerlink" title="XML参数"></a>XML参数</h4><h5 id="conf-hive-site-xml"><a href="#conf-hive-site-xml" class="headerlink" title="conf/hive-site.xml"></a><code>conf/hive-site.xml</code></h5><ul>
<li>模板：<code>conf/hive-default.xml.template</code></li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hd-master:3306/metastore_db?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.mariadb.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1234<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">&lt;property&gt;</span></span><br><span class="line"><span class="comment">	&lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;</span></span><br><span class="line"><span class="comment">	&lt;value&gt;$&#123;system:java.io.tmpdir&#125;/$&#123;system:user.name&#125;&lt;/value&gt;</span></span><br><span class="line"><span class="comment">&lt;/property&gt;</span></span><br><span class="line"><span class="comment">&lt;property&gt;</span></span><br><span class="line"><span class="comment">	&lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;</span></span><br><span class="line"><span class="comment">	&lt;valeu&gt;$&#123;system:java.io.tmpdir&#125;/$&#123;hive.session.id&#125;_resources&lt;/value&gt;</span></span><br><span class="line"><span class="comment">&lt;/property&gt;</span></span><br><span class="line"><span class="comment">&lt;property&gt;«</span></span><br><span class="line"><span class="comment">	&lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt;«</span></span><br><span class="line"><span class="comment">	&lt;value&gt;$&#123;system:java.io.tmpdir&#125;/$&#123;system:user.name&#125;/operation_logs&lt;/value&gt;«</span></span><br><span class="line"><span class="comment">	&lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt;«</span></span><br><span class="line"><span class="comment">&lt;/property&gt;«</span></span><br><span class="line"><span class="comment">所有`$&#123;system.java.io.tmpdir&#125;`都要被替换为相应的`/opt/hive/tmp`，</span></span><br><span class="line"><span class="comment">可以通过设置这两个变量即可，基本是用于设置路径</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>system:java.io.tmpdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hive/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>system:user.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">&lt;property&gt;</span></span><br><span class="line"><span class="comment">	&lt;name&gt;hive.querylog.location&lt;/name&gt;</span></span><br><span class="line"><span class="comment">	&lt;value&gt;/user/hive/logs&lt;/value&gt;</span></span><br><span class="line"><span class="comment">	&lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;</span></span><br><span class="line"><span class="comment">&lt;/property&gt;</span></span><br><span class="line"><span class="comment">这里应该不用设置，log放在本地文件系统更合适吧</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.31.129:19083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--这个是配置metastore，如果配置此选项，每次启动hive必须先启动metastore，否则hive实可直接启动--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.logging.operation.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 使用JDBCServer时需要配置，否则无法自行建立log文件夹，然后报错，手动创建可行，但是每次查询都会删除文件夹，必须查一次建一次 --&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>/user</code>开头的路径一般表示hdfs中的路径，而<code>$&#123;&#125;</code>变量开头
的路径一般表示本地文件系统路径</p>
<ul>
<li>变量<code>system:java.io.tmpdir</code>、<code>system:user.name</code>在
文件中需要自己设置，这样就避免需要手动更改出现这些
变量的地方</li>
<li><code>hive.querylog.location</code>设置在本地更好，这个日志好像
只在hive启动时存在，只是查询日志，不是hive运行日志，
hive结束运行时会被删除，并不是没有生成日志、<code>$&#123;&#125;</code>表示
HDFS路径</li>
</ul>
</li>
<li><p>配置中出现的目录（HDFS、locaL）有些手动建立</p>
<ul>
<li>HDFS的目录手动建立？</li>
<li>local不用</li>
</ul>
</li>
<li><p><code>hive.metastore.uris</code>若配置，则hive会通过metastore服务
访问元信息</p>
<ul>
<li>使用hive前需要启动metastore服务</li>
<li>并且端口要和配置文件中一样，否则hive无法访问</li>
</ul>
</li>
</ul>
<h4 id="环境设置文件-1"><a href="#环境设置文件-1" class="headerlink" title="环境设置文件"></a>环境设置文件</h4><h5 id="conf-hive-env-sh"><a href="#conf-hive-env-sh" class="headerlink" title="conf/hive-env.sh"></a><code>conf/hive-env.sh</code></h5><ul>
<li>模板：<code>conf/hive-env.sh.template</code></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/java/jdk</span><br><span class="line">export HADOOP_HOME=/opt/hadoop</span><br><span class="line">export HIVE_CONF_DIR=/opt/hive/conf</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 以上3者若在`~/.bashrc`中设置，则无需再次设置</span></span><br><span class="line">export HIVE_AUX_JARS_PATH=/opt/hive/lib</span><br></pre></td></tr></table></figure>
<h5 id="conf-hive-exec-log4j2-properties"><a href="#conf-hive-exec-log4j2-properties" class="headerlink" title="conf/hive-exec-log4j2.properties"></a><code>conf/hive-exec-log4j2.properties</code></h5><ul>
<li><p>模板：<code>hive-exec-log4j2.properties.template</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">property.hive.log.dir=/opt/hive/logs</span><br><span class="line">	# 原为`$&#123;sys:java.io.tmpdir&#125;/$&#123;sys:user.name&#125;`</span><br><span class="line">	# 即`/tmp/root`（root用户执行）</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="conf-hive-log4j2-properties"><a href="#conf-hive-log4j2-properties" class="headerlink" title="conf/hive-log4j2.properties"></a><code>conf/hive-log4j2.properties</code></h5><ul>
<li>模板：<code>hive-log4j2.properties.template</code></li>
</ul>
<h4 id="MetaStore"><a href="#MetaStore" class="headerlink" title="MetaStore"></a>MetaStore</h4><h5 id="MariaDB"><a href="#MariaDB" class="headerlink" title="MariaDB"></a>MariaDB</h5><ul>
<li><p>安装MariaDB</p>
</li>
<li><p>修改MariaDB配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp /user/share/mysql/my-huge.cnf /etc/my.cnf</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建用户，注意新创建用户可能无效，见mysql配置</p>
<ul>
<li>需要注意用户权限：创建数据库权限、修改表权限</li>
<li>初始化时Hive要自己创建数据库（<code>hive-site</code>中配置），
所以对权限比较严格的环境下，可能需要先行创建同名
数据库、赋权、删库</li>
</ul>
</li>
<li><p>下载<code>mariadb-java-client-x.x.x-jar</code>包，复制到<code>lib</code>中</p>
</li>
</ul>
<h5 id="初始化数据库"><a href="#初始化数据库" class="headerlink" title="初始化数据库"></a>初始化数据库</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> schematool -initSchema -dbType mysql</span></span><br></pre></td></tr></table></figure>
<p>这个命令要在所有配置完成之后执行</p>
<h4 id="服务设置"><a href="#服务设置" class="headerlink" title="服务设置"></a>服务设置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> hive --service metastore -p 19083 &amp;</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 启动metastore服务，端口要和hive中的配置相同</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 否则hive无法连接metastore服务，无法使用</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 终止metastore服务只能根据进程号`<span class="built_in">kill</span>`</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hive --service hiveserver2 --hiveconf hive.server2.thrift.port =10011 &amp;</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 启动JDBC Server</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 此时可以通过JDBC Client（如beeline）连接JDBC Server对</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> Hive中数据进行操作</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hive --service hiveserver2 --stop</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 停止JDBC Server</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 或者直接<span class="built_in">kill</span></span></span><br></pre></td></tr></table></figure>
<h4 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h4><h5 id="Hive可用性"><a href="#Hive可用性" class="headerlink" title="Hive可用性"></a>Hive可用性</h5><p>需要先启动hdfs、YARN、metastore database（mysql），如果有
设置独立metastore server，还需要在正确端口启动</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span>	<span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> words(id <span class="type">INT</span>, word STRING)</span><br><span class="line">		<span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot; &quot;</span><br><span class="line">		lines terminated <span class="keyword">by</span> &quot;\n&quot;;</span><br><span class="line">hive<span class="operator">&gt;</span>	load data <span class="keyword">local</span> inpath &quot;/opt/hive-test.txt&quot; overwrite <span class="keyword">into</span></span><br><span class="line">		<span class="keyword">table</span> words;</span><br><span class="line">hive<span class="operator">&gt;</span>	<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> words;</span><br></pre></td></tr></table></figure>
<h5 id="JDBCServer可用性"><a href="#JDBCServer可用性" class="headerlink" title="JDBCServer可用性"></a>JDBCServer可用性</h5><ul>
<li><p>命令行连接</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> beeline -u jdbc:hive2://localhost:10011 -n hive -p 1234</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>beeline中连接</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> beeline</span></span><br><span class="line"><span class="meta">beeline&gt;</span><span class="bash"> !connect jdbc:hive2://localhost:10011</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 然后输入用户名、密码（metastore数据库用户名密码）</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h3><h4 id="可能错误-1"><a href="#可能错误-1" class="headerlink" title="可能错误"></a>可能错误</h4><blockquote>
<p>   Failed with exception Unable to move source file</p>
</blockquote>
<ul>
<li>linux用户权限问题，无法操作原文件</li>
<li>hdfs用户权限问题，无法写入目标文件</li>
<li>hdfs配置问题，根本无法向hdfs写入：参见hdfs问题</li>
</ul>
<blockquote>
<p>   org.apache.hive.service.cli.HiveSQLException: Couldn’t find log associated with operation handle: </p>
</blockquote>
<ul>
<li><p>原因：hiveserver2查询日志文件夹不存在</p>
</li>
<li><p>可以在hive中通过</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="keyword">set</span> hive.server2.logging.operation.log.location;</span><br></pre></td></tr></table></figure>
<p>查询日志文件夹，建立即可，默认为
<code>$&#123;system:java.io.tmpdir&#125;/$&#123;system:user.name&#125;/operation_logs</code>
，并设置权限为777</p>
<ul>
<li>好像如果不设置权限为777，每次查询文件夹被删除，每
查询一次建立一次文件夹？#todo</li>
<li>在<code>hive-sitex.xml</code>中配置允许自行创建？</li>
</ul>
</li>
</ul>
<blockquote>
<p>   User: root is not allowed to impersonate hive</p>
</blockquote>
<ul>
<li><p>原因：当前用户（不一定是root）不被允许通过代理操作
hadoop用户、用户组、主机</p>
<ul>
<li>hadoop引入安全伪装机制，不允许上层系统直接将实际用户
传递给超级代理，此代理在hadoop上执行操作，避免客户端
随意操作hadoop</li>
</ul>
</li>
<li><p>配置hadoop的<code>core-site.xml</code>，使得当前用户作为超级代理</p>
</li>
</ul>
<h2 id="Tez"><a href="#Tez" class="headerlink" title="Tez"></a>Tez</h2><h3 id="依赖-2"><a href="#依赖-2" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>hadoop</li>
</ul>
<h3 id="机器环境配置-2"><a href="#机器环境配置-2" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-2"><a href="#bashrc-2" class="headerlink" title=".bashrc"></a><code>.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">export TEZ_HOME=/opt/tez</span><br><span class="line">export TEZ_CONF_DIR=$TEZ_HOME/conf</span><br><span class="line"></span><br><span class="line">for jar in `ls $TEZ_HOME | grep jar`; do</span><br><span class="line">	export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_HOME/$jar</span><br><span class="line">done</span><br><span class="line">for jar in `ls $TEZ_HOME/lib`; do</span><br><span class="line">	export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_HOME/lib/$jar</span><br><span class="line">done</span><br><span class="line"><span class="meta">	#</span><span class="bash"> this part could be replaced with line bellow</span></span><br><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$TEZ_HOME/*:$TEZ_HOME/lib/*</span><br><span class="line"><span class="meta">	#</span><span class="bash"> `hadoop-env.sh`中说`HADOOP_CLASSPATH`是Extra Java CLASSPATH</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> elements</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这意味着hadoop组件只需要把其jar包加到`HADOOP_CLASSPATH`中既可</span></span><br></pre></td></tr></table></figure>
<h4 id="HDFS-2"><a href="#HDFS-2" class="headerlink" title="HDFS"></a>HDFS</h4><ul>
<li><p>上传<code>$TEZ_HOME/share/tez.tar.gz</code>至HDFS中</p>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -mkdir /apps</span><br><span class="line">$ hdfs dfs -copyFromLocal tez.tar.gz /apps</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="HadoopOnTez"><a href="#HadoopOnTez" class="headerlink" title="HadoopOnTez"></a>HadoopOnTez</h3><p>在hadoop中配置Tez</p>
<ul>
<li><p>侵入性较强，对已有的hadoop集群全体均有影响</p>
</li>
<li><p>所有hadoop集群执行的MapReduce任务都通过tez执行</p>
<ul>
<li>这里所有的任务应该是指直接在hadoop上执行、能在
webRM上看到的任务</li>
<li>hive这样的独立组件需要独立配置</li>
</ul>
</li>
</ul>
<h4 id="XML参数-1"><a href="#XML参数-1" class="headerlink" title="XML参数"></a>XML参数</h4><h5 id="tez-site-xml"><a href="#tez-site-xml" class="headerlink" title="tez-site.xml"></a><code>tez-site.xml</code></h5><ul>
<li>模板：<code>conf/tez-default-tmplate.xml</code></li>
<li>好像还是需要复制到hadoop的配置文件夹中</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;tez.lib.uris&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;$&#123;fs.defaultFS&#125;/apps/tez.tar.gz&lt;/value&gt;</span><br><span class="line">	&lt;!--设置tez安装包位置--&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;tez.container.max.java.heap.fraction&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;0.2&lt;/value&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">内存不足时--&gt;</span><br></pre></td></tr></table></figure>
<h5 id="mapred-site-xml-2"><a href="#mapred-site-xml-2" class="headerlink" title="mapred-site.xml"></a><code>mapred-site.xml</code></h5><ul>
<li>修改<code>mapred-site.xml</code>文件：配置mapreduce基于<code>yarn-tez</code>，
（配置修改在hadoop部分也有）</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn-tez<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="环境参数"><a href="#环境参数" class="headerlink" title="环境参数"></a>环境参数</h4><h3 id="HiveOnTez"><a href="#HiveOnTez" class="headerlink" title="HiveOnTez"></a>HiveOnTez</h3><ul>
<li><p>此模式下Hive可以在mapreduce、tez计算模型下自由切换？</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.execution.engine<span class="operator">=</span>tez;</span><br><span class="line">	# 切换查询引擎为tez</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">set</span> hive.execution.engine<span class="operator">=</span>mr;</span><br><span class="line">	# 切换查询引擎为mapreduce</span><br><span class="line">	# 这些命令好像没用，只能更改值，不能更改实际查询模型</span><br></pre></td></tr></table></figure>
</li>
<li><p>只有Hive会受到影响，其他基于hadoop平台的mapreduce作业
仍然使用tez计算模型</p>
</li>
</ul>
<h4 id="Hive设置"><a href="#Hive设置" class="headerlink" title="Hive设置"></a>Hive设置</h4><ul>
<li>若已经修改了<code>mapred-site.xml</code>设置全局基于tez，则无需复制
jar包，直接修改<code>hive-site.xml</code>即可</li>
</ul>
<h5 id="Jar包复制"><a href="#Jar包复制" class="headerlink" title="Jar包复制"></a>Jar包复制</h5><p>复制<code>$TEZ_HOME</code>、<code>$TEZ_HOME/lib</code>下的jar包到<code>$HIVE_HOME/lib</code>
下即可</p>
<h5 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a><code>hive-site.xml</code></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;hive.execution.engine&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;tez&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<h3 id="其他-2"><a href="#其他-2" class="headerlink" title="其他"></a>其他</h3><h4 id="可能错误-2"><a href="#可能错误-2" class="headerlink" title="可能错误"></a>可能错误</h4><blockquote>
<p>   SLF4J: Class path contains multiple SLF4J bindings.</p>
</blockquote>
<ul>
<li>原因：包冲突的</li>
<li>解决方案：根据提示冲突包删除即可</li>
</ul>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="依赖-3"><a href="#依赖-3" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
<li>scala</li>
<li>python：一般安装anaconda，需要额外配置<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PYTHON_HOME=/opt/anaconda3</span><br><span class="line">export PATH=$PYTHON_HOME/bin:$PATH</span><br></pre></td></tr></table></figure></li>
<li>相应资源管理框架，如果不以standalone模式运行</li>
</ul>
<h3 id="机器环境配置-3"><a href="#机器环境配置-3" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-3"><a href="#bashrc-3" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/opt/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin</span><br><span class="line">export PYTHON_PATH=$PYTHON_PATH:$SPARK_HOME/python:$SPARK_HOME/python/lib/*</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 把`pyshark`、`py4j`模块对应的zip文件添加进路径</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里用的是`*`通配符应该也可以，手动添加所有zip肯定可以</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 否则无法在一般的python中对spark进行操作</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 似乎只要master节点有设置`/lib/*`添加`pyspark`、`py4j`就行</span></span><br></pre></td></tr></table></figure>
<h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><h4 id="环境设置文件-2"><a href="#环境设置文件-2" class="headerlink" title="环境设置文件"></a>环境设置文件</h4><h5 id="conf-spark-env-sh"><a href="#conf-spark-env-sh" class="headerlink" title="conf/spark-env.sh"></a><code>conf/spark-env.sh</code></h5><ul>
<li>模板：<code>conf/spark-env.sh.template</code></li>
</ul>
<p>这里应该有些配置可以省略、移除#todo</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/jdk</span><br><span class="line">export HADOOP_HOME=/opt/hadoop</span><br><span class="line">export hADOOP_CONF_DIR=/opt/hadoop/etc/hadoop</span><br><span class="line">export HIVE_HOME=/opt/hive</span><br><span class="line"></span><br><span class="line">export SCALA_HOME=/opt/scala</span><br><span class="line">export SCALA_LIBRARY=$SPARK_HOME/lib</span><br><span class="line"><span class="meta">	#</span><span class="bash"> `~/.bashrc`设置完成之后，前面这段应该就这个需要设置</span></span><br><span class="line"></span><br><span class="line">export SPARK_HOME=/opt/spark</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(hadoop classpath)</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里是执行命令获取classpath</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> todo</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里看文档的意思，应该也是类似于`<span class="variable">$HADOOP_CLASSPATH</span>`</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 可以直接添加进`<span class="variable">$CLASSPATH</span>`而不必设置此变量</span></span><br><span class="line">export SPARK_LIBRARY_PATH=$SPARK_HOME/lib</span><br><span class="line"></span><br><span class="line">export SPARK_MASTER_HOST=hd-master</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line">export SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line">export SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line">export SPARK_WORKER_MEMORY=1024m</span><br><span class="line"><span class="meta">	#</span><span class="bash"> spark能在一个container内执行多个task</span></span><br><span class="line">export SPARK_LOCAL_DIRS=$SPARK_HOME/data</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 需要手动创建</span></span><br><span class="line"></span><br><span class="line">export SPARK_MASTER_OPTS=</span><br><span class="line">export SPARK_WORKER_OPTS=</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=</span><br><span class="line">export SPARK_DAEMON_MEMORY=</span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=</span><br></pre></td></tr></table></figure>
<h5 id="文件夹建立-2"><a href="#文件夹建立-2" class="headerlink" title="文件夹建立"></a>文件夹建立</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir /opt/spark/spark_data</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> <span class="keyword">for</span> `<span class="variable">$SPARK_LOCAL_DIRS</span>`</span></span><br></pre></td></tr></table></figure>
<h4 id="Spark配置"><a href="#Spark配置" class="headerlink" title="Spark配置"></a>Spark配置</h4><h5 id="conf-slaves"><a href="#conf-slaves" class="headerlink" title="conf/slaves"></a><code>conf/slaves</code></h5><p>文件不存在，则在当前主机单节点运行</p>
<ul>
<li>模板：<code>conf/slaves.template</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hd-slave1</span><br><span class="line">hd-slave2</span><br></pre></td></tr></table></figure>
<h5 id="conf-hive-site-xml-1"><a href="#conf-hive-site-xml-1" class="headerlink" title="conf/hive-site.xml"></a><code>conf/hive-site.xml</code></h5><p>这里只是配置Spark，让Spark作为“thrift客户端”能正确连上
metastore server</p>
<ul>
<li>模板：<code>/opt/hive/conf/hive-site.xml</code></li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.31.129:19083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">description</span>&gt;</span>Thrift URI for the remote metastor. Used by metastore client to connect to remote metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>10011<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!--配置spark对外界thrift服务，以便可通过JDBC客户端存取spark--&gt;</span></span><br><span class="line">	<span class="comment">&lt;!--这里启动端口同hive的配置，所以两者不能默认同时启动--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="测试-2"><a href="#测试-2" class="headerlink" title="测试"></a>测试</h4><h5 id="启动Spark服务"><a href="#启动Spark服务" class="headerlink" title="启动Spark服务"></a>启动Spark服务</h5><p>需要启动hdfs、正确端口启动的metastore server</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> start-master.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 在执行**此命令**机器上启动master实例</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-slaves.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 在`conf/slaves`中的机器上启动worker实例</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> start-slave.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 在执行**此命令**机器上启动worker实例</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> stop-master.sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> stop-slaves.sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> stop-slave.sh</span></span><br></pre></td></tr></table></figure>
<h5 id="启动Spark-Thrift-Server"><a href="#启动Spark-Thrift-Server" class="headerlink" title="启动Spark Thrift Server"></a>启动Spark Thrift Server</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> start-thriftserver.sh --master spark://hd-master:7077 \</span></span><br><span class="line"><span class="bash">	--hiveconf hive.server2.thrift.bind.host hd-master \</span></span><br><span class="line"><span class="bash">	--hiveconf hive.server2.thrift.port 10011</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里在命令行启动thrift server时动态指定host、port</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> 如果在`conf/hive-site.xml`有配置，应该不需要</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 然后使用beeline连接thrift server，同hive</span></span><br></pre></td></tr></table></figure>
<h5 id="Spark-Sql测试"><a href="#Spark-Sql测试" class="headerlink" title="Spark-Sql测试"></a>Spark-Sql测试</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ spark<span class="operator">-</span><span class="keyword">sql</span> <span class="comment">--master spark://hd-master:7077</span></span><br><span class="line">	# 在含有配置文件的节点上启动时，配置文件中已经指定`MASTER`</span><br><span class="line">		# 因此不需要指定后面配置</span><br><span class="line"></span><br><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span><span class="operator">&gt;</span> <span class="keyword">set</span> spark.sql.shuffle.partitions<span class="operator">=</span><span class="number">20</span>;</span><br><span class="line">spark<span class="operator">-</span><span class="keyword">sql</span><span class="operator">&gt;</span> <span class="keyword">select</span> id, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> words <span class="keyword">group</span> <span class="keyword">by</span> id <span class="keyword">order</span> <span class="keyword">by</span> id;</span><br></pre></td></tr></table></figure>
<h5 id="pyspark测试"><a href="#pyspark测试" class="headerlink" title="pyspark测试"></a>pyspark测试</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ MASTER=spark://hd-master:<span class="number">7077</span> pyspark</span><br><span class="line">	<span class="comment"># 这里应该是调用`$PATH`中第一个python，如果未默认指定</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext</span><br><span class="line">sql_ctxt = HiveContext(sc)</span><br><span class="line">	<span class="comment"># 此`sc`是pyspark启动时自带的，是`SparkContext`类型实例</span></span><br><span class="line">	<span class="comment"># 每个连接只能有一个此实例，不能再次创建此实例</span></span><br><span class="line"></span><br><span class="line">ret = sql_ctxt.sql(<span class="string">&quot;show tables&quot;</span>).collect()</span><br><span class="line">	<span class="comment"># 这里语句结尾不能加`;` </span></span><br><span class="line"></span><br><span class="line">file = sc.textFile(<span class="string">&quot;hdfs://hd-master:9000/user/root/input/capacity-scheduler.xml&quot;</span>)</span><br><span class="line">file.count()</span><br><span class="line">file.first()</span><br></pre></td></tr></table></figure>
<h5 id="Scala测试"><a href="#Scala测试" class="headerlink" title="Scala测试"></a>Scala测试</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="type">MASTER</span>=spark:<span class="comment">//hd-master:7077 spark-shell \</span></span><br><span class="line">	executor-memory <span class="number">1024</span>m \</span><br><span class="line">	--total-executor-cores <span class="number">2</span> \</span><br><span class="line">	--excutor-cores <span class="number">1</span> \</span><br><span class="line">	# 添加参数启动`spark-shell`</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span>(sc)</span><br><span class="line">sqlContext.sql(<span class="string">&quot;select * from words&quot;</span>).collect().foreach(println)</span><br><span class="line">sqlContext.sql(<span class="string">&quot;select id, word from words order by id&quot;</span>).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">&quot;insert into words values(7, \&quot;jd\&quot;)&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.sql(<span class="string">&quot;select * from words&quot;</span>);</span><br><span class="line">df.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> df = spark.read.json(<span class="string">&quot;file:///opt/spark/example/src/main/resources/people.json&quot;</span>)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
<h3 id="Spark-on-YARN"><a href="#Spark-on-YARN" class="headerlink" title="Spark on YARN"></a>Spark on YARN</h3><h3 id="其他-3"><a href="#其他-3" class="headerlink" title="其他"></a>其他</h3><h4 id="可能错误-3"><a href="#可能错误-3" class="headerlink" title="可能错误"></a>可能错误</h4><blockquote>
<p>   Initial job has not accepted any resources;</p>
</blockquote>
<ul>
<li><p>原因：内存不足，spark提交application时内存超过分配给
worker节点内存</p>
</li>
<li><p>说明</p>
<ul>
<li>根据结果来看，<code>pyspark</code>、<code>spark-sql</code>需要内存比
<code>spark-shell</code>少？
（设置worker内存512m，前两者可以正常运行）</li>
<li>但是前两者的内存分配和scala不同，scala应该是提交任务
、指定内存大小的方式，这也可以从web-ui中看出来，只有
spark-shell开启时才算是<em>application</em></li>
</ul>
</li>
<li><p>解决方式</p>
<ul>
<li>修改<code>conf/spark-env.sh</code>中<code>SPARK_WORKER_MEMORY</code>更大，
（spark默认提交application内存为1024m）</li>
<li>添加启动参数<code>--executor-memory XXXm</code>不超过分配值</li>
</ul>
</li>
</ul>
<blockquote>
<pre><code>ERROR KeyProviderCache:87 - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider
</code></pre></blockquote>
<ul>
<li>无影响</li>
</ul>
<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><h3 id="依赖-4"><a href="#依赖-4" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
<li>hadoop</li>
<li>zookeeper：建议，否则日志不好管理</li>
</ul>
<h3 id="机器环境"><a href="#机器环境" class="headerlink" title="机器环境"></a>机器环境</h3><h4 id="bashrc-4"><a href="#bashrc-4" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/opt/hbase</span><br><span class="line">export PATH=$PAHT:$HBASE_HOME/bin</span><br><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_HOME/lib/*</span><br></pre></td></tr></table></figure>
<h4 id="建立目录"><a href="#建立目录" class="headerlink" title="建立目录"></a>建立目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir /tmp/hbase/tmpdir</span></span><br></pre></td></tr></table></figure>
<h3 id="HBase配置"><a href="#HBase配置" class="headerlink" title="HBase配置"></a>HBase配置</h3><h4 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h4><h5 id="conf-hbase-env-sh"><a href="#conf-hbase-env-sh" class="headerlink" title="conf/hbase-env.sh"></a><code>conf/hbase-env.sh</code></h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 不使用自带zookeeper</span></span><br></pre></td></tr></table></figure>
<h5 id="conf-zoo-cfg"><a href="#conf-zoo-cfg" class="headerlink" title="conf/zoo.cfg"></a><code>conf/zoo.cfg</code></h5><p>若设置使用独立zookeeper，需要复制zookeeper配置至HBase配置
文件夹中</p>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cp /opt/zookeeper/conf/zoo.cfg /opt/hbase/conf</span><br></pre></td></tr></table></figure>
<h4 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h4><h5 id="conf-hbase-site-xml"><a href="#conf-hbase-site-xml" class="headerlink" title="conf/hbase-site.xml"></a><code>conf/hbase-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;HBASE_HOME&#125;/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/zookeeper/zkdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="Pseudo-Distributed模式"><a href="#Pseudo-Distributed模式" class="headerlink" title="Pseudo-Distributed模式"></a>Pseudo-Distributed模式</h4><h5 id="conf-hbase-site-xml-1"><a href="#conf-hbase-site-xml-1" class="headerlink" title="conf/hbase-site.xml"></a><code>conf/hbase-site.xml</code></h5><ul>
<li>在Standalone配置上修改</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">proeperty</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hd-master:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="Fully-Distributed模式"><a href="#Fully-Distributed模式" class="headerlink" title="Fully-Distributed模式"></a>Fully-Distributed模式</h4><h5 id="conf-hbase-site-xml-2"><a href="#conf-hbase-site-xml-2" class="headerlink" title="conf/hbase-site.xml"></a><code>conf/hbase-site.xml</code></h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hd-master:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hd-master,hd-slave1,hd-slave2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/zookeeper/zkdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="测试-3"><a href="#测试-3" class="headerlink" title="测试"></a>测试</h4><ul>
<li>需要首先启动HDFS、YARN</li>
<li>使用独立zookeeper还需要先行在每个节点启动zookeeper</li>
</ul>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ start-hbase.sh</span><br><span class="line"><span class="code">	# 启动HBase服务</span></span><br><span class="line"><span class="code">$ local-regionservers.sh start 2 3 4 5</span></span><br><span class="line"><span class="code">	# 启动额外的4个RegionServer</span></span><br><span class="line"><span class="code">$ hbase shell</span></span><br><span class="line"><span class="code">hbase&gt; create &#x27;test&#x27;, &#x27;cf&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; list &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; put &#x27;test&#x27;, &#x27;row7&#x27;, &#x27;cf:a&#x27;, &#x27;value7a&#x27;</span></span><br><span class="line"><span class="code">	put &#x27;test&#x27;, &#x27;row7&#x27;, &#x27;cf:b&#x27;, &#x27;value7b&#x27;</span></span><br><span class="line"><span class="code">	put &#x27;test&#x27;, &#x27;row7&#x27;, &#x27;cf:c&#x27;, &#x27;value7c&#x27;</span></span><br><span class="line"><span class="code">	put &#x27;test&#x27;, &#x27;row8&#x27;, &#x27;cf:b&#x27;, &#x27;value8b&#x27;,</span></span><br><span class="line"><span class="code">	put &#x27;test&#x27;, &#x27;row9&#x27;, &#x27;cf:c&#x27;, &#x27;value9c&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; scan &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; get &#x27;test&#x27;, &#x27;row7&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; disable &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; enable &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbaee&gt; drop &#x27;test&#x27;</span></span><br><span class="line"><span class="code">hbase&gt; quit</span></span><br></pre></td></tr></table></figure>
<h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><h3 id="依赖-5"><a href="#依赖-5" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li><p>java</p>
</li>
<li><p>注意：zookeeper集群中工作超过半数才能对外提供服务，所以
一般配置服务器数量为奇数</p>
</li>
</ul>
<h3 id="机器环境-1"><a href="#机器环境-1" class="headerlink" title="机器环境"></a>机器环境</h3><h4 id="bashrc-5"><a href="#bashrc-5" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export ZOOKEEPER_HOME=/opt/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$ZOOKEEPER_HOME/lib</span><br></pre></td></tr></table></figure>
<h4 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h4><ul>
<li>在所有节点都需要创建相应文件夹、<code>myid</code>文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /tmp/zookeeper/zkdata /tmp/zookeeper/zkdatalog</span><br><span class="line">echo 0 &gt; /tmp/zookeeper/zkdatalog/myid</span><br><span class="line"></span><br><span class="line">ssh centos2 mkdir -p /tmp/zookeeper/zkdata /tmp/zookeeper/zkdatalog</span><br><span class="line">ssh centos3 mkdir -p /tmp/zookeeper/zkdata /tmp/zookeeper/zkdatalog</span><br><span class="line">ssh centos2 &quot;echo 2 &gt; /tmp/zookeeper/zkdata/myid&quot;</span><br><span class="line">ssh centos3 &quot;echo 3 &gt; /tmp/zookeeper/zkdata/myid&quot;</span><br></pre></td></tr></table></figure>
<h3 id="Zookeeper配置"><a href="#Zookeeper配置" class="headerlink" title="Zookeeper配置"></a>Zookeeper配置</h3><h4 id="Conf"><a href="#Conf" class="headerlink" title="Conf"></a>Conf</h4><h5 id="conf-zoo-cfg-1"><a href="#conf-zoo-cfg-1" class="headerlink" title="conf/zoo.cfg"></a><code>conf/zoo.cfg</code></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">	# The number of milliseconds of each tick</span><br><span class="line">initLimit=10</span><br><span class="line">	# The number of ticks that the initial</span><br><span class="line">	# synchronization phase can take</span><br><span class="line">syncLimit=5</span><br><span class="line">	# The number of ticks that can pass between</span><br><span class="line">	# sending a request and getting an acknowledgement</span><br><span class="line">dataDir=/tmp/zookeeper/zkdata</span><br><span class="line">dataLogDir=/tmp/zookeeper/zkdatalog</span><br><span class="line">	# the directory where the snapshot is stored.</span><br><span class="line">	# do not use /tmp for storage, /tmp here is just</span><br><span class="line">	# example sakes.</span><br><span class="line">clientPort=2181</span><br><span class="line">	# the port at which the clients will connect</span><br><span class="line"></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line">	# Be sure to read the maintenance section of the</span><br><span class="line">	# administrator guide before turning on autopurge.</span><br><span class="line">	# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span><br><span class="line">	# The number of snapshots to retain in dataDir</span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line">	# Purge task interval in hours</span><br><span class="line">	# Set to &quot;0&quot; to disable auto purge feature</span><br><span class="line"></span><br><span class="line">server.0=hd-master:2888:3888</span><br><span class="line">server.1=hd-slave1:2888:3888</span><br><span class="line">server.2=hd-slave2:2888:3888</span><br><span class="line">	# Determine the zookeeper servers</span><br><span class="line">	# fromation: server.NO=HOST:PORT1:PORT2</span><br><span class="line">		# PORT1: port used to communicate with leader</span><br><span class="line">		# PORT2: port used to reelect leader when current leader fail</span><br></pre></td></tr></table></figure>
<h5 id="dataDir-myid"><a href="#dataDir-myid" class="headerlink" title="$dataDir/myid"></a><code>$dataDir/myid</code></h5><ul>
<li><code>$dataDir</code>是<code>conf/zoo.cfg</code>中指定目录</li>
<li><code>myid</code>文件里就一个id，指明当前zookeeper server的id，服务
启动时读取文件确定其id，需要自行创建</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure>
<h4 id="启动、测试、清理"><a href="#启动、测试、清理" class="headerlink" title="启动、测试、清理"></a>启动、测试、清理</h4><p>启动zookeeper</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> zkServer.sh start</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 开启zookeeper服务</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> zookeeper服务要在各个节点分别手动启动</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> zkServer.sh status</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 查看服务状态</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> zkCleanup.sh</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 清理旧的快照、日志文件</span></span><br></pre></td></tr></table></figure>
<h2 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h2><h3 id="依赖-6"><a href="#依赖-6" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
</ul>
<h3 id="机器环境配置-4"><a href="#机器环境配置-4" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-6"><a href="#bashrc-6" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/opt/flume/bin</span><br></pre></td></tr></table></figure>
<h3 id="Flume配置"><a href="#Flume配置" class="headerlink" title="Flume配置"></a>Flume配置</h3><h4 id="环境设置文件-3"><a href="#环境设置文件-3" class="headerlink" title="环境设置文件"></a>环境设置文件</h4><h5 id="conf-flume-env-sh"><a href="#conf-flume-env-sh" class="headerlink" title="conf/flume-env.sh"></a><code>conf/flume-env.sh</code></h5><ul>
<li>模板：<code>conf/flume-env.sh.template</code></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/opt/jdk</span><br></pre></td></tr></table></figure>
<h4 id="Conf文件"><a href="#Conf文件" class="headerlink" title="Conf文件"></a>Conf文件</h4><h5 id="conf-flume-conf"><a href="#conf-flume-conf" class="headerlink" title="conf/flume.conf"></a><code>conf/flume.conf</code></h5><ul>
<li>模板：<code>conf/flume-conf.properties.template</code></li>
</ul>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">agent1.channels.ch1.type=memory</span><br><span class="line"><span class="code">	# define a memory channel called `ch1` on `agent1`</span></span><br><span class="line"><span class="code">agent1.sources.avro-source1.channels=ch1</span></span><br><span class="line"><span class="code">agent1.sources.avro-source1.type=avro</span></span><br><span class="line"><span class="code">agent1.sources.avro-source1.bind=0.0.0.0</span></span><br><span class="line"><span class="code">agent1.sources.avro-source1.prot=41414</span></span><br><span class="line"><span class="code">	# define an Avro source called `avro-source1` on `agent1` and tell it</span></span><br><span class="line"><span class="code">agent1.sink.log-sink1.channels=ch1</span></span><br><span class="line"><span class="code">agent1.sink.log-sink1.type=logger</span></span><br><span class="line"><span class="code">	# define a logger sink that simply logs all events it receives</span></span><br><span class="line"><span class="code">agent1.channels=ch1</span></span><br><span class="line"><span class="code">agent1.sources=avro-source1</span></span><br><span class="line"><span class="code">agent1.sinks=log-sink1</span></span><br><span class="line"><span class="code">	# Finally, all components have been defined, tell `agent1` which one to activate</span></span><br></pre></td></tr></table></figure>
<h4 id="启动、测试"><a href="#启动、测试" class="headerlink" title="启动、测试"></a>启动、测试</h4><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ flume-ng agent --conf /opt/flume/conf \</span><br><span class="line"><span class="code">	-f /conf/flume.conf \</span></span><br><span class="line"><span class="code">	-D flume.root.logger=DEBUG,console \</span></span><br><span class="line"><span class="code">	-n agent1</span></span><br><span class="line"><span class="code">	# the agent name specified by -n agent1` must match an agent name in `-f /conf/flume.conf`</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">$ flume-ng avro-client --conf /opt/flume/conf \</span><br><span class="line"><span class="code">	-H localhost -p 41414 \</span></span><br><span class="line"><span class="code">	-F /opt/hive-test.txt \</span></span><br><span class="line"><span class="code">	-D flume.root.logger=DEBUG, Console</span></span><br><span class="line"><span class="code">	# 测试flume</span></span><br></pre></td></tr></table></figure>
<h3 id="其他-4"><a href="#其他-4" class="headerlink" title="其他"></a>其他</h3><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><h3 id="依赖-7"><a href="#依赖-7" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
<li>zookeeper</li>
</ul>
<h3 id="机器环境变量"><a href="#机器环境变量" class="headerlink" title="机器环境变量"></a>机器环境变量</h3><h4 id="bashrc-7"><a href="#bashrc-7" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=$PATH:/opt/kafka/bin</span><br><span class="line">export KAFKA_HOME=/opt/kafka</span><br></pre></td></tr></table></figure>
<h3 id="多brokers配置"><a href="#多brokers配置" class="headerlink" title="多brokers配置"></a>多brokers配置</h3><h4 id="Conf-1"><a href="#Conf-1" class="headerlink" title="Conf"></a>Conf</h4><h5 id="config-server-1-properties"><a href="#config-server-1-properties" class="headerlink" title="config/server-1.properties"></a><code>config/server-1.properties</code></h5><ul>
<li>模板：<code>config/server.properties</code></li>
<li>不同节点<code>broker.id</code>不能相同</li>
<li>可以多编写几个配置文件，在不同节点使用不同配置文件启动</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">broker.id=0</span><br><span class="line">listeners=PLAINTEXT://:9093</span><br><span class="line">zookeeper.connect=hd-master:2181, hd-slave1:2181, hd-slave2:2181</span><br></pre></td></tr></table></figure>
<h4 id="测试-4"><a href="#测试-4" class="headerlink" title="测试"></a>测试</h4><ul>
<li>启动zookeeper</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kafka-server-start.sh /opt/kafka/config/server.properties &amp;</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 开启kafka服务（broker）</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这里是指定使用单个默认配置文件启动broker</span></span><br><span class="line"><span class="meta">		#</span><span class="bash"> 启动多个broker需要分别使用多个配置启动多次</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-server-stop.sh /opt/kafka/config/server.properties</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics.sh --create --zookeeper localhost:2181 \</span></span><br><span class="line"><span class="bash">	--replication-factor 1 \</span></span><br><span class="line"><span class="bash">	--partitions 1 \</span></span><br><span class="line"><span class="bash">	--topic test1</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 开启话题</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics.sh --list zookeeper localhost:2181</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-topics.shd --delete --zookeeper localhost:2181</span></span><br><span class="line">	--topic test1</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 关闭话题</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-producer.sh --broker-list localhost:9092 \</span></span><br><span class="line"><span class="bash">	--topic test1</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 新终端开启producer，可以开始发送消息</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer.sh --bootstrap-server localhost:9092 \</span></span><br><span class="line"><span class="bash">	--topic test1 \</span></span><br><span class="line"><span class="bash">	--from-beginning</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kafka-console-consumer.sh --zookeeper localhost:2181 \</span></span><br><span class="line"><span class="bash">	--topic test1 \</span></span><br><span class="line"><span class="bash">	--from beginning</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 新终端开启consumer，可以开始接收信息</span></span><br><span class="line"><span class="meta">	#</span><span class="bash"> 这个好像是错的</span></span><br></pre></td></tr></table></figure>
<h3 id="其他-5"><a href="#其他-5" class="headerlink" title="其他"></a>其他</h3><h2 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h2><h3 id="依赖-8"><a href="#依赖-8" class="headerlink" title="依赖"></a>依赖</h3><ul>
<li>java</li>
<li>zookeeper</li>
<li>python2.6+</li>
<li>ZeroMQ、JZMQ</li>
</ul>
<h3 id="机器环境配置-5"><a href="#机器环境配置-5" class="headerlink" title="机器环境配置"></a>机器环境配置</h3><h4 id="bashrc-8"><a href="#bashrc-8" class="headerlink" title="~/.bashrc"></a><code>~/.bashrc</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export STORM_HOME=/opt/storm</span><br><span class="line">export PAT=$PATH:$STORM_HOME/bin</span><br></pre></td></tr></table></figure>
<h3 id="Storm配置"><a href="#Storm配置" class="headerlink" title="Storm配置"></a>Storm配置</h3><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><h5 id="conf-storm-yaml"><a href="#conf-storm-yaml" class="headerlink" title="conf/storm.yaml"></a><code>conf/storm.yaml</code></h5><ul>
<li>模板：<code>conf/storm.yarml</code></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">storm.zookeeper.servers:</span><br><span class="line">	-hd-master</span><br><span class="line">	-hd-slave1</span><br><span class="line">	-hd-slave2</span><br><span class="line">storm.zookeeper.port: 2181</span><br><span class="line"></span><br><span class="line">nimbus.seeds: [hd-master]</span><br><span class="line">storm.local.dir: /tmp/storm/tmp</span><br><span class="line">nimbus.host: hd-master</span><br><span class="line">supervisor.slots.ports:</span><br><span class="line">	-6700</span><br><span class="line">	-6701</span><br><span class="line">	-6702</span><br><span class="line">	-6703</span><br></pre></td></tr></table></figure>
<h4 id="启动、测试-1"><a href="#启动、测试-1" class="headerlink" title="启动、测试"></a>启动、测试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">storm nimbus &amp;&gt; /dev/null &amp;</span><br><span class="line">storm logviewer &amp;&gt; /dev/null &amp;</span><br><span class="line">storm ui &amp;&gt; /dev/null &amp;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> master节点启动nimbus</span></span><br><span class="line"></span><br><span class="line">storm sueprvisor &amp;&gt; /dev/null &amp;</span><br><span class="line">storm logviewer &amp;&gt; /dev/nulla &amp;</span><br><span class="line"><span class="meta">	#</span><span class="bash"> worker节点启动</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">storm jar /opt/storm/example/..../storm-start.jar \</span><br><span class="line">	storm.starter.WordCountTopology</span><br><span class="line"><span class="meta">	#</span><span class="bash"> 测试用例</span></span><br><span class="line">stom kill WordCountTopology</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r3.1.1">http://hadoop.apache.org/docs/r3.1.1</a></p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/WWxwMVBsOUtoNTdMSlJ4Z1lWVnRISERsd2t6ek9MeVpEUWs0YldlZGxUdXlKdDNmMEZVV1hWaFZFYWFSNmFKL25penZodWx3UzRiaVkxcnREWCtOYUJhZWhNbWpzaUdyc1hPangycUh5RTVjRXFnZnFGdVdSTzZvVzJBcTJHKzl8aXpDK1ROWWl4N080YkFEK3QvbEVWNEJuQjFqdWdxODZQcGNoM1NqbERXST0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>