<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: Regularization - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="follow_it-verification-code" content="SVBypAPPHxjjr7Y4hHfn"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Regularization</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-25T13:53:20.000Z" title="8/25/2019, 9:53:20 PM">2019-08-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T09:40:21.000Z" title="8/4/2021, 5:40:21 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Loss/">Loss</a></span><span class="level-item">21 minutes read (About 3154 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Loss/loss_thoery.html">损失函数理论</a></h1><div class="content"><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><ul>
<li><p>矩估计：<strong>建立参数和总体矩的关系</strong>，求解参数</p>
<ul>
<li>除非参数本身即为样本矩，否则基本无应用价值</li>
<li>应用场合<ul>
<li>均值：对应二次损失 $\arg\min<em>{\mu} \sum</em>{i=1}^N (x_i - \mu)^2$</li>
<li>方差：对应二次损失?</li>
</ul>
</li>
</ul>
</li>
<li><p>极大似然估计：极大化似然函数，求解概率上最合理参数</p>
<ul>
<li>需知道（假设）总体 <strong>概率分布形式</strong></li>
<li>似然函数形式复杂，求解困难<ul>
<li>往往无法直接给出参数的解析解，只能求数值解</li>
</ul>
</li>
<li>应用场合<ul>
<li>估计回归参数：对数损失
$\mathop{\arg\min}<em>{\beta} \sum</em>{i=1}^N lnP(y_i|x_i, \beta)$</li>
</ul>
</li>
</ul>
</li>
<li><p>损失函数估计：极小化损失函数，求解损失最小的参数</p>
<ul>
<li>最泛用的参数求解方法<ul>
<li>适合求解有大量参数的待求解的问题</li>
<li>往往通过迭代方式逐步求解</li>
</ul>
</li>
<li>特别的<ul>
<li>线性回归使用 <em>MSE</em> 作为损失函数时，也被称为最小二乘估计</li>
<li>极大似然估计同对数损失函数</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>参数估计都可以找到合适损失函数，通过迭代求解损失最小化</li>
</ul>
</blockquote>
<h3 id="随机模拟估计参数"><a href="#随机模拟估计参数" class="headerlink" title="随机模拟估计参数"></a>随机模拟估计参数</h3><ul>
<li>需要<strong>设计随机模拟实验</strong>估计参数</li>
<li>应用场合<ul>
<li>蒙特卡洛类似算法：随机化损失</li>
</ul>
</li>
</ul>
<h3 id="迭代求解参数"><a href="#迭代求解参数" class="headerlink" title="迭代求解参数"></a>迭代求解参数</h3><ul>
<li><p>损失函数定义不同</p>
<ul>
<li>包含样本量数量不同</li>
<li>惩罚项设置不同</li>
</ul>
</li>
<li><p>异步更新参数</p>
<ul>
<li>同时求解参数数量：全部、部分、单个</li>
<li>参数升维</li>
</ul>
</li>
<li><p>更新方向</p>
<ul>
<li>梯度</li>
<li>海瑟矩阵</li>
<li>次梯度</li>
</ul>
</li>
<li><p>更新方式</p>
<ul>
<li>叠加惯性</li>
<li>动态学习率</li>
</ul>
</li>
</ul>
<h2 id="Loss-Models"><a href="#Loss-Models" class="headerlink" title="Loss Models"></a><em>Loss Models</em></h2><p>模型（目标函数）在样本整体的损失：度量模型整体预测效果</p>
<ul>
<li>代表模型在整体上的性质，有不同的设计形式</li>
<li><p>可以用于 <strong>设计学习策略、评价模型</strong></p>
<ul>
<li>风险函数</li>
<li>评价函数</li>
</ul>
</li>
<li><p>有时在算法中也会使用整体损失</p>
</li>
</ul>
<h3 id="Expected-Risk-Expected-Loss-Generalization-Loss"><a href="#Expected-Risk-Expected-Loss-Generalization-Loss" class="headerlink" title="Expected Risk / Expected Loss / Generalization Loss"></a><em>Expected Risk</em> / <em>Expected Loss</em> / <em>Generalization Loss</em></h3><p>期望风险（函数）：损失函数 $L(Y, f(X))$（随机变量）期望</p>
<script type="math/tex; mode=display">
R_{exp}(f) = E_p[L(Y, f(X))] = \int_{x*y} L(y,f(x))P(x,y) dxdy</script><blockquote>
<ul>
<li>$P(X, Y)$：随机变量 $(X, Y)$ 遵循的联合分布，未知</li>
</ul>
</blockquote>
<ul>
<li><p>风险函数值度量模型预测错误程度</p>
<ul>
<li>反映了学习方法的泛化能力</li>
<li>评价标准（<strong>监督学习目标</strong>）就应该是选择期望风险最小</li>
</ul>
</li>
<li><p>联合分布未知，所以才需要学习，否则可以直接计算条件分布概率，而计算期望损失需要知道联合分布，因此监督学习是一个病态问题</p>
</li>
</ul>
<h3 id="Empirical-Risk-Empirical-Loss"><a href="#Empirical-Risk-Empirical-Loss" class="headerlink" title="Empirical Risk / Empirical Loss"></a><em>Empirical Risk</em> / <em>Empirical Loss</em></h3><p>经验风险：模型关于给定训练数据集的平均损失</p>
<script type="math/tex; mode=display">\begin{align*}
R_{emp}(f) & = \sum_{i=1}^N D_i L(y_i, f(x_i;\theta)) \\
E(R_{emp}(f)) & = R_{exp}(f)
\end{align*}</script><blockquote>
<ul>
<li>$\theta$：模型参数</li>
<li>$D_i$：样本损失权重，常为 $\frac 1 N$，在 <em>Boosting</em> 框架中不同</li>
</ul>
</blockquote>
<ul>
<li><p>经验风险损失是模型 $f(x)$ 的函数</p>
<ul>
<li>训练时，模型是模型参数的函数</li>
<li>即其为模型参数函数</li>
</ul>
</li>
<li><p>根据大数定律，样本量容量 $N$ 趋于无穷时，$R<em>{emp}(f)$ 趋于 $R</em>{exp}(f)$</p>
<ul>
<li>但是现实中训练样本数目有限、很小</li>
<li>利用经验风险估计期望常常并不理想，需要对经验风险进行矫正</li>
</ul>
</li>
<li><p>例子</p>
<ul>
<li><em>maximum probability estimation</em>：极大似然估计<ul>
<li>模型：条件概率分布（贝叶斯生成模型、逻辑回归）</li>
<li>损失函数：对数损失函数</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Structual-Risk-Structual-Loss"><a href="#Structual-Risk-Structual-Loss" class="headerlink" title="Structual Risk / Structual Loss"></a><em>Structual Risk</em> / <em>Structual Loss</em></h3><p>结构风险：在经验风险上加上表示 <strong>模型复杂度</strong> 的 <em>regularizer</em>（<em>penalty term</em>）</p>
<script type="math/tex; mode=display">
R_{srm} = \frac 1 N \sum_{i=1}^N L(y_i, f(x_i)) +
    \lambda J(f)</script><blockquote>
<ul>
<li>$J(f)$：模型复杂度，定义在假设空间$F$上的泛函</li>
<li>$\lambda$：权衡经验风险、模型复杂度的系数</li>
</ul>
</blockquote>
<ul>
<li>结构风险最小化<ul>
<li>添加 <em>regularization</em>（正则化），调节损失函数（目标函数）</li>
</ul>
</li>
<li>模型复杂度 $J(f)$ 表示对复杂模型的惩罚：模型 $f$ 越复杂，复杂项 $J(f)$ 越大</li>
<li>案例<ul>
<li><em>maximum posterior probability estimation</em>：最大后验概率估计<ul>
<li>损失函数：对数损失函数</li>
<li>模型复杂度：模型先验概率对数后取负</li>
<li>先验概率对应模型复杂度，先验概率越小，复杂度越大</li>
</ul>
</li>
<li>岭回归：平方损失 + $L<em>2$ 正则化
$\mathop{\arg\min}</em>{\beta} \sum_{i=1}^N (y_i - f(x_i, \beta))^2 + |\beta|$</li>
<li><em>LASSO</em>：平方损失 + $L<em>1$ 正则化
$\mathop{\arg\min}</em>{\beta} \sum_{i=1}^N (y_i - f(x_i, \beta))^2 + |\beta|_1$</li>
</ul>
</li>
</ul>
<h2 id="Generalization-Ability"><a href="#Generalization-Ability" class="headerlink" title="Generalization Ability"></a><em>Generalization Ability</em></h2><p>泛化能力：方法学习到的模型对未知数据的预测能力，是学习方法本质、重要的性质</p>
<ul>
<li>测试误差衡量学习方法的泛化能力不可靠，其依赖于测试集，而测试集有限</li>
<li>学习方法的泛化能力往往是通过研究泛化误差的概率上界进行</li>
</ul>
<h3 id="Generalization-Error-Bound"><a href="#Generalization-Error-Bound" class="headerlink" title="Generalization Error Bound"></a>Generalization Error Bound</h3><p>泛化误差上界：泛化误差的 <strong>概率</strong> 上界</p>
<ul>
<li>是样本容量函数，样本容量增加时，泛化上界趋于 0</li>
<li>是假设空间容量函数，假设空间容量越大，模型越难学习，泛化误差上界越大</li>
</ul>
<h4 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h4><ul>
<li><p>根据 <em>Hoeffding</em> 不等式，泛化误差满足</p>
<script type="math/tex; mode=display">\begin{align*}
& \forall h \in H, & P(|E(h) - \hat E(h)| \geq \epsilon) \leq 2 e^{-2 N \epsilon^2} \\
\Rightarrow & \forall h \in H, & P(|E(h) - \hat E(h)|
   \leq \epsilon) \geq 1 - 2|H|e^{-2N\epsilon^2}
\end{align*}</script><blockquote>
<ul>
<li>$H$：假设空间</li>
<li>$N$：样本数量</li>
<li>$E(h) := R_{exp}(h)$</li>
<li>$\hat E(h) := R_{emp}(h)$</li>
</ul>
</blockquote>
</li>
<li><p>证明如下：</p>
<script type="math/tex; mode=display">\begin{align*}
P(\forall h \in H: |E(h) - \hat E(h)| \leq \epsilon|)
   & = 1 - P(\exists h \in H: |E(h) - \hat E(h)|
   \geq \epsilon) \\
& = 1 - P((|E(h_1) - \hat E(h_1) \geq \epsilon) \vee \cdots
   \vee (|E(h_{|H|}) - \hat E_{|H|}| \geq \epsilon)) \\
& \geq 1 - \sum_{i=1}^{|H|} P(|E(h_i) - \hat E(h_i)|
   \geq \epsilon) \\
& \geq 1 - 2|H|e^{-2N \epsilon^2}
\end{align*}</script></li>
<li><p>对任意 $\epsilon$，随样本数量 $m$ 增大， $|E(h) - \hat E(h)| \leq \epsilon$ 概率增大，可以使用经验误差近似泛化误差</p>
</li>
</ul>
<h4 id="二分类泛化误差上界"><a href="#二分类泛化误差上界" class="headerlink" title="二分类泛化误差上界"></a>二分类泛化误差上界</h4><ul>
<li><p>由 <em>Hoeffding</em> 不等式</p>
<script type="math/tex; mode=display">\begin{align*}
P(E(h) - \hat E(h) & \geq \epsilon) \leq exp(-2N\epsilon^2) \\
P(\exists h \in H: E(h) - \hat E(h) \geq \epsilon) & =
   P(\bigcup_{h \in H} \{ E(h) - \hat E(h) \geq \epsilon \}) \\
& \leq \sum_{h \in H} P(E(h) - \hat E(h) \geq \epsilon) \\
& \leq |H| exp(-2 N \epsilon^2)
\end{align*}</script></li>
<li><p>则 $\forall h \in H$，有</p>
<script type="math/tex; mode=display">
P(E(h) - \hat E(h) < \epsilon) \geq 1 - |H| exp(-2 N \epsilon)</script><p>则令 $\sigma = |H| exp(-2N\epsilon^2)$，则至少以概率 $1-\sigma$ 满足如下，即得到泛化误差上界</p>
<script type="math/tex; mode=display">\begin{align*}
E(h)  & \leq \hat E(h) + \epsilon(|H|, N, \sigma) \\
\epsilon(|H|, N, \sigma) & = \sqrt
   {\frac 1 {2N} (log |H| + log \frac 1 {\sigma})}
\end{align*}</script></li>
</ul>
<h3 id="Probably-Approximate-Correct-可学习"><a href="#Probably-Approximate-Correct-可学习" class="headerlink" title="Probably Approximate Correct 可学习"></a><em>Probably Approximate Correct</em> 可学习</h3><p><em>PAC</em> 可学习：在短时间内利用少量（多项式级别）样本能够找到假设 $h^{‘}$，满足</p>
<script type="math/tex; mode=display">
P(E(h^{'}) \leq \epsilon) \geq 1 - \sigma, 0 < \epsilon, \sigma < 1</script><ul>
<li><p>即需要假设满足两个 <em>PAC</em> 辨识条件</p>
<ul>
<li>近似条件：泛化误差 $E(h^{‘})$ 足够小</li>
<li>可能正确：满足近似条件概率足够大</li>
</ul>
</li>
<li><p>同等条件下</p>
<ul>
<li>模型越复杂，泛化误差越大</li>
<li>满足条件的样本数量越大，模型泛化误差越小</li>
</ul>
</li>
<li><p><em>PAC</em> 学习理论关心能否从假设空间 $H$ 中学习到好的假设 $h$</p>
<ul>
<li>由以上泛化误差可得，取 $\sigma = 2|H|e^{-2N\epsilon^2}$，则样本量满足 $N = \frac {ln \frac {2|H|} \sigma} {2 \epsilon^2}$ 时，模型是 <em>PAC</em> 可学习的</li>
</ul>
</li>
</ul>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a><em>Regularization</em></h2><p>正则化：（向目标函数）添加额外信息以求解病态问题、避免过拟合</p>
<ul>
<li><p>常应用在机器学习、逆问题求解</p>
<ul>
<li>对模型（目标函数）复杂度惩罚</li>
<li>提高学习模型的泛化能力、避免过拟合</li>
<li>学习简单模型：稀疏模型、引入组结构</li>
</ul>
</li>
<li><p>有多种用途</p>
<ul>
<li>最小二乘也可以看作是简单的正则化</li>
<li>岭回归中的 $\mathcal{l_2}$ 范数</li>
</ul>
</li>
</ul>
<h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>模型复杂度：经常作为正则化项添加作为额外信息添加的，衡量模型复杂度方式有很多种</p>
<ul>
<li><p>函数光滑限制</p>
<ul>
<li>多项式最高次数</li>
</ul>
</li>
<li><p>向量空间范数</p>
<ul>
<li>$\mathcal{L_0} - norm$：参数个数</li>
<li>$\mathcal{L_1} - norm$：参数绝对值和</li>
<li>$\mathcal{L_2}$- norm$：参数平方和</li>
</ul>
</li>
</ul>
<h3 id="mathcal-L-0-norm"><a href="#mathcal-L-0-norm" class="headerlink" title="$\mathcal{L_0} - norm$"></a>$\mathcal{L_0} - norm$</h3><ul>
<li>$\mathcal{l_0} - norm$ 特点<ul>
<li>稀疏化约束</li>
<li>解 $\mathcal{L_0}$ 范数正则化是 <em>NP-hard</em> 问题</li>
</ul>
</li>
</ul>
<h3 id="mathcal-L-1-norm"><a href="#mathcal-L-1-norm" class="headerlink" title="$\mathcal{L_1} - norm$"></a>$\mathcal{L_1} - norm$</h3><ul>
<li><p>$\mathcal{L_1} - norm$ 特点</p>
<ul>
<li>$\mathcal{L_1}$ 范数可以通过凸松弛得到 $\mathcal{L_0}$ 的近似解</li>
<li>有时候出现解不唯一的情况</li>
<li>$\mathcal{L_1}$ 范数凸但不严格可导，可以使用依赖次梯度的方法求解极小化问题</li>
</ul>
</li>
<li><p>应用</p>
<ul>
<li><em>LASSO</em></li>
</ul>
</li>
<li><p>求解</p>
<ul>
<li><em>Proximal Method</em></li>
<li><em>LARS</em></li>
</ul>
</li>
</ul>
<h3 id="mathcal-L-2-norm"><a href="#mathcal-L-2-norm" class="headerlink" title="$\mathcal{L_2} - norm$"></a>$\mathcal{L_2} - norm$</h3><ul>
<li>$\mathcal{L_2} - norm$ 特点<ul>
<li>凸且严格可导，极小化问题有解析解</li>
</ul>
</li>
</ul>
<h3 id="mathcal-L-1-L-2"><a href="#mathcal-L-1-L-2" class="headerlink" title="$\mathcal{L_1 + L_2}$"></a>$\mathcal{L_1 + L_2}$</h3><ul>
<li><p>$\mathcal{L_1 + L_2}$ 特点</p>
<ul>
<li>有组效应，相关变量权重倾向于相同</li>
</ul>
</li>
<li><p>应用</p>
<ul>
<li><em>Elastic Net</em></li>
</ul>
</li>
</ul>
<h3 id="稀疏解产生"><a href="#稀疏解产生" class="headerlink" title="稀疏解产生"></a>稀疏解产生</h3><p>稀疏解：待估参数系数在某些分量上为 0</p>
<h4 id="mathcal-L-1-norm-稀疏解的产生"><a href="#mathcal-L-1-norm-稀疏解的产生" class="headerlink" title="$\mathcal{L_1} - norm$ 稀疏解的产生"></a>$\mathcal{L_1} - norm$ 稀疏解的产生</h4><blockquote>
<ul>
<li>$\mathcal{L_1}$ 范数在参数满足 <strong>一定条件</strong> 情况下，能对 <strong>平方损失</strong> 产生稀疏效果</li>
</ul>
</blockquote>
<ul>
<li><p>在 $[-1,1]$ 内 $y=|x|$ 导数大于 $y=x^2$（除 0 点）</p>
<ul>
<li>则特征在 0 点附近内变动时，为了取到极小值，参数必须始终为 0</li>
<li>高阶项在 0 点附近增加速度较慢，所以 $\mathcal{L_1} - norm$ 能产生稀疏解是很广泛的</li>
<li>$mathcal{L_1} - norm$ 前系数（权重）越大，能够容许高阶项增加的幅度越大，即压缩能力越强</li>
</ul>
</li>
<li><p>在 0 附近导数 “不小”，即导数在 0 点非 0</p>
<ul>
<li>对多项式正则化项<ul>
<li>$\mathcal{L_1} - norm$ 项对稀疏化解起决定性作用</li>
<li>其他项对稀疏解无帮助</li>
</ul>
</li>
<li>对“非多项式”正则化项<ul>
<li>$e^{|x|}-1$、$ln(|x|+1)$ 等在0点泰勒展开同样得到 $\mathcal{L_1} - norm$ 项</li>
<li>但是此类正则化项难以计算数值，不常用</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="mathcal-L-1-norm-稀疏解推广"><a href="#mathcal-L-1-norm-稀疏解推广" class="headerlink" title="$\mathcal{L_1} - norm$ 稀疏解推广"></a>$\mathcal{L_1} - norm$ 稀疏解推广</h4><ul>
<li><p>正负差异化：在正负设置权重不同的 $\mathcal{L_1}$，赋予在正负不同的压缩能力，甚至某侧完全不压缩</p>
</li>
<li><p>分段函数压缩：即只要保证在 0 点附近包含 $\mathcal{L_1}$ 用于产生稀疏解，远离 0 处可以设计为常数等不影响精确解的值</p>
<ul>
<li><p><em>Smoothly Clipped Absolute Deviation</em></p>
<script type="math/tex; mode=display">
R(x|\lambda, \gamma) = \left \{ \begin{array} {l}
  \lambda|x| \qquad & if |x| \leq \lambda \\
  \frac {2\gamma\lambda|x| - x^2 - {\lambda}^2 }
      {2(\gamma - 1)} &
      if \gamma< |x| <\gamma\lambda \\
  \frac { {\lambda}^2(\gamma+1)} 2 &
      if |x| \geq \gamma\lambda
\end{array} \right.</script></li>
<li><p><em>Derivate of SCAD</em></p>
<script type="math/tex; mode=display">
R(x; \lambda, \gamma) = \left \{ \begin{array} {l}
  \lambda \qquad & if |x| \leq \gamma \\
  \frac {\gamma\lambda - |x|} {\gamma - 1} &
      if \lambda < |x| < \gamma\lambda \\
  0 & if |x| \geq \gamma\lambda
\end{array} \right.</script></li>
<li><p><em>Minimax Concave Penalty</em></p>
<script type="math/tex; mode=display">
R_{\gamma}(x;\lambda) = \left \{ \begin{array} {l}
  \lambda|x| - \frac {x^2} {2\gamma} \qquad &
      if |x| \leq \gamma\lambda \\
  \frac 1 2 \gamma{\lambda}^2 &
      if |x| > \gamma\lambda
\end{array} \right.</script></li>
</ul>
</li>
<li><p>分指标：对不同指标动态设置 $\mathcal{L_0}$ 系数</p>
<ul>
<li><em>Adaptive Lasso</em>：$\lambda \sum_J w_jx_j$</li>
</ul>
</li>
</ul>
<h4 id="稀疏本质"><a href="#稀疏本质" class="headerlink" title="稀疏本质"></a>稀疏本质</h4><p>稀疏本质：极值、<strong>不光滑</strong>，即导数符号突然变化</p>
<ul>
<li><p>若某约束项导数符号突然变化、其余项在该点处导数为 0，为保证仍然取得极小值，解会聚集（极小）、疏远（极大）该点（类似坡的陡峭程度）</p>
<ul>
<li>即此类不光滑点会<strong>抑制解的变化</strong>，不光滑程度即导数变化幅度越大，抑制解变化能力越强，即吸引、排斥解能力越强</li>
<li>容易构造压缩至任意点的约束项</li>
<li>特殊的，不光滑点为 0 时，即得到稀疏解</li>
</ul>
</li>
<li><p>可以设置的多个极小不光滑点，使得解都在不连续集合中</p>
<ul>
<li>可以使用三角函数、锯齿函数等构造，但此类约束项要起效果，必然会使得目标函数非凸<ul>
<li>但是多变量场合，每个变量实际解只会在某个候选解附近，其邻域内仍然是凸的</li>
<li>且锯齿函数这样的突变非凸可能和凸函数具有相当的优秀性质</li>
</ul>
</li>
<li>当这些点均为整数时，这似乎可以近似求解 <strong>整数规划</strong></li>
</ul>
</li>
</ul>
<h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a><em>Early Stopping</em></h2><p><em>Early Stopping</em>：提前终止（训练）</p>
<ul>
<li><em>Early Stopping</em> 也可以被视为是 <em>regularizing on time</em><ul>
<li>迭代式训练随着迭代次数增加，往往会有学习复杂模型的倾向</li>
<li>对时间施加正则化，可以减小模型复杂度、提高泛化能力</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T09:19:32.000Z" title="8/4/2021, 5:19:32 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Technique/">ML Technique</a><span> / </span><a class="link-muted" href="/categories/ML-Technique/Neural-Network/">Neural Network</a></span><span class="level-item">12 minutes read (About 1782 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Technique/Neural-Network/batch_normalization.html">Batch Normalization</a></h1><div class="content"><h2 id="Internal-Covariate-Shift"><a href="#Internal-Covariate-Shift" class="headerlink" title="Internal Covariate Shift"></a><em>Internal Covariate Shift</em></h2><p><em>ICS</em>：由于网络参数变化，引起内部节点（输入）数据分布发生变化的过程</p>
<ul>
<li><p>网络中层与层之间高度耦合，具有强关联性</p>
<ul>
<li>网络中任意层都可以视为单独网络</li>
<li>上层输入可视为作为当前层外部输入</li>
</ul>
</li>
<li><p>随训练进行，网络中参数不断发生改变</p>
<ul>
<li>任意层中参数变化会导致之后层输入发生改变</li>
<li>高层需要不断适应输入分布的改变，即其输入分布性质影响
该层训练</li>
<li>由此导致模型训练困难</li>
</ul>
</li>
</ul>
<h3 id="负面影响"><a href="#负面影响" class="headerlink" title="负面影响"></a>负面影响</h3><ul>
<li><p>上层网络需要不断调整输入适应数据分布变换，降低网络学习
效率</p>
</li>
<li><p>输入数据量级不稳定、各维度数据量级差距不稳定</p>
<ul>
<li>降低学习效率<ul>
<li>小量级维度参数要求更小的学习率</li>
<li>否则参数可能在最优解附近反复波动</li>
</ul>
</li>
<li>容易出现梯度消失，难以训练饱和非线性模型<ul>
<li>大量级维度训练过程中容易陷入梯度饱和区，参数更新
速度慢，减缓网络收敛速度</li>
<li>训练过程中参数更新更有可能使得输入移向激活函数
饱和区</li>
<li>且该效应随着网络深度加深被进一步放大</li>
</ul>
</li>
<li>参数初始化需要更复杂考虑</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>还可以使用非饱和激活函数ReLU等避免陷入梯度饱和区</li>
</ul>
</blockquote>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p><em>Batch Normalization</em>：规范化batch数据，使样本<strong>各维度</strong>
标准化，即均值为0、方差为1</p>
<script type="math/tex; mode=display">\begin{align*}
\y & = BN_{\gamma, \beta}(z) = \gamma \odot \hat z + \beta \\
\hat z & = \frac {z - E(z)} {\sqrt {Var(z) + \epsilon}}
\end{align*}</script><blockquote>
<ul>
<li>$B$：mini-batch</li>
<li>$z, y$：<strong>某层</strong>输入向量、规范化后输入向量
  （即以个神经元中激活前标量值$z=Wx+b$为一维）</li>
<li>$\odot$：逐元素乘积</li>
<li>$E(x)$：均值使用移动平均均值</li>
<li>$Var(x)$：方差使用移动平均无偏估计</li>
<li>$\gamma, \beta$：待学习向量，用于<strong>恢复网络的表示能力</strong></li>
<li>$\epsilon$：为数值计算稳定性添加</li>
</ul>
</blockquote>
<ul>
<li><p>BN可以视为<em>whitening</em>的简化</p>
<ul>
<li>简化计算过程：避免过高的运算代价、时间</li>
<li>保留数据信息：未改变网络每层各特征之间相关性</li>
</ul>
</li>
<li><p>BN层引入可学习参数$\gamma, \beta$以恢复数据表达能力</p>
<ul>
<li>Normalization操作缓解了ICS问题，使得每层输入稳定
，也导致数据表达能力的缺失</li>
<li>输入分布均值为0、方差为1时，经过sigmoid、tanh激活
函数时，容易陷入其线性区域</li>
<li>$\gamma = \sqrt {Var(z)}, \beta = E(z)$时为等价变换
，并保留原始输入特征分布信息</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>Whitening</em>：白化，对输入数据变换使得各特征同均值、
  同方向、不相关，可以分为PCA白化、ZCA白化</li>
</ul>
</blockquote>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ul>
<li><p>规范化在每个神经元内部非线性激活前$z=Wu$进行，而不是
[也]在上一层输出$u$上进行，即包含BN最终为</p>
<script type="math/tex; mode=display">
z = act(BN(Wu))</script><blockquote>
<ul>
<li>$act$：激活函数</li>
<li>偏置$b$：可以被省略，BN中减去均值</li>
</ul>
</blockquote>
<ul>
<li>$u$的分布形状可以在训练过程中改变</li>
<li>而$u$两次正则化无必要</li>
<li>$z=Wu$分布更可能对称、稠密、类似高斯分布</li>
</ul>
</li>
<li><p>以batch统计量作为整体训练样本均值、方差估计</p>
<ul>
<li>每层均需存储均值、方差的移动平均统计量用于测试时
归一化测试数据</li>
</ul>
</li>
<li><p>对卷积操作，考虑卷积特性，不是只为激活函数（即卷积核）
学习$\gamma, \beta$，而是为每个<em>feature map</em>学习
（即每个卷积核、对每个特征图层分别学习）</p>
</li>
</ul>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><ul>
<li><p>预测过程中各参数（包括均值、方差）为定值，BN仅仅对数据
做了线性变换</p>
<ul>
<li><p>使用训练总体的无偏统计量对测试数据归一化
（训练时存储）</p>
<script type="math/tex; mode=display">\begin{align*}
\mu_{test} & = E(\mu_{batch}) \\
\sigma^2_{test} = \frac m {m-1} E(\sigma^2_{batch})
\end{align*}</script></li>
<li><p>还可以使用样本指数加权平均统计量</p>
</li>
</ul>
</li>
</ul>
<h3 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h3><blockquote>
<ul>
<li>BN通过规范化输入数据各维度分布减少<em>ICS</em>，使得网络中每层
  输入数据分布相对稳定</li>
</ul>
</blockquote>
<ul>
<li><p>实现网络层与层之间的解耦</p>
<ul>
<li>方便迁移学习</li>
<li>加速模型学习速度：后层网络无需不断适应输入分布变化，
利于提高神经网络学习速度</li>
</ul>
</li>
<li><p>降低模型对网络超参数、初始值敏感度，使得网络学习更加稳定</p>
<ul>
<li>简化调参过程</li>
<li>允许使用更大的学习率提高学习效率</li>
</ul>
<script type="math/tex; mode=display">\begin{align*}
BN(Wu) & = BN((aW)u) \\
\frac {\partial BN(aWu)} {\partial u} & = \frac
   {\partial BN(Wu)} {\partial u} \\
\frac {BN(aWu)} {\partial aW} & = \frac 1 a \frac
   {\partial BN(Wu)} {\partial W}
\end{align*}</script><blockquote>
<ul>
<li>$a$：假设某层权重参数变动$a$倍</li>
</ul>
</blockquote>
<ul>
<li>激活函数函数输入不受权重$W$放缩影响</li>
<li>梯度反向传播更稳定，权重$W$的Jacobian矩阵将包含接近
1的奇异值，保持梯度稳定反向传播</li>
</ul>
</li>
<li><p>允许网络使用饱和激活函数（sigmoid、tanh等），而不至于
停滞在饱和处，缓解梯度消失问题</p>
<ul>
<li>深度网络的复杂性容易使得网络变化积累到上层网络中，
导致模型容易进入激活函数梯度饱和区</li>
</ul>
</li>
<li><p>有正则化作用，提高模型泛化性能，减少对Dropout的需求</p>
<ul>
<li>不同batch均值、方差有所不同，为网络学习过程增加随机
噪声</li>
<li>与Dropout关闭神经元给网络带来噪声类似，一定程度上
有正则化效果</li>
</ul>
</li>
</ul>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>层归一化：假设非线性激活前的输入随机变量分布接近，可以直接
基于每层所有非线性激活前输入估计均值、方差</p>
<script type="math/tex; mode=display">\begin{align*}
\mu^l & = \frac 1 H \sum_{i=1}^H h_i^l \\
\sigma^l &= \sqrt {\frac 1 H \sum_{i=1}^H (h_i^l - \mu^l)^2} \\
h^l & = W^l x^{l-1} + b^l \\
LN(h^l) & = \frac {g^l} {\sigma^l} \odot (h^l - \mu^l) + b^l \\
x^l & = g(LN(h^l))
\end{align*}</script><blockquote>
<ul>
<li>$h^l$：第$l$隐层激活前值</li>
<li>$\mu^l, \sigma^l$：第$l$隐层对应LN均值、方差
  （标量，是同层神经元激活前值统计量）</li>
</ul>
</blockquote>
<ul>
<li><p>相对于BN，其适应范围更广</p>
<ul>
<li>循环神经网络中，BN无法处理长于训练序列的测试序列</li>
<li>BN无法应用到在线学习、超大分布式模型任务，此时训练
batch较小，计算的均值、方差无法有效代表训练总体</li>
</ul>
</li>
<li><p>LN假设非线性激活前输入随机变量分布接近，而CNN网络中图像
边缘对应kernel大量隐藏单元未被激活，假设不成立，所以
CNN网络中LN效果没有BN效果好</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T11:35:20.000Z" title="8/4/2021, 7:35:20 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Technique/">ML Technique</a><span> / </span><a class="link-muted" href="/categories/ML-Technique/Neural-Network/">Neural Network</a></span><span class="level-item">5 minutes read (About 812 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Technique/Neural-Network/dropout.html">Dropout</a></h1><div class="content"><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p><em>Dropout</em>：<strong>训练时</strong>根据随机隐藏部分神经元、对应连接边避免
过拟合</p>
<h3 id="固定概率丢弃"><a href="#固定概率丢弃" class="headerlink" title="固定概率丢弃"></a>固定概率丢弃</h3><p>Dropout最简单方法：设置固定概率p，对每个神经元以概率p判定
是否需要保留</p>
<script type="math/tex; mode=display">\begin{align*}
y & = f(W * d(x) + b) \\
d(x) & = \left \{ \begin{array}{l}
    m \odot x, & 训练阶段
    px, & 测试阶段
\end{array} \right.
\end{align*}</script><blockquote>
<ul>
<li>$d(x)$：丢弃函数</li>
<li>$m \in {0, 1}^d$：丢弃掩码，通过概率为p的伯努利
  分布随机生成</li>
</ul>
</blockquote>
<ul>
<li><p>$p$可以设置为0.5，对大部分网络、任务比较有效</p>
<ul>
<li>此时随机生成多的网络最具多样性</li>
</ul>
</li>
<li><p>训练时</p>
<ul>
<li>激活神经元数量为原来的p倍</li>
<li>每个batch分别进行drop，相当于对每个batch都有独特网络</li>
</ul>
</li>
<li><p>测试时</p>
<ul>
<li>所有神经元都被激活，造成训练、测试时网络输出不一致，
需将每个神经元输出乘p避免</li>
<li>也相当于把不同网络做平均</li>
</ul>
</li>
<li><p>在预测时，类似bagging技术将多个模型组合</p>
<ul>
<li>只是类似，各个drop后的子网并不独立，在不同子网中相同
神经元的权重相同</li>
<li>多个模型组合组合可以一定程度上抵消过拟合</li>
<li>因为在训练时子网中部分神经元被drop，剩余部分权重相较
完全网络有$\frac 1 {1-p}$，所以在完整网络中，各部分
权重需要$ * (1-p)$</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>讲道理应该是隐藏部分神经元而不是连接，否则会使神经元偏向
  某些输入，还不如隐藏部分神经元，这样可以让神经元随机降低
  样本权重，理论上能减弱过拟合</li>
</ul>
</blockquote>
<h3 id="丢弃方法"><a href="#丢弃方法" class="headerlink" title="丢弃方法"></a>丢弃方法</h3><ul>
<li><p>输入层神经元丢弃率更接近1，使得输入变化不会太大</p>
<ul>
<li>输入层神经元丢失时，相当于给数据增加噪声，提高网络
稳健性</li>
</ul>
</li>
<li><p>循环神经网络丢弃</p>
<ul>
<li>不能直接丢弃隐状态，会损害循环网络在时间维度上的记忆
能力</li>
<li>简单方法：可以考虑对非循环连接进行随机丢弃</li>
<li>变分丢弃法：根据贝叶斯对丢弃法是对参数的采样解释，
采样参数需要每个时刻保持不变<ul>
<li>需要对参数矩阵的每个元素随机丢弃</li>
<li>所有时刻使用相同的丢弃掩码</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h3><ul>
<li><p>集成学习解释</p>
<ul>
<li>每次丢弃，相当于从原网络采样得到子网络</li>
<li>每次迭代，相当于训练不同的子网络，共享原始网络参数</li>
<li>最终网络可以近似看作是集成了指数个不同网络的组合模型</li>
</ul>
</li>
<li><p>贝叶斯学习解释</p>
<ul>
<li>对需要学习的网络$y = f(x, \theta)$，贝叶斯学习假设
参数$\theta$为随机向量</li>
<li><p>设先验分布为$q(\theta)$，贝叶斯方法预测为</p>
<script type="math/tex; mode=display">\begin{align*}
E_{q(\theta)}[y] &  = \int_q f(x, \theta) q(\theta)
  d\theta \\
& \approx \frac 1 M \sum_{m=1}^M f(x, \theta_m)
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li>$f(x, \theta_m)$：第$m$次应用丢弃方法的网络</li>
<li>$\theta_m$：对全部参数的采样</li>
</ul>
</blockquote>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T08:54:07.000Z" title="7/16/2021, 4:54:07 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Technique/">ML Technique</a><span> / </span><a class="link-muted" href="/categories/ML-Technique/Feature-Engineering/">Feature Engineering</a></span><span class="level-item">20 minutes read (About 2943 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Technique/Feature-Engineering/data_preprocessing.html">数据预处理</a></h1><div class="content"><h2 id="数据说明"><a href="#数据说明" class="headerlink" title="数据说明"></a>数据说明</h2><h3 id="数据模式"><a href="#数据模式" class="headerlink" title="数据模式"></a>数据模式</h3><ul>
<li><p>结构化数据：行数据，可用二维表逻辑表达数据逻辑、存储在数据库中</p>
<ul>
<li>可以看作是关系型数据库中一张表</li>
<li>行：记录、元组，表示一个样本信息</li>
<li>列：字段、属性，有清晰定义</li>
</ul>
</li>
<li><p>非结构化数据：相对于结构化数据而言，不方便用二维逻辑表达的数据</p>
<ul>
<li>包含信息无法用简单数值表示<ul>
<li>没有清晰列表定义</li>
<li>每个数据大小不相同</li>
</ul>
</li>
<li>研究方向<ul>
<li>社交网络数据</li>
<li>文本数据</li>
<li>图像、音视频</li>
<li>数据流</li>
</ul>
</li>
<li>针对不同类型数据、具体研究方面有不同的具体分析方法，不存在普适、可以解决所有具体数据的方法</li>
</ul>
</li>
<li><p>半结构化数据：介于完全结构化数据、完全无结构数据之间的数据</p>
<ul>
<li>一般是自描述的，数据结构和内容混合、没有明显区分</li>
<li>树、图（<em>XML</em>、<em>HTML</em> 文档也可以归为半结构化数据）</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>结构化数据：先有结构、再有数据</li>
<li>半结构化数据：先有数据、再有结构</li>
</ul>
</blockquote>
<h3 id="数据拼接"><a href="#数据拼接" class="headerlink" title="数据拼接"></a>数据拼接</h3><ul>
<li>利用外键拼接不同来源的数据时，注意不同数据间粒度差异<ul>
<li>外键低于问题标签粒度时，考虑对数据作聚合操作再拼接<ul>
<li>保证拼接后用于训练的记录粒度和问题一致</li>
<li>避免维度爆炸</li>
<li>各数据来源数据厚薄不同，会改变数据分布</li>
</ul>
</li>
<li>外键高于、等于目标粒度时，可考虑直接直接连接</li>
</ul>
</li>
</ul>
<h2 id="数据问题"><a href="#数据问题" class="headerlink" title="数据问题"></a>数据问题</h2><h3 id="稀疏特征"><a href="#稀疏特征" class="headerlink" title="稀疏特征"></a>稀疏特征</h3><ul>
<li>产生原因<ul>
<li>数据缺失</li>
<li>统计数据频繁 0 值</li>
<li>特征工程技术，如：独热编码</li>
</ul>
</li>
</ul>
<h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3><h4 id="产生原因"><a href="#产生原因" class="headerlink" title="产生原因"></a>产生原因</h4><ul>
<li>信息暂时无法获取、成本高</li>
<li>信息被遗漏</li>
<li>属性不存在</li>
</ul>
<h4 id="缺失值影响"><a href="#缺失值影响" class="headerlink" title="缺失值影响"></a>缺失值影响</h4><ul>
<li>建模将丢失大量有用信息</li>
<li>模型不确定性更加显著、蕴含规则更难把握</li>
<li>包含空值可能使得建模陷入混乱，导致不可靠输出</li>
</ul>
<h4 id="缺失利用"><a href="#缺失利用" class="headerlink" title="缺失利用"></a>缺失利用</h4><ul>
<li><p>直接使用含有缺失值特征：有些方法可以完全处理、不在意缺失值</p>
<ul>
<li>分类型变量可以将缺失值、异常值单独作为特征的一种取值</li>
<li>数值型变量也可以离散化，类似分类变量将缺失值单独分箱</li>
</ul>
</li>
<li><p>删除含有缺失值特征</p>
<ul>
<li>一般仅在特征缺失率比较高时才考虑采用，如缺失率达到 90%、95%</li>
</ul>
</li>
</ul>
<h4 id="插值补全"><a href="#插值补全" class="headerlink" title="插值补全"></a>插值补全</h4><ul>
<li><p>非模型补全缺失值</p>
<ul>
<li>均值、中位数、众数</li>
<li>同类/前后均值、中位数、众数</li>
<li>固定值</li>
<li>矩阵补全</li>
<li>最近邻补全：寻找与样本最接近样本相应特征补全</li>
</ul>
</li>
<li><p>手动补全：根据对所在领域理解，手动对缺失值进行插补</p>
<ul>
<li>需要对问题领域有很高认识理解</li>
<li>缺失较多时费时、费力</li>
</ul>
</li>
<li><p>建模预测：回归、决策树模型预测</p>
<ul>
<li>若其他特征和缺失特征无关，预测结果无意义</li>
<li>若预测结果相当准确，缺失属性也没有必要纳入数据集</li>
</ul>
</li>
<li><p>多重插补：认为待插补值是随机的</p>
<ul>
<li>通常估计处待插补值</li>
<li>再加上<strong>不同噪声</strong>形成多组可选插补值</li>
<li>依据某准则，选取最合适的插补值</li>
</ul>
</li>
<li><p>高维映射：<em>one-hot</em> 编码增加维度表示某特征缺失</p>
<ul>
<li>保留所有信息、未人为增加额外信息</li>
<li>可能会增加数据维度、增加计算量</li>
<li>需要样本量较大时效果才较好</li>
</ul>
</li>
<li><p>压缩感知：利用信号本身具有的<strong>稀疏性</strong>，从部分观测样本中恢复原信号</p>
<ul>
<li>感知测量阶段：对原始信号进行处理以获得稀疏样本表示<ul>
<li>傅里叶变换</li>
<li>小波变换</li>
<li>字典学习</li>
<li>稀疏编码</li>
</ul>
</li>
<li>重构恢复阶段：基于稀疏性从少量观测中恢复信号</li>
</ul>
</li>
</ul>
<h3 id="异常值"><a href="#异常值" class="headerlink" title="异常值"></a>异常值</h3><blockquote>
<ul>
<li>异常值/离群点：样本中数值明显偏离其余观测值的个别值</li>
</ul>
</blockquote>
<p>异常值分析：检验数据是否有录入错误、含有不合常理的数据</p>
<h4 id="非模型异常值检测"><a href="#非模型异常值检测" class="headerlink" title="非模型异常值检测"></a>非模型异常值检测</h4><ul>
<li><p>简单统计</p>
<ul>
<li>观察数据统计型描述、散点图</li>
<li>箱线图：利用箱线图四分位距对异常值进行检测</li>
</ul>
</li>
<li><p>$3\sigma$ 原则：取值超过均值 3 倍标准差，可以视为异常值</p>
<ul>
<li>依据小概率事件发生可能性“不存在”</li>
<li>数据最好近似正态分布</li>
</ul>
</li>
</ul>
<h4 id="模型异常值检测"><a href="#模型异常值检测" class="headerlink" title="模型异常值检测"></a>模型异常值检测</h4><ul>
<li><p>基于模型预测：构建概率分布模型，计算对象符合模型的概率，将低概率对象视为异常点</p>
<ul>
<li>分类模型：异常点为不属于任何类的对象</li>
<li>回归模型：异常点为原理预测值对象</li>
<li>特点<ul>
<li>基于统计学理论基础，有充分数据和所用的检验类型知识时，检验可能非常有效</li>
<li>对多元数据，可用选择少，维度较高时，检测效果不好</li>
</ul>
</li>
</ul>
</li>
<li><p>基于近邻度的离群点检测：对象离群点得分由其距离 <em>k-NN</em> 的距离确定</p>
<ul>
<li><em>k</em> 取值会影响离群点得分，取 <em>k-NN</em> 平均距离更稳健</li>
<li>特点<ul>
<li>简单，但时间复杂度高 $\in O(m^2)$，不适合大数据集</li>
<li>方法对参数 <em>k</em> 取值敏感</li>
<li>使用全局阈值，无法处理具有不同密度区域的数据集</li>
</ul>
</li>
</ul>
</li>
<li><p>基于密度的离群点检测</p>
<ul>
<li>定义密度方法<ul>
<li><em>k-NN</em> 分类：<em>k</em> 个最近邻的平均距离的倒数</li>
<li><em>DSSCAN</em> 聚类中密度：对象指定距离 <em>d</em> 内对象个数</li>
</ul>
</li>
<li>特点<ul>
<li>给出定量度量，即使数据具有不同区域也能很好处理</li>
<li>时间复杂度 $\in O^(m^2)$，对低维数据使用特点数据结构可以达到 $\in O(mlogm)$</li>
<li>参数难以确定，需要确定阈值</li>
</ul>
</li>
</ul>
</li>
<li><p>基于聚类的离群点检测：不属于任何类别簇的对象为离群点</p>
<ul>
<li>特点<ul>
<li>（接近）线性的聚类技术检测离群点高度有效</li>
<li>簇、离群点互为补集，可以同时探测</li>
<li>聚类算法本身对离群点敏感，类结构不一定有效，可以考虑：对象聚类、删除离群点再聚类</li>
<li>检测出的离群点依赖类别数量、产生簇的质量</li>
</ul>
</li>
</ul>
</li>
<li><p><em>One-class SVM</em></p>
</li>
<li><p><em>Isolation Forest</em></p>
</li>
</ul>
<h4 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h4><ul>
<li><p>删除样本</p>
<ul>
<li>简单易行</li>
<li>观测值很少时，可能导致样本量不足、改变分布</li>
</ul>
</li>
<li><p>视为缺失值处理</p>
<ul>
<li>作为缺失值不做处理</li>
<li>利用现有变量信息，对异常值进行填补</li>
<li>全体/同类/前后均值、中位数、众数修正</li>
<li>将缺失值、异常值单独作为特征的一种取值</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>很多情况下，要先分析异常值出现的可能原因，判断异常值是否为<strong>真异常值</strong></li>
</ul>
</blockquote>
<h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><h4 id="创造新样本"><a href="#创造新样本" class="headerlink" title="创造新样本"></a>创造新样本</h4><ul>
<li><p>对数据集重采样</p>
<ul>
<li>尝试随机采样、非随机采样</li>
<li>对各类别尝试不同采样比例，不必保持 1:1 违反现实情况</li>
<li>同时使用过采样、欠采样</li>
</ul>
</li>
<li><p>属性值随机采样</p>
<ul>
<li>从类中样本每个特征随机取值组成新样本</li>
<li>基于经验对属性值随机采样</li>
<li>类似朴素贝叶斯方法：假设各属性之间相互独立进行采样，但是无法保证属性之前的线性关系</li>
</ul>
</li>
<li><p>对模型进行惩罚</p>
<ul>
<li>类似 <em>AdaBoosting</em>：对分类器小类样本数据增加权值</li>
<li>类似 <em>Bayesian</em>分类：增加小类样本错分代价，如：<em>penalized-SVM</em>、<em>penalized-LDA</em></li>
<li>需要根据具体任务尝试不同惩罚矩阵</li>
</ul>
</li>
</ul>
<h4 id="新角度理解问题"><a href="#新角度理解问题" class="headerlink" title="新角度理解问题"></a>新角度理解问题</h4><ul>
<li><p>将小类样本视为异常点：问题变为异常点检测、变化趋势检测</p>
<ul>
<li>尝试不同分类算法</li>
<li>使用 <em>one-class</em> 分类器</li>
</ul>
</li>
<li><p>对问题进行分析，将问题划分为多个小问题</p>
<ul>
<li>大类压缩为小类</li>
<li>使用集成模型训练多个分类器、组合</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>需要具体问题具体分析</li>
</ul>
</blockquote>
<h4 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h4><ul>
<li>尝试其他评价指标：准确度在不平衡数据中不能反映实际情况<ul>
<li>混淆矩阵</li>
<li>精确度</li>
<li>召回率</li>
<li><em>F1</em> 得分</li>
<li><em>ROC</em> 曲线</li>
<li><em>Kappa</em></li>
</ul>
</li>
</ul>
<h3 id="数据量缺少"><a href="#数据量缺少" class="headerlink" title="数据量缺少"></a>数据量缺少</h3><h4 id="图片数据扩充"><a href="#图片数据扩充" class="headerlink" title="图片数据扩充"></a>图片数据扩充</h4><p><em>Data Agumentation</em>：根据先验知识，在保留特点信息的前提下，对原始数据进行适当变换以达到扩充数据集的效果</p>
<ul>
<li><p>对原始图片做变换处理</p>
<ul>
<li>一定程度内随机旋转、平移、缩放、裁剪、填充、左右翻转，这些变换对应目标在不同角度观察效果</li>
<li>对图像中元素添加噪声扰动：椒盐噪声、高斯白噪声</li>
<li>颜色变换</li>
<li>改变图像亮度、清晰度、对比度、锐度</li>
</ul>
</li>
<li><p>先对图像进行特征提取，在特征空间进行变换，利用通用数据
扩充、上采样方法</p>
<ul>
<li><em>SMOTE</em></li>
</ul>
</li>
<li><p><em>Fine-Tuning</em> 微调：直接接用在大数据集上预训练好的模型，在小数据集上进行微调</p>
<ul>
<li>简单的迁移学习</li>
<li>可以快速寻外效果不错针对目标类别的新模型</li>
</ul>
</li>
</ul>
<h2 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h2><blockquote>
<ul>
<li>正则化：<strong>针对单个样本</strong>，将每个样本缩放到单位范数</li>
<li>归一化：针对单个属性，需要用到所有样本在该属性上值</li>
</ul>
</blockquote>
<h3 id="Normalizaion"><a href="#Normalizaion" class="headerlink" title="Normalizaion"></a><em>Normalizaion</em></h3><p>归一化/标准化：将特征/数据缩放到指定大致相同的数值区间</p>
<ul>
<li>某些算法要求数据、特征数值具有零均值、单位方差</li>
<li>消除样本数据、特征之间的量纲/数量级影响<ul>
<li>量级较大属性占主导地位</li>
<li>降低迭代收敛速度：梯度下降时，梯度方向会偏离最小值，学习率必须非常下，否则容易引起<strong>宽幅震荡</strong></li>
<li>依赖样本距离的算法对数据量机敏感</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>决策树模型不需要归一化，归一化不会改变信息增益（比），<em>Gini</em> 指数变化</li>
</ul>
</blockquote>
<h4 id="Min-Max-Scaling"><a href="#Min-Max-Scaling" class="headerlink" title="Min-Max Scaling"></a><em>Min-Max Scaling</em></h4><p>线性函数归一化：对原始数据进行线性变换，映射到 $[0, 1]$ 范围</p>
<script type="math/tex; mode=display">
X_{norm} = \frac {X - X_{min}} {X_{max} - X_{min}}</script><blockquote>
<ul>
<li>训练集、验证集、测试集都使用训练集归一化参数</li>
</ul>
</blockquote>
<h4 id="Z-Score-Scaling"><a href="#Z-Score-Scaling" class="headerlink" title="Z-Score Scaling"></a><em>Z-Score Scaling</em></h4><p>零均值归一化：将原始数据映射到均值为 0，标准差为 1 的分布上</p>
<script type="math/tex; mode=display">
Z = \frac {X - \mu} {\sigma}</script><h4 id="其他一些变换方式"><a href="#其他一些变换方式" class="headerlink" title="其他一些变换方式"></a>其他一些变换方式</h4><ul>
<li>对数变换：$X^{‘} = lg(X)$</li>
<li>反余切函数变换：$X^{‘} = \frac {2 arctan(x)} {\pi}$</li>
<li><em>Sigmoid</em> 变换：$X^{‘} = \frac 1 {1 + e^{-x}}$</li>
<li>模糊向量变：$X^{‘} = \frac 1 2 + \frac 1 2 sin \frac {X - \frac{max(X) - min(X)} 2} {max(X) - min(X)} * \pi$</li>
</ul>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a><em>Regularization</em></h3><p>正则化：将样本/特征<strong>某个范数</strong>缩放到单位 1</p>
<script type="math/tex; mode=display">\begin{align*}
\overrightarrow x_i & = (
    \frac {x_i^{(1)}} {L_p(\overrightarrow x_i)},
    \frac {x_i^{(2)}} {L_p(\overrightarrow x_i)}, \cdots,
    \frac {x_i^{(d)}} {L_p(\overrightarrow x_i)})^T \\
L_p(\overrightarrow x_i) & = (|x_i^{(1)}|^p + |x_i^{(2)}|^p + 
    \cdots + |x_i^{(d)}|^p)^{1/p}
\end{align*}</script><blockquote>
<ul>
<li>$L_p$：样本的 $L_p$ 范数</li>
</ul>
</blockquote>
<ul>
<li>使用内积、二次型、核方法计算样本之间相似性时，正则化很有用</li>
</ul>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/WWxwMVBsOUtoNTdMSlJ4Z1lWVnRISERsd2t6ek9MeVpEUWs0YldlZGxUdXlKdDNmMEZVV1hWaFZFYWFSNmFKL25penZodWx3UzRiaVkxcnREWCtOYUJhZWhNbWpzaUdyc1hPangycUh5RTVjRXFnZnFGdVdSTzZvVzJBcTJHKzl8aXpDK1ROWWl4N080YkFEK3QvbEVWNEJuQjFqdWdxODZQcGNoM1NqbERXST0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>