<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: Model Enhencement - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"Hexo","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/ML-Theory/">ML Theory</a></li><li class="is-active"><a href="#" aria-current="page">Model Enhencement</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-29T10:12:36.000Z" title="8/29/2019, 6:12:36 PM">2019-08-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-08-29T10:12:43.000Z" title="8/29/2019, 6:12:43 PM">2019-08-29</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">a few seconds read (About 1 word)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/lightgbm.html">LightGBM</a></h1><div class="content"><h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-23T00:39:04.000Z" title="7/23/2019, 8:39:04 AM">2019-07-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-23T00:39:04.000Z" title="7/23/2019, 8:39:04 AM">2019-07-23</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">6 minutes read (About 890 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/stacking.html">Stacked Generalization</a></h1><div class="content"><h2 id="Stacked-Generalization"><a href="#Stacked-Generalization" class="headerlink" title="Stacked Generalization"></a>Stacked Generalization</h2><p>堆栈泛化：使用<strong>多种模型</strong>分别训练训练，将其结果叠加作为下层
模型的输入，最终得到预测输出</p>
<p><img src="/imgs/stacking.png" alt="stacking"></p>
<ul>
<li><p>属于异源集成模型，可以视为</p>
<ul>
<li><p>复合函数</p>
<p><img src="/imgs/stacking_workflow_2.png" alt="stacing_workflow_2"></p>
</li>
<li><p>短路网络</p>
<p><img src="/imgs/stacking_workflow_1.png" alt="stacing_workflow_1"></p>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>从某种意义上，复杂模型都是stacking</li>
</ul>
</blockquote>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>不同模型侧重于获取数据不同方面的特征</p>
<ul>
<li>使用基学习器抽取数据特征进行表示学习，提取不同角度的
数据高维特征</li>
<li>考虑到使用全量训练数据训练、预测作为下层模型输入会
导致过拟合，可使用K折交叉验证避免过拟合</li>
<li>有些基学习器只使用适合其部分特征训练<ul>
<li>GBDT、DNN适合低维稠密特征</li>
</ul>
</li>
</ul>
</li>
<li><p>元学习器组合多个基学习器的输出</p>
<ul>
<li>从数据高维特征学习数据模式，具有更好的泛化能力，避免
过拟合</li>
</ul>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>输入：模型$M<em>1, M_2, \cdots, M_d$、训练特征：$X</em>{n*m}$、
  训练标签$Y_{n}$、测试特征$X^{‘}$</li>
<li>输出：stacking模型、预测标签</li>
</ul>
</blockquote>
<ul>
<li><p>将训练数据K折划分，对第$i$轮划分</p>
<ul>
<li>使用模型$M<em>1, M_2, \cdots, M_d$分别在相应训练集
$[X[:n_i,:], X[n</em>{i+1}:,:]]$、
$[Y[:n<em>i], Y[n</em>{i+1}:]]$上训练</li>
<li>在相应验证集$X[n<em>i:n</em>{i+1}, :]$上验证、并记录验证
结果</li>
<li>将验证集验证结果叠加得到部分样本新特征
$N[n<em>i: n</em>{i+1}, d]$</li>
</ul>
</li>
<li><p>将K轮划分得到的部分新特征拼接得到训练集的完整新特征
$N_{n * d}$，将新特征作为输入，训练下层模型，得到最终
stacking模型</p>
</li>
<li><p>将测试特征如上作为输入经过两层模型预测，得到最终预测结果</p>
</li>
</ul>
<blockquote>
<ul>
<li>以上以2层stacking为例，有深层stacking</li>
</ul>
</blockquote>
<h2 id="常用模型"><a href="#常用模型" class="headerlink" title="常用模型"></a>常用模型</h2><h3 id="基学习器"><a href="#基学习器" class="headerlink" title="基学习器"></a>基学习器</h3><ul>
<li>交叉项、原始特征本身也可以视为线性基学习器学习到的特征</li>
</ul>
<blockquote>
<ul>
<li>具体模型参见
  <em>ml_specification/rec_system/ctr_stacking_models</em></li>
</ul>
</blockquote>
<h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p><img src="img/gbdt_in_stacking.png" alt="gbdt_in_stacking"></p>
<blockquote>
<ul>
<li>各树中各节点对应元学习器一维输入特征</li>
</ul>
</blockquote>
<ul>
<li><p>适合低维稠密通用特征，对输入特征分布没有要求</p>
</li>
<li><p>GBDT树根据熵增益（Gini系数增益）划分节点，每条路径
都代表一定区分能力</p>
<ul>
<li>以叶子节点（路径）作为特征，相当于自动进行特征
转换、组合、选择、离散化，得到<strong>高维组合特征</strong></li>
</ul>
</li>
<li><p>GDBT相较于单棵树、或RF更适合stacking</p>
<ul>
<li>单棵树表达能力弱，无法表达多个有区分性特征组合，
集成模型可将样本映射为多个特征</li>
<li>GBDT拟合残差意味着各树对样本区分度不同，对各特征
区别对待更合理</li>
</ul>
</li>
</ul>
<h4 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h4><ul>
<li>适合普通稠密特征、embedding特征</li>
<li>模型表达能力强，能抽取有良好分布数据的深层次特征，提高
模型准确性、泛化能力</li>
<li>容易扩充其他类别特征，如：图片、文字</li>
</ul>
<h3 id="元学习器"><a href="#元学习器" class="headerlink" title="元学习器"></a>元学习器</h3><ul>
<li><p>LR</p>
<ul>
<li>适合低维稀疏特征，可对所有特征离散化以引入非线性</li>
</ul>
</li>
<li><p>FM</p>
<ul>
<li>适合低维稀疏特征</li>
<li>LR基础上自动组合二阶交叉项</li>
</ul>
</li>
<li><p>Linear：训练模型、对训练结果线性加权</p>
</li>
</ul>
<p>?</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">20 minutes read (About 2987 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/model_enhancement.html">Model Enhancement</a></h1><div class="content"><h2 id="Emsemble-Learning"><a href="#Emsemble-Learning" class="headerlink" title="Emsemble Learning"></a>Emsemble Learning</h2><blockquote>
<ul>
<li>集成学习：训练多个基模型，并将其组合起来，以达到更好的
  预测能力、泛化能力、稳健性</li>
<li><em>base learner</em>：基模型，基于<strong>独立样本</strong>建立的、一组
  <strong>具有相同形式</strong>的模型中的一个</li>
<li>组合预测模型：由基模型组合，即集成学习最终习得模型</li>
</ul>
</blockquote>
<ul>
<li><p>源于样本均值抽样分布思路</p>
<ul>
<li>$var(\bar{X}) = \sigma^2 / n$</li>
<li>基于独立样本，建立一组具有相同形式的基模型</li>
<li>预测由这组模型共同参与</li>
<li>组合预测模型稳健性更高，类似于样本均值抽样分布方差
更小</li>
</ul>
</li>
<li><p>关键在于</p>
<ul>
<li>获得多个独立样本的方法</li>
<li>组合多个模型的方法</li>
</ul>
</li>
</ul>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul>
<li><p><em>homogenous ensemble</em>：同源集成，基学习器属于同一类型</p>
<ul>
<li><em>bagging</em></li>
<li><em>boosting</em></li>
</ul>
</li>
<li><p><em>heterogenous ensemble</em>：异源集成，基学习器不一定属于同
一类型</p>
<ul>
<li><em>[genralization] stacking</em></li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Target</th>
<th>Data</th>
<th>parallel</th>
<th>Classifier</th>
<th>Aggregation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Bagging</td>
<td>减少方差</td>
<td>基于boostrap随机抽样，抗异常值、噪声</td>
<td>模型间并行</td>
<td>同源不相关基学习器，一般是树</td>
<td>分类：投票、回归：平均</td>
</tr>
<tr>
<td>Boosting</td>
<td>减少偏差</td>
<td>基于误分分步</td>
<td>模型间串行</td>
<td>同源若学习器</td>
<td>加权投票</td>
</tr>
<tr>
<td>Stacking</td>
<td>减少方差、偏差</td>
<td>K折交叉验证数据、基学习器输出</td>
<td>层内模型并行、层间串行</td>
<td>异质强学习器</td>
<td>元学习器</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<ul>
<li>以上都是指原始版本、主要用途</li>
</ul>
</blockquote>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>提升方法：将弱可学习算法<strong>提升</strong>为强可学习算法的组合元算法</p>
<ul>
<li>属于加法模型：即基函数的线性组合</li>
<li>各模型之间存在依赖关系</li>
</ul>
<p><img src="/imgs/boosting.png" alt="boosting"></p>
<h4 id="分类Boosting"><a href="#分类Boosting" class="headerlink" title="分类Boosting"></a>分类Boosting</h4><blockquote>
<ul>
<li><strong>依次</strong>学习多个基分类器</li>
<li>每个基分类器<strong>依之前分类结果调整权重</strong></li>
<li><strong>堆叠</strong>多个分类器提高分类准确率</li>
</ul>
</blockquote>
<ul>
<li><p>boosting通过组合多个误分率略好于随机猜测的分类器得到
误分率较小的分类器，因此boosting适合这两类问题</p>
<ul>
<li>个体之间难度有很大不同，boosting能够更加关注较难的
个体</li>
<li>学习器对训练集敏感，boosting驱使学习器在趋同的、
“较难”的分布上学习，此时boosting就和bagging一样能够
使得模型更加稳健（但原理不同）</li>
</ul>
</li>
<li><p>boosting能减小预测方差、偏差、过拟合</p>
<ul>
<li><p>直觉上，使用在不同的样本上训练的基学习器加权组合，
本身就能减小学习器的随机变动</p>
</li>
<li><p>基于同样的理由，boosting同时也能减小偏差</p>
</li>
<li><p>过拟合对集成学习有些时候有正面效果，其带来多样性，
使模型泛化能力更好，前提是样本两足够大，否则小样本
仍然无法提供多样性</p>
</li>
</ul>
</li>
</ul>
<h4 id="回归Boosting"><a href="#回归Boosting" class="headerlink" title="回归Boosting"></a>回归Boosting</h4><blockquote>
<ul>
<li><strong>依次</strong>训练多个基学习器</li>
<li>每个基学习器以<strong>之前学习器拟合残差</strong>为目标</li>
<li><strong>堆叠</strong>多个学习器减少整体损失</li>
</ul>
</blockquote>
<ul>
<li><p>boosting组合模型整体损失（结构化风险）</p>
<script type="math/tex; mode=display">
R_{srm} = \sum_{i=1}^N l(y_i, \hat y_i) +
   \sum_{t=1}^M \Omega(f_t)</script><blockquote>
<ul>
<li>$l$：损失函数</li>
<li>$f_t$：基学习器</li>
<li>$\Omega(f_t)$：单个基学习器的复杂度罚</li>
<li>$N, M$：样本数目、学习器数目</li>
</ul>
</blockquote>
</li>
<li><p>基学习器损失</p>
<script type="math/tex; mode=display">
obj^{(t)} = \sum_{i=1}^N l(y_i, \hat y_i^{(t)}) +
   \Omega(f_t)</script></li>
</ul>
<h4 id="最速下降法"><a href="#最速下降法" class="headerlink" title="最速下降法"></a>最速下降法</h4><p>使用线性函数拟合$l(y_i, \hat y_i^{(t)})$</p>
<script type="math/tex; mode=display">\begin{align*}
obj^{(t)} & = \sum_i^N l(y_i, \hat y_i^{(t-1)} + f_t(x_i)) +
    \Omega(f_t) \\
& \approx \sum_{i=1}^N [l(y_i, \hat y^{(t-1)}) + g_i f_t(x_i)]
    + \Omega(f_t)
\end{align*}</script><blockquote>
<ul>
<li>$g<em>i = \partial</em>{\hat y} l(y_i, \hat y^{t-1})$</li>
</ul>
</blockquote>
<ul>
<li>一次函数没有极值</li>
<li>将所有样本损失视为向量（学习器权重整体施加），则负梯度
方向损失下降最快，考虑使用负梯度作为伪残差</li>
</ul>
<h4 id="Newton法"><a href="#Newton法" class="headerlink" title="Newton法"></a>Newton法</h4><p>使用二次函数拟合$l(y_i, \hat y_i^{(t)}$</p>
<script type="math/tex; mode=display">\begin{align*}
obj^{(t)} & = \sum_i^N l(y_i, \hat y_i^{(t-1)} + f_t(x_i)) +
    \Omega(f_t) \\
& \approx \sum_{i=1}^N [l(y_i, \hat y^{(t-1)}) + g_i f_t(x_i)
    + \frac 1 2 h_i f_t^2(x_i)] + \Omega(f_t) \\
\end{align*}</script><blockquote>
<ul>
<li>$h<em>i = \partial^2</em>{\hat y} l(y_i, \hat y^{t-1})$</li>
</ul>
</blockquote>
<ul>
<li>二次函数本身有极值</li>
<li>可以结合复杂度罚综合考虑，使得每个基学习器损失达到最小</li>
</ul>
<h3 id="Boosting-amp-Bagging"><a href="#Boosting-amp-Bagging" class="headerlink" title="Boosting&amp;Bagging"></a>Boosting&amp;Bagging</h3><ul>
<li><p>基分类器足够简单时，boosting表现均显著好于bagging</p>
<ul>
<li>仅靠单次决策（单个属性、属性组合）分类</li>
</ul>
</li>
<li><p>使用C4.5树作为基分类器时，boosting仍然具有优势，但是不够
有说服力</p>
</li>
</ul>
<blockquote>
<ul>
<li>结论来自于<em>Experiments with a New Boosting Algorithm</em></li>
</ul>
</blockquote>
<h4 id="Boosting-amp-Bagging-1"><a href="#Boosting-amp-Bagging-1" class="headerlink" title="Boosting&amp;Bagging"></a>Boosting&amp;Bagging</h4><ul>
<li><p>基分类器足够简单时，boosting表现均显著好于bagging</p>
<ul>
<li>仅靠单次决策（单个属性、属性组合）分类</li>
</ul>
</li>
<li><p>使用C4.5树作为基分类器时，boosting仍然具有优势，但是不够
有说服力</p>
</li>
</ul>
<blockquote>
<ul>
<li>结论来自于<em>Experiments with a New Boosting Algorithm</em></li>
</ul>
</blockquote>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><em>probably approximately correct</em>：概率近似正确，在概率近似
正确学习的框架中</p>
<ul>
<li><p><em>strongly learnable</em>：强可学习，一个概念（类），如果存在
一个多项式的学习算法能够学习它，并且<strong>正确率很高</strong>，那么
就称为这个概念是强可学习的</p>
</li>
<li><p><em>weakly learnable</em>：弱可学习，一个概念（类），如果存在
一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测
略好，称此概念为弱可学习的</p>
</li>
<li><p><em>Schapire</em>证明：在PAC框架下强可学习和弱可学习是等价的</p>
</li>
</ul>
<h3 id="具体措施"><a href="#具体措施" class="headerlink" title="具体措施"></a>具体措施</h3><blockquote>
<ul>
<li>弱学习算法要比强学习算法更容易寻找，所以具体实施提升就是
  需要解决的问题</li>
</ul>
</blockquote>
<ul>
<li><p><strong>改变训练数据权值、概率分布的方法</strong></p>
<ul>
<li>提高分类错误样本权值、降低分类正确样本权值</li>
</ul>
</li>
<li><p><strong>将弱学习器组合成强学习器的方法</strong></p>
<ul>
<li><em>competeing</em></li>
<li><em>simple majority voting</em></li>
<li><em>weighted majority voting</em></li>
<li><em>confidence-based weighting</em></li>
</ul>
</li>
</ul>
<h3 id="学习器组合方式"><a href="#学习器组合方式" class="headerlink" title="学习器组合方式"></a>学习器组合方式</h3><blockquote>
<ul>
<li>很多模型无法直接组合，只能组合预测结果</li>
</ul>
</blockquote>
<ul>
<li><p><em>simple majority voting</em>/<em>simple average</em>：简单平均</p>
<script type="math/tex; mode=display">
h = \frac 1 K \sum_{k=1}_K h_k</script><blockquote>
<ul>
<li>$h_k$：第k个预测</li>
</ul>
</blockquote>
</li>
<li><p><em>weighted majority voting</em>/<em>weighted average</em>：加权平均</p>
<script type="math/tex; mode=display">
h = \frac {\sum_{k=1}^K w_k h_k} {\sum_{k=1}^K w_k}</script><blockquote>
<ul>
<li>$w_k$：第k个预测权重，对分类器可以是准确率</li>
</ul>
</blockquote>
</li>
<li><p><em>competing voting</em>/<em>largest</em>：使用效果最优者</p>
</li>
<li><p><em>confidence based weighted</em>：基于置信度加权</p>
<script type="math/tex; mode=display">\begin{align*}
h = \arg\max_{y \in Y} \sum_{k=1}^K ln(\frac {1 - e_k}
   {e_k}) h_k
\end{align*}</script><blockquote>
<ul>
<li>$e_k$：第k个模型损失</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="Meta-Learning"><a href="#Meta-Learning" class="headerlink" title="Meta Learning"></a>Meta Learning</h2><p>元学习：自动学习关于关于机器学习的元数据的机器学习子领域</p>
<ul>
<li><p>元学习主要目标：使用学习到元数据解释，自动学习如何
<em>flexible</em>的解决学习问题，借此提升现有学习算法性能、
学习新的学习算法，即学习学习</p>
</li>
<li><p>学习算法灵活性即可迁移性，非常重要</p>
<ul>
<li>学习算法往往基于某个具体、假象的数据集，有偏</li>
<li>学习问题、学习算法有效性之间的关系没有完全明白，对
学习算法的应用有极大限制</li>
</ul>
</li>
</ul>
<h3 id="要素"><a href="#要素" class="headerlink" title="要素"></a>要素</h3><ul>
<li>元学习系统必须包含子学习系统</li>
<li>学习经验通过提取元知识获得经验，元知识可以在先前单个
数据集，或不同的领域中获得</li>
<li>学习<em>bias</em>（影响用于模型选择的前提）必须动态选择<ul>
<li><em>declarative bias</em>：声明性偏见，确定假设空间的形式
，影响搜索空间的大小<ul>
<li>如：只允许线性模型</li>
</ul>
</li>
<li><em>procedural bias</em>：过程性偏见，确定模型的优先级<ul>
<li>如：简单模型更好</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Recurrent-Neural-networks"><a href="#Recurrent-Neural-networks" class="headerlink" title="Recurrent Neural networks"></a><em>Recurrent Neural networks</em></h3><p>RNN：<em>self-referential</em> RNN理论上可以通过反向传播学习到，
和反向传播完全不同的权值调整算法</p>
<h3 id="Meta-Reinforcement-Learning"><a href="#Meta-Reinforcement-Learning" class="headerlink" title="Meta Reinforcement Learning"></a><em>Meta Reinforcement Learning</em></h3><p>MetaRL：RL智能体目标是最大化奖励，其通过不断提升自己的学习
算法来加速获取奖励，这也涉及到自我指涉</p>
<h2 id="Additional-Model"><a href="#Additional-Model" class="headerlink" title="Additional Model"></a>Additional Model</h2><p>加法模型：将模型<strong>视为</strong>多个基模型加和而来</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M \beta_m b(x;\theta_m)</script><blockquote>
<ul>
<li>$b(x;\theta_m)$：基函数</li>
<li>$\theta_m$：基函数的参数</li>
<li>$\beta_m$：基函数的系数</li>
</ul>
</blockquote>
<ul>
<li><p>则相应风险极小化策略</p>
<script type="math/tex; mode=display">
\arg\min_{\beta_m, \theta_m} \sum_{i=1}^N
   L(y_i, \sum_{m=1}^M \beta_m b(x_i;\theta_m))</script><blockquote>
<ul>
<li>$L(y, f(x))$：损失函数</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Forward-Stagewise-Algorithm"><a href="#Forward-Stagewise-Algorithm" class="headerlink" title="Forward Stagewise Algorithm"></a>Forward Stagewise Algorithm</h3><p>前向分步算法：从前往后，每步只学习<strong>加法模型</strong>中一个基函数
及其系数，逐步逼近优化目标函数，简化优化复杂度</p>
<ul>
<li><p>即每步只求解优化</p>
<script type="math/tex; mode=display">
\arg\min_{\beta, \theta} \sum_{i=1}^N
   L(y_i, \hat f_m(x_i) + \beta b(x_i;\theta))</script><blockquote>
<ul>
<li>$\hat f_m$：前m轮基函数预测值加和</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><blockquote>
<ul>
<li>输入：训练数据集$T={(x_1,y_1), \cdots, (x_N,y_N)}$，损失
  函数$L(y,f(x))$，基函数集${b(x;\theta)}$</li>
<li>输出：加法模型$f(x)$</li>
</ul>
</blockquote>
<ul>
<li><p>初始化$f_0(x)=0$</p>
</li>
<li><p>对$m=1,2,\cdots,M$，加法模型中M个基函数</p>
<ul>
<li><p>极小化损失函数得到参数$\beta_m, \theta_m$</p>
<script type="math/tex; mode=display">
(\beta_m, \theta_m) = \arg\min_{\beta, \theta}
  \sum_{i=1}^N L(y_i, f_{m-1}(x_1) +
  \beta b(x_i; \theta))</script></li>
<li><p>更新</p>
<script type="math/tex; mode=display">
f_m(x) = f_{m-1}(x) + \beta_m b(x;y_M)</script></li>
</ul>
</li>
<li><p>得到加法模型</p>
<script type="math/tex; mode=display">
f(x) = f_M(x) = \sum_{i=1}^M \beta_m b(x;\theta_m)</script></li>
</ul>
<h3 id="AdaBoost-amp-前向分步算法"><a href="#AdaBoost-amp-前向分步算法" class="headerlink" title="AdaBoost&amp;前向分步算法"></a>AdaBoost&amp;前向分步算法</h3><p>AdaBoost（基分类器loss使用分类误差率）是前向分步算法的特例，
是由基本分类器组成的加法模型，损失函数是指数函数</p>
<ul>
<li><p>基函数为基本分类器时加法模型等价于AdaBoost的最终分类器
$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$</p>
</li>
<li><p>前向分步算法的损失函数为指数函数$L(y,f(x))=exp(-yf(x))$
时，学习的具体操作等价于AdaBoost算法具体操作</p>
<ul>
<li><p>假设经过m-1轮迭代，前向分步算法已经得到</p>
<script type="math/tex; mode=display">\begin{align*}
f_{m-1}(x) & = f_{m-2}(x) + \alpha_{m-1}G_{m-1}(x) \\
  & = \alpha_1G_1(x) + \cdots +
  \alpha_{m-1}G_{m-1}(x)
\end{align*}</script></li>
<li><p>经过第m迭代得到$\alpha_m, G_m(x), f_m(x)$，其中</p>
<script type="math/tex; mode=display">\begin{align*}
(\alpha_m, G_m(x)) & = \arg\min_{\alpha, G}
      \sum_{i=1}^N exp(-y_i(f_{m-1}(x_i) +
      \alpha G(x_i))) \\
  & = \arg\min_{\alpha, G} \sum_{i=1}^N \bar w_{m,i}
      exp(-y_i \alpha G(x_i))
\end{align*}</script><blockquote>
<ul>
<li>$\bar w<em>{m,i}=exp(-y_i f</em>{m-1}(x_i))$：不依赖
$\alpha, G$</li>
</ul>
</blockquote>
</li>
<li><p>$\forall \alpha &gt; 0$，使得损失最小应该有
（提出$\alpha$）</p>
<script type="math/tex; mode=display">\begin{align*}
G_m^{*}(x) & = \arg\min_G \sum_{i=1}^N \bar w_{m,i}
      exp(-y_i f_{m-1}(x_i)) \\
  & = \arg\min_G \sum_{i=1}^N \bar w_{m,i}
      I(y_i \neq G(x_i))
\end{align*}</script><p>此分类器$G_m^{*}$即为使得第m轮加权训练误差最小分类器
，即AdaBoost算法的基本分类器</p>
</li>
<li><p>又根据</p>
<script type="math/tex; mode=display">\begin{align*}
\sum_{i=1}^N \bar w_{m,i} exp(-y_i \alpha G(x_i)) & =
  \sum_{y_i = G_m(x_i)} \bar w_{m,i} e^{-\alpha} +
  \sum_{y_i \neq G_m(x_i)} \bar w_{m,i} e^\alpha \\
& = (e^\alpha - e^{-\alpha}) \sum_{i=1}^N (\bar w_{m,i}
  I(y_i \neq G(x_i))) + e^{-\alpha}
  \sum_{i=1}^N \bar w_{m,i}
\end{align*}</script><p>带入$G_m^{*}$，对$\alpha$求导置0，求得极小值为</p>
<script type="math/tex; mode=display">\begin{align*}
\alpha_m^{*} & = \frac 1 2 log \frac {1-e_m} {e_m} \\
e_m & = \frac {\sum_{i=1}^N (\bar w_{m,i}
      I(y_i \neq G_m(x_i)))}
  {\sum_{i=1}^N \bar w_{m,i}} \\
& = \frac {\sum_{i=1}^N (\bar w_{m,i}
      I(y_i \neq G_m(x_i)))} {Z_m} \\
& = \sum_{i=1}^N w_{m,i} I(y_i \neq G_m(x_i))
\end{align*}</script><blockquote>
<ul>
<li>$w_{m,i}, Z_M$同AdaBoost中</li>
</ul>
</blockquote>
<p>即为AdaBoost中$\alpha_m$</p>
</li>
<li><p>对权值更新有</p>
<script type="math/tex; mode=display">
\bar w_{m+1,i} = \bar w_{m,i} exp(-y_i \alpha_m G_m(x))</script><p>与AdaBoost权值更新只相差规范化因子$Z_M$</p>
</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:11:28.000Z" title="7/16/2021, 3:11:28 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">15 minutes read (About 2228 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/adaboost.html">AdaBoost</a></h1><div class="content"><h2 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h2><p>通过改变训练样本权重，学习多个分类器，并将分类器进行线性
组合，提高分类性能</p>
<ul>
<li>对离群点、奇异点敏感</li>
<li>对过拟合不敏感</li>
</ul>
<h3 id="Boosting实现"><a href="#Boosting实现" class="headerlink" title="Boosting实现"></a>Boosting实现</h3><blockquote>
<ul>
<li><p>改变训练数据权值或概率分布：提高分类错误样本权值、降低
  分类正确样本权值</p>
</li>
<li><p>弱分类器组合：加权多数表决，即加大分类误差率小的弱分类器
  权值，使其在表决中起更大作用；减小分类误差率大的弱分类器
  权值，使其在表决中起更小作用</p>
</li>
</ul>
</blockquote>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p><img src="/imgs/adaboost_steps.png" alt="adaboost_steps"></p>
<blockquote>
<ul>
<li>输入：训练数据集$T={(x_1, y_1), \cdots, (x_N, y_N)}$，
  弱分类器算法$G(x)$<blockquote>
<ul>
<li>$x_i \in \mathcal{X \subset R^n}$</li>
<li>$y_i \in \mathcal{Y} = {-1, +1 }$</li>
</ul>
</blockquote>
</li>
<li>输出：最终分类器$G(x)$</li>
</ul>
</blockquote>
<ul>
<li><p>初始化训练数据权值分布：
$D<em>1=(w</em>{11}, \cdots, w<em>{1N}), w</em>{1i}=\frac 1 N$</p>
</li>
<li><p>对$m=1,2,\cdots,M$（即训练M个弱分类器）</p>
<ul>
<li><p>使用具有<strong>权值分布</strong>$D_m$的训练数据学习，得到基本
分类器</p>
<script type="math/tex; mode=display">
G_m(x):\mathcal{X} \rightarrow \{-1, +1\}</script></li>
<li><p>计算$G_m(x)$在训练数据集上的<strong>分类误差率</strong></p>
<script type="math/tex; mode=display">\begin{align*}
e_m & = P(G_m(x_i)) \neq y_i) \\
  & = \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i) \\
  & = \sum_{G_m(x_i) \neq y_i} w_{mi}
\end{align*}</script></li>
<li><p>计算$G_m(x)$组合为最终分类器时权重</p>
<script type="math/tex; mode=display">
\alpha = \frac 1 2 log \frac {1-e_m} {e_m}</script><blockquote>
<ul>
<li>$\alpha_m$表示就简单分类器$G_m(x)$在最终分类器中
的重要性，随$e_m$减小而增加
（弱分类器保证$e_m \leq 1/2$）</li>
</ul>
</blockquote>
</li>
<li><p>更新训练集权值分布</p>
<script type="math/tex; mode=display">\begin{align*}
D_{m+1} & = (w_{m+1,1}, \cdots, w_{m+1,N}) \\
w_{m+1,i} & = \frac {w_{mi}} {Z_m}
  exp(-\alpha y_i G_m(x_i)) = \left \{
  \begin{array}{l}
      \frac {w_mi} {Z_m} e^{-\alpha_m},
          & G_m(x_i) = y_i \\
      \frac {w_mi} {Z_m} e^{\alpha_m},
          & G_m(x_i) \neq y_i \\
  \end{array} \right. \\
Z_m & = \sum_{i=1}^N w_{mi} exp(-\alpha_m y_i G_m(x_i))
\end{align*}</script><blockquote>
<ul>
<li>$Z<em>m$：规范化因子，是第m轮调整后的权值之和，其
使得$D</em>{m+1}$成为概率分布</li>
<li>误分类样本权值相当于被放大
$e^{2\alpha_m} = \frac {e_m} {1 - e_m}$倍</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>构建基本分类器线性组合</p>
<script type="math/tex; mode=display">
f(x) = \sum_{m=1}^M \alpha_m G_m(x)</script><p>得到最终分类器</p>
<script type="math/tex; mode=display">
G(x) = sign(f(x)) = sign(\sum_{m=1}^M \alpha_m G_m(x))</script><blockquote>
<ul>
<li>这里$\alpha_m$没有规范化，和不为1，规范化没有必要</li>
<li>$f(x)$符号决定分类预测结果，绝对值大小表示分类确信度</li>
</ul>
</blockquote>
</li>
</ul>
<blockquote>
<ul>
<li>AdaBoost中分类器学习和之后的分类误差率“无关”，基分类器
  学习算法中的loss不是分类误差率，可以是其他loss，只是需要
  考虑训练数据的权值分布<blockquote>
<ul>
<li>好像基学习器的loss就要是和集成部分调权的loss一致<h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1></li>
<li><strong>按权值分布有放回的抽样</strong>，在抽样集上进行训练</li>
<li>各样本loss按权重加权，类似分类误差率中加权</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h3 id="训练误差边界"><a href="#训练误差边界" class="headerlink" title="训练误差边界"></a>训练误差边界</h3><p>AdaBoost算法最终分类器的训练误差边界为</p>
<script type="math/tex; mode=display">
\frac 1 N \sum_{i=1}^N I(G(x_i) \neq y_i) \leq
    \frac 1 N \sum_i exp(-y_if(x_i)) = \prod_m Z_m</script><ul>
<li><p>$G(x_i) \neq y_i$时，$y_if(x_i)&lt;0$，所以
$exp(-y_i f(x_i)) \geq 1$，则不等式部分可证</p>
</li>
<li><script type="math/tex; mode=display">\begin{align*}
\frac 1 N \sum_i exp(-y_i f(x_i))
   & = \frac 1 N \sum_i exp(-\sum_{m=1}^M
       \alpha_m y_i G_m(x_i)) \\
   & = \sum_i (w_{1,i} \prod_{m=1}^M
       exp(-\alpha_m y_i G_m(x_i))) \\
   & = \sum_i (Z_1 w_{2,i} \prod_{m=2}^M
       exp(-\alpha_m y_i G_m(x_i))) \\
   & = \prod_{m=1}^M Z_i \sum_i w_{M+1,i} \\
   & = \prod_{m=1}^M Z_i
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li>AdaBoost训练误差边界性质的关键：权重调整与基本分类器权重
  调整<strong>共系数</strong>（形式不完全一样）</li>
<li>这也是AdaBoost权重调整设计的依据，方便给出误差上界</li>
</ul>
</blockquote>
<h4 id="二分类训练误差边界"><a href="#二分类训练误差边界" class="headerlink" title="二分类训练误差边界"></a>二分类训练误差边界</h4><script type="math/tex; mode=display">
\prod_{m=1}^M Z_m = \prod_{m=1}^M (2\sqrt{e_m(1-e_m)})
    = \prod_{m=1}^M \sqrt{(1-4\gamma_m^2)}
    \leq exp(-2\sum_{m=1}^M \gamma_m^2)</script><blockquote>
<ul>
<li>$\gamma_m = \frac 1 2 - e_m$</li>
</ul>
</blockquote>
<ul>
<li><script type="math/tex; mode=display">\begin{align*}
Z_m & = \sum_{i=1}^N w_{m,i} exp(-\alpha y_i G_m(x_i)) \\
   & = \sum_{y_i = G_m(x_i)} w_{m,i}e^{-\alpha_m} +
       \sum_{y_i \neq G_m(x_i)} w_{m,i}e^{\alpha_m} \\
   & = (1-e_m)e^{-\alpha_m} + e_m e^{\alpha_m} \\
   & = 2\sqrt{e_m(1-e_m)} \\
   & = \sqrt{1-4\gamma^2}
\end{align*}</script></li>
<li><p>由$\forall x \in [0, 0.5], e^{-x} &gt; \sqrt{1-2x}$可得，
$\sqrt{1-4\gamma_m^2} \leq exp(-2\gamma_m^2)$</p>
</li>
</ul>
<blockquote>
<ul>
<li>二分类AdaBoost误差边界性质的关键：$\alpha$的取值，也是
  前向分步算法（损失函数）要求</li>
<li>若存$\gamma &gt; 0$，对所有m有$\gamma_m \geq \gamma$，则<script type="math/tex; mode=display">
  \frac 1 N \sum_{i=1}^N I(G(x_i) \neq y_i) \neq
      exp(-2M\gamma^2)</script>  即AdaBoost的训练误差是<strong>指数下降</strong>的</li>
<li>分类器下界$\gamma$可以未知，AdaBoost能适应弱分类器各自
  训练误差率，所以称为<em>adptive</em></li>
</ul>
</blockquote>
<h2 id="Adaboost-M1"><a href="#Adaboost-M1" class="headerlink" title="Adaboost.M1"></a><em>Adaboost.M1</em></h2><p>Adaboost.M1是原版AdaBoost的多分类升级版，基本思想同Adaboost</p>
<h3 id="Boosting实现-1"><a href="#Boosting实现-1" class="headerlink" title="Boosting实现"></a>Boosting实现</h3><ul>
<li><p>基分类器组合方式</p>
<ul>
<li>仍然是加权投票，且投票权重同Adaboost</li>
<li>出于多分类考虑，没有使用<code>sign</code>符号函数</li>
</ul>
</li>
<li><p>改变训练数据权值或概率分布：和Adaboost形式稍有不同，但
相对的错误分类样本提升比率完全相同</p>
<ul>
<li>被上个分类器错误分类样本，权值保持不变</li>
<li>被上个分类器正确分类样本，权值缩小比例是Adaboost平方</li>
</ul>
</li>
</ul>
<h3 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h3><ul>
<li><p>输入</p>
<ul>
<li>训练集：$T={x_i, y_i}, i=1,\cdots,N; y_i \in C, C={c_1, \cdots, c_m}$</li>
<li>训练轮数：T</li>
<li>弱学习器：I</li>
</ul>
</li>
<li><p>输出：提升分类器</p>
<script type="math/tex; mode=display">
H(x) = \arg\max_{y \in C} \sum_{m=1}^M
   ln(\frac 1 {\beta_m}) [h_m(x) = y]</script><blockquote>
<ul>
<li>$h_t, h_t(x) \in C$：分类器</li>
<li>$\beta_t$：分类器权重</li>
</ul>
</blockquote>
</li>
</ul>
<p><img src="/imgs/adaboostm1_steps.png" alt="adaboostm1_steps"></p>
<h3 id="误分率上界"><a href="#误分率上界" class="headerlink" title="误分率上界"></a>误分率上界</h3><blockquote>
<ul>
<li>对弱学习算法产生的伪损失$\epsilon<em>1,\cdots,\epsilon_t$，
  记$\gamma_t = 1/2 \epsilon_t$，最终分类器$h</em>{fin}$误分率
  上界有<script type="math/tex; mode=display">
  \frac 1 N |\{i: h_{fin}(x_i) \neq y_i \}| \leq
      \prod_{t-1}^T \sqrt {1-4\gamma^2} \leq
      exp(-2 \sum_{t-1}^T \gamma^2)</script></li>
</ul>
</blockquote>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>Adaboost.M1和Adaboost基本上没有区别</p>
<ul>
<li>类别数目为2的Adaboost.M1就是Adaboost</li>
<li>同样无法处理对误分率高于0.5的情况，甚至在多分类场合，
误分率小于0.5更加难以满足</li>
<li>理论误分率上界和Adaboost相同</li>
</ul>
<h2 id="Adaboost-M2"><a href="#Adaboost-M2" class="headerlink" title="Adaboost.M2"></a><em>Adaboost.M2</em></h2><p>AdaboostM2是AdaboostM1的进阶版，更多的利用了基分类器信息</p>
<ul>
<li>要求基学习器能够输出更多信息：输出对样本分别属于各类别
的置信度向量，而不仅仅是最终标签</li>
<li>要求基分类器更加精细衡量错误：使用伪损失代替误分率
作为损失函数</li>
</ul>
<h3 id="Psuedo-Loss"><a href="#Psuedo-Loss" class="headerlink" title="Psuedo-Loss"></a><em>Psuedo-Loss</em></h3><script type="math/tex; mode=display">\begin{align*}
L & = \frac 1 2 \sum_{(i,y) \in B} D_{i,y}
    (1 - h(x_i, y_i) + h(x_i, y)) \\
& = \frac 1 2 \sum_{i=1}^N D_i (1 - h(x_i, y_i) +
    \sum_{y \neq y_i} (w_{i,y} h(x_i, y)))
\end{align*}</script><blockquote>
<ul>
<li>$D$：权重分布（行和为1，但不满足列和为1）<blockquote>
<ul>
<li>$D_{i,y}$：个体$x_i$中错误标签$y$的权重，代表从个体
 $x_i$中识别出错误标签$y$的重要性</li>
</ul>
</blockquote>
</li>
<li>$B = {(i, y)|y \neq y_i, i=1,2,\cdots,N }$</li>
<li>$w$：个体各错误标签权重边际分布</li>
<li>$h(x, y)$：模型$h$预测样本$x$为$y$的置信度<blockquote>
<ul>
<li>$h(x_i,y_i)$：预测正确的置信度</li>
<li>$h(x_i,y), y \neq y_i$：预测$x_i$为错误分类$y$置信度</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li>伪损失函数同时考虑了样本和<strong>标签</strong>的权重分布</li>
<li>通过改变此分布，能够更明确的关注难以预测的个体标签，
而不仅仅个体</li>
</ul>
<h3 id="Boosting实现-2"><a href="#Boosting实现-2" class="headerlink" title="Boosting实现"></a>Boosting实现</h3><ul>
<li><p>改变数据权值或者概率分布</p>
<ul>
<li>使用<em>psuedo-loss</em>替代误分率，以此为导向改变权值</li>
<li>对多分类每个错误分类概率分别计算错误占比，在此基础上
分别计算</li>
</ul>
</li>
<li><p>基分类器组合方式：同Adaboost.M1</p>
</li>
</ul>
<h3 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a>步骤</h3><p><img src="/imgs/adaboostm2_steps.png" alt="adaboostm2_steps"></p>
<h3 id="训练误差上界"><a href="#训练误差上界" class="headerlink" title="训练误差上界"></a>训练误差上界</h3><blockquote>
<ul>
<li>对弱学习算法产生的伪损失$\epsilon<em>1,\cdots,\epsilon_t$，
  记$\gamma_t = 1/2 \epsilon_t$，最终分类器$h</em>{fin}$误分率
  上界有</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\frac 1 N |\{i: h_{fn}(x_i) \neq y_i \}| \leq
    (M-1) \prod_{t-1}^T \sqrt {1-4\gamma^2} \leq
    (M-1) exp(-2 \sum_{t-1}^T \gamma^2)</script><h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>基于伪损失的Adaboost.M2能够提升稍微好于随机预测的分类器</p>
</li>
<li><p>Adaboosting.M2能够较好的解决基分类器对噪声的敏感性，但是
仍然距离理论最优<em>Bayes Error</em>有较大差距，额外误差主要
来自于</p>
<ul>
<li>训练数据</li>
<li>过拟合</li>
<li>泛化能力</li>
</ul>
</li>
<li><p>控制权值可以有效的提升算法，减小最小训练误差、过拟合
、泛化能力</p>
<ul>
<li>如对权值使用原始样本比例作为先验加权</li>
</ul>
</li>
<li><p>其分类结果不差于AdaBoost.M1（在某些基分类器、数据集下）</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">6 minutes read (About 847 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/bagging.html">Bagging</a></h1><div class="content"><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a><em>Bagging</em></h2><p><em>bagging</em>：<em>bootstrap aggregating</em>，每个分类器随机从原样本
中做<strong>有放回的随机抽样</strong>，在抽样结果上训练基模型，最后根据
多个基模型的预测结果产生最终结果</p>
<ul>
<li>核心为bootstrap重抽样自举</li>
</ul>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ul>
<li><p>建模阶段：通过boostrap技术获得k个自举样本
$S_1, S_2,…, S_K$，以其为基础建立k个相同类型模型
$T_1, T_2,…, T_K$</p>
</li>
<li><p>预测阶段：组合K个预测模型</p>
<ul>
<li>分类问题：K个预测模型“投票”</li>
<li>回归问题：K个预测模型平均值</li>
</ul>
</li>
</ul>
<h3 id="模型性质"><a href="#模型性质" class="headerlink" title="模型性质"></a>模型性质</h3><ul>
<li>相较于单个基学习器，Bagging的优势<ul>
<li>分类Bagging几乎是最优的贝叶斯分类器</li>
<li>回归Bagging可以通过降低方差（主要）降低均方误差</li>
</ul>
</li>
</ul>
<h4 id="预测误差"><a href="#预测误差" class="headerlink" title="预测误差"></a>预测误差</h4><p>总有部分观测未参与建模，预测误差估计偏乐观</p>
<ul>
<li><p><em>OOB</em>预测误差：<em>out of bag</em>，基于袋外观测的预测误差，
对每个模型，使用没有参与建立模型的样本进行预测，计算预测
误差</p>
</li>
<li><p>OOB观测比率：样本总量n较大时有</p>
<script type="math/tex; mode=display">
r = (1 - \frac 1 n)^n \approx \frac 1 e = 0.367</script><ul>
<li>每次训练样本比率小于10交叉验证的90%</li>
</ul>
</li>
</ul>
<h2 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a><em>Random Forest</em></h2><p>随机森林：随机建立多个有较高预测精度、弱相关（甚至不相关）
的决策树（基础学习器），多棵决策树共同对新观测做预测</p>
<ul>
<li><p>RF是Bagging的扩展变体，在以决策树为基学习器构建Bagging
集成模型的基础上，在训练过程中引入了<strong>随机特征选择</strong></p>
</li>
<li><p>适合场景</p>
<ul>
<li>数据维度相对较低、同时对准确率有要求</li>
<li>无需很多参数调整即可达到不错的效果</li>
</ul>
</li>
</ul>
<h3 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h3><ul>
<li><p>样本随机：Bootstrap自举样本</p>
</li>
<li><p>输入属性随机：对第i棵决策树通过随机方式选取K个输入变量
构成候选变量子集$\Theta_I$</p>
<ul>
<li><p>Forest-Random Input：随机选择$k=log_2P+1或k=\sqrt P$
个变量</p>
</li>
<li><p>Forest-Random Combination</p>
<ul>
<li>随机选择L个输入变量x</li>
<li>生成L个服从均匀分布的随机数$\alpha$</li>
<li>做线性组合
$v<em>j = \sum</em>{i=1}^L \alpha_i x_i, \alpha_i \in [-1, 1]$</li>
<li>得到k个由新变量v组成的输入变量子集$\Theta_i$</li>
</ul>
</li>
</ul>
</li>
<li><p>在候选变量子集中选择最优变量构建决策树</p>
<ul>
<li>生成决策树时不需要剪枝</li>
</ul>
</li>
<li><p>重复以上步骤构建k棵决策树，用一定集成策略组合多个决策树</p>
<ul>
<li>简单平均/随机森林投票</li>
</ul>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li><p>样本抽样、属性抽样引入随机性</p>
<ul>
<li>基学习器估计误差较大，但是组合模型偏差被修正</li>
<li>不容易发生过拟合、对随机波动稳健性较好</li>
<li>一定程度上避免贪心算法带来的局部最优局限</li>
</ul>
</li>
<li><p>数据兼容性</p>
<ul>
<li>能够方便处理高维数据，“不用做特征选择”</li>
<li>能处理分类型、连续型数据</li>
</ul>
</li>
<li><p>训练速度快、容易实现并行</p>
</li>
<li><p>其他</p>
<ul>
<li>可以得到变量重要性排序</li>
<li>启发式操作</li>
<li>优化操作</li>
</ul>
</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>决策树数量过多时，训练需要资源多</li>
<li>模型解释能力差，有点黑盒模型</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">29 minutes read (About 4279 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/gradient_boosting.html">Boosting</a></h1><div class="content"><h2 id="Gredient-Boosting"><a href="#Gredient-Boosting" class="headerlink" title="Gredient Boosting"></a>Gredient Boosting</h2><p><em>GB</em>：（利用）梯度提升，将提升问题视为优化问题，前向分步算法
利用最速下降思想实现</p>
<ul>
<li><p>一阶展开拟合损失函数，沿负梯度方向迭代更新</p>
<ul>
<li>损失函数中，模型的样本预测值$f(x)$是因变量</li>
<li>即$f(x)$应该沿着损失函数负梯度方向变化</li>
<li>即下个基学习器应该以负梯度方向作为优化目标，即负梯度
作为<strong>伪残差</strong></li>
</ul>
<blockquote>
<ul>
<li>类似复合函数求导</li>
</ul>
</blockquote>
</li>
<li><p>对基学习器预测值求解最优加权系数</p>
<ul>
<li>最速下降法中求解更新步长体现</li>
<li>前向分布算法中求解基学习器权重</li>
</ul>
</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>基学习器拟合目标：损失函数的负梯度在当前模型的值</p>
<script type="math/tex; mode=display">
-\left [ \frac {\partial L(y, \hat y_i)}
    {\partial y_i} \right ]_{\hat y_i=\hat y_i^{(t-1)}}</script><h4 id="平方损失"><a href="#平方损失" class="headerlink" title="平方损失"></a>平方损失</h4><p>平方损失：$L(y, f(x)) = \frac 1 2 (y - f(x))^2$（回归）</p>
<ul>
<li><p>第m-1个基学习器伪残差为</p>
<script type="math/tex; mode=display">
r_{m,i} = y_i - f_{m-1}(x_i), i=1,2,\cdots,N</script><blockquote>
<ul>
<li>$N$：样本数量</li>
</ul>
</blockquote>
</li>
<li><p>第m个基学习器为</p>
<script type="math/tex; mode=display">\begin{align*}
h_m & = \arg\min_h \sum_{i=1}^N \frac 1 2
   (y_i - (f_{m-1}(x_i) + h(x)))^2 \\
& = \arg\min_h \sum_{i=1}^N \frac 1 2
   (C_{m,i} - h(x))^2 \\
C_{m,i} & = y_i - f_{m-1}(x_i)
\end{align*}</script></li>
<li><p>第m轮学习器组合为</p>
<script type="math/tex; mode=display">
f_m = f_{m-1} + \alpha_m h_m</script><blockquote>
<ul>
<li>$\alpha_m$：学习率，留给之后基模型学习空间</li>
</ul>
</blockquote>
<ul>
<li>这里只是形式上表示模型叠加，实际上树模型等不可加，
应该是模型预测结果叠加</li>
</ul>
</li>
</ul>
<h4 id="指数损失"><a href="#指数损失" class="headerlink" title="指数损失"></a>指数损失</h4><p>指数损失：$L(y, f(x)) = e^{-y f(x)}$（分类）</p>
<ul>
<li><p>第m-1个基学习器伪残差</p>
<script type="math/tex; mode=display">
r_{m,i} = -y_i e^{-y_i f_{m-1}(x_i)}, i=1,2,\cdots,N</script></li>
<li><p>基学习器、权重为</p>
<script type="math/tex; mode=display">\begin{align*}
h_m & = \arg\min_h \sum_{i=1}^N exp(-y_i(f_{m-1}(x_i)
   + \alpha f(x_i))) \\
& = \arg\min_h \sum_{i=1}^N C_{m,i}
   exp(-y_i \alpha f(x_i)) \\
C_{m,i} & = exp(-y_i f_{m-1}(x_i))
\end{align*}</script></li>
<li><p>第m轮学习器组合为</p>
<script type="math/tex; mode=display">
f_m = f_{m-1} + \alpha_m h_m</script></li>
</ul>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><blockquote>
<ul>
<li>输入：训练数据集$T={(x_1, y_1), \cdots, (x_N, y_N)}$，
  损失函数$L(y, f(x))$<blockquote>
<ul>
<li>$x_i \in \mathcal{X \subset R^n}$</li>
<li>$y_i \in \mathcal{Y} = {-1, +1 }$</li>
</ul>
</blockquote>
</li>
<li>输出：回归树$\hat f(x)$</li>
</ul>
</blockquote>
<ul>
<li><p>初始化模型</p>
<script type="math/tex; mode=display">
\hat y_i^{(0)} = \arg\min_{\hat y} \sum_{i=1}^N
   L(y_i, \hat y)</script></li>
<li><p>对$m=1,2,\cdots,M$（即训练M个若分类器）</p>
<ul>
<li><p>计算伪残差</p>
<script type="math/tex; mode=display">
r_i^{(t)} = -\left [ \frac {\partial L(y, \hat y_i)}
  {\partial y_i} \right ]_{\hat y_i=\hat y_i^{(t-1)}}</script></li>
<li><p>基于${(x_i, r_i^{(t)})}$生成基学习器$h_t(x)$</p>
</li>
<li><p>计算最优系数</p>
<script type="math/tex; mode=display">
\gamma = \arg\min_\gamma \sum_{i=1}^N
  L(y_i, \hat y_i^{(t-1)} + \gamma h_t(x_i))</script></li>
<li><p>更新预测值</p>
<script type="math/tex; mode=display">
\hat y_i^{(t)} = \hat y_i^{(t-1)} + \gamma_t h_t (x)</script></li>
</ul>
</li>
<li><p>得到最终模型</p>
<script type="math/tex; mode=display">
\hat f(x) = f_M(x) = \sum_{t=1}^M \gamma_t h_t(x)</script></li>
</ul>
<h3 id="Gradient-Boosted-Desicion-Tree"><a href="#Gradient-Boosted-Desicion-Tree" class="headerlink" title="Gradient Boosted Desicion Tree"></a>Gradient Boosted Desicion Tree</h3><p><em>GBDT</em>：梯度提升树，以回归树为基学习器的梯度提升方法</p>
<ul>
<li><p>GBDT会累加所有树的结果，本质上是回归模型（毕竟梯度）</p>
<ul>
<li>所以一般使用CART回归树做基学习器</li>
<li>当然可以实现分类效果</li>
</ul>
</li>
<li><p>损失函数为平方损失（毕竟回归），则相应伪损失/残差</p>
<script type="math/tex; mode=display">
r_{t,i} = y_i - f_{t-1}(x_i), i=1,2,\cdots,N</script></li>
</ul>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li>准确率、效率相较于RF有一定提升</li>
<li>能够灵活的处理多类型数据</li>
<li>Boosting类算法固有的基学习器之间存在依赖，难以并行训练
数据，比较可行的并行方案是在每轮选取最优特征切分时，并行
处理特征</li>
</ul>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p><em>Extreme Gradient Boost</em>/<em>Newton Boosting</em>：前向分步算法利用
Newton法思想实现</p>
<ul>
<li><p>二阶展开拟合损失函数</p>
<ul>
<li>损失函数中，模型的样本预测值$\hat y_i$是因变量</li>
<li>将损失函数对$\hat y_i$二阶展开拟合</li>
<li>求解使得损失函数最小参数</li>
</ul>
</li>
<li><p>对基学习器预测值求解最优加权系数</p>
<ul>
<li>阻尼Newton法求解更新步长体现</li>
<li>前向分布算法中求解基学习器权重</li>
<li>削弱单个基学习器影响，让后续基学习器有更大学习空间</li>
</ul>
</li>
</ul>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><ul>
<li><p>第t个基分类器损失函数</p>
<script type="math/tex; mode=display">\begin{align*}
obj^{(t)} & = \sum_{i=1}^N l(y_i, \hat y_i^{(t)}) +
   \Omega(f_t) \\

& = \sum_i^N l(y_i, \hat y_i^{(t-1)} + f_t(x_i)) +
   \Omega(f_t) \\

& \approx \sum_{i=1}^N [l(y_i, \hat y^{(t-1)}) + g_i
   f_t(x_i) + \frac 1 2 h_i f_t^2(x_i)] + \Omega(f_t) \\

& = \sum_{i=1}^N [l(y_i, \hat y^{(t-1)}) + g_i f_t(x_i) +
   \frac 1 2 h_i f_t^2(x_i)] + \gamma T_t +
   \frac 1 2 \lambda \sum_{j=1}^T {w_j^{(t)}}^2 \\

\Omega(f_t) & = \gamma T_t + \frac 1 2 \lambda
   \sum_{j=1}^T {w_j^{(t)}}^2
\end{align*}</script><blockquote>
<ul>
<li>$f_t$：第t个基学习器</li>
<li>$f_t(x_i)$：第t个基学习器对样本$x_i$的取值</li>
<li>$g<em>i = \partial</em>{\hat y} l(y_i, \hat y^{t-1})$</li>
<li>$h<em>i = \partial^2</em>{\hat y} l(y_i, \hat y^{t-1})$</li>
<li>$\Omega(f_t)$：单个基学习器的复杂度罚</li>
<li>$T_t$：第t个基学习器参数数量，即$L_0$罚<blockquote>
<ul>
<li>线性回归基学习器：回归系数数量</li>
<li>回归树基学习器：叶子节点数目</li>
</ul>
</blockquote>
</li>
<li>$\gamma$：基学习器$L_0$罚系数，模型复杂度惩罚系数</li>
<li>$w_j = f_t$：第t个基学习器参数值，即$L_2$罚<blockquote>
<ul>
<li>线性回归基学习器：回归系数值</li>
<li>回归树基学习器：叶子节点</li>
</ul>
</blockquote>
</li>
<li>$\lambda$：基学习器$L_2$罚系数，模型贡献惩罚系数</li>
<li>$\approx$：由二阶泰勒展开近似</li>
</ul>
</blockquote>
</li>
<li><p>对损失函数进行二阶泰勒展开（类似牛顿法）拟合原损失函数，
同时利用一阶、二阶导数求解下个迭代点</p>
</li>
<li><p>正则项以控制模型复杂度</p>
<ul>
<li>降低模型估计误差，避免过拟合</li>
<li>$L_2$正则项也控制基学习器的学习量，给后续学习器留下
学习空间</li>
</ul>
</li>
</ul>
<h3 id="树基学习器"><a href="#树基学习器" class="headerlink" title="树基学习器"></a>树基学习器</h3><p>XGBoost Tree：以回归树为基学习器的XGBoost模型</p>
<ul>
<li><p>模型结构说明</p>
<ul>
<li>基学习器类型：CART</li>
<li>叶子节点取值作惩罚：各叶子节点取值差别不应过大，否则
说明模型不稳定，稍微改变输入值即导致输出剧烈变化</li>
<li>树复杂度惩罚：叶子结点数量</li>
</ul>
</li>
<li><p>XGBoost最终损失（结构风险）有</p>
<script type="math/tex; mode=display">\begin{align*}
R_{srm} & = \sum_{i=1}^N l(y_i, \hat y_i) +
   \sum_{t=1}^M \Omega(f_t)
\end{align*}</script><blockquote>
<ul>
<li>$N, M$：样本量、基学习器数量</li>
<li>$\hat y_i$：样本$i$最终预测结果</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h4><ul>
<li><p>以树作基学习器时，第$t$基学习器损失函数为</p>
<script type="math/tex; mode=display">\begin{align*}
obj^{(t)} & = \sum_{i=1}^N l(y_i, \hat y_i^{(t)}) +
   \Omega(f_t) \\

& \approx \sum_{i=1}^N [l(y_i, \hat y^{(t-1)}) + g_i
   f_t(x_i) + \frac 1 2 h_i f_t^2(x_i)] + \gamma T_t
   + \frac 1 2 \lambda \sum_{j=1}^T {w_j^{(t)}}^2 \\

& = \sum_{j=1}^{T_t} [(\sum_{i \in I_j} g_i) w_j^{(t)} +
   \frac 1 2 (\sum_{i \in I_j} h_i + \lambda)
   {w_j^{(t)}}^2] + \gamma T_t + \sum_{i=1}^N
   l(y_i, \hat y^{(t)}) \\

& = \sum_{j=1}^{T_t} [G_i w_j^{(t)} + \frac 1 2
   (H_j + \lambda){w_j^{(t)}}^2] + \gamma T_t +
   \sum_{i=1}^N l(y_i, \hat y^{(t)}) \\

& = \sum_{j=1}^{T_t} [G_i w_j^{(t)} + \frac 1 2
   (H_j + \lambda)(w_j^{(t)})^2] + \gamma T_t +
   \sum_{i=1}^N l(y_i, \hat y^{(t)}) \\

\end{align*}</script><blockquote>
<ul>
<li>$f_t, T_t$：第t棵回归树、树叶子节点</li>
<li>$f_t(x_i)$：第t棵回归树对样本$x_i$的预测得分</li>
<li>$w_j^{(t)} = f_t(x)$：第t棵树中第j叶子节点预测得分</li>
<li>$g<em>i = \partial</em>{\hat y} l(y_i, \hat y^{t-1})$</li>
<li>$h<em>i = \partial^2</em>{\hat y} l(y_i, \hat y^{t-1})$</li>
<li>$I_j$：第j个叶结点集合</li>
<li>$G<em>j = \sum</em>{i \in I_j} g_i$</li>
<li>$H<em>j = \sum</em>{i \in I_j} h_i$</li>
</ul>
</blockquote>
<ul>
<li><p>对回归树，正则项中含有$(w_j^{(t)})^2$作为惩罚，能够
和损失函数二阶导合并，不影响计算</p>
</li>
<li><p>模型复杂度惩罚项惩罚项是针对树的，定义在叶子节点上，
而平方损失是定义在样本上，合并时将其改写</p>
</li>
</ul>
</li>
<li><p>第t棵树的整体损失等于<strong>其各叶子结点损失加和</strong>，且
各叶子结点取值之间独立</p>
<ul>
<li><p>则第t棵树各叶子结点使得损失最小的最优取值如下
（$G_j, H_j$是之前所有树的预测得分和的梯度取值，在
当前整棵树的构建中是定值，所以节点包含样本确定后，
最优取值即可确定）</p>
<script type="math/tex; mode=display">
w_j^{(*)} = -\frac {\sum_{i \in I_j} g_i}
  {\sum_{i \in I_j} h_i + \lambda}
= -\frac {G_j} {H_j + \lambda}</script></li>
<li><p>整棵树结构分数（最小损失）带入即可得</p>
<script type="math/tex; mode=display">
obj^{(t)} = -\frac 1 2 \sum_{j=i}^M \frac {G_j^2}
  {H_j + \lambda} + \gamma T</script></li>
<li><p>则在结点分裂为新节点时，树损失变化量为</p>
<script type="math/tex; mode=display">
l_{split} = \frac 1 2 \left [
\frac {(\sum_{i \in I_L} g_i)^2} {\sum_{i \in I_L h_i}
  + \lambda} +
\frac {(\sum_{i \in I_R} g_i)^2} {\sum_{i \in I_R h_i}
  + \lambda} -
\frac {(\sum_{i \in I} g_i)^2} {\sum_{i \in I h_i} +
  \lambda}
\right ] - \gamma</script><blockquote>
<ul>
<li>$I_L, I_R$：结点分裂出的左、右结点</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>则最后应根据树损失变化量确定分裂节点、完成树的分裂，精确
贪心分裂算法如下</p>
<p><a href="/imgs/xgb_exact_greedy_algorithm_for_split_finding.png">!xgb_exact_greedy_algorithm_for_split_finding</a></p>
<ul>
<li><p>对于连续型特征需遍历所有可能切分点</p>
<ul>
<li>对特征排序</li>
<li>遍历数据，计算上式给出的梯度统计量、损失变化</li>
</ul>
</li>
<li><p>不适合数据量非常大、或分布式场景</p>
</li>
</ul>
</li>
</ul>
<h4 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h4><ul>
<li><p><em>shrinkage</em>：对新学习的树使用系数$\eta$收缩权重</p>
<ul>
<li>类似SGD中学习率，降低单棵树的影响，给后续基模型留下
学习空间</li>
</ul>
</li>
<li><p><em>column subsampling</em>：列抽样</p>
<ul>
<li>效果较传统的行抽样防止过拟合效果更好
（XGB也支持行抽样）</li>
<li>加速计算速度</li>
</ul>
</li>
</ul>
<h3 id="XGB树分裂算法"><a href="#XGB树分裂算法" class="headerlink" title="XGB树分裂算法"></a>XGB树分裂算法</h3><blockquote>
<ul>
<li>线性回归作为基学习器时，XGB相当于L0、L2正则化的
  Logistic回归、线性回归</li>
</ul>
</blockquote>
<h4 id="近似分割算法"><a href="#近似分割算法" class="headerlink" title="近似分割算法"></a>近似分割算法</h4><p>XGB近似分割算法：根据特征分布选取分位数作为候选集，将连续
特征映射至候选点划分桶中，统计其中梯度值、计算最优分割点</p>
<p><a href="/imgs/xgb_approximate_algorithm_for_split_finding.png">!xgb_approximate_algorithm_for_split_finding</a></p>
<ul>
<li><p>全局算法：在树构建初始阶段即计算出所有候选分割点，之后
所有构建过程均使用同样候选分割点</p>
<ul>
<li>每棵树只需计算一次分割点的，步骤少</li>
<li>需要计算更多候选节点才能保证精度</li>
</ul>
</li>
<li><p>局部算法：每次分裂都需要重新计算候选分割点</p>
<ul>
<li>计算步骤多</li>
<li>总的需要计算的候选节点更少</li>
<li>适合构建较深的树</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>分位点采样算法参见
  <em>ml_model/model_enhancement/gradient_boost</em></li>
</ul>
</blockquote>
<h4 id="Sparsity-aware-Split-Finding"><a href="#Sparsity-aware-Split-Finding" class="headerlink" title="Sparsity-aware Split Finding"></a>Sparsity-aware Split Finding</h4><p>稀疏特点分裂算法：为每个树节点指定默认分裂方向，缺失值对应
样本归为该方向</p>
<p><img src="/imgs/xgb_sparsity_aware_split_finding.png" alt="xgb_sparsity_aware_split_finding"></p>
<ul>
<li><p>仅处理非缺失值，算法复杂度和随无缺失数据集大小线性增加，
减少计算量</p>
</li>
<li><p>按照升许、降序分别扫描样本两轮，以便将缺失值样本分别归为
两子节点，确定最优默认分裂方向</p>
<p><img src="/imgs/xgb_sparsity_aware_split_finding_example.png" alt="xgb_sparsity_aware_split_finding_example"></p>
</li>
</ul>
<h3 id="XGB系统设计"><a href="#XGB系统设计" class="headerlink" title="XGB系统设计"></a>XGB系统设计</h3><h4 id="Column-Block-for-Parallel-Learning"><a href="#Column-Block-for-Parallel-Learning" class="headerlink" title="Column Block for Parallel Learning"></a>Column Block for Parallel Learning</h4><blockquote>
<ul>
<li>建树过程中最耗时的部分为寻找最优切分点，而其中最耗时部分
  为数据排序</li>
</ul>
</blockquote>
<p>XGB对每列使用block结构存储数据</p>
<ul>
<li><p>每列block内数据为CSC压缩格式</p>
<ul>
<li>特征排序一次，之后所有树构建可以复用（忽略缺失值）</li>
<li>存储样本索引，以便计算样本梯度</li>
<li>方便并行访问、处理所有列，寻找分裂点</li>
</ul>
</li>
<li><p>精确贪心算法：将所有数据（某特征）放在同一block中</p>
<ul>
<li>可同时对所有叶子分裂点进行计算</li>
<li>一次扫描即可得到所有叶子节点的分割特征点候选者统计
数据</li>
</ul>
</li>
<li><p>近似算法：可以使用多个block、分布式存储数据子集</p>
<ul>
<li>对local策略提升更大，因为local策略需要多次生成分位点
候选集</li>
</ul>
</li>
</ul>
<h4 id="Cache-aware-Access"><a href="#Cache-aware-Access" class="headerlink" title="Cache-aware Access"></a>Cache-aware Access</h4><blockquote>
<ul>
<li>列block结构通过索引获取数据、计算梯度，会导致非连续内存
  访问，降低CPU cache命中率</li>
</ul>
</blockquote>
<ul>
<li><p>精确贪心算法：使用<em>cache-aware prefetching</em></p>
<ul>
<li>对每个线程分配连续缓冲区，读取梯度信息存储其中，再
统计梯度信息</li>
<li>对样本数量较大时更有效</li>
</ul>
</li>
<li><p>近似算法：合理设置block大小为block中最多的样本数</p>
<ul>
<li>过大容易导致命中率低、过小导致并行化效率不高</li>
</ul>
</li>
</ul>
<h4 id="Blocks-for-Out-of-core-Computation"><a href="#Blocks-for-Out-of-core-Computation" class="headerlink" title="Blocks for Out-of-core Computation"></a>Blocks for Out-of-core Computation</h4><ul>
<li><p>数据量过大不能全部存放在主存时，将数据划分为多个block
存放在磁盘上，使用独立线程将block读入主存
（这个是指数据划分为块存储、读取，不是列block）</p>
</li>
<li><p>磁盘IO提升</p>
<ul>
<li><em>block compression</em>：将block按列压缩，读取后使用额外
线程解压</li>
<li><em>block sharding</em>：将数据分配至不同磁盘，分别使用线程
读取至内存缓冲区</li>
</ul>
</li>
</ul>
<h2 id="分位点采样算法—XGB"><a href="#分位点采样算法—XGB" class="headerlink" title="分位点采样算法—XGB"></a>分位点采样算法—XGB</h2><h3 id="Quantile-Sketch"><a href="#Quantile-Sketch" class="headerlink" title="Quantile Sketch"></a>Quantile Sketch</h3><h4 id="样本点权重"><a href="#样本点权重" class="headerlink" title="样本点权重"></a>样本点权重</h4><blockquote>
<ul>
<li>根据已经建立的$t-1$棵树可以得到数据集在已有模型上误差，
  采样时根据误差对样本分配权重，对误差大样本采样粒度更大</li>
</ul>
</blockquote>
<ul>
<li><p>将树按样本点计算损失改写如下</p>
<script type="math/tex; mode=display">
\sum_{i=1}^N \frac 1 2 h_i(f_t(x_i) - \frac {g_i} {h_i})^2
   + \Omega(f_t) + constant</script></li>
<li><p>则对各样本，其损失为$f_t(x_i) - \frac {g_i} {h_i}$
平方和$h_i$乘积，考虑到$f_t(x_i)$为样本点在当前树预测
得分，则可以</p>
<ul>
<li>将样本点损失视为“二次损失”</li>
<li>将$\frac {g_i} {h_i}$视为样本点“当前标签”</li>
<li>相应将$h_i$视为<strong>样本点权重</strong></li>
</ul>
</li>
<li><p>样本权重取值示例</p>
<ul>
<li>二次损失：$h_i$总为2，相当于不带权</li>
<li>交叉熵损失：$h_i=\hat y(1-\hat y)$为二次函数，
则$\hat y$接近0.5时权重取值大，此时该样本预测值
也确实不准确，符合预期</li>
</ul>
</li>
</ul>
<h4 id="Rank函数"><a href="#Rank函数" class="headerlink" title="Rank函数"></a>Rank函数</h4><ul>
<li><p>记集合$D={(x_1, h_1), \cdots, (x_n, h_n)}$</p>
</li>
<li><p>定义rank函数$r_D: R \rightarrow [0, +\infty)$如下</p>
<script type="math/tex; mode=display">
r_D(z) = \frac 1 {\sum_{(x, h) \in D} h}
   \sum_{(x, h) \in D, x < z} h</script><ul>
<li>即集合$D$中权重分布中给定取值分位数</li>
<li>即取值小于给定值样本加权占比，可视为加权秩</li>
</ul>
</li>
</ul>
<h4 id="分位点抽样序列"><a href="#分位点抽样序列" class="headerlink" title="分位点抽样序列"></a>分位点抽样序列</h4><ul>
<li><p>分位点抽样即为从集合$D$特征值中抽样，找到升序点序列
$S = {s_1, \cdots, s_l}$满足</p>
<script type="math/tex; mode=display">
|r_D(s_j - r_D(s_{j+1})| < \epsilon</script><blockquote>
<ul>
<li>$\epsilon$：采样率，序列长度$l = 1/\epsilon$</li>
<li>$s<em>1 = \min</em>{i} x_i$：特征最小值</li>
<li><p>$s<em>l = \max</em>{i} x_i$：特征最大值</p>
</li>
<li><p>各样本等权分位点抽样已有成熟方法，加权分位点抽样方法
 为XGB创新，如下</p>
</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Weighted-Quantile-Sketch"><a href="#Weighted-Quantile-Sketch" class="headerlink" title="Weighted Quantile Sketch"></a>Weighted Quantile Sketch</h3><h4 id="Formalization"><a href="#Formalization" class="headerlink" title="Formalization"></a>Formalization</h4><ul>
<li><p>记$D<em>k={(x</em>{1,k}, h<em>1), \cdots, (x</em>{n,k}, h_n)}$为各
训练样本第$k$维特征、对应二阶导数</p>
<ul>
<li>考虑到数据点可能具有相同$x, h$取值，$D_k$为可能包含
重复值的multi-set</li>
</ul>
</li>
<li><p>对于多重集$D$，额外定义两个rank函数</p>
<script type="math/tex; mode=display">\begin{align*}
r_D^{-}(y) & = \sum_{(x,h) \in D, x<y} h \\
r_D^{+}(y) & = \sum_{(x,h) \in D, x \leq y} h
\end{align*}</script><p>定义相应权重函数为</p>
<script type="math/tex; mode=display">
w_D(y) = r_D^{+}(y) - r_D^{-}(y) =
   \sum_{(x,h) \in D, x=y} h</script></li>
<li><p>多重集$D$上全部权重和定义为</p>
<script type="math/tex; mode=display">
w(D) = \sum_{(x, w) \in D} w</script></li>
</ul>
<h4 id="Quantile-Summary-of-Weighted-Data"><a href="#Quantile-Summary-of-Weighted-Data" class="headerlink" title="Quantile Summary of Weighted Data"></a>Quantile Summary of Weighted Data</h4><ul>
<li><p>定义加权数据上的quantile summary为
$Q(D)=(S, \tilde r_D^{+}, \tilde r_D^{-}, \tilde w_D)$</p>
<ul>
<li><p>$S$为$D$中特征取值抽样升序序列，其最小、最大值分别
为$D$中特征最小、最大值</p>
</li>
<li><p>$\tilde r_D^{+}, \tilde r_D^{-}, \tilde w_D$为定义在
$S$上的函数，满足</p>
<script type="math/tex; mode=display">\begin{align*}
\tilde r_D^{-}(x_i) & \leq r_D^{-}(x_i) \\
\tilde r_D^{+}(x_i) & \leq r_D^{+}(x_i) \\
\tilde w_D(x_i) & \leq w_D(x_i) \\
\tilde r_D^{-}(x_i) + \tilde w_D(x_i) & \leq
  \tilde r_D^{-}(x_{i+1}) \\
\tilde r_D^{+}(x_i) + \tilde w_D(x_i) & \leq
  \tilde r_D^{+}(x_{i+1}) \\
\end{align*}</script></li>
</ul>
</li>
<li><p>$Q(D)$满足如下条件时，称为
$\epsilon$-approximate quantile summary</p>
<script type="math/tex; mode=display">
\forall y \in D_X, \tilde r_D^{+}(y) - \tilde r_D(y) -
   \tilde w_D(y) \leq \epsilon w(D)</script><ul>
<li>即对任意$y$的秩估计误差在$\epslion$之内</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>$\phi-quantile$：秩位于$\phi * N$的元素（一般向下取整）</li>
<li>$\epsilon-\phi-quantile$：秩位于区间
  $[(\phi-\epsilon)<em>N, (\phi+\epsilon)</em>N]$的元素</li>
</ul>
</blockquote>
<h4 id="构建-epsilon-Approximate-Qunatile-Summary"><a href="#构建-epsilon-Approximate-Qunatile-Summary" class="headerlink" title="构建$\epsilon$-Approximate Qunatile Summary"></a>构建$\epsilon$-Approximate Qunatile Summary</h4><ul>
<li><p>初始化：在小规模数据集
$D={(x_1,h_1), \cdots, (x_n,h_n)}$上构建初始
初始quantile summary
$Q(D)=(S, \tilde r_D^{+}, \tilde r_D^{-}, \tilde w_D)$
满足</p>
<script type="math/tex; mode=display">\begin{align*}
\tilde r_D^{-}(x_i) & \leq r_D^{-}(x_i) \\
\tilde r_D^{+}(x_i) & \leq r_D^{+}(x_i) \\
\tilde w_D(x_i) & \leq w_D(x_i)
\end{align*}</script><ul>
<li>即初始化$Q(D)$为0-approximate summary</li>
</ul>
</li>
<li><p><em>merge operation</em>：记
$Q(D<em>1)=(S_1, \tilde r</em>{D<em>1}^{+}, \tilde r</em>{D<em>1}^{-}, \tilde w</em>{D<em>1})$、
$Q(D_2)=(S_2, \tilde r</em>{D<em>2}^{+}, \tilde r</em>{D<em>2}^{-}, \tilde w</em>{D_2})$、
$D = D_1 \cup D_2$，则归并后的
$Q(D)=(S, \tilde r_D^{+}, \tilde r_D^{-}, \tilde w_D)$
定义为</p>
<script type="math/tex; mode=display">\begin{align*}
S & S_1 \cup S_2 \\
\tilde r_D^{-}(x_i) & = \tilde r_{D_1}^{-}(x_i) +
   \tilde r_{D_2}^{-}(x_i) \\
\tilde r_D^{+}(x_i) & = \tilde r_{D_1}^{+}(x_i) +
   \tilde r_{D_2}^{+}(x_i) \\
\tilde w_D(x_i) & = \tilde w_{D_1}(x_i) +
   \tilde w_{D_2}(x_i)
\end{align*}</script></li>
<li><p><em>prune operation</em>：从给定
$Q(D)=(S, \tilde r_D^{+}, \tilde r_D^{-}, \tilde w_D)$，
（其中$S = {x_1, \cdots, x_k }$），构建新的summary
$\acute Q(D)=(\acute S, \tilde r_D^{+}, \tilde r_D^{-}, \tilde w_D)$</p>
<ul>
<li><p>仅定义域从$S$按如下操作抽取
$\acute S={\acute x<em>1, \cdots, \acute x</em>{b+1}}$</p>
<script type="math/tex; mode=display">
\acute x_i = g(Q, \frac {i-1} b w(D))</script></li>
<li><p>$g(Q, d)$为查询函数，对给定quantile summary $Q$、
秩$d$返回秩最接近$d$的元素</p>
<p><img src="/imgs/xgb_weighted_quantile_sketch_query_function.png" alt="xgb_weighted_quantile_sketch_query_function"></p>
</li>
</ul>
</li>
</ul>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6371777973" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>