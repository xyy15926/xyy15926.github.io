<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: NLP - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="follow_it-verification-code" content="SVBypAPPHxjjr7Y4hHfn"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">NLP</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-08-04T14:47:06.000Z" title="8/4/2020, 10:47:06 PM">2020-08-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T14:47:06.000Z" title="8/4/2021, 10:47:06 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Specification/">ML Specification</a><span> / </span><a class="link-muted" href="/categories/ML-Specification/NLP/">NLP</a></span><span class="level-item">10 minutes read (About 1457 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Specification/NLP/word2vec.html">Word2Vec</a></h1><div class="content"><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Word2Vec：word embeding的一种，使用层次化softmax、负采样
训练词向量</p>
<h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p>层次Softmax</p>
<p><img src="/imgs/word2vec_hierarchical_softmax.png" alt="word2vec_hierarchical_softmax"></p>
<ul>
<li><p>对所有词向量求和取平均作为输入层到隐层的映射
（特指CBOW模型）</p>
</li>
<li><p>使用霍夫曼树代替从隐藏层到输出softmax层的映射</p>
</li>
</ul>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>softmax需要对$m$个类别求出softmax概率，参数多、计算复杂</p>
</li>
<li><p>考虑将$m$个类别划分为多个二分类sigmoid，即</p>
<ul>
<li>将总类别划分为两组</li>
<li>依次判断数据点属于哪组</li>
<li>直至数据点所属组仅包含一个类别</li>
</ul>
</li>
<li><p>则多个sigmoid划分构成一棵二叉树，树叶子节点即为$m$
类别</p>
<ul>
<li>二叉树结构可以由多种，最优二叉树应该使得对整个
数据集而言，sigmoid判断次数最少</li>
<li>即应该使用按照数据点频数构建的霍夫曼树</li>
<li>霍夫曼树</li>
</ul>
</li>
</ul>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ul>
<li><p>输入$x^T$所属类别霍夫曼编码为$d={d_1,\cdots,d_M}$，
则应最大化如下似然函数</p>
<script type="math/tex; mode=display">\begin{align*}
\prod_{i=1}^M P(d_i|x, w_{j_i}) & = \prod_{i=1}^M
   [\sigma(x^T w_{j_i} + b_{j_i})]^{d_i}
   [1 - \sigma(x^T w_{j_i} + b_{j_i})]^{1-d_i} \\
P(d_i|x, w_{j_i}) & = \left \{ \begin{array}{l}
   1 - \sigma(x^T w_{j_i} + b_{j_i}), & d_i = 0 \\
   \sigma(x^T w_{j_i} + b_{j_i}), & d_i = 1
   \end{array} \right. \\
\sigma(z) & = \frac 1 {1 + e^z}
\end{align*}</script><blockquote>
<ul>
<li>$w_j, b_j$：节点$j$对应sigmoid参数</li>
<li>$P(d_i)$：以sigmoid激活值作为正例概率
 （也可以其作为负例概率，但似然函数需更改）</li>
</ul>
</blockquote>
</li>
<li><p>则对数似然函数为</p>
<script type="math/tex; mode=display">
L = log \prod_{i=1}^M P(d_i|x, w_{j_i}) = \sum_{i=1}^M
   d_i log [\sigma(x^T w_{j_i} + b_{j_i})]
   {1-d_i} log [1 - \sigma(x^T w_{j_i} + b_{j_i})]</script></li>
</ul>
<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><ul>
<li><p>则参数$w_{j_M}$梯度如下</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial L} {\partial w_{j_M}} & =
   d_M [1-\sigma(x^T w_{j_M} + b_{j_M})] x -
   (1 - d_M) \sigma(x^T w_{j_M} + b_{j_M}) x \\
& = (d_M - \sigma(x^T w_{j_M} + b_{j_M})) x
\end{align*}</script></li>
<li><p>词向量$x$梯度如下</p>
<script type="math/tex; mode=display">
\frac {\partial L} {\partial x} = \sum_{i=1}^M
   (d_i - \sigma(x^T w_{j_i} + b_{j_i})) w_{j_i}</script></li>
</ul>
<h3 id="CBOW流程"><a href="#CBOW流程" class="headerlink" title="CBOW流程"></a>CBOW流程</h3><blockquote>
<ul>
<li>特征词周围上下文词均使用梯度更新，<strong>更新输入</strong></li>
</ul>
</blockquote>
<ul>
<li>基于预料训练样本建立霍夫曼树</li>
<li>随机初始化模型参数$w$、词向量$w$</li>
<li><p>对训练集中每个样本 $(context(x), x)$（$2C$个上下文）如下
计算，直至收敛</p>
<ul>
<li><p>置：$e=0, x<em>w=\frac 1 {2C} \sum</em>{c=1}^{2C} x_c$</p>
</li>
<li><p>对$x$的霍夫曼编码 $d={d_1, \cdots, d_M}$ 中 $d_i$ 计算</p>
<script type="math/tex; mode=display">\begin{align*}
\sigma_i & = \sigma(x_w^T w_{j_i} + b_{j_i}) \\
g & = (d_i - \sigma_i) \eta \\
e & = e + g w_{j_i} \\
w_{j_i} & = w_{j_i} + g x_w
\end{align*}</script></li>
<li><p>更新 $2C$ 上下文词对应词向量</p>
<script type="math/tex; mode=display">
x_i = x_i + e</script></li>
</ul>
</li>
</ul>
<h3 id="Skip-Gram流程"><a href="#Skip-Gram流程" class="headerlink" title="Skip-Gram流程"></a>Skip-Gram流程</h3><blockquote>
<ul>
<li>考虑上下文是相互的，则 $P(x<em>{context}|x)$ 最大化时，$P(x|x</em>{context})$ 也最大</li>
<li>为在迭代窗口（样本）内更新仅可能多词向量，应该最大化 $P(x|x_{context})$，使用梯度更新上下文 $2C$ 个词向量，<strong>更新输出</strong>（条件概率中更新条件）</li>
</ul>
</blockquote>
<ul>
<li>基于预料训练样本建立霍夫曼树</li>
<li>随机初始化模型参数 $w$、词向量 $w$</li>
<li><p>对训练集中每个样本 $(x, context(x))$、每个样本中上下文词向量 $x_c$（$2C$ 个上下文），训练直至收敛</p>
<ul>
<li><p>置：$e=0$</p>
</li>
<li><p>对 $x$ 的霍夫曼编码 $d={d_1, \cdots, d_M}$ 中 $d_i$ 计算</p>
<script type="math/tex; mode=display">\begin{align*}
\sigma_i & = \sigma(x_c^T w_{j_i} + b_{j_i}) \\
g & = (d_i - \sigma_i) \eta \\
e & = e + g w_{j_i} \\
w_{j_i} & = w_{j_i} + g x_c
\end{align*}</script></li>
<li><p>更新 $2C$ 上下文词对应词向量</p>
<script type="math/tex; mode=display">
x_c = x_c + e</script></li>
</ul>
</li>
</ul>
<h2 id="Negtive-Sampling"><a href="#Negtive-Sampling" class="headerlink" title="Negtive Sampling"></a>Negtive Sampling</h2><p>负采样</p>
<h3 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h3><ul>
<li>通过负采样得到$neg$个负例</li>
<li>对正例、负采样负例建立二元逻辑回归</li>
</ul>
<h3 id="模型、梯度"><a href="#模型、梯度" class="headerlink" title="模型、梯度"></a>模型、梯度</h3><ul>
<li><p>对类别为$j$正例、负采样负例应有如下似然函数、对数似然
函数</p>
<script type="math/tex; mode=display">\begin{align*}
P(context(x), x) & = \sigma(x^T w_j)
   \prod_{i=1}^{neg} (1 - \sigma(x^T w_j)) \\
L & = log P(context(x), x) \\
& = \sum_{i=0}^{neg} [y_i log(\sigma(x^T w_j)) + 
   (1 - y_i) log(\sigma (x^T w_j))]
\end{align*}</script><blockquote>
<ul>
<li>$y_i$：样本点标签，$y_0$为正例、其余负例</li>
</ul>
</blockquote>
</li>
<li><p>同普通LR二分类，得到参数、词向量梯度</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial L} {\partial w_j} & =
   (y_i - \sigma(x^T w_j)) x \\
\frac {\partial L} {\partial x} & = \sum_{i=1}^{neg}
   (y_i - \sigma(x^T w_j)) w_j
\end{align*}</script></li>
</ul>
<h3 id="负采样方法"><a href="#负采样方法" class="headerlink" title="负采样方法"></a>负采样方法</h3><ul>
<li><p>每个词对应采样概率为词频取$3/4$次幂后加权</p>
<script type="math/tex; mode=display">
p(x_0) = \frac {count(x_0)^{3/4}}
   {\sum_{x \in vocab} count(x)^{3/4}}</script></li>
</ul>
<h3 id="CBOW流程-1"><a href="#CBOW流程-1" class="headerlink" title="CBOW流程"></a>CBOW流程</h3><ul>
<li>随机初始化所有模型参数、词向量</li>
<li>对每个训练样本$(context(x_0), x_0)$负采样$neg$个中心词
$x_i$，考虑$x_0$为类别$j$</li>
<li><p>在以上训练集$context(x<em>0), x_0, x_1, \cdots, x</em>{neg}$中
训练直至收敛</p>
<ul>
<li><p>置：$e=0, x<em>w=\frac 1 {2C} \sum</em>{c=1}^{2C} x_c$</p>
</li>
<li><p>对样本$x<em>0, x_1, \cdots, x</em>{neg}$，计算</p>
<script type="math/tex; mode=display">\begin{align*}
\sigma_i & = \sigma(x_w^T w_j + b_j) \\
g & = (y_i - \sigma_i) \eta \\
e & = e + g w_j \\
w_j & = w_j + g x_w
\end{align*}</script></li>
<li><p>更新$2C$上下文词对应词向量</p>
<script type="math/tex; mode=display">
x_i = x_i + e</script></li>
</ul>
</li>
</ul>
<h3 id="Skip-gram中心词"><a href="#Skip-gram中心词" class="headerlink" title="Skip-gram中心词"></a>Skip-gram中心词</h3><blockquote>
<ul>
<li>类似Hierarchical Softmax思想，更新输出$2C$个词向量</li>
</ul>
</blockquote>
<ul>
<li>随机初始化所有模型参数、词向量</li>
<li>对每个训练样本$(context(x_0), x_0)$负采样$neg$个中心词
$x_i$，考虑$x_0$为类别$j$</li>
<li><p>以上训练集$context(x<em>0), x_0, x_1, \cdots, x</em>{neg}$中，
对每个上下文词向量$x_c$如下训练直至收敛</p>
<ul>
<li><p>置：$e=0$</p>
<script type="math/tex; mode=display">\begin{align*}
\sigma_i & = \sigma(x_c^T w_j + b_j) \\
g & = (y_i - \sigma_i) \eta \\
e & = e + g w_j \\
w_j & = w_j + g x_c
\end{align*}</script></li>
<li><p>更新$2C$上下文词对应词向量</p>
<script type="math/tex; mode=display">
x_c = x_c + e</script></li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-14T12:58:18.000Z" title="7/14/2019, 8:58:18 PM">2019-07-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T08:43:03.000Z" title="7/16/2021, 4:43:03 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Specification/">ML Specification</a><span> / </span><a class="link-muted" href="/categories/ML-Specification/NLP/">NLP</a></span><span class="level-item">21 minutes read (About 3137 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Specification/NLP/features_extractions.html">文本预处理</a></h1><div class="content"><h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><ul>
<li>去除噪声文档、文档中垃圾数据</li>
<li>停用词去除</li>
<li>词根还原（英文）</li>
<li>分词（中文）</li>
<li>词性标注</li>
<li>短语识别</li>
<li>词频统计</li>
</ul>
<h2 id="汉语分词"><a href="#汉语分词" class="headerlink" title="汉语分词"></a>汉语分词</h2><p>分词：添加合适的显性词语边界标志，使所形成的词串反映句子本意</p>
<ul>
<li><p>分词是正确处理中文信息的基础</p>
<ul>
<li>文本基于单字</li>
<li>书面表达方式以汉字作为最小单位</li>
<li>词之间没有显性界限标志</li>
</ul>
</li>
<li><p>用单个汉字作特征，不考虑词语含义，直接利用汉字在文本中
出现的<strong>统计特性</strong>对文本进行划分</p>
<ul>
<li>直观明了</li>
<li>操作简单</li>
<li>对西语文本划分非常容易（使用空格划分）</li>
</ul>
</li>
<li><p>使用词作为特征</p>
<ul>
<li>词是中文语义的最小信息单位，可以更好的反映句子中信息</li>
<li>分析难度更高，中文文本中词之间没有分隔标记，正确分词
是关键</li>
</ul>
</li>
</ul>
<h3 id="分词方法"><a href="#分词方法" class="headerlink" title="分词方法"></a>分词方法</h3><ul>
<li><p>基于词典</p>
<ul>
<li><em>FMM</em>：正向最大匹配分词</li>
<li><em>BMM</em>：逆向最大匹配分词</li>
<li>BM法：双向扫描法</li>
<li>逐词遍历</li>
</ul>
</li>
<li><p>基于统计模型</p>
<ul>
<li>N-最短路径</li>
<li>HMM</li>
<li>N元语法</li>
<li>由字构词的汉语分词方法</li>
</ul>
</li>
</ul>
<h3 id="分词难点"><a href="#分词难点" class="headerlink" title="分词难点"></a>分词难点</h3><h4 id="歧义切分"><a href="#歧义切分" class="headerlink" title="歧义切分"></a>歧义切分</h4><ul>
<li><p>分词规范</p>
<ul>
<li>分词单位<ul>
<li>二字、三字以及结合紧密、使用稳定的</li>
<li>四字成语</li>
<li>四字词或结合紧密、使用稳定的四字词组</li>
</ul>
</li>
<li>五字、五字以上谚语、格言等，分开后如不违背原有组合
意义，应切分</li>
</ul>
</li>
<li><p>歧义切分</p>
<ul>
<li>交集型切分歧义</li>
<li>组合型切分歧义</li>
</ul>
</li>
</ul>
<h4 id="未登录词识别"><a href="#未登录词识别" class="headerlink" title="未登录词识别"></a>未登录词识别</h4><blockquote>
<ul>
<li>词表词：记录在词表中的词</li>
<li>未登录词：词表中没有的词、或已有训练语料中未曾出现词
  （此时也称为<em>out of vocabulary</em>）</li>
</ul>
</blockquote>
<ul>
<li><p>真实文本切分中，未登录词总数大约9成是专有名词，其余为
新词</p>
</li>
<li><p>未登录词对分词精度影响是歧义词的10倍</p>
</li>
<li><p>命名实体识别：实体名词、专业名词</p>
<ul>
<li>界定规则不存在太大分歧、构成形式有一定规律</li>
<li>在文本中只占8.7%，引起分词错误率59.2%</li>
</ul>
</li>
</ul>
<h4 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h4><p>词性标注：在给定句子中判定每个词的语法范畴，确定词性并加以
标注的过程</p>
<ul>
<li><p><em>POS</em>作为特征可以更好的识别词语之间关系</p>
<ul>
<li><p>词性标注计数为<em>phrase chunking</em>词组组块的界定、
<em>entities and relationship</em>实体与关系的识别打下良好
基础，有利于深入探索文本语义信息</p>
</li>
<li><p>词组的形式提高了特征向量的语义含量，使得向量更稀疏</p>
</li>
</ul>
</li>
<li><p>难点</p>
<ul>
<li>汉语缺乏词形态变化</li>
<li>常用词兼类现象严重：占11%</li>
<li>研究者主观原因：不同语料库有不同规定、划分方法</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>part of speech</em>：<em>POS</em>，词性</li>
</ul>
</blockquote>
<h3 id="Forward-Maximum-Matching-Method"><a href="#Forward-Maximum-Matching-Method" class="headerlink" title="Forward Maximum Matching Method"></a><em>Forward Maximum Matching Method</em></h3><p><em>FMM</em>：正向最大匹配分词</p>
<ul>
<li><p>步骤</p>
<ul>
<li>记词典中最长此表包含汉字数量为M</li>
<li>从材料中选取前$m = M$个汉字去作为匹配字段，查找分词
词典<ul>
<li>若存在匹配词，则将其切分出</li>
<li>否则$m = m - 1$，重复</li>
</ul>
</li>
<li>重复直至材料分词完毕</li>
</ul>
</li>
<li><p>特点</p>
<ul>
<li>对交叉歧义、组合歧义没有解决办法</li>
<li>错误切分率为$\frac 1 {169}$</li>
</ul>
</li>
</ul>
<h3 id="Backward-Maximum-Matching-Method"><a href="#Backward-Maximum-Matching-Method" class="headerlink" title="Backward Maximum Matching Method"></a><em>Backward Maximum Matching Method</em></h3><p><em>BMM</em>：逆向最大匹配分词</p>
<ul>
<li><p>步骤：类似FMM，仅从材料/句子末尾开始处理</p>
</li>
<li><p>特点</p>
<ul>
<li>错误切分率$\frac 1 {245}$，较FMM更有效</li>
</ul>
</li>
</ul>
<h3 id="Bi-direction-Matching-Method"><a href="#Bi-direction-Matching-Method" class="headerlink" title="Bi-direction Matching Method"></a><em>Bi-direction Matching Method</em></h3><p>BM法：双向扫描法</p>
<ul>
<li><p>步骤：比较FMM、BMM法切分结果，决定正确切分</p>
</li>
<li><p>特点</p>
<ul>
<li>可以识别分词中交叉语义</li>
</ul>
</li>
</ul>
<h3 id="N-最短路径"><a href="#N-最短路径" class="headerlink" title="N-最短路径"></a>N-最短路径</h3><ul>
<li><p>思想</p>
<ul>
<li><p>考虑待切分字串$S=c_1 c_2 \cdots c_n$，其中$c_i$为
单个字、$n$为串长</p>
</li>
<li><p>建立节点数为$n+1$的切分有向无环图，各节点编号为
$V_0, V_1, \cdots, V_n$</p>
<ul>
<li>相邻节点间存在边</li>
<li>若$w=c<em>i c</em>{i+1} \cdots c<em>j$是一个词，则节点
$v</em>{i-1}, v_j$直接存在边</li>
<li>所有边距离均为1</li>
</ul>
</li>
<li><p>求有图无环图中最短路径</p>
</li>
</ul>
</li>
</ul>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>算法时间复杂度为$O(n<em>N</em>K)$</p>
<blockquote>
<ul>
<li>$n$：字串长度</li>
<li>$N$：最短路径数目</li>
<li>$k$：某个字作为词末端字的平均次数</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="改进—考虑噪声"><a href="#改进—考虑噪声" class="headerlink" title="改进—考虑噪声"></a>改进—考虑噪声</h4><p>基于统计信息的粗分模型</p>
<ul>
<li><p>考虑词串$W$经过信道传输，由于噪声干扰丢失词界切分标志，
到输出端为字串$C$</p>
</li>
<li><p>N-最短路径词语粗分模型可以改进为：求N个候选切分$W$，使得
概率$P(W|C)$为前N个最大值</p>
<script type="math/tex; mode=display">
P(W|C) = \frac {P(W) P(C|W)} {P(C)}</script><blockquote>
<ul>
<li>$P(C)$：字串概率，常数</li>
<li>$P(C|W)$：仅有</li>
</ul>
</blockquote>
</li>
<li><p>采用一元统计模型，设$W=w_1w_2\cdots W_m$是字串
$S=c_1c_2\cdots c_n$的切分结果，则其切分概率为</p>
<script type="math/tex; mode=display">\begin{align*}
P(W) & = \prod_{i=1}^m P(w_i) \\
P^{*}(W) = -ln P(w) = \sum_{i=1}^m (-ln P(W_i))
\end{align*}</script><blockquote>
<ul>
<li>$P(w_i)$：词$w_i$出现概率，在大规模预料训练的基础上
 通过极大似然方法得到</li>
</ul>
</blockquote>
</li>
<li><p>则$-lnP(w_i)$可看作是词$w_i$在切分有向无环图中对应距离，
改进N-最短路径方法</p>
</li>
</ul>
<h3 id="由字构词"><a href="#由字构词" class="headerlink" title="由字构词"></a>由字构词</h3><h4 id="假设、背景"><a href="#假设、背景" class="headerlink" title="假设、背景"></a>假设、背景</h4><blockquote>
<ul>
<li>思想：将分词过程看作字分类问题，认为每个字在构造特定词语
  时，<strong>占据确定的位置</strong></li>
</ul>
</blockquote>
<ul>
<li>中文词一般不超过4个字，字位数量很小<ul>
<li>首部B</li>
<li>词中M</li>
<li>词尾E</li>
<li>单独成词S</li>
</ul>
</li>
<li>部分汉字按一定方式分布，有规律</li>
<li>利用相对固定的字推断相对不定的字的位置问题</li>
<li>虽然无法将所有词列入词典，但字基本稳定</li>
</ul>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ul>
<li>对所有字根据预定义的特征进行<strong>词位特征学习</strong>，获得概率
模型</li>
<li>在带待分字串上根据字与字之间的结合紧密程度得到词位的分类
结果</li>
<li>根据词位定义直接获得最终分词结果</li>
</ul>
<h4 id="Productivity"><a href="#Productivity" class="headerlink" title="Productivity"></a><em>Productivity</em></h4><p>能产度：词$c_i$在词位$t_j$的能产度定义为</p>
<script type="math/tex; mode=display">
P_{c_i}(t_j) = \frac {count(c_i, t_j)}
    \sum_{t_j \in T} count(c_i, t_j)</script><blockquote>
<ul>
<li>$T = {B, B_2, B_3, M, E, S}$</li>
</ul>
</blockquote>
<ul>
<li><p>主词位：给定字在其上能产度高于0.5的词位</p>
<p>|标记|B|B2|B3|M|E|S|总字量|
|——-|——-|——-|——-|——-|——-|——-|——-|
|字量|1634|156|27|33|1438|632|3920|
|百分比|31.74|3.03|0.52|0.64|27.94|12.28|76.16|</p>
<blockquote>
<ul>
<li>MSRA2005语料库中有主词位的字量分布</li>
</ul>
</blockquote>
</li>
<li><p>自由字：没有主词位的字</p>
<ul>
<li>自由字是基于词位分类的分词操作得以有效进行的的基础
之一</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>字：不仅限于汉字，包括标点、外文字母、注音符号、数字等
  任何可能文字符号</li>
</ul>
</blockquote>
<h4 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h4><ul>
<li>能平衡词表词、未登录词</li>
<li>简化分词系统设计<ul>
<li>无需强调词表词信息</li>
<li>无需设置特定未登录词识别模块</li>
</ul>
</li>
</ul>
<h3 id="分词评价指标"><a href="#分词评价指标" class="headerlink" title="分词评价指标"></a>分词评价指标</h3><ul>
<li>正确率</li>
<li>召回率</li>
<li>F-测度值</li>
</ul>
<h2 id="Vector-Space-Model"><a href="#Vector-Space-Model" class="headerlink" title="Vector Space Model"></a><em>Vector Space Model</em></h2><p>向量空间模型：自然语言处理常用模型</p>
<blockquote>
<ul>
<li><em>document</em>：文档，句子、段落、整篇文章</li>
<li><em>term/feature</em>：词根、词、短语、其他</li>
<li><em>weight</em>：项的权重，每个特征项在文档中重要程度</li>
</ul>
</blockquote>
<h3 id="相似度比较"><a href="#相似度比较" class="headerlink" title="相似度比较"></a>相似度比较</h3><ul>
<li><p>内积</p>
<script type="math/tex; mode=display">
sim(D_1, D_2) = \sum_{k=1}^n w_{1,k} w_{2,k}</script></li>
<li><p>Cosine相似度</p>
<script type="math/tex; mode=display">
cos(D_1, D_2) = cos \theta = \frac
   {\sum_{k=1}^n w_{1,k} w_{2,k}}
   {\sqrt{\sum_{k=1}^n w_{1,k}^2 \sum_{k=1}^n w_{2,k}^2}}</script></li>
</ul>
<h3 id="权重"><a href="#权重" class="headerlink" title="权重"></a>权重</h3><ul>
<li>布尔权重：$bw_{t,d} = {0, 1}$</li>
<li><em>TF</em>：绝对词频，$TF<em>{t,d} = \frac {n</em>{t,d}} {n_d}$</li>
<li><em>IDF</em>：倒排文档频度，$IDF_{t,d} = log \frac M {m_t}$</li>
<li><em>TF-IDF</em>：$TF-IDF<em>{t,d} = TF</em>{t,d} * IDF_{t,d}$</li>
<li><em>TF-IWF</em>：$TF<em>IWF</em>{t,d}= TF<em>{t,d} log \frac {\sum</em>{t=1}^T \sum<em>{d=1}^N n</em>{t,d}} {\sum<em>{t=1} n</em>{t,d}}$</li>
</ul>
<blockquote>
<ul>
<li>$t_{t,d}$：文档$d$中出现特征$t$的次数</li>
<li>$t_d$：文档$d$中出现总词数</li>
<li>$m_t$：训练集中出现特征$t$文档数</li>
<li>$M$：训练集中文档总数</li>
<li>$K$：特征总数量</li>
</ul>
</blockquote>
<h4 id="特征加权"><a href="#特征加权" class="headerlink" title="特征加权"></a>特征加权</h4><ul>
<li><p>特征加权主要包括三个部分（层次）</p>
<ul>
<li>局部加权：使用词语在文档中的统计量</li>
<li>全局加权：词语在整个数据集中的统计量</li>
<li>标准化</li>
</ul>
</li>
<li><p>一般化特征加权表达式</p>
<script type="math/tex; mode=display">
L_d(w) G(w) N_d</script><blockquote>
<ul>
<li>$L_d(w)$：词$w$在文档$d$中的局部权重</li>
<li>$G(w)$：词$w$在文档集合中的全局权重</li>
<li>$N_d$：文档d的标准化因子</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Document-Frequency"><a href="#Document-Frequency" class="headerlink" title="Document Frequency"></a><em>Document Frequency</em></h3><p><em>DF</em>：文档频率，文本数据中包含某词条的文档数目</p>
<ul>
<li><p>通过文档频率进行特征选择：按文档频率大小对词条进行排序</p>
<ul>
<li><p>将DF小于某阈值的词删除</p>
<ul>
<li>稀有词项全局影响力不大</li>
<li>文档若有稀有词向，通常也会有常见词项</li>
</ul>
<blockquote>
<ul>
<li>和通常信息获取观念抵触：稀有更有代表性</li>
</ul>
</blockquote>
</li>
<li><p>将DF大于某阈值的词删除</p>
<ul>
<li>太频繁词词项没有区分度</li>
</ul>
</li>
</ul>
</li>
<li><p>容易实现、可扩展性好</p>
</li>
</ul>
<h3 id="其他指标"><a href="#其他指标" class="headerlink" title="其他指标"></a>其他指标</h3><ul>
<li><p>信息增益/互信息</p>
</li>
<li><p>卡方统计量</p>
</li>
</ul>
<h2 id="Latent-Semantic-Analysis"><a href="#Latent-Semantic-Analysis" class="headerlink" title="Latent Semantic Analysis"></a><em>Latent Semantic Analysis</em></h2><p><em>LSA</em>：潜在语义分析</p>
<ul>
<li><p>文本分析中常用的降维技术</p>
<ul>
<li>特征重构方法</li>
<li>很好解决了同义词、一词多义等现象给文本分析造成的困难</li>
</ul>
</li>
<li><p>理论依据、假设</p>
<ul>
<li>认为有潜在语义结构隐含在文档中词语的上下文使用模式中</li>
<li>而文档词频共现矩阵在一定程度可以反映词和不同主题之间
关系</li>
</ul>
</li>
<li><p>以文档词频矩阵为基础进行分析</p>
<ul>
<li>得到向量空间模型中文档、词的高维表示</li>
<li>并通过投影形成文档、词在潜在语义空间中的相对稠密的
低维表示，缩小问题规模</li>
<li>通过这种低维表示解释出“文档-语义-词语”之间的联系</li>
</ul>
</li>
<li><p>数学描述</p>
<ul>
<li>LSA将每个文本视为以词语/特征为维度的空间的点，包含
语义的文本出现在空间中分布服从某种语义结构</li>
<li>LSA将每个词视为以文档为维度的空间中点</li>
<li>文档由词语构成，词语需要放在文档中理解，体现词语和
文档之间的双重概率关系</li>
</ul>
</li>
</ul>
<h3 id="应用SVD分解"><a href="#应用SVD分解" class="headerlink" title="应用SVD分解"></a>应用SVD分解</h3><ul>
<li><p>词频共现矩阵$X=(x_{d,t})$：文档、词语的共现频率矩阵</p>
<ul>
<li>其中每行代表文档向量</li>
<li>每列代表词语向量</li>
<li>元素$x_{d,t}$表示文档$d$中词$t$出现的频率</li>
</ul>
</li>
<li><p>对词频共现矩阵$X$进行SVD分解得到$X=U \Sigma V^T$</p>
</li>
<li><p>仅保留$\Sigma$中满足阈值要求的较大的前$r$特征值，
其余置为0，得到
$\tilde X = \tilde U \tilde \Sigma \tilde V^T$，达到信息
过滤、去除噪声的目的</p>
<ul>
<li>$A = \tilde X$：矩阵特征分解后的文档词频矩阵近似</li>
<li>$T = \tilde U$：文档和潜在语义的关系矩阵近似</li>
<li>$S = \tilde V$：词语和潜在语义的关系矩阵近似</li>
<li>$D = \tilde \Sigma$：各潜在语义的重要程度</li>
</ul>
</li>
</ul>
<h4 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h4><ul>
<li><p>从数据压缩角度：近似矩阵是秩为$K$的前提下，矩阵$X$的最小
二乘意义下最佳近似</p>
</li>
<li><p>r值过大会增加运算量，一般选择K使得贡献率满足</p>
<script type="math/tex; mode=display">
\sum_{i=1}^r d_i / \sum_{i=1}^K d_i \geq \theta</script><blockquote>
<ul>
<li>$\theta$：阈值</li>
<li>$K$：原始词频共现矩阵秩</li>
</ul>
</blockquote>
</li>
<li><p>LSA缺点</p>
<ul>
<li>SVD的向量元素有正、有负，性质难以解释</li>
<li>SVD的实际意义不够明确，难以控制词义据类的效果</li>
<li>涉及高维矩阵运算</li>
</ul>
</li>
</ul>
<h3 id="相似关系计算"><a href="#相似关系计算" class="headerlink" title="相似关系计算"></a>相似关系计算</h3><ul>
<li><p>潜在语义空间中存在：词-词、文本-文本、词-文本3种关系，
可以通过近似矩阵$T, S, D$计算</p>
</li>
<li><p>比较词汇两两相似度：“正向乘法”</p>
<script type="math/tex; mode=display">A A^T = T S D^T D S^T T^T = T S^2 T^T</script></li>
<li><p>比较文本两两相似度：“逆向乘法”</p>
<script type="math/tex; mode=display">A^T A = T^T S^T D D^T S T = T^T S^2 T</script></li>
<li><p>词汇、文本两两相似度：就是原始矩阵$X$的近似矩阵本身$A$</p>
<script type="math/tex; mode=display">A = T * S * D^T</script></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-13T04:03:12.000Z" title="7/13/2019, 12:03:12 PM">2019-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T08:35:29.000Z" title="7/16/2021, 4:35:29 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Specification/">ML Specification</a><span> / </span><a class="link-muted" href="/categories/ML-Specification/NLP/">NLP</a></span><span class="level-item">4 minutes read (About 555 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Specification/NLP/abstract.html">NLP 总述</a></h1><div class="content"><h2 id="文本挖掘"><a href="#文本挖掘" class="headerlink" title="文本挖掘"></a>文本挖掘</h2><ul>
<li><p>文本处理：将非结构化数据转换为结构化数据</p>
</li>
<li><p>预测建模</p>
<ul>
<li>文本分类：根据观察到的对象特征值预测其他特征值</li>
</ul>
</li>
<li><p>描述建模</p>
<ul>
<li>文本聚类：对数据对象进行概括，以看到数据对象的最重要
特征<ul>
<li>适应范围非常广</li>
</ul>
</li>
<li>聚类分析</li>
</ul>
</li>
<li><p>基于相似度方法</p>
<ul>
<li>需要用户显式指定相似度函数</li>
<li>聚类算法根据相似度的计算结果将相似文本分在同一个组</li>
<li>每个文本只能属于一个组，因此也成为“硬聚类”</li>
</ul>
</li>
<li><p>基于模型的方法</p>
<ul>
<li>文本有多个标签，也成为“软聚类”</li>
</ul>
</li>
</ul>
<h2 id="话题检测"><a href="#话题检测" class="headerlink" title="话题检测"></a>话题检测</h2><p>找出文档中的K个话题，计算每个文档对话题的覆盖率</p>
<h3 id="话题表示方法"><a href="#话题表示方法" class="headerlink" title="话题表示方法"></a>话题表示方法</h3><h4 id="基于单个词"><a href="#基于单个词" class="headerlink" title="基于单个词"></a>基于单个词</h4><h4 id="基于词分布"><a href="#基于词分布" class="headerlink" title="基于词分布"></a>基于词分布</h4><h5 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h5><blockquote>
<ul>
<li>输入<blockquote>
<ul>
<li>N个文档构成的文本集C</li>
<li>话题个数K</li>
<li>词典V</li>
</ul>
</blockquote>
</li>
<li>输出<blockquote>
<ul>
<li>K个话题的分布
 $(\theta_1, \theta2, \cdots, \theta_K)$</li>
<li>N个文档在K个话题上的概率分布
 $(\pi_1, \pi_2, \cdots, \pi_N)$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>词向量：将向量表示词</p>
<ul>
<li><p><em>1-of-N representation</em>/<em>one hot representation</em>：one-hot
表示词</p>
<p><img src="/imgs/word_vector_one_hot.png" alt="word_vector"></p>
<ul>
<li>词向量维度为整个词汇表大小</li>
<li>简单、效率不高</li>
</ul>
</li>
<li><p><em>distributed representation</em>：embedding思想，通过训练，
将词映射到较短词向量中</p>
<p><img src="/imgs/word_vector_embedding.png" alt="word_vector"></p>
<ul>
<li>词向量维度自定义</li>
<li>容易分析词之间关系</li>
</ul>
</li>
</ul>
<h3 id="Continuous-Bag-of-Words"><a href="#Continuous-Bag-of-Words" class="headerlink" title="Continuous Bag-of-Words"></a>Continuous Bag-of-Words</h3><p><em>CBOW</em>：输入特征词上下文相关词对应词向量，输出特征词的词向量</p>
<ul>
<li>CBOW使用词袋模型<ul>
<li>特征词上下文相关从平等，不考虑和关注的词之间的距离</li>
</ul>
</li>
</ul>
<h3 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h3><p><em>Skip-Gram</em>：输入特征词词向量，输出softmax概率靠前的词向量</p>
<h2 id="神经网络词向量"><a href="#神经网络词向量" class="headerlink" title="神经网络词向量"></a>神经网络词向量</h2><p>神经网络词向量：使用神经网络训练词向量</p>
<ul>
<li><p>一般包括三层：输入层、隐层、输出softmax层</p>
</li>
<li><p>从隐藏层到输出softmax层计算量很大</p>
<ul>
<li>需要计算所有词的softmax概率，再去找概率最大值</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-13T04:03:12.000Z" title="7/13/2019, 12:03:12 PM">2019-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T08:44:53.000Z" title="7/16/2021, 4:44:53 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Specification/">ML Specification</a><span> / </span><a class="link-muted" href="/categories/ML-Specification/NLP/">NLP</a></span><span class="level-item">a few seconds read (About 0 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Specification/NLP/rnn_language_models.html">RNN 语言模型</a></h1><div class="content"></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/WWxwMVBsOUtoNTdMSlJ4Z1lWVnRISERsd2t6ek9MeVpEUWs0YldlZGxUdXlKdDNmMEZVV1hWaFZFYWFSNmFKL25penZodWx3UzRiaVkxcnREWCtOYUJhZWhNbWpzaUdyc1hPangycUh5RTVjRXFnZnFGdVdSTzZvVzJBcTJHKzl8aXpDK1ROWWl4N080YkFEK3QvbEVWNEJuQjFqdWdxODZQcGNoM1NqbERXST0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>