<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: Optimization - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"Hexo","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Optimization</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">7 minutes read (About 1121 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/unconstrained_specials.html">无约束优化特殊问题</a></h1><div class="content"><h2 id="正定二次目标函数"><a href="#正定二次目标函数" class="headerlink" title="正定二次目标函数"></a>正定二次目标函数</h2><script type="math/tex; mode=display">
min f(x) = \frac 1 2 x^T G x + r^T x + \sigma</script><h2 id="非线性最小二乘"><a href="#非线性最小二乘" class="headerlink" title="非线性最小二乘"></a>非线性最小二乘</h2><script type="math/tex; mode=display">\begin{align*}
f(x) & = \frac 1 2 \sum_{i=1}^m r^2_i(x) \\
& = \frac 1 2 r(x) r^T(x)
\end{align*}</script><blockquote>
<ul>
<li>$r_i(x)$：通常为非线性函数</li>
<li>$r(x) = (r_1(x), \cdots, r_n(x))^T$</li>
<li>$x \in R^n, m \geq n$</li>
</ul>
</blockquote>
<p>则</p>
<script type="math/tex; mode=display">\begin{align*}
\nabla f(x) & = \sum_{i=1}^m \nabla r_i(x)
    r_i(x) \\
& = J(x)^T r(x) \\

\nabla^2 f(x) & = \sum_{i=1}^m \nabla r_i(x)
    r_i(x) + \sum_{i=1}^m r_i \nabla^2 r_i(x) \\
& = J(x)^T J(x) + \sum_{i=1}^m r_i(x) \nabla^2 r_i(x)
\end{align*}</script><blockquote>
<ul>
<li><script type="math/tex; mode=display">
  J(x) = \begin{bmatrix}
  \frac {\partial r_1} {\partial x_1} &
      \frac {\partial r_1} {\partial x_2} & \cdots &
      \frac {\partial r_1} {\partial x_n} \\
  \frac {\partial r_2} {\partial x_1} &
      \frac {\partial r_2} {\partial x_2} & \cdots &
      \frac {\partial r_2} {\partial x_n} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac {\partial r_m} {\partial x_1} &
      \frac {\partial r_m} {\partial x_2} & \cdots &
      \frac {\partial r_m} {\partial x_n}
  \end{bmatrix}
  = \begin{bmatrix}
  \nabla r_1(x)^T \\
  \nabla r_2(x)^T \\
  \vdots \\
  \nabla r_m(x)^T \\
  \end{bmatrix}</script>  为$r(x)$的Jacobi矩阵</li>
</ul>
</blockquote>
<h3 id="Gauss-Newton法"><a href="#Gauss-Newton法" class="headerlink" title="Gauss-Newton法"></a>Gauss-Newton法</h3><p>Newton法中为简化计算，略去其Hesse矩阵中
$\sum_{i=1}^m r_i(x) \nabla^2 r_i(x)$项，即直接求解
方程组</p>
<script type="math/tex; mode=display">
J(x^{(k)})^T J(x^{(k)}) d = -J(x^{(k)})^T r(x^{(k)})</script><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>同Newton法，仅求解Newton方程改为求解以上方程组</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>实际问题中</p>
<ul>
<li>局部解$x^{ <em> }$对应的目标函数值$f(x^{ </em> })$接近0
时，$|r(x^{(k)})|$较小</li>
<li>曲线$r_i(x)$接近直线，
$\nabla^2 r_i(x) \approx 0$</li>
</ul>
<p>采用Gauss-Newton法效果较好，否则效果一般</p>
</li>
<li><p>矩阵$J(x^{(k)})^T J(x^{(k)})$是半正定矩阵，当Jacobi矩阵
列满秩时为正定矩阵，此时虽然$d^{(k)}$是下降方向，但仍需
类似修正牛顿法增加一维搜索策略保证目标函数值不上升</p>
</li>
</ul>
<h3 id="Levenberg-Marquardt方法"><a href="#Levenberg-Marquardt方法" class="headerlink" title="Levenberg-Marquardt方法"></a>Levenberg-Marquardt方法</h3><p>但$J(x^{(k)})$中各列线性相关、接近线性相关，则求解
Newton-Gauss方法中的方程组会出现困难，可以改为求解</p>
<script type="math/tex; mode=display">
(J(x^{(k)})^T J(x^{(k)}) + vI) d = -J(x^{(k)})^T r(x^{(k)})</script><blockquote>
<ul>
<li>$v$：迭代过程中需要调整的参数，LM方法的关键即如何调整</li>
</ul>
</blockquote>
<h4 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h4><blockquote>
<ul>
<li>若$d(v)$是以上方程组的解，则$|d(v)|^2$是$v$的连续下降
  函数，且$v \rightarrow +\infty, |d(v)| \rightarrow 0$</li>
</ul>
</blockquote>
<ul>
<li><p>$J(x^{(k)})^T J(x^{(k)})$是对称半正定矩阵，则存在正交阵</p>
<script type="math/tex; mode=display">
(P^{(k)})^T J(x^{(k)})^T J(x^{(k)}) P^{(k)} =
   \Lambda^{(k)}</script></li>
<li><p>则可以解出$|d(v)|^2$</p>
</li>
</ul>
<blockquote>
<ul>
<li>增大$v$可以限制$|d^{(k)}|$，所以LM方法也被称为阻尼最小
  二乘法</li>
</ul>
</blockquote>
<h4 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h4><blockquote>
<ul>
<li>若$d(v)$是以上方程的解，则$d(v)$是$f(x)$在$x^{(k)}$处的
  下降方向，且$v \rightarrow + \infty$时，$d(v)$的方向与
  $-J(x^{(k)})^T r(x^{(k)})$方向一致</li>
</ul>
</blockquote>
<ul>
<li>下降方向：$\nabla f(x^{(k)}) d(v) &lt; 0$即可</li>
<li>方向一致：夹角余弦</li>
</ul>
<blockquote>
<ul>
<li>$v$充分大时，LM方法产生的搜索方向$d^{(k)}$和负梯度方向
  一致</li>
</ul>
</blockquote>
<h4 id="参数调整方法"><a href="#参数调整方法" class="headerlink" title="参数调整方法"></a>参数调整方法</h4><p>使用梯度、近似Hesse矩阵定义二次函数</p>
<script type="math/tex; mode=display">
q(d) = f(x^{(k)}) + (J(x^{(k)})^T r(x^{(k)}))^T d +
    \frac 1 2 d^T (J(x^{(k)})^T J(x^{(k)})) d</script><blockquote>
<ul>
<li><p>其增量为</p>
<script type="math/tex; mode=display">\begin{align*}
  \Delta q^{(k)} & = q(d^{(k)}) - q(0) \\
  & = (J(x^{(k)})^T r(x^{(k)}))^T d^{(k)} + \frac 1 2
      (d^{(k)})^T (J(x^{(k)})^T J(x^{(k)})) d^{(k)}
  \end{align*}</script></li>
<li><p>目标函数增量</p>
<script type="math/tex; mode=display">\begin{align*}
  \Delta f^{(k)} & = f(x^{(k)} + d^{(k)}) - f(x^{(k)}) \\
  & = f(x^{(k+1)}) - f(x^{(k)})
  \end{align*}</script></li>
<li><p>定义$\gamma_k = \frac {\Delta f^{(k)}} {\Delta q^{(k)}}$</p>
</li>
</ul>
</blockquote>
<ul>
<li><p>$\gamma_k$接近1说明$\Delta f^{(k)}$接近$\Delta q^{(k)}$</p>
<ul>
<li>即$f(x^{(k)} + d^{(k+1)})$接近$q(d^{(k)})$</li>
<li>即$f(x)$在$x^{(k)}$附近接近二次函数</li>
<li>即使用Gauss-Newton方法求解最小二乘问题效果较好</li>
<li>即LM方法求解时$v$参数应该较小</li>
</ul>
</li>
<li><p>$\gamma_k$接近0说明$\Delta f^{(k)}$与$\Delta q^{(k)}$
近似程度不好</p>
<ul>
<li>$d^{(k)}$不应取得过大，应减少$d^{(k)}$得模长</li>
<li>应该增加参数$v$进行限制</li>
<li>迭代方向趋近于负梯度方向</li>
</ul>
</li>
<li><p>$\gamma_k$适中时，认为参数$v$选取合适，不做调整</p>
<ul>
<li>临界值通常为0.25、0.75</li>
</ul>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始点$x^{(1)}$、初始参数$v$（小值）、精度要求$\epsilon$
，置k=k+1</p>
</li>
<li><p>若$|J(x^{(k)})^T r(x^{(k)})| &lt; \epsilon$，则停止计算，
得到问题解$x^{(k)}$，否则求解线性方程组</p>
<script type="math/tex; mode=display">
(J(x^{(k)})^T J(x^{(k)}) + v_kI) d = -J(x^{(k)})^T
  r(x^{(k)})</script><p>得到$d^{(k)}$</p>
</li>
<li><p>置$x^{(k+1)} = x^{(k)} + d^{(k)}$，计算$\gamma_k$</p>
</li>
<li><p>若</p>
<ul>
<li>$\gamma &lt; 0.25$，置$v_{k+1} = 4 v_k$</li>
<li>$\gamma &gt; 0.75$，置$v_{k+1} = v_k / 2$</li>
<li>否则置$v_{k+1} = v_k$</li>
</ul>
</li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-13T16:16:47.000Z" title="7/14/2019, 12:16:47 AM">2019-07-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T08:42:31.000Z" title="8/4/2021, 4:42:31 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Optimization/">Optimization</a></span><span class="level-item">8 minutes read (About 1136 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Optimization/robust_optimization.html">Robust Optimization</a></h1><div class="content"><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>稳健优化：利用凸理论、对偶理论中概念，使得凸优化问题中的解
对参数的<em>bounded uncertainty</em>有限不确定性（波动）不敏感</p>
<ul>
<li><p>稳健优化在机器学习涉及方面：不确定优化、过拟合</p>
<ul>
<li><em>Connecting Consistency</em></li>
<li><em>Generalization Ability</em></li>
<li><em>Sparsity</em></li>
<li><em>Stability</em></li>
</ul>
</li>
<li><p>不确定性来源</p>
<ul>
<li>模型选择错误</li>
<li>假设不成立</li>
<li>忽略必要因素</li>
<li>经验分布、函数无法正确估计整体分布</li>
</ul>
</li>
<li><p>过拟合判断依据</p>
<ul>
<li><em>metric entropy</em></li>
<li><em>VC-dimension</em></li>
</ul>
</li>
</ul>
<h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>优化问题对问题参数的扰动非常敏感，以至于解经常不可行、次优</p>
<ul>
<li><p><em>Stochastic Programming</em>：使用概率描述参数不确定性，</p>
</li>
<li><p>稳健优化则假设问题参数在某个给定的先验范围内随意变动</p>
<ul>
<li><p>不考虑参数的分布</p>
</li>
<li><p>利用概率论的理论，而不用付出计算上的代价</p>
</li>
</ul>
</li>
</ul>
<h3 id="策略（最优化问题）"><a href="#策略（最优化问题）" class="headerlink" title="策略（最优化问题）"></a>策略（最优化问题）</h3><script type="math/tex; mode=display">
\begin{align*}
\min_x & : f_0(x) \\
s.t. & : f_i(x) \leq 0, i=1,2,\cdots,m
\end{align*}</script><script type="math/tex; mode=display">
\begin{align*}
\min_x & : f_0(x) \\
s.t. & : f_i(x, u_i) \leq 0, \forall u_i \in \mathcal{U}_i,
    i=1,2,\cdots,m
\end{align*}</script><ul>
<li>$\mathcal{U}_i $：<em>uncertainty set</em>，不确定集</li>
</ul>
<h3 id="Computational-Tractablity"><a href="#Computational-Tractablity" class="headerlink" title="Computational Tractablity"></a><em>Computational Tractablity</em></h3><p>稳健优化易解性：在满足标准或一点违反
<em>Slater-like regularity conditions</em>情况下，求解稳健优化问题
等同于求解对以下凸集$\mathcal{X(U)}$的划分（求出凸集）</p>
<script type="math/tex; mode=display">
\mathcal{X(U)} \overset {\triangle} {=}
    \{ x: f_i(x, u_i) \leq 0,
    \forall u_i \in \mathcal{U}_i, i=1,2,\cdots,m \}</script><ul>
<li><p>若存在高效算法能确定$x \in \mathcal{X(U)}$、或者能够提供
分离超平面，那么问题可以在多项式时间中求解</p>
</li>
<li><p>即使所有的约束函数$f_i$都是凸函数，此时$\mathcal{X(U)}$
也是凸集，也有可能没有高效算法能够划分出$\mathcal{X(U)}$</p>
</li>
<li><p>然而在大部分情况下，稳健化后的问题都能高效求解下，和原
问题复杂度相当</p>
</li>
</ul>
<h4 id="复杂度说明"><a href="#复杂度说明" class="headerlink" title="复杂度说明"></a>复杂度说明</h4><ul>
<li>LP + Polyhedra Uncertainty：LP</li>
<li>LP + Ellipsoidal Uncertainty：SOCP</li>
<li>CQP + Ellipsoidal Uncertainty：SDP</li>
<li>SDP + Ellipsoodal Uncertainty：NP-hard</li>
</ul>
<blockquote>
<ul>
<li><em>LP</em>：Linear Program，线性规划</li>
<li><em>SOCP</em>：Second-Order Cone Program，二阶锥规划</li>
<li><em>CQP</em>：Convex Quadratic Program，凸二次规划</li>
<li><em>SDP</em>：Semidefinite Program，半定规划</li>
<li><em>Polyhedra Uncertainty</em>：多项式类型不确定</li>
<li><em>Ellipsodial Uncertainty</em>：椭圆类型不确定</li>
<li><em>NP-hard</em>：NP难问题，至少和NPC问题一样困难得问题</li>
</ul>
</blockquote>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p><em>Linear Programs with Polyhedral Uncertainty</em></p>
<h3 id="概率解释、结果"><a href="#概率解释、结果" class="headerlink" title="概率解释、结果"></a>概率解释、结果</h3><ul>
<li><p>稳健优化的计算优势很大程度上来源于，其形式是固定的，不再
需要考虑概率分布，只需要考虑不确定集</p>
</li>
<li><p>计算优势使得，即使不确定性是随机、且分布已知，稳健优化
仍然具有吸引力</p>
</li>
<li><p>在一些概率假定下，稳健优化可以给出稳健化问题解的某些
概率保证，如：可行性保证（在给定约束下，解能以多大概率
不超过约束）</p>
</li>
</ul>
<h3 id="Uncertainty-Set"><a href="#Uncertainty-Set" class="headerlink" title="Uncertainty Set"></a><em>Uncertainty Set</em></h3><h4 id="Atomic-Uncertainty-Set"><a href="#Atomic-Uncertainty-Set" class="headerlink" title="Atomic Uncertainty Set"></a><em>Atomic Uncertainty Set</em></h4><p>原子不确定集</p>
<script type="math/tex; mode=display">
\begin{align*}
(I) & 0 \in \mathcal{U}_0 \\
(II) & \forall w_0 \in R^n: \sup_{u \in \mathcal{U}_0
    [-w_0^T u^{'} < +\infty
\end{align}</script><h2 id="Robust-Optimization-and-Adversary-Resistant-Learning"><a href="#Robust-Optimization-and-Adversary-Resistant-Learning" class="headerlink" title="Robust Optimization and Adversary Resistant Learning"></a>Robust Optimization and Adversary Resistant Learning</h2><p>即稳健优化在机器学习中处理不确定性（随机的、对抗性的）</p>
<ul>
<li><p>稳健优化中在机器学习中应用</p>
</li>
<li><p>稳健学习在很多学习任务中都有提出</p>
<ul>
<li>学习和规划</li>
<li>Fisher线性判别分析</li>
<li>PCA</li>
</ul>
</li>
</ul>
<p>这里考虑经典的<strong>二分类软阈值SVM</strong></p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{w,b,\xi}: \quad & \mathcal{ r(w,b) +
    C\sum_{i=1}^m \xi_i} \\
s.t.: & \xi_i \geq [1-y_i(<w,x_i> + b)], i=1,\cdots,m; \\
    & \xi_i \geq 0, i=1,\cdots,m;
\end{align*}</script><h3 id="Corrupted-Location"><a href="#Corrupted-Location" class="headerlink" title="Corrupted Location"></a>Corrupted Location</h3><ul>
<li><p>椭圆不确定集：随机导致的</p>
</li>
<li><p>正则化项使用</p>
<ul>
<li>传统的二范数，一范数同样使用的稀疏的解</li>
</ul>
</li>
<li><p>概率解释：风险控制</p>
</li>
</ul>
<h3 id="Missing-Data"><a href="#Missing-Data" class="headerlink" title="Missing Data"></a>Missing Data</h3><ul>
<li><p>多项式不确定：对抗删除数据（alpha go）</p>
</li>
<li><p>使用无效特征消去偏置</p>
</li>
<li><p>对max损失取对偶得到min带入得到SOCP</p>
</li>
</ul>
<h2 id="Robust-Optimization-and-Regularization"><a href="#Robust-Optimization-and-Regularization" class="headerlink" title="Robust Optimization and Regularization"></a>Robust Optimization and Regularization</h2><ul>
<li><p>统一从稳健优化的角度解释学习算法中的优秀性质</p>
<ul>
<li>正则化</li>
<li>稀疏</li>
<li>一致性</li>
</ul>
</li>
<li><p>指导寻找新的算法</p>
<ul>
<li><p>大数定理、中心极限定理表明即使各个特征上随机不确定项
是独立的，其本身也会有强烈的耦合倾向，表现出相同特征
、像会相互影响一样</p>
</li>
<li><p>这促使寻找新的稳健算法，其中随机不确定项是耦合的</p>
</li>
</ul>
</li>
</ul>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><ul>
<li></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-26T17:01:10.000Z" title="6/27/2019, 1:01:10 AM">2019-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-06-26T17:01:10.000Z" title="6/27/2019, 1:01:10 AM">2019-06-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">7 minutes read (About 999 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/lagrange_duality.html">Lagrange 对偶</a></h1><div class="content"><h2 id="Langrangian-Duality"><a href="#Langrangian-Duality" class="headerlink" title="Langrangian Duality"></a><em>Langrangian Duality</em></h2><p>拉格朗日对偶</p>
<ul>
<li><p>考虑优化问题：找到$f(x)$满足约束的最好下界</p>
<script type="math/tex; mode=display">
z^{*} = \min_{x} f(x) \\
\begin{align*}
s.t. \quad & g_i(x) \leq 0, i=1,2,\cdots,m \\
   & x \in X
\end{align*}</script></li>
<li><p>考虑方程组</p>
<script type="math/tex; mode=display">
\left \{ \begin{array}{l}
f(x) < v \\
g_i(x) \leq 0, i=1,2,\cdots,m
\end{array} \right.</script><ul>
<li><p><strong>方程组无解</strong>：$v$是优化问题的一个下界</p>
</li>
<li><p><strong>方程组有解</strong>：则可以推出</p>
<script type="math/tex; mode=display">
\forall \lambda \geq 0, \exists x, 
f(x) + \sum_{i=1}^m \lambda_ig_i(x) < v</script><blockquote>
<ul>
<li>显然，取$g_1 + g_2 = 0, g_1(x) &gt; 0$是反例，不能
推出原方程有解</li>
</ul>
</blockquote>
</li>
<li><p>由以上方程组有解逆否命题：方程组无解<strong>充分条件</strong>如下</p>
<script type="math/tex; mode=display">
\exists \lambda \geq 0,
\min_{x} f(x) + \sum _{i=1}^m \lambda_ig_i(x) \geq v</script></li>
</ul>
</li>
<li><p>由此方法推出的最好下界，即拉格朗日对偶问题</p>
<script type="math/tex; mode=display">
v^{*} = \max_{\lambda \geq 0} \min_{x} f(x) +
   \sum_{i=1}^m \lambda_ig_i(x)</script></li>
</ul>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><ul>
<li><p>拉格朗日对偶对实数域上的优化问题都存在，对目标函数、
约束函数都没有要求</p>
</li>
<li><p>强对偶定理：$v^{<em>} = z^{</em>}$，需要$f,g$满足特定条件才成立</p>
<ul>
<li>线性规划</li>
<li>半正定规划</li>
<li>凸优化</li>
</ul>
<blockquote>
<ul>
<li>即需要给约束条件加以限制，使得<script type="math/tex; mode=display">
 \forall \lambda \geq 0, \exists x, 
 f(x) + \sum_{i=1}^m \lambda_ig_i(x) < v</script> 是上述方程组有解的冲要条件</li>
</ul>
</blockquote>
</li>
<li><p>弱对偶定理：$v^{<em>} \leq z^{</em>}$，永远成立（以上即可证）</p>
<ul>
<li>通过弱对偶定理，可以得到原问题的一个下界</li>
<li>对求解原问题有帮助，比如：分支界限法中快速求下界</li>
</ul>
</li>
<li><p>对偶问题相关算法往往原问题算法在实际应用中往往更加有效</p>
<ul>
<li><em>dual-simplex</em></li>
<li><em>primal-dual interior point method</em></li>
<li><em>augmented Lagrangian Method</em></li>
</ul>
</li>
</ul>
<h2 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h2><p>约束最优化问题</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min_{x \in R^n} & f(x) \\
s.t. & c_i(x) \leq 0, i = 1,2,\cdots,k \\
& h_j(x) = 0, j = 1,2,\cdots,l
\end{array}</script><h3 id="Generalized-Lagrange-Function"><a href="#Generalized-Lagrange-Function" class="headerlink" title="Generalized Lagrange Function"></a><em>Generalized Lagrange Function</em></h3><ul>
<li><p>引入<em>Generalized Lagrange Function</em></p>
<script type="math/tex; mode=display">
L(x, \alpha, \beta) = f(x) + \sum_{i=1}^k \alpha_i
   c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><blockquote>
<ul>
<li>$x=(x_1, x_2, \cdots, x_n) \in R^n$</li>
<li>$\alpha_i \geq 0, \beta_j$：拉格朗日乘子</li>
</ul>
</blockquote>
</li>
<li><p>考虑关于x的函数</p>
<script type="math/tex; mode=display">
\theta_P(x) = \max_{\alpha, \beta: \alpha_i \geq 0}
   L(x, \alpha, \beta)</script><blockquote>
<ul>
<li>$P$：primal，原始问题</li>
</ul>
</blockquote>
<ul>
<li><p>若x满足原始问题的两组约束条件，则$\theta_P(x)=f(x)$</p>
</li>
<li><p>若x违反等式约束j，取$\beta_j \rightarrow \infty$，
则有$\theta_P(x) \rightarrow \infty$</p>
</li>
<li><p>若x违反不等式约束i，取$\alpha_i \rightarrow \infty$
，则有$\theta_P(x) \rightarrow \infty$</p>
</li>
</ul>
<p>则有</p>
<script type="math/tex; mode=display">\theta_P(x) = \left \{ \begin{array}{l}
f(x), & x 满足原始问题约束条件 \\
+\infty, & 其他
\end{array} \right.</script></li>
<li><p>则极小化问题，称为广义拉格朗日函数的极小极大问题</p>
<script type="math/tex; mode=display">
\min_x \theta_P(x) = \max_{\alpha, \beta: \alpha_i \geq 0}
   L(x, \alpha, \beta)</script><p>与原始最优化问题等价，两问题最优值相同，记为</p>
<script type="math/tex; mode=display">
p^{*} = \min_x \theta_P(x)</script></li>
</ul>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><ul>
<li><p>定义</p>
<script type="math/tex; mode=display">
\theta_D (\alpha, \beta) = \min_x L(x, \alpha, \beta)</script></li>
<li><p>再考虑极大化$\theta_D(\alpha, \beta)$，得到广义拉格朗日
函数的极大极小问题，即</p>
<script type="math/tex; mode=display">
\max_{\alpha, \beta: \alpha \geq 0}
   \theta_D(\alpha, \beta) =
   \max_{\alpha, \beta: \alpha \geq 0} \min_x
   L(x, \alpha, \beta)</script><p>表示为约束最优化问题如下</p>
<script type="math/tex; mode=display">\begin{align*}
\max_{\alpha, \beta} & \theta_D(\alpha, \beta) =
   \max_{\alpha, \beta} \min_x L(x, \alpha, \beta) \\
s.t. & \alpha_i \geq 0, i=1,2,\cdots,k
\end{align*}</script><p>称为原始问题的对偶问题，其最优值定义记为</p>
<script type="math/tex; mode=display">
d^{*} = \max_{\alpha, \beta: \alpha \geq 0}
   \theta_D(\alpha, \beta)</script></li>
</ul>
<h2 id="原始、对偶问题关系"><a href="#原始、对偶问题关系" class="headerlink" title="原始、对偶问题关系"></a>原始、对偶问题关系</h2><h3 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h3><blockquote>
<ul>
<li>若原始问题、对偶问题都有最优值，则<script type="math/tex; mode=display">
  d^{*} = \max_{\alpha, \beta: \alpha \geq 0} \min_x
      L(x, \alpha, \beta) \leq
  \min_x \max_{\alpha, \beta: \alpha \geq 0}
      L(x, \alpha, \beta) = p^{*}</script></li>
</ul>
</blockquote>
<ul>
<li><p>$\forall x, \alpha, \beta$有</p>
<script type="math/tex; mode=display">
\theta_D(\alpha, \beta) = \min_x L(x, \alpha, \beta)
   \leq L(x, \alpha, \beta) \leq
   \max_{\alpha, \beta: \alpha \geq 0} = \theta_P(x)</script><p>即</p>
<script type="math/tex; mode=display">
\theta_D(\alpha, \beta) \leq \theta_P(x)</script></li>
<li><p>而原始、对偶问题均有最优值，所以得证</p>
</li>
</ul>
<blockquote>
<ul>
<li>设$x^{<em>}$、$\alpha^{</em>}, \beta^{<em>}$分别是原始问题、对偶
  问题的可行解，且$d^{</em>} = p^{*}$，则其分别是原始问题、
  对偶问题的最优解</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:26:19.000Z" title="8/4/2021, 11:26:19 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">16 minutes read (About 2327 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/gredient_based.html">Gradient Descent Method</a></h1><div class="content"><h2 id="思想：最速下降-amp-牛顿"><a href="#思想：最速下降-amp-牛顿" class="headerlink" title="思想：最速下降&amp;牛顿"></a>思想：最速下降&amp;牛顿</h2><p>对目标函数$f(x)$在$x^{(1)}$进行展开</p>
<script type="math/tex; mode=display">
f(x) = f(x^{(1)}) + \nabla f(x^{(1)})(x - x^{(1)})+
    \frac 1 2 \nabla^2 f(x^{(1)})(x - x^{(1)})^2 +
    o((x - x^{(1)})^2)</script><blockquote>
<ul>
<li>最速下降法：只保留一阶项，即使用线性函数近似原目标函数</li>
<li>Newton法：保留一阶、二阶项，即使用二次函数近似</li>
</ul>
</blockquote>
<ul>
<li><p>利用近似函数求解元素问题极小值</p>
<ul>
<li>最速下降法：<strong>线性函数无极值，需要确定步长、迭代</strong></li>
<li>Newton法：<strong>二次函数有极值，直接求导算出极值、迭代</strong></li>
</ul>
</li>
<li><p>最速下降法</p>
<ul>
<li>只考虑一阶导：甚至说根本没有考虑拟合原目标函数</li>
</ul>
</li>
<li><p>Newton法</p>
<ul>
<li>考虑二阶导：每步迭代还考虑了二阶导，即当前更新完毕
后，下一步能够更好的更新（二阶导的意义）</li>
<li>甚至从后面部分可以看出，Newton法甚至考虑是全局特征，
不只是局部性质（前提目标函数性质足够好）</li>
<li>二次函数拟合更接近函数极值处的特征</li>
</ul>
</li>
</ul>
<h2 id="最速下降算法"><a href="#最速下降算法" class="headerlink" title="最速下降算法"></a>最速下降算法</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>设$x=x(t)$为最优点$x$从初始点、沿负梯度方向经过的曲线，
则有</p>
<script type="math/tex; mode=display">\left \{ \begin{array}{l}
& \frac {dx(t)} {dt} = -\nabla f(x(t)) \\
& x(t_1) = x^{(1)}
\end{array} \right.</script><blockquote>
<ul>
<li>$t_1, x^{(1)}$：初始时刻、初始位置</li>
</ul>
</blockquote>
</li>
<li><p>可以证明，$x(t)$解存在，且$t \rightarrow \infty$时，有
$x(t) \rightarrow x^{ * }$，即得到无约束问题最优解</p>
</li>
<li><p>但微分方程组求解可能很麻烦，可能根本无法求解</p>
<ul>
<li>考虑将以上曲线离散化，每次前进到“不应该”前进为止</li>
<li>然后更换方向，逐步迭代得到最优解</li>
</ul>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>搜索方向最速下降方向：负梯度方向</li>
<li>终止准则：$\nabla f(x^{(k)})=0$</li>
</ul>
</blockquote>
<ol>
<li><p>取初始点$x^{(1)}$，置k=1</p>
</li>
<li><p>若$\nabla f(x^{(k)})=0$，则停止计算，得到最优解，
否则置</p>
<script type="math/tex; mode=display">d^{(k)} = -\nabla f(x^{(k)})</script><p>以负梯度作为前进方向</p>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) =
  f(x^{(k)} + \alpha d^{(k)})</script><p>得$\alpha_k$前进步长，置</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}</script></li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
<blockquote>
<ul>
<li>最速下降算法不具有二次终止性</li>
</ul>
</blockquote>
<h2 id="叠加惯性"><a href="#叠加惯性" class="headerlink" title="叠加惯性"></a>叠加惯性</h2><p>模拟物体运动时惯性：指数平滑更新步长</p>
<p><img src="/imgs/momentum.png" alt="momentum"></p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a><em>Momentum</em></h3><p>冲量方法：在<strong>原始更新步</strong>上叠加上次更新步，类似指数平滑</p>
<script type="math/tex; mode=display">
v^{(t)} = \gamma v^{(t-1)} + (1 - \gamma) \eta
    \bigtriangledown_\theta L(\theta^{(t-1)}) \\
\theta^{(t)} = \theta^{(t-1)} - v^{(t)}</script><blockquote>
<ul>
<li>$v^{(t)}$：第$t$步时第k个参数更新步</li>
<li>$L(\theta)$：往往是batch损失函数</li>
</ul>
</blockquote>
<ul>
<li>更新参数时，一定程度<strong>保持</strong>上次更新方向</li>
<li>可以在一定程度上保持稳定性，学习速度更快</li>
<li>能够越过部分局部最优解</li>
</ul>
<h3 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a><em>Nesterov Momentum</em></h3><p><em>NGA</em>：在使用冲量修正最终方向基础上，使用冲量对当前
<strong>参数位置</strong>进行修正，即使用“未来”位置计算梯度</p>
<ul>
<li>先使用冲量更新一步</li>
<li>再在更新后位置计算新梯度进行第二步更新</li>
</ul>
<script type="math/tex; mode=display">
v^{(t)} = \gamma v^{(t-1)} + \eta \bigtriangledown_\theta
    L(\theta^{(t-1)} - \gamma v^{(t-1)}) \\

\theta^{(t)} = \theta^{(t-1)} - v^{(t)}</script><h2 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h2><ul>
<li>学习率太小收敛速率缓慢、过大则会造成较大波动</li>
<li>在训练过程中动态调整学习率大小较好</li>
</ul>
<blockquote>
<ul>
<li>模拟退火思想：达到一定迭代次数、损失函数小于阈值时，减小
  学习速率</li>
</ul>
</blockquote>
<p><img src="/imgs/param_estimation_comparion_1.png" alt="param_estimation_comparion_1">
<img src="/imgs/param_estimation_comparion_2.png" alt="param_estimation_comparion_2"></p>
<h3 id="Vanilla-Gradient-Descent"><a href="#Vanilla-Gradient-Descent" class="headerlink" title="Vanilla Gradient Descent"></a><em>Vanilla Gradient Descent</em></h3><p>每次迭代减小学习率$\eta$</p>
<script type="math/tex; mode=display">
\eta^{(t)} = \frac \eta {\sqrt {t+1}} \\

\theta^{(t)} = \theta^{(t-1)} - \eta^{(t)}
    \bigtriangledown_\theta L(\theta^{(t-1)})</script><ul>
<li>学习率逐渐减小，避免学习后期参数在最优解附近反复震荡</li>
</ul>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a><em>Adagrad</em></h3><p><em>adaptive gradient</em>：训练中<strong>不同参数</strong>学习率随着迭代次数、
梯度动态变化，使得参数收敛更加平稳</p>
<script type="math/tex; mode=display">
v^{(t)}_k = \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\

\theta^{(t)}_k = \theta^{(t-1)}_k - \frac \eta
    {\sqrt {\sum_{i=0}^{t-1} (v^{(i)}_k)^2 + \epsilon}}
    v^{(t)}_k</script><blockquote>
<ul>
<li>$\epsilon$：fuss factor，避免分母为0</li>
<li>$\theta^{(t)}_k$：第t轮迭代完成后待估参数第k个分量
  （之前未涉及参数间不同，统一为向量）</li>
</ul>
</blockquote>
<ul>
<li><p>特点</p>
<ul>
<li>较大梯度参数真正学习率会被拉小；较小梯度真正学习率
参数被拉小幅度较小</li>
<li>可以和异步更新参数结合使用，给不常更新参数更大学习率</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>在训练后期，分母中梯度平方累加很大，学习步长趋于0，
收敛速度慢（可能触发阈值，提前结束训练）</li>
</ul>
</li>
</ul>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a><em>RMSprop</em></h3><p><em>root mean square prop</em>：指数平滑更新学习率分母</p>
<script type="math/tex; mode=display">
v^{(t)}_k = \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\

\theta^{(t)}_k = \theta^{(t-1)}_k - \frac \eta
    {\sqrt { \gamma \sum_{i=1}^{t-1}(v^{(i)}_k)^2 +
        (1 - \gamma)((v^{(t)})^2 + \epsilon}
    } v^{(t)}</script><ul>
<li>赋予当前梯度更大权重，减小学习率分母，避免学习速率下降
太快</li>
</ul>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><em>Adam</em></h3><p><em>adptive moment estimation</em>：指数平滑更新步、学习率分母</p>
<script type="math/tex; mode=display">
\begin{align*}
v^{(t)}_k & = \gamma_1 v^{(t-1)}_k + (1 - \gamma_1)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\
s^{(t)}_k & = \gamma_2 s^{(t-1)}_k + (1 - \gamma_2)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\hat{v^{(t)}_k} & = \frac {v^{(t)}_k} {1 - \gamma_1^t} \\
\hat{s^{(t)}_k} & = \frac {s^{(t)}_k} {1 - \gamma_2^t} \\

\theta^{(t)}_k & = \theta^{(t-1)}_k - \frac \eta
    {\sqrt{\hat{s^{(t)}_k} + \epsilon}} \hat{v^{(t)}_k}
\end{align*}</script><blockquote>
<ul>
<li>$\gamma_1$：通常为0.9</li>
<li>$\gamma_2$：通常为0.99</li>
<li>$\hat{v^{(t)}_k} = \frac {v^{(t)}_k} {1 - \gamma_1^t}$
  ：权值修正，使得过去个时间步，小批量随机梯度权值之和为1</li>
</ul>
</blockquote>
<ul>
<li><p>利用梯度的一阶矩$v^{(t)}$、二阶矩$s^{(t)}$动态调整每个
参数学习率</p>
</li>
<li><p>类似于<em>mommentum</em>、<em>RMSprop</em>结合</p>
</li>
<li><p>经过偏执矫正后，每次迭代学习率都有确定范围，参数比较平稳</p>
</li>
</ul>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a><em>Adadelta</em></h3><p>指数平滑更新学习率（分子）、学习率分母</p>
<script type="math/tex; mode=display">
\begin{align*}
s^{(t)}_k & = \gamma_1 s^{(t-1)}_k + (1 - \gamma_1)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\hat{v^{(t)}_k} & = \sqrt {\frac {\Delta \theta^{(t-1)}_k + \epsilon}
    {s^{(t)}_k + \epsilon}}
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\Delta \theta^{(t)}_k & = \gamma_1 \Delta \theta^{(t-1)}_k +
    (1 - \gamma_1) \hat{v^{(t)}_k}^2 \\

\theta^{(t)}_k & = \theta^{(t)}_k - \hat{v^{(t)}_k}
\end{align*}</script><blockquote>
<ul>
<li>$s, \Delta \theta$共用超参$\gamma_1$</li>
</ul>
</blockquote>
<ul>
<li>在<em>RMSprop</em>基础上，使用$\sqrt {\Delta \theta}$作为学习率</li>
<li>$\hat v$：中超参$\gamma_1$在分子、分母“抵消”，模型对
超参不敏感</li>
</ul>
<h2 id="样本量"><a href="#样本量" class="headerlink" title="样本量"></a>样本量</h2><h3 id="Singular-Loss-Stocastic-Gradient-Descent"><a href="#Singular-Loss-Stocastic-Gradient-Descent" class="headerlink" title="Singular Loss/Stocastic Gradient Descent"></a>Singular Loss/Stocastic Gradient Descent</h3><p><em>SGD</em>：用模型在某个样本点上的损失极小化目标函数、计算梯度、
更新参数</p>
<ul>
<li><p>单点损失度量模型“一次”预测的好坏</p>
<ul>
<li>代表模型在单点上的优劣，无法代表模型在总体上性质</li>
<li>具有很强随机性</li>
</ul>
</li>
<li><p>单点损失不常用，SGD范围也不局限于单点损失</p>
</li>
</ul>
<blockquote>
<ul>
<li>损失函数具体参见<em>ml_xxxxx</em></li>
</ul>
</blockquote>
<h3 id="全局估计"><a href="#全局估计" class="headerlink" title="全局估计"></a>全局估计</h3><p>全局损失：用模型在全体样本点上损失极小化目标函数、计算梯度、
更新参数</p>
<script type="math/tex; mode=display">
\theta^{(t)} = \theta^{(t-1)} - \eta \bigtriangledown_\theta
    L_{total}(\theta_{(t-1)})</script><blockquote>
<ul>
<li>$\theta^{(t)}$：第t步迭代完成后待估参数</li>
<li>$\eta$：学习率</li>
<li>$L<em>{total}(\theta) = \sum</em>{i=1}^N L(\theta, x_i, y_i)$：
  训练样本整体损失</li>
<li>$N$：训练样本数量</li>
</ul>
</blockquote>
<ul>
<li><p>若损失函数有解析解、样本量不大，可<strong>一步更新（计算）</strong>
完成（传统参数估计场合）</p>
<ul>
<li>矩估计</li>
<li>最小二乘估计</li>
<li>极大似然估计</li>
</ul>
</li>
<li><p>否则需要迭代更新参数</p>
<ul>
<li>样本量较大场合</li>
<li>并行计算</li>
</ul>
</li>
</ul>
<h3 id="Mini-Batch-Loss"><a href="#Mini-Batch-Loss" class="headerlink" title="Mini-Batch Loss"></a>Mini-Batch Loss</h3><p><em>mini-batch loss</em>：用模型在某个batch上的损失极小化目标函数、
计算梯度、更新参数</p>
<script type="math/tex; mode=display">
\theta^{(t)} = \theta^{(t-1)} - \eta \bigtriangledown_\theta
    L_{batch}(\theta^{(t-1)})</script><blockquote>
<ul>
<li>$L<em>{batch}(\theta)=\sum</em>{i \in B} L(\theta, x_i, y_i)$：
  当前batch整体损失</li>
<li>$B$：当前更新步中，样本组成的集合batch</li>
</ul>
</blockquote>
<ul>
<li><p>batch-loss是模型在batch上的特征，对整体的代表性取决于
batch大小</p>
<ul>
<li>batch越大对整体代表性越好，越稳定；越小对整体代表
越差、不稳定、波动较大、难收敛</li>
<li>batch大小为1时，就是SGD</li>
<li>batch大小为整个训练集时，就是经验（结构）风险</li>
</ul>
</li>
<li><p>batch-loss是学习算法中最常用的loss，SGD优化常指此</p>
<ul>
<li>实际中往往是使用batch-loss替代整体损失，表示经验风险
极小化</li>
<li>batch-loss同样可以带正则化项，表示结构风险极小化</li>
<li>损失极值：SVM（几何间隔最小）</li>
</ul>
</li>
</ul>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>适合样本量较大、无法使用样本整体估计使用</li>
<li>一定程度能避免局部最优（随机batch可能越过局部极值）</li>
<li>开始阶段收敛速度快</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li><p>限于每次只使用单batch中样本更新参数，batch-size较小时，
结果可能不稳定，往往很难得到最优解</p>
</li>
<li><p>无法保证良好的收敛性，学习率小收敛速度慢，学习率过大
则损失函数可能在极小点反复震荡</p>
</li>
<li><p>对所有参数更新应用相同学习率，没有对低频特征有优化
（更的学习率）</p>
</li>
<li><p>依然容易陷入局部最优点</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">5 minutes read (About 752 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/newtons.html">Newton&#039;s Method</a></h1><div class="content"><h2 id="Newton法"><a href="#Newton法" class="headerlink" title="Newton法"></a>Newton法</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>若$x^{ * }$是无约束问题局部解，则有</p>
<script type="math/tex; mode=display">\nabla f(x^{ * }) = 0</script><p>可求解此问题，得到无约束问题最优解</p>
</li>
<li><p>原始问题是非线性，考虑求解其线性逼近，在初始点$x^{(1)}$
处泰勒展开</p>
<script type="math/tex; mode=display">
\nabla f(x) \approx \nabla f(x^{(1)})
   + \nabla^2 f(x^{(1)})(x - x^{(1)})</script><p>解得</p>
<script type="math/tex; mode=display">
x^{(2)} = x^{(1)} - (\nabla^2 f(x^{(1)}))^{-1}
   \nabla f(x^{(1)})</script><p>作为$x^{ * }$的第二次近似</p>
</li>
<li><p>不断迭代，得到如下序列</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + d^{(k)}</script><blockquote>
<ul>
<li>$d^{(k)}$：Newton方向，即以下方程解<script type="math/tex; mode=display">
 \nabla^2 f(x^{(k)}) d = -\nabla
     f(x^{(k)})</script></li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ul>
<li><p>初始点 $x^{(1)}$、精度要求 $\epsilon$，置 $k=1$</p>
</li>
<li><p>考虑 $|\nabla f(x^{(k)})| \leq \epsilon$</p>
<ul>
<li>若满足，则停止计算，得到最优解 $x^{(k)}$</li>
<li><p>否则求解如下方程，得到 $d^{(k)}$</p>
<script type="math/tex; mode=display">
\nabla^2 f(x^{(k)}) d = -\nabla f(x^{(k)})</script></li>
</ul>
</li>
<li><p>如下设置，并转2</p>
<script type="math/tex; mode=display">x^{(k+1)} = x^{(k)} + d^{(k)}, k = k+1</script></li>
</ul>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>优点</p>
<ul>
<li>产生点列 ${x^{k}}$ 若收敛，则具有二阶收敛速率</li>
<li>具有二次终止性，事实上对正定二次函数，一步即可收敛</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>可能会在某步迭代时目标函数值上升</li>
<li>当初始点 $x^{(1)}$ 距离最优解 $x^{ * }$ 时，产生的点列
可能不收敛，或者收敛到鞍点</li>
<li>需要计算 <em>Hesse</em> 矩阵<ul>
<li>计算量大</li>
<li><em>Hesse</em> 矩阵可能不可逆，算法终止</li>
<li><em>Hesse</em> 矩阵不正定，<em>Newton</em> 方向可能不是下降方向</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="阻尼-修正-Newton-法"><a href="#阻尼-修正-Newton-法" class="headerlink" title="阻尼/修正 Newton 法"></a>阻尼/修正 <em>Newton</em> 法</h2><ul>
<li>克服 <em>Newton</em> 法目标函数值上升的缺点</li>
<li>一定程度上克服点列可能不收敛缺点</li>
</ul>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul>
<li><p>初始点 $x^{(1)}$、精度要求 $\epsilon$，置 $k=1$</p>
</li>
<li><p>考虑 $|\nabla f(x^{(k)})| \leq \epsilon$</p>
<ul>
<li>若满足，停止计算，得到最优解 $x^{(k)}$</li>
<li>否则求解如下方程得到 $d^{(k)}$<script type="math/tex; mode=display">
\nabla^2 f(x^{(k)}) d = -\nabla f(x^{(k)})</script></li>
</ul>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) = f(x^{(k)} + \alpha d^{(k)})</script><p>得到 $\alpha_k$，如下设置并转2</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}, k = k+1</script></li>
</ul>
<h2 id="其他改进"><a href="#其他改进" class="headerlink" title="其他改进"></a>其他改进</h2><ul>
<li><em>Newton</em> 法、修正 <em>Newton</em> 法的改进方向<ul>
<li>结合最速下降方向修正迭代方向</li>
<li><em>Hesse</em> 矩阵不正定情形下的替代</li>
</ul>
</li>
</ul>
<h3 id="结合最速下降方向"><a href="#结合最速下降方向" class="headerlink" title="结合最速下降方向"></a>结合最速下降方向</h3><blockquote>
<ul>
<li>将 <em>Newton</em> 方向和最速下降方向结合</li>
</ul>
</blockquote>
<ul>
<li><p>设 $\theta_k$ 是 $<d^{(k)}, -\nabla f(x^{(k)})>$ 之间夹角，显然希望 $\theta &lt; \frac \pi 2$</p>
</li>
<li><p>则置限制条件 $\eta$，取迭代方向</p>
<script type="math/tex; mode=display">
d^{(k)} = \left \{ \begin{array}{l}
   d^{(k)}, & cos\theta_k \geq \eta \\
   -\nabla f(x^{(k)}), & 其他
\end{array} \right.</script></li>
</ul>
<h3 id="Negative-Curvature"><a href="#Negative-Curvature" class="headerlink" title="Negative Curvature"></a><em>Negative Curvature</em></h3><blockquote>
<ul>
<li>当 <em>Hesse</em> 矩阵非正定时，选择负曲率下降方向 $d^{(k)}$（一定存在）</li>
</ul>
</blockquote>
<ul>
<li><p><em>Hesse</em> 矩阵非正定时，一定存在负特征值、相应特征向量 $u$</p>
<ul>
<li><p>取负曲率下降方向作为迭代方向</p>
<script type="math/tex; mode=display">
d^{(k)} = -sign(u^T \nabla f(x^{(k)})) u</script></li>
<li><p>$x^{(k)}$ 处负曲率方向 $d^{(k)}$ 满足</p>
<script type="math/tex; mode=display">
{d^{(k)}}^T \nabla^2 f(x^{(k)}) d^{(k)} < 0</script></li>
</ul>
</li>
</ul>
<h3 id="修正-Hesse-矩阵"><a href="#修正-Hesse-矩阵" class="headerlink" title="修正 Hesse 矩阵"></a>修正 <em>Hesse</em> 矩阵</h3><ul>
<li><p>取迭代方向 $d^{(k)}$ 为以下方程的解</p>
<script type="math/tex; mode=display">
(\nabla^2 f(x^{(k)}) + v_k I) d = -\nabla f(x^{k})</script></li>
</ul>
<blockquote>
<ul>
<li>$v_k$：大于 $\nabla^2 f(x^{(k)})$ 最大负特征值绝对值</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-05-25T11:55:48.000Z" title="5/25/2019, 7:55:48 PM">2019-05-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-02T09:30:14.000Z" title="8/2/2021, 5:30:14 PM">2021-08-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a></span><span class="level-item">23 minutes read (About 3403 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/optimizer_physical.html">物理查询优化</a></h1><div class="content"><h2 id="查询代价估算"><a href="#查询代价估算" class="headerlink" title="查询代价估算"></a>查询代价估算</h2><h3 id="代价模型"><a href="#代价模型" class="headerlink" title="代价模型"></a>代价模型</h3><p>代价估计模型：基于CPU代价、IO代价</p>
<script type="math/tex; mode=display">\begin{align*}
总代价 &= IO代价 + CPU代价 \\
COST &= P * CPUTimePerPage + W * T
\end{align*}</script><blockquote>
<ul>
<li>$P$：计划访问的页面数</li>
<li>$CPUTimePerPage$：读取每个页面的时间花费</li>
<li>$T$：访问的元组数，索引扫描应包括索引读取花费<blockquote>
<ul>
<li>反映CPU代价，因为访问页面上的元组需要解析元组结构，
 消耗CPU</li>
</ul>
</blockquote>
</li>
<li>$W$：<em>selectivity</em>，选择率/权重因子，表明IO、CPU的相关性</li>
</ul>
</blockquote>
<h3 id="Selectivity"><a href="#Selectivity" class="headerlink" title="Selectivity"></a><em>Selectivity</em></h3><p>选择率：在关系R中，满足<strong>条件</strong><code>A &lt;cond_op&gt; a</code>的元组数R和
所有元组数N的比值</p>
<ul>
<li>在CBO中占有重要地位</li>
<li>其精确程度直接影响最优计划的选择</li>
</ul>
<h4 id="估计方法"><a href="#估计方法" class="headerlink" title="估计方法"></a>估计方法</h4><ul>
<li><p><em>Non-Parametric Method</em>：非参方法，使用ad-hoc数据结构、
直方图维护属性值分布</p>
</li>
<li><p><em>Parametric Method</em>：参数方法，使用预先估计的分布函数
逼近真实分布</p>
</li>
<li><p><em>Curve Fitting</em>：曲线拟合法，使用多项式函数、最小标准差
逼近属性值分布</p>
</li>
<li><p><em>Sampling</em>：抽样法，从数据库中抽取部分元组，针对样本进行
查询，收集统计数据</p>
<ul>
<li>需要足够多样本被测试才能达到足够精度</li>
</ul>
</li>
<li><p>综合法</p>
</li>
</ul>
<h2 id="单表扫描算法"><a href="#单表扫描算法" class="headerlink" title="单表扫描算法"></a>单表扫描算法</h2><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h2 id="两表联接算法"><a href="#两表联接算法" class="headerlink" title="两表联接算法"></a>两表联接算法</h2><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><h3 id="Nested-Loop"><a href="#Nested-Loop" class="headerlink" title="Nested Loop"></a><em>Nested Loop</em></h3><p>嵌套循环联接算法：扫描外表，读取记录根据<code>join</code>字段上的
<strong>索引</strong>去内表中查询</p>
<ul>
<li>适合场景<ul>
<li>外表记录较少（&lt;1w）</li>
<li>内表已经创建索引、性能较好</li>
<li>inner、left outer、left semi、left antisemi join</li>
</ul>
</li>
</ul>
<h4 id="嵌套循环联接算法"><a href="#嵌套循环联接算法" class="headerlink" title="嵌套循环联接算法"></a>嵌套循环联接算法</h4><ul>
<li>搜索时扫描整个表、索引</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> each row R1 in the outer table:</span><br><span class="line">	<span class="keyword">for</span> each row R2 in the inner table:</span><br><span class="line">		<span class="keyword">if</span> R1 join with R2:</span><br><span class="line">			<span class="keyword">return</span> (R1, R2)</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>外部循环逐行消耗外部输入表，当其数据量很大时可以并行扫描
  内表</li>
<li>内表被外表驱动：内部循环为每个外部行执行，在内表中搜索
  匹配行</li>
</ul>
</blockquote>
<h4 id="基于块嵌套循环联接算法"><a href="#基于块嵌套循环联接算法" class="headerlink" title="基于块嵌套循环联接算法"></a>基于块嵌套循环联接算法</h4><ul>
<li>每次IO申请以“块”为单位尽量读入多个页面</li>
<li>改进获取元组的方式</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> each chunk c1 of t1</span><br><span class="line">	<span class="keyword">if</span> c1 <span class="keyword">not</span> in memory:</span><br><span class="line">		read chunk c1 to memory</span><br><span class="line">	<span class="keyword">for</span> each row r1 in chunk c1:</span><br><span class="line">		<span class="keyword">for</span> each chunk c2 of t2:</span><br><span class="line">			<span class="keyword">if</span> c2 <span class="keyword">not</span> in memory:</span><br><span class="line">				read chunk c2 into memory</span><br><span class="line">			<span class="keyword">for</span> each row r2 in c2:</span><br><span class="line">				<span class="keyword">if</span> r1 join with r2:</span><br><span class="line">					<span class="keyword">return</span>(R1, R2)</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>内存循环最后一个块使用后作为下次循环循环使用的第一个块
  可以节省一次IO</li>
</ul>
</blockquote>
<h4 id="索引嵌套循环联接算法"><a href="#索引嵌套循环联接算法" class="headerlink" title="索引嵌套循环联接算法"></a>索引嵌套循环联接算法</h4><ul>
<li>索引嵌套循环连结：在内表中搜索时使用索引，可以加快联接
速度</li>
<li>临时索引嵌套循环连结：为查询临时生成索引作为查询计划的
一部分，查询完成后立刻将索引破坏</li>
</ul>
<h3 id="Sort-Merge-Join"><a href="#Sort-Merge-Join" class="headerlink" title="(Sort)Merge Join"></a><em>(Sort)Merge Join</em></h3><p>排序归并联接算法</p>
<ul>
<li>适合场景<ul>
<li>联接字段已经排序，如B+树索引</li>
<li>inner、left outer、left semi、left anti semi、
right outer、right semi、right anti semi join、union</li>
<li>等值、非等值联接，除<code>!=/&lt;&gt;</code></li>
</ul>
</li>
</ul>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ul>
<li><p>确保两个关联表都是按照关联字段进行排序</p>
<ul>
<li>若关联字段已经有排序一致的可用索引，可以利用索引直接
进行merge join操作</li>
<li>否则先对关联字段进行排序，表过大无法一次载入内存时
需要分块载入</li>
</ul>
</li>
<li><p>从每个表分别取记录开始匹配（升序）</p>
<ul>
<li>若符合关联条件，放入结果集</li>
<li>否则丢关联字段较小记录，取对应表中下条记录继续
匹配，直到整个循环结束</li>
<li>对于多对join，通常需要使用临时表进行操作<h1 id="todo-1"><a href="#todo-1" class="headerlink" title="todo"></a>todo</h1></li>
</ul>
</li>
</ul>
<h3 id="Hash-Join"><a href="#Hash-Join" class="headerlink" title="Hash Join"></a><em>Hash Join</em></h3><p>哈希联接：利用Hash Match联接</p>
<ul>
<li><p>HJ处理代价非常高，是服务器内存、CPU头号杀手，需要对数据
进行分区时，还会造成大量异步磁盘I/O，避免大数据的HJ，
尽量转化为高效的SMJ、NLJ</p>
<ul>
<li>表结构设计：冗余字段</li>
<li>索引调整设计</li>
<li>SQL优化</li>
<li>冗余表：静态表存储统计结果</li>
</ul>
</li>
<li><p>类似任何hash算法，内存小、数据偏斜严重时，散列冲突会比较
严重，此时应该考虑使用NIJ</p>
</li>
<li><p>适合场景</p>
<ul>
<li>两表数据量相差非常大</li>
<li>对CPU消耗明显，需要CPU资源充足</li>
<li>只适合（不）等值查询</li>
</ul>
</li>
</ul>
<h4 id="In-Memory-Hash-Join"><a href="#In-Memory-Hash-Join" class="headerlink" title="In-Memory Hash Join"></a><em>In-Memory Hash Join</em></h4><p><img src="/imgs/db_hash_join.png" alt="db_hash_join"></p>
<h5 id="build阶段"><a href="#build阶段" class="headerlink" title="build阶段"></a><em>build</em>阶段</h5><p>以操作涉及字段为hash key构造hash表</p>
<ul>
<li><p>从构造输入表中取记录，使用hash函数生成hash值</p>
</li>
<li><p>hash值对应hash表中的buckets，若一个hash值对应多个桶，
则使用链表将联接桶</p>
</li>
<li><p>构造输入表处理完毕之后，其中记录都被桶关联</p>
</li>
</ul>
<blockquote>
<ul>
<li>build表构建的hash表需要频繁访问，最好能全部加载在内存中
  ，因此尽量选择小表，避免使用GHJ</li>
</ul>
</blockquote>
<h5 id="probe阶段"><a href="#probe阶段" class="headerlink" title="probe阶段"></a><em>probe</em>阶段</h5><ul>
<li><p>从探测输入中取记录，使用同样hash函数生成hash值</p>
</li>
<li><p>根据hash值，在构造阶段构造的hash表中搜索对应桶</p>
</li>
</ul>
<blockquote>
<ul>
<li>为避免冲突，bucket可能会联接到其他bucket，探测操作
  会搜索整个冲突链上的buckets查找匹配记录</li>
</ul>
</blockquote>
<h5 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h5><p>以下操作内部实现其实都是hash join，只是对应算符不同而已</p>
<ul>
<li><p>join操作</p>
<ul>
<li>使用join字段计算hash值</li>
<li>使用<strong>顶端输入</strong>构造hash表，<strong>底端输入</strong>进行探测</li>
<li>按照联接类型规定的模式输出（不）匹配项</li>
<li>若多个联接使用相同的联接列，这些操作将分组为一个
哈希组</li>
</ul>
</li>
<li><p>grouby操作、unique操作</p>
<ul>
<li>使用groupby字段、所有select字段计算hash值</li>
<li>使用输入构造hash表，删除重复项、计算聚合表达式</li>
<li>扫描hash表输出所有项</li>
</ul>
</li>
<li><p>union操作、需要去除重复记录操作</p>
<ul>
<li>所有select字段计算hash值</li>
<li>第一个输入构建hash表，删除重复项</li>
<li>第二个输入进行探测<ul>
<li>若第二个输入没有重复项，直接返回没有匹配的项，
扫描hash表返回所有项</li>
<li>若第二个输入有重复项，则应该需要继续构建hash表，
最后统一输出整个hash表</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Grace-Hash-Join"><a href="#Grace-Hash-Join" class="headerlink" title="Grace Hash Join"></a><em>Grace Hash Join</em></h4><p><em>grace hash join</em>：磁盘分块HJ</p>
<ul>
<li><p>将两表按照相同hash函数分配至不同分片中</p>
<ul>
<li>在磁盘上为各分片、表建立相应文件</li>
<li>对表输入计算哈希值，根据哈希值写入分片、表对应文件</li>
</ul>
</li>
<li><p>再对不同分片进行普通<em>in-memory hash join</em></p>
<ul>
<li>若分片依然不能全部加载至内存，可以继续使用
<em>grace hash join</em></li>
</ul>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">grace_hash_join</span>(t1, t2):</span><br><span class="line">	<span class="comment">// Grace Hash Join实现</span></span><br><span class="line">	<span class="comment">// 输入：待join表t1、t2</span></span><br><span class="line">	<span class="keyword">for</span> row in t1:</span><br><span class="line">		hash_val = <span class="built_in">hash_func</span>(row)</span><br><span class="line">		N = hash_val % PART_COUNT</span><br><span class="line">		write row to file t1_N</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> row in t2:</span><br><span class="line">		hash_val = <span class="built_in">hash_func</span>(row)</span><br><span class="line">		N = hash_val % PART_COUNT</span><br><span class="line">		write row to file t2_N</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="number">0</span>, PART_COUNT):</span><br><span class="line">		<span class="built_in">join</span>(t1_i, t2_i)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>分片数量<code>PART_COUNT</code>决定磁盘IO效率</p>
<ul>
<li>分片数量过小：无法起到分治效果，分片仍然需要进行
<em>grace hash join</em>，降低效率</li>
<li>分片数量过大：磁盘是块设备，每次刷盘刷一定数量块才
高效，频繁刷盘不经济</li>
<li>即分片数量在保证刷盘经济的情况下，越大越好，这需要
优化器根据表统计信息确定</li>
</ul>
</li>
<li><p>特点</p>
<ul>
<li>有磁盘I/O代价，会降低效率</li>
<li>适合参与join表非常大，无法同时载入内存中</li>
</ul>
</li>
</ul>
<h4 id="Hybrid-Hash-Join"><a href="#Hybrid-Hash-Join" class="headerlink" title="Hybrid Hash Join"></a><em>Hybrid Hash Join</em></h4><p><em>hybrid hash join</em>：GHJ基础上结合IMHJ的改进</p>
<ul>
<li>对build表分片过程中，尽量多把完整分片保留在内存中</li>
<li>对probe表分片时，对应分片可以直接进行probe操作</li>
</ul>
<blockquote>
<ul>
<li><em>hybrid hash join</em>有时也被直接视为<em>grace hash join</em>，
  不做区分</li>
</ul>
</blockquote>
<h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><ul>
<li><p>资源消耗</p>
<ul>
<li>HJ：CPU计算、内存（磁盘）中创建临时hash表</li>
<li>SMJ：磁盘I/O（扫描表、索引）</li>
<li>NLJ：磁盘I/O</li>
</ul>
</li>
<li><p>性能</p>
<ul>
<li><p>通常情况：HJ &gt; NPJ &lt;&gt; SMJ</p>
<ul>
<li>全表扫描比索引范围扫描再进行表访问更可取时，SMJ
优于NPJ？？？</li>
<li>而表特别小、特别大时，全表扫描优于索引范围扫描</li>
</ul>
</li>
<li><p>但若关联字段已排序，SMJ性能最优</p>
</li>
</ul>
</li>
<li><p>首条搜索结果</p>
<ul>
<li>NPJ能快速返回首条搜索结果</li>
<li>HJ、SMJ返回首条结果较慢</li>
</ul>
</li>
</ul>
<h2 id="多表联接算法"><a href="#多表联接算法" class="headerlink" title="多表联接算法"></a>多表联接算法</h2><p>多表联接算法：找到最优连接顺序（执行路径）</p>
<ul>
<li><p>表联接顺序对于查询结果没有影响，但是对资源消耗、性能影响
巨大</p>
</li>
<li><p>随着需要联接表数目增加，可能的联接排列非常多，基本不能
对所有可能穷举分析</p>
<ul>
<li><em>left-deep tree</em>/<em>linear (processing)tree</em>：$n!$</li>
<li><em>bushy tree</em>：$\frac {2(n-1)!} {(n-1)!}$
（包括left-deep tree、right-deep tree）</li>
</ul>
<p><img src="/imgs/left_deep_tree_bushy_tree.png" alt="left_deep_tree_bushy_tree"></p>
</li>
<li><p>事实上查询优化器不会穷尽搜索所有可能联接排列，而是使用
启发式算法进行搜索</p>
</li>
</ul>
<h3 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a><em>Dynamic Programming</em></h3><p>动态规划算法：依次求解各数量表最优联接顺序，直到求出最终结果</p>
<ol>
<li><p>构造第一层关系：每个关系的最优路径就是关系的最优单表扫描
方式</p>
</li>
<li><p>迭代依次构造之后n-1层关系联接最优解</p>
<ul>
<li>左深联接树方式：将第k-1层每个关系同第1层关系联接</li>
<li>紧密树联接方式：将第m(m &gt; 2)层每个关系同第k-m层关系
联接</li>
</ul>
<p><img src="/imgs/left_deep_tree_bushy_tree.png" alt="left_deep_tree_bushy_tree"></p>
</li>
</ol>
<h3 id="Heuristic-Algorithm"><a href="#Heuristic-Algorithm" class="headerlink" title="Heuristic Algorithm"></a><em>Heuristic Algorithm</em></h3><h3 id="Greedy-Algorithm"><a href="#Greedy-Algorithm" class="headerlink" title="Greedy Algorithm"></a><em>Greedy Algorithm</em></h3><p>贪心算法：认为每次连接表的连接方式都是最优的，即从未联接表中
选择使得下次联接代价最小者</p>
<ul>
<li><p>多表排序一般为</p>
<ul>
<li>常量表最前</li>
<li>其他表按可访问元组数量升序排序</li>
</ul>
</li>
<li><p>贪心算法得到的联接方式都是最优的</p>
<ul>
<li>则每次联接主要求解要联接表对象的最佳访问方式</li>
<li>即每次代价估计的重点在于单表扫描的代价</li>
</ul>
</li>
<li><p>求解结束后，局部最优查询计划生成</p>
<ul>
<li>得到左深树</li>
<li>最初始表位于最左下端叶子节点处</li>
</ul>
</li>
</ul>
<h3 id="System-R"><a href="#System-R" class="headerlink" title="System R"></a><em>System R</em></h3><p><em>System R</em>：对动态规划算法的改进</p>
<ul>
<li>保留子树查询最优、次优查询计划，用于上层查询计划生成，
使得查询计划整体较优</li>
</ul>
<h3 id="Genetic-Algorithm"><a href="#Genetic-Algorithm" class="headerlink" title="Genetic Algorithm"></a><em>Genetic Algorithm</em></h3><p>遗传算法：模拟自然界生物进化过程，采用人工进化的方式对目标
空间进行搜索</p>
<ul>
<li>本质是高效、并行、全局搜索方法</li>
<li>能在搜索过程中自动获取、积累有关搜索空间的知识，并自适应
的控制搜索过程以求的最佳解</li>
</ul>
<h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><ul>
<li>将问题域中可能解看作是染色体，将其编码为符号串的形式</li>
<li>对染色体群体反复进行基于遗传学的操作：选择、交叉、变异</li>
<li>根据预定目标适应度函数对每个个体进行评价，不断得到更优
群体，从中全局并行搜索得到优化群体中最优个体</li>
</ul>
<h4 id="实体"><a href="#实体" class="headerlink" title="实体"></a>实体</h4><ul>
<li><em>population</em>：群体，GA的遗传搜索空间</li>
<li><em>individual</em>：个体，搜索空间中可能解</li>
<li><em>chromosome</em>：染色体，个体特征代表<ul>
<li>由若干段基因组成</li>
<li>GA中基本操作对象</li>
</ul>
</li>
<li><em>gene</em>：基因<ul>
<li>染色体片段</li>
</ul>
</li>
<li><em>fitness</em>：适应度，个体对环境的适应程度</li>
</ul>
<h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h4><ul>
<li><p><em>selection</em>：选择，根据个体适应度在群体中按照一定概率
选择个体作为父本</p>
<ul>
<li>适应度大个体被选择概率高</li>
<li>体现了适者生存、优胜劣汰的进化规则</li>
</ul>
</li>
<li><p><em>crossover</em>：交叉，将父本个体按照一定概率随机交换基因
形成新个体</p>
</li>
<li><p><em>mutate</em>：变异，按照一定概率随机改变某个体基因值</p>
</li>
</ul>
<h4 id="涉及问题"><a href="#涉及问题" class="headerlink" title="涉及问题"></a>涉及问题</h4><ul>
<li><p>串编码方式</p>
<ul>
<li>把问题的各种参数用二进串进行编码构成子串</li>
<li>把子串拼接成染色体<blockquote>
<ul>
<li>串长度、编码方式对算法收敛影响极大</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>适应度/对象函数确定</p>
<ul>
<li>一般可以把问题模型函数作为对象函数</li>
</ul>
</li>
<li><p>GA超参设置</p>
<ul>
<li>群体大小$n$：过小难以求出最优解，过大难收敛，一般取
$n = 30 ~ 160$</li>
<li>交叉概率$P_c$：太小难以前向搜索，太大容易破坏高适应
值结构，一般取$P_c = 0.25 ~ 0.75$</li>
<li>变异概率$P_m$：太小难以产生新结构，太大则变为单纯
随机搜索，一般取$P_m = 0.01 ~ 0.2$</li>
</ul>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><ol>
<li>随机初始化种群</li>
<li>估初始种群：为种群每个个体计算适应值、排序</li>
<li>若没有达到预定演化数，则继续，否则结束算法</li>
<li>选择父体<ul>
<li>杂交：得到新个体</li>
<li>变异：对新个体变异</li>
</ul>
</li>
<li>计算新个体适应值，把适应值排名插入种群，淘汰最后个体</li>
<li>重复3</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-04-22T17:32:09.000Z" title="4/23/2019, 1:32:09 AM">2019-04-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-04-22T17:32:09.000Z" title="4/23/2019, 1:32:09 AM">2019-04-23</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">12 minutes read (About 1864 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/quasi_newtons.html">Quasi-Newton Method/Variable Metric Method</a></h1><div class="content"><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>拟Newton法/变度量法：不需要求解Hesse矩阵，使用一阶导构造
二阶信息的近似矩阵</p>
<ul>
<li><p>使用迭代过程中信息，创建近似矩阵$B^{(k)}$代替Hesse矩阵</p>
</li>
<li><p>用以下方程组替代Newton方程，其解$d^{(k)}$作为搜索方向</p>
<script type="math/tex; mode=display">
B^{(k)} d = - \triangledown f(x^{(k)})</script></li>
</ul>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>考虑$\triangledown f(x)$在$x^{(k+1)}$处泰勒展开</p>
<script type="math/tex; mode=display">
\triangledown f(x) \approx \triangledown f(x^{(k+1)})
   + \triangledown^2 f(x^{(k+1)})(x - x^{(k+1)})</script></li>
<li><p>取$x = x^{(k)}$，有</p>
<script type="math/tex; mode=display">\begin{align*}
\triangledown f(x^{(k+1)}) - \triangledown f(x^{(k)})
   & \approx \triangledown^2 f(x^{(x+1)})
   (x^{(k+1) } - x^{(k)}) \\
\triangledown^2 f(x^{k+1}) s^{(k)} & \approx y^{(k)}
\end{align*}</script><blockquote>
<ul>
<li>$s^{(k)} = x^{(k+1)} - x^{(k)}$</li>
<li>$y^{(k)} = \triangledown f(x^{(k+1)}) - \triangledown f(x^{(k)})$</li>
</ul>
</blockquote>
</li>
<li><p>要求$B^{(k)}$近似$\triangledown^2 f(x^{(k)})$，带入并将
$\approx$改为$=$，得到拟Newton方程</p>
<script type="math/tex; mode=display">
B^{(k+1)} s^{(k)} = y^{(k)}</script><p>并假设$B^{(k)}$对称</p>
</li>
<li><p>拟Newton方程不能唯一确定$B^{(k+1)}$，需要附加条件，自然
的想法就是$B^{(k+1)}$可由$B^{(k)}$修正得到，即</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} + \Delta B^{(k)}</script><p>且修正项$\Delta B^{(k)}$具有“简单形式”</p>
</li>
</ul>
<h2 id="Hesse矩阵修正"><a href="#Hesse矩阵修正" class="headerlink" title="Hesse矩阵修正"></a>Hesse矩阵修正</h2><h3 id="对称秩1修正"><a href="#对称秩1修正" class="headerlink" title="对称秩1修正"></a>对称秩1修正</h3><p>认为简单指矩阵秩小：即认为$\Delta B^{(k)}$秩为最小值1</p>
<ul>
<li><p>设$\Delta B^{(k)} = u v^T$，带入有</p>
<script type="math/tex; mode=display">\begin{align*}
y^{(k)} & = B^{(k+1)} s^{(k)} \\
& = B^{(k)} s^{(k)} + (v^T s^{(k)}) u \\
y^{(k)} - B^{(k)} s^{(k)} & = (v^T s^{(k)}) u
\end{align*}</script><ul>
<li>这里有的书会设$\Delta B^{(k)} = \alpha u v^T$，
其实对向量没有必要</li>
<li>$v^T s^{(k)}$是数，所以$u$必然与共线，同理也没有必要
考虑系数，直接取相等即可</li>
<li>而且系数不会影响最终结果</li>
</ul>
</li>
<li><p><strong>可取</strong>$u = y^{(k)} - B^{(k)} s{(k)}$，取$v$满足
$v^T s^{(k)}  = 1$</p>
</li>
<li><p>由$B^{(k)}$的对称性、并希望$B^{(k+1)}$保持对称，需要
$u, v$共线，则有</p>
<script type="math/tex; mode=display">\begin{align*}
v & = \lambda u = \lambda (y^{(k)} - B^{(k)} s^{(k)}) \\
1 & = \lambda (y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)}
\end{align*}</script></li>
<li><p>得到$B^{(k)}$的对称秩1修正公式</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} + \frac {(y^{(k) - B^{(k)} s^{(k)}})
   (y^{(k)} - B^{(k)} s^{(k)})^T}
   {(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)}}</script></li>
</ul>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始点$x^{(1)}$、初始矩阵$B^{(1)} = I$、精度要求
$\epsilon$、置$k=1$</p>
</li>
<li><p>若$|\triangledown f(x^{(k)})| \leq \epsilon$，停止计算
，得到解$x^{(k)}$，否则求解以下方程得到$d^{(k)}$</p>
<script type="math/tex; mode=display">
B^{(k)} d = -\triangle f(x^{(k)})</script></li>
<li><p>一维搜索，求解</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha)=f(x^{(k)} + \alpha d^{(k)})</script><p>得到$\alpha_k$，置$x^{(k+1)}=x^{(k)} + \alpha_k d^{(k)}$</p>
</li>
<li><p>修正$B^{(k)}$</p>
<script type="math/tex; mode=display">\begin{align*}
s^{(k)} & = x^{(k+1)} - x^{(k)} \\
y^{(k)} & = \triangledown f(x^{(k+1)}) -
  \triangledown f(x^{(k)}) \\
B^{(k+1)} & = B^{(k)} + \frac {(y^{(k) - B^{(k)} s^{(k)}})
  (y^{(k)} - B^{(k)} s^{(k)})^T}
  {(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)}}
\end{align*}</script></li>
<li><p>置$k = k+1$，转2</p>
</li>
</ol>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>缺点</p>
<ul>
<li><p>要求$(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)} \neq 0$，
否则无法继续计算</p>
</li>
<li><p>不能保证正定性传递，只有
$(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)} &gt; 0$才能保证
$B^{(k+1)}$也正定</p>
</li>
<li><p>即使$(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)} &gt; 0$，
也可能很小，容易产生较大的舍入误差</p>
</li>
</ul>
</li>
</ul>
<h3 id="对称秩2修正"><a href="#对称秩2修正" class="headerlink" title="对称秩2修正"></a>对称秩2修正</h3><ul>
<li><p>为克服秩1修正公式缺点，考虑$\Delta B^{(k)}$秩为2，设</p>
<script type="math/tex; mode=display">
\Delta B^{(k)} = u^{(1)} (v^{(1)})^T
   + u^{(2)} (v^{(2)})^T</script></li>
<li><p>带入拟Newton方程有</p>
<script type="math/tex; mode=display">
B^{(k)} s^{(k)} + ((v^{(1)})^T s^{(k)}) u^{(1)} +
   ((v^{(2)})^T s^{(k)}) u^{(2)} = y^{(k)}</script></li>
<li><p>类似的取</p>
<script type="math/tex; mode=display">\left \{ \begin{array}{l}
u^{(1)} = y^{(k)} \\
(v^{(1)})^T s^{(k)} = 1
\end{array} \right.</script><script type="math/tex; mode=display">\left \{ \begin{array}{l}
u^{(2)} = -B^{(k)} s^{(k)} \\
(v^{(2)})^T s^{(k)} = 1
\end{array} \right.</script></li>
<li><p>同秩1公式保持对称性推导，得到对称秩2修正公式/BFGS公式</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} - \frac {B^{(k)} s^{(k)}
   (s^{(k)})^T B^{(k)}} {(s^{(k)})^T B^{(k)} s^{(k)}}
   + \frac {y^{(k)} (y^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script></li>
</ul>
<h3 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h3><p>类似同秩1修正算法，仅第4步使用对称秩2修正公式</p>
<h2 id="Hesse逆修正"><a href="#Hesse逆修正" class="headerlink" title="Hesse逆修正"></a>Hesse逆修正</h2><h3 id="对称秩2修正-1"><a href="#对称秩2修正-1" class="headerlink" title="对称秩2修正"></a>对称秩2修正</h3><ul>
<li><p>考虑直接构造近似于$(\triangledown^2 f(x^{(k)}))^{-1}$的
矩阵$H^{(k)}$</p>
</li>
<li><p>这样无需求解线性方程组，直接计算</p>
<script type="math/tex; mode=display">
d^{(k)} = -H^{(k)} \triangledown f(x^{(k)})</script></li>
<li><p>相应拟Newton方程为</p>
<script type="math/tex; mode=display">
H^{(k+1)} y^{(k)} = s^{(k)}</script></li>
<li><p>可得$H^{(k)}$的对称秩1修正公式</p>
<script type="math/tex; mode=display">
H^{(k+1)} = H^{(k)} + \frac {(s^{(k)} - H^{(k)} y^{(k)})
   (s^{(k)} - H^{(k)} y^{(k)})T}
   {(s^{(k)} - H^{(k)} y^{(k)})^T y^{(k)}}</script></li>
<li><p>可得$H^{(k)}$的对称秩2修正公式/DFP公式</p>
<script type="math/tex; mode=display">
H^{(k+1)} = H^{(k)} - \frac {H^{(k)} y^{(k)} (y^{(k)})^T
   H^{(k)}} {(y^{(k)})^T H^{(k)} y^{(k)}} +
   \frac {s^{(k)} (s^{(k)})^T} {(s^{(k)})^T y^{(k)}}</script></li>
</ul>
<h4 id="DFP算法"><a href="#DFP算法" class="headerlink" title="DFP算法"></a>DFP算法</h4><p>类似BFGS算法，只是</p>
<ul>
<li>使用$H^{(k)}$计算更新方向</li>
<li>使用$H^{(k)}$的对称秩2修正公式修正</li>
</ul>
<blockquote>
<ul>
<li>对正定二次函数，BFGS算法和DFP算法效果相同</li>
<li>对一般可微（非正定二次函数），一般认为BFGS算法在收敛性质
  、数值计算方面均由于DFP算法</li>
</ul>
</blockquote>
<h3 id="Hesse逆的BFGS算法"><a href="#Hesse逆的BFGS算法" class="headerlink" title="Hesse逆的BFGS算法"></a>Hesse逆的BFGS算法</h3><ul>
<li><p>考虑</p>
<script type="math/tex; mode=display">\begin{align*}
B^{(k+1)} & = B^{(k)} + u^{(1)} (v^{(1)})^T +
   u^{(2)} (v^{(2)})^T \\
H^{(k+1)} & = (B^{(k+1)})^{-1} \\
& = (B^{(k)} + u^{(1)} (v^{(1)})^T + u^{(2)}
   (v^{(2)})^T)^{-1} \\
\end{align*}</script></li>
<li><p>两次利用<em>Sherman-Morrison</em>公式，可得</p>
<script type="math/tex; mode=display">
H^{(k+1)} = (I - \frac {s^{(k)} (y^{(k)})^T} 
   {(y^{(k)})^T s^{(k)}})
   H^{(k)}
   (I - \frac {s^{(k)} (y^{(k)})^T}
       {(y^{(k)})^T s^{(k)}})^T
   + \frac {s^{(k)} (s^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script></li>
</ul>
<h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><ul>
<li><p>还可以进一步展开</p>
<script type="math/tex; mode=display">
H^{(k+1)} = H^{(k)} + (\frac 1 {(s^{(k)})^T y^{(k)}} +
   \frac {(y^{(k)})^T H^{(k)} y^{(k)}}
   {((s^{(k)})^T y^{(k)})^2}) s^{(k)} (s^{(k)})^T
   - \frac 1 {(s^{(k)})^T y^{(k)}}
   (H^{(k)} y^{(k)} (s^{(k)})^T +
   s^{(k)} (y^{(k)})^T H^{(k)})</script></li>
</ul>
<h2 id="变度量法的基本性质"><a href="#变度量法的基本性质" class="headerlink" title="变度量法的基本性质"></a>变度量法的基本性质</h2><h3 id="算法的下降性"><a href="#算法的下降性" class="headerlink" title="算法的下降性"></a>算法的下降性</h3><h4 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h4><blockquote>
<ul>
<li>设$B^{(k)}$（$H^{(k)}$）是正定对称矩阵，且有
  $(s^{(k)})^T y^{(k)} &gt; 0$，则由BFGS（DFS）公式构造的
  $B^{(k+1)}$（$H^{(k+1)}$）是正定对称的</li>
</ul>
</blockquote>
<ul>
<li><p>考虑$B^{(k)}$对称正定，有
$B^{(k)} = (B^{(k)})^{1/2} (B^{(k)})^{1/2}$</p>
</li>
<li><p>带入利用柯西不等式即可证</p>
</li>
</ul>
<blockquote>
<ul>
<li>中间插入正定矩阵的向量内积不等式也称为广义柯西不等式</li>
</ul>
</blockquote>
<h4 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h4><blockquote>
<ul>
<li>若$d^{(k)}$v是下降方向，且<strong>一维搜索是精确的</strong>，设
  $B^{(k)}$（$H^{(k)}$）是正定对称矩阵，则有BFGS（DFP）
  公式构造的$B^{(k+1)}$（$H^{(k+1)}$）是正定对称的</li>
</ul>
</blockquote>
<ul>
<li>精确一维搜索$(d^{(k)})^T \triangledown f(x^{(k+1)}) = 0$</li>
<li>则有$(s^{(k)})^T y^{(k)} &gt; 0$</li>
</ul>
<h4 id="定理3"><a href="#定理3" class="headerlink" title="定理3"></a>定理3</h4><blockquote>
<ul>
<li>若用BFGS算法（DFP算法）求解无约束问题，设初始矩阵
  $B^{(1)}$（$H^{(1)}$）是正定对称矩阵，且一维搜索是精确的
  ，若$\triangledown f(x^{(k)}) \neq 0$，则产生搜索方向
  $d^{(k)}$是下降方向</li>
</ul>
</blockquote>
<ul>
<li>结合上2个结论，数学归纳法即可</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li><p>若每步迭代一维搜索精确，或满足$(s^{(k)})^T y^{(k)} &gt; 0$</p>
<ul>
<li>停止在某一稳定点</li>
<li>或产生严格递减的序列${f(x^{(k)})}$</li>
</ul>
</li>
<li><p>若目标函数满足一定条件我，可以证明变度量法产生的点列
${x^{(k)}}$收敛到极小点，且收敛速率超线性</p>
</li>
</ul>
<h3 id="搜索方向共轭性"><a href="#搜索方向共轭性" class="headerlink" title="搜索方向共轭性"></a>搜索方向共轭性</h3><blockquote>
<ul>
<li>用变度量法BFGS（DFP）算法求解正定二次函数</li>
</ul>
</blockquote>
<pre><code>$$
min f(x) = \frac 1 2 x^T G x + r^T x + \sigma
$$

若一维搜索是精确的，假设已经进行了m次迭代，则
</code></pre><blockquote>
<ul>
<li><p>搜索方向$d^{(1)}, \cdots, d^{(m)}$是m个非零的G共轭方向</p>
</li>
<li><p>对于$j = 1, 2, \cdots, m$，有</p>
</li>
</ul>
</blockquote>
<pre><code>$$
B^&#123;(m+1)&#125; s^&#123;(j)&#125; = y^&#123;(j)&#125;
(H^&#123;(m+1)&#125; y^&#123;(j)&#125; = s^&#123;(j)&#125;)
$$

且$m = n$时有吧

$$
B^&#123;(n+1)&#125; = G(H^&#123;(n+1)&#125; = G^&#123;-1&#125;)
$$
</code></pre><h3 id="变度量法二次终止"><a href="#变度量法二次终止" class="headerlink" title="变度量法二次终止"></a>变度量法二次终止</h3><blockquote>
<ul>
<li>若一维搜索是精确的，则变度量法（BFGS、DFP）具有二次终止</li>
</ul>
</blockquote>
<ul>
<li><p>若$\triangle f(x^{(k)}) = 0, k \leq n$，则得到最优解
$x^{(k)}$</p>
</li>
<li><p>否则得到的搜索方向是共轭的，由扩展空间子定理，
$x^{(n+1)}$是最优解</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-04-09T18:35:42.000Z" title="4/10/2019, 2:35:42 AM">2019-04-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-02T09:30:55.000Z" title="8/2/2021, 5:30:55 PM">2021-08-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a><span> / </span><a class="link-muted" href="/categories/Database/Spark/">Spark</a></span><span class="level-item">15 minutes read (About 2177 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/Spark/spark_joinreorder.html">SparkSQL2.4中启用CBO时JoinReorder分析</a></h1><div class="content"><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="Spark-Join方式"><a href="#Spark-Join方式" class="headerlink" title="Spark Join方式"></a>Spark Join方式</h3><p>SparkSQL目前支持三种join方式</p>
<ul>
<li><p><em>broadcast hash join</em>：将小表广播分发到大表所在的结点上
，并行在各节点上进行hash join</p>
<ul>
<li>仅适合内表非常小的场合</li>
</ul>
</li>
<li><p><em>shuffle hash join</em>：按照join key分区，每个结点独立并行
进行hash join</p>
<ul>
<li>类似分布式GHJ，不同块位于不同结点</li>
</ul>
</li>
<li><p><em>sort merge join</em>：按照join key分区，在各节点独立并行<em>SMJ</em></p>
<ul>
<li>spark当前shuffle算法使用<em>sort-based shuffle</em>算法</li>
<li>理论上shuffle过后各分区数据已经排序完毕，无需再次
sort，效率很高</li>
</ul>
</li>
</ul>
<h3 id="Join类型"><a href="#Join类型" class="headerlink" title="Join类型"></a>Join类型</h3><p>SparkSQL支持的Join类型可以分为以下两类</p>
<ul>
<li><p>顺序结果无关Join</p>
<ul>
<li><em>inner join</em></li>
<li><em>(full)outer join</em></li>
</ul>
</li>
<li><p>顺序结果相关Join</p>
<ul>
<li><em>left(outer) join</em></li>
<li><em>right(outer) join</em></li>
<li><em>left semi join</em></li>
<li><em>right semi join</em></li>
</ul>
</li>
</ul>
<p>考虑到JoinReorder的结果</p>
<ul>
<li><p>仅支持连接重排序的连接类型只可能是<em>inner join</em>
<em>outer join</em></p>
</li>
<li><p>而<em>outer join</em>重排序虽然不影响结果，但是处理不方便，所以
联接重排序一般仅限于<em>inner join</em>？？？</p>
<ul>
<li>有些情况下RBO可以将外联接等价转换为内联接</li>
<li>SparkSQL2.4中支持的连接重排序仅限于内连接</li>
</ul>
</li>
</ul>
<h3 id="Cost-Based-Opitimization-Optimizer"><a href="#Cost-Based-Opitimization-Optimizer" class="headerlink" title="Cost-Based Opitimization/Optimizer"></a><em>Cost-Based Opitimization/Optimizer</em></h3><p><em>CBO</em>：基于成本的优化（器）</p>
<ul>
<li><p>根据SQL的执行成本制定、优化查询作业执行计划，生成可能
的执行计划中代价最小的计划</p>
<ul>
<li>数据表统计数据<ul>
<li>基/势</li>
<li>唯一值数量</li>
<li>空值数量</li>
<li>平均、最大长度</li>
</ul>
</li>
<li>SQL执行路径I/O</li>
<li>网络资源</li>
<li>CPU使用情况</li>
</ul>
</li>
<li><p>在SparkSQL Hash Join中可以用于</p>
<ul>
<li>选择正确hash建表方</li>
<li>选择正确join类型：广播hash、全洗牌hash</li>
<li>join reorder：调整多路join顺序</li>
</ul>
</li>
<li><p>CBO本身需要耗费一定资源，需要平衡CBO和查询计划优化程度</p>
<ul>
<li>数据表的数据统计资源耗费</li>
<li>优化查询计划即时资源耗费</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>CBO是相较于<em>Rule-Based Optimization</em>的概念</li>
</ul>
</blockquote>
<h4 id="CBO中的独特概念"><a href="#CBO中的独特概念" class="headerlink" title="CBO中的独特概念"></a>CBO中的独特概念</h4><ul>
<li><p><em>cardinality</em>：集的势，结果集的行数</p>
<ul>
<li>表示SQL执行成本值</li>
<li>SQL执行返回的结果集包含的行数越多，成本越大</li>
</ul>
</li>
<li><p><em>selectivity</em>：可选择率，施加指定谓语条件后返回结果集的
记录数占未施加任何谓语条件的原始结果记录数的比率</p>
<ul>
<li>值越小，说明可选择性越好</li>
<li>值越大，说明可选择性越差，成本值越大</li>
</ul>
</li>
</ul>
<h2 id="Join-Reorder"><a href="#Join-Reorder" class="headerlink" title="Join Reorder"></a><em>Join Reorder</em></h2><p>Join Reorder：基于CBO的多表连接顺序重排</p>
<ul>
<li><p>用统计信息预估的基修正join顺序</p>
</li>
<li><p>主要涉及到以下两个方面</p>
<ul>
<li>查询代价估算</li>
<li>多表连接顺序搜索算法</li>
</ul>
</li>
</ul>
<h3 id="查询代价估计"><a href="#查询代价估计" class="headerlink" title="查询代价估计"></a>查询代价估计</h3><h4 id="代价模型"><a href="#代价模型" class="headerlink" title="代价模型"></a>代价模型</h4><ul>
<li><p>单个join操作成本</p>
<script type="math/tex; mode=display">
cost = weight * cardinality + (1 - weight)*size</script><blockquote>
<ul>
<li>carinality：对应CPU成本</li>
<li>size：对应IO成本</li>
</ul>
</blockquote>
</li>
<li><p>join树的成本是所有中间join成本总和</p>
</li>
</ul>
<h4 id="Filter-Selectivity估计"><a href="#Filter-Selectivity估计" class="headerlink" title="Filter Selectivity估计"></a><em>Filter Selectivity</em>估计</h4><p>过滤选择率：估计应用谓词表达式过滤的选择率</p>
<h5 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h5><ul>
<li><p><code>AND</code>：左侧过滤条件选择率、右侧过滤条件选择率之积</p>
<script type="math/tex; mode=display">
fs(a AND b) = fs(a) * fs(b)</script></li>
<li><p><code>OR</code>：左侧、右侧过滤条件选择率之和，减去其乘积</p>
<script type="math/tex; mode=display">
fs(a OR b) = fs(a) + fs(b) - fs(a) * fs(b)</script></li>
<li><p><code>NOT</code>：1减去原始过滤条件选择率</p>
<script type="math/tex; mode=display">
fs(NOT a) = 1.0 - fs(a)</script></li>
</ul>
<h5 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h5><ul>
<li><p><code>=</code>：等于条件</p>
<ul>
<li>若常数取值在当前列取值范围之外，则过滤选择率为0</li>
<li>否则根据柱状图、均匀分布得到过滤选择率</li>
</ul>
</li>
<li><p><code>&lt;</code>：小于条件</p>
<ul>
<li>若常数取值小于当前列最小值，则过滤选择率为0</li>
<li>否则根据柱状图、均匀分数得到过滤选择率</li>
</ul>
</li>
</ul>
<h4 id="Join-Carinality估计"><a href="#Join-Carinality估计" class="headerlink" title="Join Carinality估计"></a><em>Join Carinality</em>估计</h4><p>联接基：估计联接操作结果的基</p>
<ul>
<li><p><em>inner</em>：其他基估计值可由inner join计算</p>
<script type="math/tex; mode=display">
num(A IJ B) = \frac {num(A) * num(B)}
   {max(distinct(A.k), distinct(B.k))}</script><blockquote>
<ul>
<li><code>num(A)</code>：join操作前表A的有效记录数</li>
<li><code>distinct(A.k)</code>：表A中列k唯一值数量</li>
</ul>
</blockquote>
</li>
<li><p><em>left-outer</em>：取inner join、左表中基较大者</p>
<script type="math/tex; mode=display">
num(A LOJ B) = max(num(A IJ B), num(A))</script></li>
<li><p><em>right-outer</em>：取inner join、右表中基较大者</p>
<script type="math/tex; mode=display">
num(A ROJ B) = max(num(A IJ B), num(B))</script></li>
<li><p><em>full-outer</em></p>
<script type="math/tex; mode=display">
num(A FOJ B) = num(A ROJ B) + num(A ROJ B) - num(A IJ B)</script></li>
</ul>
<h3 id="多表连接顺序搜索算法"><a href="#多表连接顺序搜索算法" class="headerlink" title="多表连接顺序搜索算法"></a>多表连接顺序搜索算法</h3><p>SparkSQL2.4中使用动态规划算法对可能联接顺序进行搜索，从中
选择最优的联接顺序作为执行计划</p>
<ul>
<li><p>最优子结构：一旦前k个表联接顺序确定，则联接前中间表和
第k+1个表方案和前k个表的联接顺序无关</p>
</li>
<li><p>动态规划表：从单表代价开始，逐层向上计算各层多表联接代价
，直到求得所有表联接最小代价</p>
</li>
<li><p>减少搜索空间启发式想法：尽可能优先有谓词限制的内连接、
中间表</p>
</li>
</ul>
<h4 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h4><ul>
<li>优势：动态规划算法能够求得整个搜索空间中最优解</li>
<li>缺陷：当联接表数量增加时，算法需要搜索的空间增加的非常快
，计算最优联接顺序代价很高</li>
</ul>
<h2 id="PostgreSQL"><a href="#PostgreSQL" class="headerlink" title="PostgreSQL"></a>PostgreSQL</h2><h3 id="代价模型-1"><a href="#代价模型-1" class="headerlink" title="代价模型"></a>代价模型</h3><p>Postgres的查询代价估计模型基于CPU开销、IO开销，另外还增加
了<strong>启动代价</strong></p>
<script type="math/tex; mode=display">
总代价 = 启动代价 + IO代价 + CPU代价</script><h3 id="动态规划算法"><a href="#动态规划算法" class="headerlink" title="动态规划算法"></a>动态规划算法</h3><p>类似SparkSQL2.4多表连接算法（假设联接n个表）</p>
<ol>
<li><p>构造第一层关系：每个关系的最优路径就是关系的最优单表扫描
方式</p>
</li>
<li><p>迭代依次构造之后n-1层关系联接最优解</p>
<ul>
<li>左深联接树方式：将第k-1层每个关系同第1层关系联接</li>
<li>紧密树联接方式：将第m(m &gt; 2)层每个关系同第k-m层关系
联接</li>
</ul>
<p><img src="/imgs/left_deep_tree_bushy_tree.png" alt="left_deep_tree_bushy_tree"></p>
</li>
</ol>
<h3 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h3><p>遗传算法：模拟自然界生物进化过程，采用人工进化的方式对目标
空间进行搜索</p>
<ul>
<li>本质是高效、并行、全局搜索方法</li>
<li>能在搜索过程中自动获取、积累有关搜索空间的知识，并自适应
的控制搜索过程以求的最佳解</li>
</ul>
<h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><ul>
<li>将问题域中可能解看作是染色体，将其编码为符号串的形式</li>
<li>对染色体群体反复进行基于遗传学的操作：选择、交叉、变异</li>
<li>根据预定目标适应度函数对每个个体进行评价，不断得到更优
群体，从中全局并行搜索得到优化群体中最优个体</li>
</ul>
<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><h3 id="代价模型-2"><a href="#代价模型-2" class="headerlink" title="代价模型"></a>代价模型</h3><script type="math/tex; mode=display">
总代价 = IO代价 + CPU代价</script><ul>
<li>因为多表联接顺序采用贪心算法，多个表已经按照一定规则排序
（可访问元组数量升序排序）</li>
<li>所以MySQL认为，找到每个表的最小花费就是最终联接最小代价</li>
</ul>
<h3 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h3><p>贪心算法：认为每次连接表的连接方式都是最优的，即从未联接表中
选择使得下次联接代价最小者</p>
<ul>
<li><p>多表排序一般为</p>
<ul>
<li>常量表最前</li>
<li>其他表按可访问元组数量升序排序</li>
</ul>
</li>
<li><p>贪心算法得到的联接方式都是最优的</p>
<ul>
<li>则每次联接主要求解要联接表对象的最佳访问方式</li>
<li>即每次代价估计的重点在于单表扫描的代价</li>
</ul>
</li>
<li><p>求解结束后，局部最优查询计划生成</p>
<ul>
<li>得到左深树</li>
<li>最初始表位于最左下端叶子节点处</li>
</ul>
</li>
</ul>
<h2 id="优化方案"><a href="#优化方案" class="headerlink" title="优化方案"></a>优化方案</h2><p>以下分别从查询代价估计、多表连接顺序搜索算法给出方案</p>
<h3 id="查询代价估计-1"><a href="#查询代价估计-1" class="headerlink" title="查询代价估计"></a>查询代价估计</h3><ul>
<li><p>考虑在现有代价模型上增加网络通信开销</p>
<script type="math/tex; mode=display">
cost = \alpha * cardinality + \beta * size + \gamma netcost</script></li>
<li><p>在现有直方图估计选择率基础上，增加选择率估计方法</p>
<ul>
<li><p><em>Parametric Method</em>：参数方法，使用预先估计分布函数
逼近真实分布</p>
</li>
<li><p><em>Curve Fitting</em>：曲线拟合法，使用多项式函数、最小
标准差逼近属性值分布</p>
</li>
</ul>
</li>
</ul>
<h3 id="多表连接顺序搜索算法-1"><a href="#多表连接顺序搜索算法-1" class="headerlink" title="多表连接顺序搜索算法"></a>多表连接顺序搜索算法</h3><p>考虑到动态规划算法随着联接表数量增加时，计算代价过于庞大，
可以考虑引入其他算法优化多表连接顺序</p>
<ul>
<li>遗传算法</li>
<li>退火算法</li>
<li>贪心算法</li>
</ul>
<ul>
<li>遗传算法</li>
<li>退火算法</li>
<li>贪心算法</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-04-09T15:04:11.000Z" title="4/9/2019, 11:04:11 PM">2019-04-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-04-09T15:04:11.000Z" title="4/9/2019, 11:04:11 PM">2019-04-09</time></span><span class="level-item"><a class="link-muted" href="/categories/Database/">Database</a></span><span class="level-item">12 minutes read (About 1758 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Database/optimizer.html">DBMS 查询优化</a></h1><div class="content"><h2 id="查询优化技术"><a href="#查询优化技术" class="headerlink" title="查询优化技术"></a>查询优化技术</h2><p>查询优化技术：求解给定查询语句的高效执行计划过程</p>
<ul>
<li>目标：在数据库查询优化引擎生成执行策略的过程中，尽量减小
查询总开销</li>
<li><p>SQL层面上的局部优化，区别于数据库调优的全局优化</p>
</li>
<li><p>广义数据库查询优化</p>
<ul>
<li>查询重用技术</li>
<li>查询重写规则</li>
<li>查询算法优化技术</li>
<li>并行查询优化技术</li>
<li>分布式查询优化技术</li>
</ul>
</li>
<li><p>狭义数据库查询优化</p>
<ul>
<li>查询重写规则：代数/逻辑优化，RBO</li>
<li>查询算法优化技术：非代数/物理优化，CBO</li>
</ul>
</li>
</ul>
<h3 id="代数-逻辑优化"><a href="#代数-逻辑优化" class="headerlink" title="代数/逻辑优化"></a>代数/逻辑优化</h3><p>代数/逻辑优化：依据关系代数的等价变换做逻辑变换</p>
<ul>
<li>语法级：查询语句层、基于语法进行优化</li>
<li>代数级：使用形式逻辑、关系代数原理进行优化</li>
<li>语义级：根据完整性约束，对查询语句进行语义理解，推知可
优化操作</li>
</ul>
<h3 id="非代数-物理优化"><a href="#非代数-物理优化" class="headerlink" title="非代数/物理优化"></a>非代数/物理优化</h3><p>非代数/物理优化：根据数据读取、表连接方式、排序等技术对查询
进行优化</p>
<ul>
<li>物理级：物理优化技术，基于代价估计模型，比较得出各执行
方式中代价最小者<ul>
<li>查询算法优化：运用基于代价估算的多表连接算法求解最小
花费计算</li>
</ul>
</li>
</ul>
<h2 id="查询重用技术"><a href="#查询重用技术" class="headerlink" title="查询重用技术"></a>查询重用技术</h2><p>查询重用：尽可能利用先前执行的结果，以节约全过程时间、减少
资源消耗</p>
<ul>
<li>查询结果的重用：分配缓冲块存放SQL语句、最后结果集</li>
<li>查询计划的重用：缓存查询语句执行计划、相应语法树结构</li>
<li>优势：节约CPU、IO消耗</li>
<li>弊端<ul>
<li>结果集很大回消耗放大内存资源</li>
<li>同样SQL不同用户获取的结果集可能不完全相同</li>
</ul>
</li>
</ul>
<h2 id="查询重写规则"><a href="#查询重写规则" class="headerlink" title="查询重写规则"></a>查询重写规则</h2><p>查询重写：查询语句的等价转换</p>
<ul>
<li>基于关系代数，关系代数的等价变换规则为查询重写提供了理论
支持</li>
<li>查询重写后，查询优化器可能生成多个连接路径，可以从候选者
中择优</li>
</ul>
<h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><ul>
<li>将查询转换为等价、效率更高的形式<ul>
<li>低效率谓词转换为高效率谓词</li>
<li>消除重复条件</li>
</ul>
</li>
<li>将查询重写为等价、简单、不受表顺序限制的形式，为
<strong>物理查询阶段提供更多选择</strong></li>
</ul>
<h3 id="优化方向"><a href="#优化方向" class="headerlink" title="优化方向"></a>优化方向</h3><ul>
<li>过程性查询转换为描述性查询：视图重写</li>
<li>复杂查询尽可能转换为多表连接查询：嵌套子查询、外连接、
嵌套连接等</li>
<li>低效率谓词转换为高效率谓词：等价谓词重写</li>
<li>利用（不）等式性质简化<code>where</code>、<code>having</code>、<code>on</code>条件</li>
</ul>
<h2 id="查询算法优化技术"><a href="#查询算法优化技术" class="headerlink" title="查询算法优化技术"></a>查询算法优化技术</h2><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><h3 id="Rule-Based-Optimizer"><a href="#Rule-Based-Optimizer" class="headerlink" title="Rule-Based Optimizer"></a><em>Rule-Based Optimizer</em></h3><p>RBO：基于规则的优化器</p>
<ul>
<li><p>对AST/LP进行遍历，模式匹配能够满足特定规则的结点，进行
等价转换，得到等价的另一棵树</p>
<ul>
<li>剪枝：删除一些无用计算</li>
<li>合并：合并多个计算步骤</li>
</ul>
</li>
<li><p>经验式、启发式的固定<em>transformation</em>，手动设置（硬编码）
在数据库中规则决定SQL执行计划</p>
</li>
</ul>
<h4 id="经典优化规则"><a href="#经典优化规则" class="headerlink" title="经典优化规则"></a>经典优化规则</h4><ul>
<li><p><em>predicate pushdown</em>：谓词下推</p>
<p><img src="/imgs/sql_optimization_predicate_pushdown.png" alt="sql_optimization_predicate_pushdown"></p>
</li>
<li><p><em>constant folding</em>：常量累加</p>
<p><img src="/imgs/sql_optimization_constant_folding.png" alt="sql_optimization_constant_folding"></p>
</li>
<li><p><em>column pruning</em>：列值裁剪</p>
<p><img src="/imgs/sql_optimization_column_pruning.png" alt="sql_optimization_column_pruning"></p>
</li>
<li><p><em>combine limits</em>：Limits合并</p>
</li>
<li><p>inner join只访问单表：降为semi join</p>
</li>
</ul>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>操作简单、能快速确定连接方式</p>
</li>
<li><p>规则虽然有效但不敏感</p>
<ul>
<li>数据分布发生变化时，RBO是不感知的</li>
</ul>
</li>
<li><p>基于RBO生成的执行计划不能确保是最优的</p>
<ul>
<li>启发式规则只能排除一些明显不好的存取路径</li>
</ul>
</li>
</ul>
<h3 id="Cost-Base-Optimizer"><a href="#Cost-Base-Optimizer" class="headerlink" title="Cost-Base Optimizer"></a><em>Cost-Base Optimizer</em></h3><p>CBO：基于成本的优化器</p>
<ul>
<li><p>根据SQL的执行成本制定、优化查询作业执行计划，生成可能
的执行计划中代价最小的计划</p>
<ul>
<li>数据表统计数据<ul>
<li>基/势</li>
<li>唯一值数量</li>
<li>空值数量</li>
<li>平均、最大长度</li>
</ul>
</li>
<li>SQL执行路径I/O</li>
<li>网络资源</li>
<li>CPU使用情况</li>
</ul>
</li>
<li><p>以上执行信息获取方式取决于不同平台、数据库</p>
<ul>
<li>执行SQL前抽样分析数据</li>
<li>每次执行SQL都会记录统计信息</li>
</ul>
</li>
<li><p>特殊概念</p>
<ul>
<li><p><em>cardinality</em>：集的势，结果集的行数</p>
<ul>
<li>表示SQL执行成本值</li>
<li>SQL执行返回的结果集包含的行数越多，成本越大</li>
</ul>
</li>
<li><p><em>selectivity</em>：可选择率，施加指定谓语条件后返回
结果集的记录数占未施加任何谓语条件的原始结果集记录数
的比率</p>
<ul>
<li>值越小，说明可选择性越好</li>
<li>值越大，说明可选择性越差，成本值越大</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="常见优化规则"><a href="#常见优化规则" class="headerlink" title="常见优化规则"></a>常见优化规则</h4><ul>
<li>hash-join<ul>
<li>选择正确hash建表方</li>
<li>选择正确join类型：广播hash、全洗牌hash</li>
<li>join reorder：调整多路join顺序<ul>
<li>尽量减小中间shuffle数据集大小，达到最优输出</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h4><ul>
<li>对各种可能情况进行量化比较，可以得到花费最小的情况</li>
<li>CBO本身需要耗费一定资源，需要平衡CBO和查询计划优化程度<ul>
<li>数据表的数据统计资源耗费</li>
<li>优化查询计划即时资源耗费，如果组合情况比较多则花费
判断时间较多</li>
</ul>
</li>
</ul>
<h2 id="并行查询优化技术"><a href="#并行查询优化技术" class="headerlink" title="并行查询优化技术"></a>并行查询优化技术</h2><p>并行数据库系统中查询优化的目标：寻找具有最小响应时间的查询
执行计划</p>
<ul>
<li><p>具有最小执行代价的计划不一定具有最快相应时间，需要考虑
把查询工作分解为可以并行运行的子工作</p>
</li>
<li><p>查询能否并行取决于</p>
<ul>
<li>系统中可用资源</li>
<li>CPU数目</li>
<li>运算中特定代数运算符</li>
</ul>
</li>
<li><p>查询并行可以分为</p>
<ul>
<li><p>操作内并行：将同一操作如单表扫描、两表连接、排序操作
等分解为多个独立子操作</p>
</li>
<li><p>操作间并行：一条SQL查询语句分解为多个子操作</p>
</li>
</ul>
</li>
</ul>
<h2 id="分布式查询优化技术"><a href="#分布式查询优化技术" class="headerlink" title="分布式查询优化技术"></a>分布式查询优化技术</h2><p>分布式数据库系统：查询策略优化、局部处理优化是查询优化重点</p>
<ul>
<li><p>查询策略优化：主要是数据传输策略优化</p>
<ul>
<li>主要考虑因素：数据的通信开销</li>
<li>主要目标：以减少传输次数、数据量</li>
</ul>
</li>
<li><p>局部处理优化：传统单结点数据库的查询优化技术</p>
</li>
<li><p>代价估计模型：总代价 = IO代价 + CPU代价 + 通信代价</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:25:02.000Z" title="8/4/2021, 11:25:02 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">11 minutes read (About 1640 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/conjugate_gradient.html">Conjugate Gradient Method</a></h1><div class="content"><h2 id="共轭方向"><a href="#共轭方向" class="headerlink" title="共轭方向"></a>共轭方向</h2><blockquote>
<ul>
<li><p>设G为$n * n$阶正定对称矩阵，若$d^{(1)}, d^{(2)}$满足</p>
<script type="math/tex; mode=display">(d^{(1)})^T G d^{(2)} = 0</script><p>  则称$d^{(1)}, d^{(2)}$关于G共轭</p>
</li>
<li><p>类似正交方向，若$d^{(1)},\cdots,d^{(k)}(k \leq n)$关于
  G两两共轭，则称其为G的k个共轭方向</p>
</li>
</ul>
</blockquote>
<ul>
<li>特别的，$G=I$时，共轭方向就是正交方向</li>
</ul>
<h3 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h3><blockquote>
<ul>
<li>设目标函数为<script type="math/tex; mode=display">
  f(w) = \frac 1 2 w^T w + r^T w + \sigma</script>  $q^{(1)}, \cdots, q^{(k)}$是$k, k \leq n$个非零正交方向
  ，从任意初始点$w^{(1)}$出发，依次沿着以上正交方向做
  <strong>精确一维搜索</strong>，得到$w^{(1)}, \cdots, w^{(k+1)}$，
  则$w^{(k+1)}$是$f(w)$在线性流形<script type="math/tex; mode=display">
  \bar W_k = \{w = w^{(1)} + \sum_{i=1}^k \alpha_i q^{(i)}
      | -\infty < \alpha_i < +\infty \}</script>  上的唯一极小点，特别的k=n时，$w^{(n+1)}$是$f(w)$在整个
  空间上的唯一极小点</li>
</ul>
</blockquote>
<ul>
<li><p>$\bar W_k$上的存在唯一极小点$\hat w^{(k)}$，在所有方向
都是极小点，所以有</p>
<script type="math/tex; mode=display">
<\triangledown f(\hat w^{(k)}), q^{(i)}> = 0, i=1,2,..</script></li>
<li><p>将$\hat w^{(k)}$由正交方向表示带入梯度，求出系数表达式</p>
</li>
<li><p>解精确搜索步长，得到$w^{(k+1)}$系数表达式</p>
</li>
</ul>
<h3 id="扩展子空间定理"><a href="#扩展子空间定理" class="headerlink" title="扩展子空间定理"></a>扩展子空间定理</h3><blockquote>
<ul>
<li>设目标函数为<script type="math/tex; mode=display">
  f(w) = \frac 1 2 x^T G x + r^T x + \sigma</script>  $d^{(1)}, \cdots, d^{(k)}$是$k, k \leq n$个非零正交方向
  ，从任意初始点$x^{(1)}$出发，依次沿着以上正交方向做
  <strong>精确一维搜索</strong>，得到$x^{(1)}, \cdots, x^{(k+1)}$，
  则$x^{(k+1)}$是$f(x)$在线性流形<script type="math/tex; mode=display">
  \bar x_k = \{x = x^{(1)} + \sum_{i=1}^k \alpha_i d^{(i)}
      | -\infty < \alpha_i < +\infty \}</script>  上的唯一极小点，特别的k=n时，$x^{(n+1)}$是$f(x)$在整个
  空间上的唯一极小点</li>
</ul>
</blockquote>
<ul>
<li>引进变换$w = \sqrt G x$即可证</li>
</ul>
<blockquote>
<ul>
<li>在以上假设下，有<script type="math/tex; mode=display">
  <\triangledown f(x^{(k+1)}), d^{(i)}> = 0, i=1,2...</script></li>
</ul>
</blockquote>
<h2 id="Conjugate-Gradient-Method"><a href="#Conjugate-Gradient-Method" class="headerlink" title="Conjugate Gradient Method"></a><em>Conjugate Gradient Method</em></h2><p>共轭梯度法</p>
<h3 id="对正定二次函数函数"><a href="#对正定二次函数函数" class="headerlink" title="对正定二次函数函数"></a>对正定二次函数函数</h3><script type="math/tex; mode=display">
f(x) = \frac 1 2 x^T G x + r^T x + \sigma</script><ul>
<li><p>任取初始点$x^{(1)}$，若$\triangledown f(x^{(1)}) = 0$，
停止计算，得到极小点$x^{(1)}$，否则取</p>
<script type="math/tex; mode=display">
d^{(1)} = -\triangledown f(x^{(1)})</script></li>
<li><p>沿着$d^{(1)}$方向进行精确一维搜索得到$x^{(2)}$，若
$\triangledown f(x^{(2)}) \neq 0$，令</p>
<script type="math/tex; mode=display">
d^{(2)} = -\triangledown f(x^{(2)}) + \beta_1^{(2)}
   d^{(1)}</script><p>且满足$(d^{(1)})^T G d^{(2)} = 0$，即二者共轭，可得</p>
<script type="math/tex; mode=display">
\beta_1^{(2)} = \frac {(d^{(1)})^T G \triangledown
   f(x^{(2)})} {((d^{(1)})^T G d^{(1)})}</script><ul>
<li>这里$d^{(2)}$方向的构造方式是为类似构造后面$d^{(k)}$
，得到能方便表示的系数</li>
<li>类似于将向量组$\triangledown f(x^{(i)})$正交化</li>
</ul>
</li>
<li><p>如此重复搜索，若$\triangledown f^(x^{i)}) \neq 0$，构造
$x^{(k)}$处搜索方向$d^{(k)}$如下</p>
<script type="math/tex; mode=display">\begin{align*}
0 & = (d^{(i)})^T G d^{(k)} \\
& = -(d^{(i)})T G \triangledown f(x^{(k)}) +
   \sum_{j=1}^{k-1} \beta_j^{(k)} (d^{(i)})^T G d^{(j)} \\
& = -(d^{(i)})^T G \triangledown f(x^{(k)}) +
   \beta_i^{(k)} (d^{(i)})^T G d^{(i)}
\end{align*}</script><p>可得</p>
<script type="math/tex; mode=display">
\beta_i^{(k)} = \frac {(d^{(i)})^T G \triangledown
   f(x^{(k)})} {(d^{(i)})^T G d^{(i)}}</script><p>此时$d^{(k)}$与前k-1个方向均关于G共轭，此k个方向是G的k个
共轭方向，由扩展空间子定理，$x^{(k+1)}$是整个空间上极小</p>
</li>
</ul>
<h4 id="计算公式简化"><a href="#计算公式简化" class="headerlink" title="计算公式简化"></a>计算公式简化</h4><p>期望简化$d^{(k)}$的计算公式</p>
<ul>
<li><p>由扩展子空间定理推论有
$\triangledown f(x^{(k)})^T d^{(i)} = 0, i=1,2…,k-1$
结合以上$d^{(k)}$的构造公式，有</p>
<script type="math/tex; mode=display">\begin{align*}
& \triangledown f(x^{(k)})^T \triangledown f(x^{(i)}) \\
= & \triangledown f(x^{(k)})^T ( -d^{(i)} +
   \beta_1^{(i)} d^{(1)} + \cdots +
   \beta_{i-1}^{(i)} d^{(i-1)} ) \\
= & 0, i=1,2,...,k-1
\end{align*}</script></li>
<li><p>则有</p>
<script type="math/tex; mode=display">\begin{align*}
(d^{(i)})^T G \triangledown f(x^{(k)}) & =
   \triangledown f(x^{(k)})^T G d^{(i)} \\
& = \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   G (x^{(i+1)} - x^{(i)}) \\
& = \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   (\triangledown f(x^{(i+1)}) -
   \triangledown f(x^{(i)})) \\
& = 0, i=1,2,\cdots,k-2
\end{align*}</script><blockquote>
<ul>
<li>$d^{(k)} = \frac 1 {\alpha_i} x^{(i+1)} - x^{(i)}$</li>
</ul>
</blockquote>
</li>
<li><p>所以上述$d^{(k)}$构造公式可以简化为</p>
<script type="math/tex; mode=display">
d^{(k)} = -\triangledown f(x^{(k)}) + \beta_{k-1}
   d^{(k-1)}</script></li>
<li><p>类似以上推导有</p>
<script type="math/tex; mode=display">\begin{align*}
(d^{(k-1)})^T G \triangledown f(x^{(k)}) & =
   \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)})) \\
& = \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   \triangledown f(x^{(k)}) \\
\end{align*}</script><script type="math/tex; mode=display">\begin{align*}
(d^{(k-1)})^T G d^{(k-1)} & = \frac 1 {\alpha_i}
   (d^{(k-1)})^T (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)})) \\
& = -\frac 1 {\alpha_i} (d^{(k-1)})^T
   \triangledown f(x^{(x-1)}) \\
& = -\frac 1 {\alpha_i} (\triangledown f(x^{(k-1)}) -
   \beta_{k-2}d^{(k-2)})^T \triangledown f(x^{(x-1)}) \\
& = -\frac 1 {\alpha_i} \triangledown f(x^{(k-1)})^T
   \triangledown f(x^{(k-1)})
\end{align*}</script><p>最终的得到简化后系数$\beta_{k-1}, k&gt;1$的PRP公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\triangledown f(x^{(k)})^T
   (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)}))}
   {\triangledown f(x^{(k-1)})^T
       \triangledown f(x^{(k-1)})}</script><p>或FR公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\|\triangledown f(x^{(k)})\|^2}
   {\|\triangledown f(x^{(k-1)}) \|^2}</script></li>
</ul>
<blockquote>
<ul>
<li><p>以上推导虽然是根据正定二次函数得出的推导，但是仍适用于
  一般可微函数</p>
</li>
<li><p>$\beta _ {k-1}$给出两种计算方式，应该是考虑到目标函数
  可能不是标准正定二次函数、一维搜索数值计算不精确性</p>
</li>
<li><p>将$\beta _ {k-1}$分子、分母推导到不同程度可以得到其他
  公式</p>
</li>
</ul>
</blockquote>
<ul>
<li><p>Growder-Wolfe公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\triangledown f(x^{(k)})^T
   (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)}))}
   {(d^{(k-1)})^T (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)}))}</script></li>
<li><p>Dixon公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\triangledown f(x^{(k)})^T
   \triangledown f(x^{(k)})}
   {(d^{(k-1)})^T \triangledown f(x^{(k-1)})}</script></li>
</ul>
<h3 id="FR-PRP算法"><a href="#FR-PRP算法" class="headerlink" title="FR/PRP算法"></a>FR/PRP算法</h3><ol>
<li><p>初始点$x^{(1)}$、精度要求$\epsilon$，置k=1</p>
</li>
<li><p>若$|\triangledown f(x^{(k)}) | \leq \epsilon$，停止
计算，得到解$x^{(k)}$，否则置</p>
<script type="math/tex; mode=display">
d^{(k)} = -\triangledown f(x^{(k)}) + \beta_{k-1}d^{(k-1)}</script><p>其中$\beta_{k-1}=0, k=1$，或由上述公式计算</p>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) = f(x^{(k)} -
  \alpha d^{(k)})</script><p>得$\alpha_k$，置$x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}$</p>
</li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
<blockquote>
<ul>
<li>实际计算中，n步重新开始的FR算法优于原始FR算法</li>
<li>PRP算法中
  $\triangledown f(x^{(k)}) \approx \triangledown f(x^{(k-1)})$
  时，有$\beta_{k-1} \approx 0$，即
  $d^{(k)} \approx -\triangledown f(x^{(k)})$，自动重新开始</li>
<li>试验表明，对大型问题，PRP算法优于FR算法</li>
</ul>
</blockquote>
<h3 id="共轭方向下降性"><a href="#共轭方向下降性" class="headerlink" title="共轭方向下降性"></a>共轭方向下降性</h3><blockquote>
<ul>
<li>设$f(x)$具有连续一阶偏导，假设一维搜索是精确的，使用共轭
  梯度法求解无约束问题，若$\triangledown f(x^{(k)}) \neq 0$
  则搜索方向$d^{(k)}$是$x^{(k)}$处的下降方向</li>
</ul>
</blockquote>
<ul>
<li>将$d^{(k)}$导入即可</li>
</ul>
<h3 id="算法二次终止性"><a href="#算法二次终止性" class="headerlink" title="算法二次终止性"></a>算法二次终止性</h3><blockquote>
<ul>
<li>若一维搜索是精确的，则共轭梯度法具有二次终止性</li>
</ul>
</blockquote>
<ul>
<li><p>对正定二次函数，共轭梯度法至多n步终止，否则</p>
<ul>
<li>目标函数不是正定二次函数</li>
<li>或目标函数没有进入正定二次函数区域，</li>
</ul>
</li>
<li><p>此时共轭没有意义，搜索方向应该重新开始，即令</p>
<script type="math/tex; mode=display">
d^{(k)} = -\triangledown f(x^{(k)})</script><p>即算法每n次重新开始一次，称为n步重新开始策略</p>
</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/tags/Optimization/">Previous</a></div><div class="pagination-next"><a href="/tags/Optimization/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/tags/Optimization/">1</a></li><li><a class="pagination-link is-current" href="/tags/Optimization/page/2/">2</a></li><li><a class="pagination-link" href="/tags/Optimization/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6371777973" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>