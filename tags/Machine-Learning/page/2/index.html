<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: Machine Learning - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"Hexo","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Machine Learning</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-29T10:12:36.000Z" title="8/29/2019, 6:12:36 PM">2019-08-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-08-29T10:12:43.000Z" title="8/29/2019, 6:12:43 PM">2019-08-29</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">a few seconds read (About 1 word)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/lightgbm.html">LightGBM</a></h1><div class="content"><h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-26T01:53:43.000Z" title="8/26/2019, 9:53:43 AM">2019-08-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-08-26T01:53:48.000Z" title="8/26/2019, 9:53:48 AM">2019-08-26</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">10 minutes read (About 1504 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/online_optimization.html">在线最优化</a></h1><div class="content"><h2 id="Truncated-Gradient"><a href="#Truncated-Gradient" class="headerlink" title="Truncated Gradient"></a>Truncated Gradient</h2><h3 id="L1正则化法"><a href="#L1正则化法" class="headerlink" title="L1正则化法"></a>L1正则化法</h3><p>L1正则化</p>
<script type="math/tex; mode=display">
w^{(t+1)} = w^{(t)} - \eta^{(t)}g^{(t)} - \eta^{(t)} \lambda sgn(w^{(t)})</script><blockquote>
<ul>
<li>$\lambda$：正则化项参数</li>
<li>$sgn$：符号函数</li>
<li>$g^{(t)}=\nabla_w L(w^{(t)}, Z^{(t)})$：损失函数对参数梯度</li>
</ul>
</blockquote>
<ul>
<li>L1正则化项在0处不可导，每次迭代使用次梯度计算正则项梯度</li>
<li>OGD中每次根据观测到的一个样本进行权重更新
（所以后面正则项次梯度只考虑非0处？？？）</li>
</ul>
<h3 id="简单截断法"><a href="#简单截断法" class="headerlink" title="简单截断法"></a>简单截断法</h3><p>简单截断法：以$k$为窗口，当$t/k$非整数时，使用标准SGD迭代，
否则如下更新权重</p>
<script type="math/tex; mode=display">\begin{align*}
w^{(t+1)} & = T_0 (w^{(t)} - \eta^{(t)} G^{(t)}, \theta) \\

T_0(v_i, \theta) & = \left \{ \begin{array}{l}
    0, & |v_i| \leq \theta \\
    v_i, & otherwise
\end{array} \right.
\end{align*}</script><blockquote>
<ul>
<li>$w^{(t)}$：模型参数</li>
<li>$g^{(t)}$：损失函数对模型参数梯度</li>
<li>$T_0$：截断函数</li>
<li>$\theta$：控制参数稀疏性</li>
</ul>
</blockquote>
<h3 id="截断梯度法"><a href="#截断梯度法" class="headerlink" title="截断梯度法"></a>截断梯度法</h3><p>截断梯度法：以$k$为窗口，当$t/k$非整数时，使用标准SGD迭代，
否则如下更新权重</p>
<script type="math/tex; mode=display">\begin{align*}
w^{(t+1)} & = T_1(w^{(t)} - \eta^{(t)} g^{(t)}, \lambda^{(t)} \eta^{(t)},
    \theta) \\

T_1(v_i, \alpha, \theta) & = \left \{ \begin{array}{l}
    max(0, v_i - \alpha), & v_i \in [0, \theta] \\
    min(0, v_1 + \alpha), & v_i \in [-\theta, 0] \\
    v_i, & otherwise
\end{array} \right.
\end{align*}</script><blockquote>
<ul>
<li>$\lambda, \theta$：控制参数$w$稀疏性</li>
</ul>
</blockquote>
<ul>
<li><p>对简单截断的改进，避免在实际（OgD）中参数因训练不足过小
而被错误截断，造成特征丢失</p>
<p><img src="/imgs/truncated_gradient_compared_with_l1.png" alt="truncated_gradient_compared_with_l1"></p>
</li>
</ul>
<h2 id="Forward-Backward-Spliting"><a href="#Forward-Backward-Spliting" class="headerlink" title="Forward-Backward Spliting"></a>Forward-Backward Spliting</h2><p>FOBOS：前向后向切分，权重更新方式为<em>proximal method</em>如下</p>
<script type="math/tex; mode=display">\begin{align*}
w^{(t.5)} & = w^{(t)} - \eta^{(t)} g^{(t)} \\
w^{(t+1)} & = \arg\min_w \{ \frac 1 2 \|w - w^{(t.5)}\|
    + \eta^{(t+0.5)} \Phi(w) \} \\
& = \arg\min_w \{ \frac 1 2 \|w - w^{(t)} + \eta^{(t)} g^{(t)}\|
    + \eta^{(t+0.5)} \Phi(w) \}
\end{align*}</script><h3 id="L1-FOBOS"><a href="#L1-FOBOS" class="headerlink" title="L1-FOBOS"></a>L1-FOBOS</h3><p>L1-FOBOS：即令$Phi(w)=\lambda |w|_1$，则根据可加性如下</p>
<script type="math/tex; mode=display">\begin{align*}
w^{(t+1)} & = \arg\min_w \sum_{i=1}^N (\frac 1 2 (w_i - v_i)^2
    + \tilde \lambda |w_i|)
w_i^{(t+1)} = \arg\min_{w_i} (\frac 1 2 (w_i - v_i)^2
    + \tilde \lambda |w_i|)
\end{align*}</script><blockquote>
<ul>
<li>$V=[v_1, v_2, \cdots, v_N]:=w^{(t.5)}$：为方便</li>
<li>$\tilde \lambda := \eta^{t.5} \lambda$：为方便</li>
<li>$\eta^{t.5}$：学习率，常取
  $\eta^{(t)} \in \theta(\frac 1 {\sqrt t})$</li>
</ul>
</blockquote>
<ul>
<li><p>则对$w_i$求次梯度、分类讨论，解得</p>
<script type="math/tex; mode=display">
w_i^{(t+1)} = \left \{ \begin{array}{l}
   v_i - \tilde \lambda, & v_i > \tilde \lambda \\
   0, & |v_i| < \tilde \lambda \\
   v_i + \tilde \lambda, & v_i < -\tilde \lambda
\end{array} \right.</script><ul>
<li><p>可以理解为：到当前样本为止，维度权重小于阈值
$\eta^{(t.5)} \lambda$）时，认为该维度不够重要，
权重置为0</p>
</li>
<li><p>可视为$k=1, \theta=\infty$的Tg算法</p>
</li>
</ul>
</li>
<li><p>另外，显然有$w_i^{(t+1)} v_i \geq 0$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac 1 2 (w_i^{(t+1)} - v_i)^2 + \tilde \lambda |w_i^{(t+1)}|
& = \frac 1 2((w_i^{(t+1)})^2 - 2 w_i^{(t+1)} v_i + v_i^2)
   + \tilde \lambda |w_i^{(t+1)}| \\
& \leq \frac 1 2 v_i^2
\end{align*}</script><blockquote>
<ul>
<li>考虑$w_i^{(t+1)}$使得目标函数最小，带入$w=0$则得</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="Regularized-Dual-Averaging"><a href="#Regularized-Dual-Averaging" class="headerlink" title="Regularized Dual Averaging"></a>Regularized Dual Averaging</h2><p>RDA算法：正则对偶平均算法，权重更新方式为
<strong>包含[增广]正则项的最速下降</strong></p>
<script type="math/tex; mode=display">
w^{(t+1)} = \arg\min_w {\frac 1 t \sum_{r=1}^t g^{(r)} w + \Phi(w)
    + \frac {\beta^{(t)}} t h(w)}</script><ul>
<li><p>目标函数包括三个部分</p>
<ul>
<li>$\frac 1 t \sum_{r=1}^t g^{(r)} w$：包含之前所有梯度
均值</li>
<li>$\Phi(w)$：正则项</li>
<li>$\frac {\beta^{(t)}} t h(w)$：额外正则项，严格凸，且
不影响稀疏性</li>
</ul>
</li>
<li><p>相较于TG、FOBOS是从另一方面求解在线最优化，更有效地提升
特征权重稀疏性</p>
</li>
</ul>
<h3 id="L1-RDA"><a href="#L1-RDA" class="headerlink" title="L1 RDA"></a>L1 RDA</h3><p>L1 RDA：令$\Phi(w) := \lambda |w|_1$，
再设$h(w) := |w|_2^2$，根据可加性则有</p>
<script type="math/tex; mode=display">\begin{align*}
w^{(t+1)} & = \arg\min_w \{ \frac 1 t \sum_{r=1}^t <g^{(t)}, w>
    + \lambda \|w\|_1 + \frac {\gamma} {2\sqrt t} \|w\|_2^2 \} \\
w_i^{(t+1)} & = \arg\min_{w_i} \{bar g_i^{(t)} w_i + \lambda |w_i|
    \frac {\gamma} {2 \sqrt t} w_i^2 \}
\end{align*}</script><blockquote>
<ul>
<li>$\lambda &gt; 0, \gamma &gt; 0$</li>
<li>$\bar g<em>i^{(t)} = \frac 1 t \sum</em>{r=1}^t g_i^{(r)}$</li>
</ul>
</blockquote>
<ul>
<li><p>对$w_i$求次梯度、置零、求解得</p>
<script type="math/tex; mode=display">
w_i^{(t+1)} = \left \{ \begin{array}{l}
   -\frac {\sqrt t} {\gamma} (\bar g^{(t)} - \lambda),
       & \bar g_i^{(t)} > \lambda \\
   0, & |\bar g_i^{(t)}| \leq \lambda \\
   -\frac {\sqrt t} {\gamma} (\bar g^{(t)} + \lambda),
       & \bar g_i^{(t)} < -\lambda \\
\end{array} \right.</script><ul>
<li>可以理解为：某维度梯度累计均值绝对值$|bar g_i^{(t)}$
小于阈值$\lambda$时，对应权重被置零、产生稀疏性</li>
</ul>
</li>
<li><p>相较于L1-FOBOS的截断</p>
<ul>
<li>截断阈值为常数，更加激进、容易产生稀疏性</li>
<li>截断判断对象为梯度累加均值，避免由于训练不足而产生
截断</li>
<li>只需条件$\lambda$参数，容易权衡精度、稀疏性</li>
</ul>
</li>
</ul>
<h2 id="Follow-the-Regularized-Leader"><a href="#Follow-the-Regularized-Leader" class="headerlink" title="Follow the Regularized Leader"></a>Follow the Regularized Leader</h2><p>FTRL：综合考虑L1-RDA、L1-FOBOS</p>
<h3 id="L1-FOBOS、L1-RDA变换"><a href="#L1-FOBOS、L1-RDA变换" class="headerlink" title="L1-FOBOS、L1-RDA变换"></a>L1-FOBOS、L1-RDA变换</h3><ul>
<li><p>将L1-FOBOS类似近端算法收敛证明中展开、去除无关项、放缩，
得到类似L1-RDA目标函数</p>
<script type="math/tex; mode=display">\begin{align*}
w^{(t+1)} & = \arg\min_w \{ \frac 1 2 \|w - w^{(t)} +
   \eta^{(t)} g^{(t)}\| + \eta^{(t)} \lambda \|w\|_1 \} \\
& = \arg\min_w \{ g^{(t)} w + \lambda \|w\|_1 +
   \frac 1 {2 \eta^{(t)}} \|w - w^{(t)}\|_2^2 \}
\end{align*}</script></li>
<li><p>将L1-RDA目标函数整体整体放缩，得到</p>
<script type="math/tex; mode=display">
w^{(t+1)} = \arg\min_w \{ g^{(1:t)} w + t \lambda \|w\|_1
   + \frac 1 {2\eta^{(t)}} \|w - 0\|_2^2 \}</script><blockquote>
<ul>
<li>$g^{(1:t)} := \sum_{r=1}^t g^{(r)}$</li>
</ul>
</blockquote>
</li>
<li><p>FTRL综合考虑L1-FOBOS、L1-RDA，得到目标函数</p>
<script type="math/tex; mode=display">
w^{(t+1)} = \arg\min_w \{ g^{(1:t)} W + \lambda_1 \|w\|_1
   + \frac {\lambda_2} 2 \|w\|_2^2 + \frac 1 2
   \sum_{r=1}^t \sigma^{(r)} \|w - w^{(r)}\|_2^2 \}</script><ul>
<li>使用累加梯度更新，避免因训练不充分错误截断</li>
<li>包含L1-FOBOS、L1-RDA全部正则化项</li>
</ul>
</li>
</ul>
<h3 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h3><ul>
<li><p>将FTRL中最后一项拆分、去除无关项</p>
<script type="math/tex; mode=display">\begin{align*}
w^{(t+1)} & = \arg\min_w \{(g^{(1:t)} - \sum_{r=1}^t
   \sigma^{(r)} w^{(r)})w + \lambda_1 \|w\|_1 +
   \frac 1 2 (\lambda_2 + \sum_{r=1}^t \sigma^{(r)})
   \|w\|_2^2 + \frac 1 2 \sum_{r=1}^t \sigma^{(r)}
   \|w^{(r)}\|_2^2 \} \\
& = \arg\min_w \{ z^{(t)} w + \lambda_1 \|w\|_1
   + \frac 1 2 (\lambda_2 + \sum_{r=1}^t \sigma^{(r)})
   \|w\|_2^2 \} \\
z^{(t)} &= g^{(1:t)} - \sum_{r=1}^t \sigma^{(r)} w^{(r)}
\end{align*}</script></li>
<li><p>则同样根据可加性，对各分量求次梯度、置零、求解得</p>
<script type="math/tex; mode=display">
w_i^{(t+1)} = \left \{ \begin{array}{l}
   \frac 1 {\lambda_1 + \sum_{r=1}^t \sigma^{(r)}}
       (z_i^{(t)} - \lambda_1 z_i), & z_i > \lambda_1 \\
   0, & |z_i^{(t)}| \leq \lambda_1 \\
   \frac 1 {\lambda_1 + \sum_{r=1}^t \sigma^{(r)}}
       (z_i^{(t)} + \lambda_1 z_i), & z_i < -\lambda_1 \\
\end{array} \right.</script></li>
<li><p>其中学习率$\eta$为类似Adagrad优化器的学习率，但包括可学习
参数$\alpha, \beta$</p>
<script type="math/tex; mode=display">
\eta_i^{(t)} = \frac {\alpha} {\beta + \sqrt{\sum_{r=1}^t
   (g_i^{(r)})^2}}</script></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-25T13:53:20.000Z" title="8/25/2019, 9:53:20 PM">2019-08-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T09:40:21.000Z" title="8/4/2021, 5:40:21 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Loss/">Loss</a></span><span class="level-item">21 minutes read (About 3154 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Loss/loss_thoery.html">损失函数理论</a></h1><div class="content"><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><ul>
<li><p>矩估计：<strong>建立参数和总体矩的关系</strong>，求解参数</p>
<ul>
<li>除非参数本身即为样本矩，否则基本无应用价值</li>
<li>应用场合<ul>
<li>均值：对应二次损失 $\arg\min<em>{\mu} \sum</em>{i=1}^N (x_i - \mu)^2$</li>
<li>方差：对应二次损失?</li>
</ul>
</li>
</ul>
</li>
<li><p>极大似然估计：极大化似然函数，求解概率上最合理参数</p>
<ul>
<li>需知道（假设）总体 <strong>概率分布形式</strong></li>
<li>似然函数形式复杂，求解困难<ul>
<li>往往无法直接给出参数的解析解，只能求数值解</li>
</ul>
</li>
<li>应用场合<ul>
<li>估计回归参数：对数损失
$\mathop{\arg\min}<em>{\beta} \sum</em>{i=1}^N lnP(y_i|x_i, \beta)$</li>
</ul>
</li>
</ul>
</li>
<li><p>损失函数估计：极小化损失函数，求解损失最小的参数</p>
<ul>
<li>最泛用的参数求解方法<ul>
<li>适合求解有大量参数的待求解的问题</li>
<li>往往通过迭代方式逐步求解</li>
</ul>
</li>
<li>特别的<ul>
<li>线性回归使用 <em>MSE</em> 作为损失函数时，也被称为最小二乘估计</li>
<li>极大似然估计同对数损失函数</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>参数估计都可以找到合适损失函数，通过迭代求解损失最小化</li>
</ul>
</blockquote>
<h3 id="随机模拟估计参数"><a href="#随机模拟估计参数" class="headerlink" title="随机模拟估计参数"></a>随机模拟估计参数</h3><ul>
<li>需要<strong>设计随机模拟实验</strong>估计参数</li>
<li>应用场合<ul>
<li>蒙特卡洛类似算法：随机化损失</li>
</ul>
</li>
</ul>
<h3 id="迭代求解参数"><a href="#迭代求解参数" class="headerlink" title="迭代求解参数"></a>迭代求解参数</h3><ul>
<li><p>损失函数定义不同</p>
<ul>
<li>包含样本量数量不同</li>
<li>惩罚项设置不同</li>
</ul>
</li>
<li><p>异步更新参数</p>
<ul>
<li>同时求解参数数量：全部、部分、单个</li>
<li>参数升维</li>
</ul>
</li>
<li><p>更新方向</p>
<ul>
<li>梯度</li>
<li>海瑟矩阵</li>
<li>次梯度</li>
</ul>
</li>
<li><p>更新方式</p>
<ul>
<li>叠加惯性</li>
<li>动态学习率</li>
</ul>
</li>
</ul>
<h2 id="Loss-Models"><a href="#Loss-Models" class="headerlink" title="Loss Models"></a><em>Loss Models</em></h2><p>模型（目标函数）在样本整体的损失：度量模型整体预测效果</p>
<ul>
<li>代表模型在整体上的性质，有不同的设计形式</li>
<li><p>可以用于 <strong>设计学习策略、评价模型</strong></p>
<ul>
<li>风险函数</li>
<li>评价函数</li>
</ul>
</li>
<li><p>有时在算法中也会使用整体损失</p>
</li>
</ul>
<h3 id="Expected-Risk-Expected-Loss-Generalization-Loss"><a href="#Expected-Risk-Expected-Loss-Generalization-Loss" class="headerlink" title="Expected Risk / Expected Loss / Generalization Loss"></a><em>Expected Risk</em> / <em>Expected Loss</em> / <em>Generalization Loss</em></h3><p>期望风险（函数）：损失函数 $L(Y, f(X))$（随机变量）期望</p>
<script type="math/tex; mode=display">
R_{exp}(f) = E_p[L(Y, f(X))] = \int_{x*y} L(y,f(x))P(x,y) dxdy</script><blockquote>
<ul>
<li>$P(X, Y)$：随机变量 $(X, Y)$ 遵循的联合分布，未知</li>
</ul>
</blockquote>
<ul>
<li><p>风险函数值度量模型预测错误程度</p>
<ul>
<li>反映了学习方法的泛化能力</li>
<li>评价标准（<strong>监督学习目标</strong>）就应该是选择期望风险最小</li>
</ul>
</li>
<li><p>联合分布未知，所以才需要学习，否则可以直接计算条件分布概率，而计算期望损失需要知道联合分布，因此监督学习是一个病态问题</p>
</li>
</ul>
<h3 id="Empirical-Risk-Empirical-Loss"><a href="#Empirical-Risk-Empirical-Loss" class="headerlink" title="Empirical Risk / Empirical Loss"></a><em>Empirical Risk</em> / <em>Empirical Loss</em></h3><p>经验风险：模型关于给定训练数据集的平均损失</p>
<script type="math/tex; mode=display">\begin{align*}
R_{emp}(f) & = \sum_{i=1}^N D_i L(y_i, f(x_i;\theta)) \\
E(R_{emp}(f)) & = R_{exp}(f)
\end{align*}</script><blockquote>
<ul>
<li>$\theta$：模型参数</li>
<li>$D_i$：样本损失权重，常为 $\frac 1 N$，在 <em>Boosting</em> 框架中不同</li>
</ul>
</blockquote>
<ul>
<li><p>经验风险损失是模型 $f(x)$ 的函数</p>
<ul>
<li>训练时，模型是模型参数的函数</li>
<li>即其为模型参数函数</li>
</ul>
</li>
<li><p>根据大数定律，样本量容量 $N$ 趋于无穷时，$R<em>{emp}(f)$ 趋于 $R</em>{exp}(f)$</p>
<ul>
<li>但是现实中训练样本数目有限、很小</li>
<li>利用经验风险估计期望常常并不理想，需要对经验风险进行矫正</li>
</ul>
</li>
<li><p>例子</p>
<ul>
<li><em>maximum probability estimation</em>：极大似然估计<ul>
<li>模型：条件概率分布（贝叶斯生成模型、逻辑回归）</li>
<li>损失函数：对数损失函数</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Structual-Risk-Structual-Loss"><a href="#Structual-Risk-Structual-Loss" class="headerlink" title="Structual Risk / Structual Loss"></a><em>Structual Risk</em> / <em>Structual Loss</em></h3><p>结构风险：在经验风险上加上表示 <strong>模型复杂度</strong> 的 <em>regularizer</em>（<em>penalty term</em>）</p>
<script type="math/tex; mode=display">
R_{srm} = \frac 1 N \sum_{i=1}^N L(y_i, f(x_i)) +
    \lambda J(f)</script><blockquote>
<ul>
<li>$J(f)$：模型复杂度，定义在假设空间$F$上的泛函</li>
<li>$\lambda$：权衡经验风险、模型复杂度的系数</li>
</ul>
</blockquote>
<ul>
<li>结构风险最小化<ul>
<li>添加 <em>regularization</em>（正则化），调节损失函数（目标函数）</li>
</ul>
</li>
<li>模型复杂度 $J(f)$ 表示对复杂模型的惩罚：模型 $f$ 越复杂，复杂项 $J(f)$ 越大</li>
<li>案例<ul>
<li><em>maximum posterior probability estimation</em>：最大后验概率估计<ul>
<li>损失函数：对数损失函数</li>
<li>模型复杂度：模型先验概率对数后取负</li>
<li>先验概率对应模型复杂度，先验概率越小，复杂度越大</li>
</ul>
</li>
<li>岭回归：平方损失 + $L<em>2$ 正则化
$\mathop{\arg\min}</em>{\beta} \sum_{i=1}^N (y_i - f(x_i, \beta))^2 + |\beta|$</li>
<li><em>LASSO</em>：平方损失 + $L<em>1$ 正则化
$\mathop{\arg\min}</em>{\beta} \sum_{i=1}^N (y_i - f(x_i, \beta))^2 + |\beta|_1$</li>
</ul>
</li>
</ul>
<h2 id="Generalization-Ability"><a href="#Generalization-Ability" class="headerlink" title="Generalization Ability"></a><em>Generalization Ability</em></h2><p>泛化能力：方法学习到的模型对未知数据的预测能力，是学习方法本质、重要的性质</p>
<ul>
<li>测试误差衡量学习方法的泛化能力不可靠，其依赖于测试集，而测试集有限</li>
<li>学习方法的泛化能力往往是通过研究泛化误差的概率上界进行</li>
</ul>
<h3 id="Generalization-Error-Bound"><a href="#Generalization-Error-Bound" class="headerlink" title="Generalization Error Bound"></a>Generalization Error Bound</h3><p>泛化误差上界：泛化误差的 <strong>概率</strong> 上界</p>
<ul>
<li>是样本容量函数，样本容量增加时，泛化上界趋于 0</li>
<li>是假设空间容量函数，假设空间容量越大，模型越难学习，泛化误差上界越大</li>
</ul>
<h4 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h4><ul>
<li><p>根据 <em>Hoeffding</em> 不等式，泛化误差满足</p>
<script type="math/tex; mode=display">\begin{align*}
& \forall h \in H, & P(|E(h) - \hat E(h)| \geq \epsilon) \leq 2 e^{-2 N \epsilon^2} \\
\Rightarrow & \forall h \in H, & P(|E(h) - \hat E(h)|
   \leq \epsilon) \geq 1 - 2|H|e^{-2N\epsilon^2}
\end{align*}</script><blockquote>
<ul>
<li>$H$：假设空间</li>
<li>$N$：样本数量</li>
<li>$E(h) := R_{exp}(h)$</li>
<li>$\hat E(h) := R_{emp}(h)$</li>
</ul>
</blockquote>
</li>
<li><p>证明如下：</p>
<script type="math/tex; mode=display">\begin{align*}
P(\forall h \in H: |E(h) - \hat E(h)| \leq \epsilon|)
   & = 1 - P(\exists h \in H: |E(h) - \hat E(h)|
   \geq \epsilon) \\
& = 1 - P((|E(h_1) - \hat E(h_1) \geq \epsilon) \vee \cdots
   \vee (|E(h_{|H|}) - \hat E_{|H|}| \geq \epsilon)) \\
& \geq 1 - \sum_{i=1}^{|H|} P(|E(h_i) - \hat E(h_i)|
   \geq \epsilon) \\
& \geq 1 - 2|H|e^{-2N \epsilon^2}
\end{align*}</script></li>
<li><p>对任意 $\epsilon$，随样本数量 $m$ 增大， $|E(h) - \hat E(h)| \leq \epsilon$ 概率增大，可以使用经验误差近似泛化误差</p>
</li>
</ul>
<h4 id="二分类泛化误差上界"><a href="#二分类泛化误差上界" class="headerlink" title="二分类泛化误差上界"></a>二分类泛化误差上界</h4><ul>
<li><p>由 <em>Hoeffding</em> 不等式</p>
<script type="math/tex; mode=display">\begin{align*}
P(E(h) - \hat E(h) & \geq \epsilon) \leq exp(-2N\epsilon^2) \\
P(\exists h \in H: E(h) - \hat E(h) \geq \epsilon) & =
   P(\bigcup_{h \in H} \{ E(h) - \hat E(h) \geq \epsilon \}) \\
& \leq \sum_{h \in H} P(E(h) - \hat E(h) \geq \epsilon) \\
& \leq |H| exp(-2 N \epsilon^2)
\end{align*}</script></li>
<li><p>则 $\forall h \in H$，有</p>
<script type="math/tex; mode=display">
P(E(h) - \hat E(h) < \epsilon) \geq 1 - |H| exp(-2 N \epsilon)</script><p>则令 $\sigma = |H| exp(-2N\epsilon^2)$，则至少以概率 $1-\sigma$ 满足如下，即得到泛化误差上界</p>
<script type="math/tex; mode=display">\begin{align*}
E(h)  & \leq \hat E(h) + \epsilon(|H|, N, \sigma) \\
\epsilon(|H|, N, \sigma) & = \sqrt
   {\frac 1 {2N} (log |H| + log \frac 1 {\sigma})}
\end{align*}</script></li>
</ul>
<h3 id="Probably-Approximate-Correct-可学习"><a href="#Probably-Approximate-Correct-可学习" class="headerlink" title="Probably Approximate Correct 可学习"></a><em>Probably Approximate Correct</em> 可学习</h3><p><em>PAC</em> 可学习：在短时间内利用少量（多项式级别）样本能够找到假设 $h^{‘}$，满足</p>
<script type="math/tex; mode=display">
P(E(h^{'}) \leq \epsilon) \geq 1 - \sigma, 0 < \epsilon, \sigma < 1</script><ul>
<li><p>即需要假设满足两个 <em>PAC</em> 辨识条件</p>
<ul>
<li>近似条件：泛化误差 $E(h^{‘})$ 足够小</li>
<li>可能正确：满足近似条件概率足够大</li>
</ul>
</li>
<li><p>同等条件下</p>
<ul>
<li>模型越复杂，泛化误差越大</li>
<li>满足条件的样本数量越大，模型泛化误差越小</li>
</ul>
</li>
<li><p><em>PAC</em> 学习理论关心能否从假设空间 $H$ 中学习到好的假设 $h$</p>
<ul>
<li>由以上泛化误差可得，取 $\sigma = 2|H|e^{-2N\epsilon^2}$，则样本量满足 $N = \frac {ln \frac {2|H|} \sigma} {2 \epsilon^2}$ 时，模型是 <em>PAC</em> 可学习的</li>
</ul>
</li>
</ul>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a><em>Regularization</em></h2><p>正则化：（向目标函数）添加额外信息以求解病态问题、避免过拟合</p>
<ul>
<li><p>常应用在机器学习、逆问题求解</p>
<ul>
<li>对模型（目标函数）复杂度惩罚</li>
<li>提高学习模型的泛化能力、避免过拟合</li>
<li>学习简单模型：稀疏模型、引入组结构</li>
</ul>
</li>
<li><p>有多种用途</p>
<ul>
<li>最小二乘也可以看作是简单的正则化</li>
<li>岭回归中的 $\mathcal{l_2}$ 范数</li>
</ul>
</li>
</ul>
<h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><p>模型复杂度：经常作为正则化项添加作为额外信息添加的，衡量模型复杂度方式有很多种</p>
<ul>
<li><p>函数光滑限制</p>
<ul>
<li>多项式最高次数</li>
</ul>
</li>
<li><p>向量空间范数</p>
<ul>
<li>$\mathcal{L_0} - norm$：参数个数</li>
<li>$\mathcal{L_1} - norm$：参数绝对值和</li>
<li>$\mathcal{L_2}$- norm$：参数平方和</li>
</ul>
</li>
</ul>
<h3 id="mathcal-L-0-norm"><a href="#mathcal-L-0-norm" class="headerlink" title="$\mathcal{L_0} - norm$"></a>$\mathcal{L_0} - norm$</h3><ul>
<li>$\mathcal{l_0} - norm$ 特点<ul>
<li>稀疏化约束</li>
<li>解 $\mathcal{L_0}$ 范数正则化是 <em>NP-hard</em> 问题</li>
</ul>
</li>
</ul>
<h3 id="mathcal-L-1-norm"><a href="#mathcal-L-1-norm" class="headerlink" title="$\mathcal{L_1} - norm$"></a>$\mathcal{L_1} - norm$</h3><ul>
<li><p>$\mathcal{L_1} - norm$ 特点</p>
<ul>
<li>$\mathcal{L_1}$ 范数可以通过凸松弛得到 $\mathcal{L_0}$ 的近似解</li>
<li>有时候出现解不唯一的情况</li>
<li>$\mathcal{L_1}$ 范数凸但不严格可导，可以使用依赖次梯度的方法求解极小化问题</li>
</ul>
</li>
<li><p>应用</p>
<ul>
<li><em>LASSO</em></li>
</ul>
</li>
<li><p>求解</p>
<ul>
<li><em>Proximal Method</em></li>
<li><em>LARS</em></li>
</ul>
</li>
</ul>
<h3 id="mathcal-L-2-norm"><a href="#mathcal-L-2-norm" class="headerlink" title="$\mathcal{L_2} - norm$"></a>$\mathcal{L_2} - norm$</h3><ul>
<li>$\mathcal{L_2} - norm$ 特点<ul>
<li>凸且严格可导，极小化问题有解析解</li>
</ul>
</li>
</ul>
<h3 id="mathcal-L-1-L-2"><a href="#mathcal-L-1-L-2" class="headerlink" title="$\mathcal{L_1 + L_2}$"></a>$\mathcal{L_1 + L_2}$</h3><ul>
<li><p>$\mathcal{L_1 + L_2}$ 特点</p>
<ul>
<li>有组效应，相关变量权重倾向于相同</li>
</ul>
</li>
<li><p>应用</p>
<ul>
<li><em>Elastic Net</em></li>
</ul>
</li>
</ul>
<h3 id="稀疏解产生"><a href="#稀疏解产生" class="headerlink" title="稀疏解产生"></a>稀疏解产生</h3><p>稀疏解：待估参数系数在某些分量上为 0</p>
<h4 id="mathcal-L-1-norm-稀疏解的产生"><a href="#mathcal-L-1-norm-稀疏解的产生" class="headerlink" title="$\mathcal{L_1} - norm$ 稀疏解的产生"></a>$\mathcal{L_1} - norm$ 稀疏解的产生</h4><blockquote>
<ul>
<li>$\mathcal{L_1}$ 范数在参数满足 <strong>一定条件</strong> 情况下，能对 <strong>平方损失</strong> 产生稀疏效果</li>
</ul>
</blockquote>
<ul>
<li><p>在 $[-1,1]$ 内 $y=|x|$ 导数大于 $y=x^2$（除 0 点）</p>
<ul>
<li>则特征在 0 点附近内变动时，为了取到极小值，参数必须始终为 0</li>
<li>高阶项在 0 点附近增加速度较慢，所以 $\mathcal{L_1} - norm$ 能产生稀疏解是很广泛的</li>
<li>$mathcal{L_1} - norm$ 前系数（权重）越大，能够容许高阶项增加的幅度越大，即压缩能力越强</li>
</ul>
</li>
<li><p>在 0 附近导数 “不小”，即导数在 0 点非 0</p>
<ul>
<li>对多项式正则化项<ul>
<li>$\mathcal{L_1} - norm$ 项对稀疏化解起决定性作用</li>
<li>其他项对稀疏解无帮助</li>
</ul>
</li>
<li>对“非多项式”正则化项<ul>
<li>$e^{|x|}-1$、$ln(|x|+1)$ 等在0点泰勒展开同样得到 $\mathcal{L_1} - norm$ 项</li>
<li>但是此类正则化项难以计算数值，不常用</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="mathcal-L-1-norm-稀疏解推广"><a href="#mathcal-L-1-norm-稀疏解推广" class="headerlink" title="$\mathcal{L_1} - norm$ 稀疏解推广"></a>$\mathcal{L_1} - norm$ 稀疏解推广</h4><ul>
<li><p>正负差异化：在正负设置权重不同的 $\mathcal{L_1}$，赋予在正负不同的压缩能力，甚至某侧完全不压缩</p>
</li>
<li><p>分段函数压缩：即只要保证在 0 点附近包含 $\mathcal{L_1}$ 用于产生稀疏解，远离 0 处可以设计为常数等不影响精确解的值</p>
<ul>
<li><p><em>Smoothly Clipped Absolute Deviation</em></p>
<script type="math/tex; mode=display">
R(x|\lambda, \gamma) = \left \{ \begin{array} {l}
  \lambda|x| \qquad & if |x| \leq \lambda \\
  \frac {2\gamma\lambda|x| - x^2 - {\lambda}^2 }
      {2(\gamma - 1)} &
      if \gamma< |x| <\gamma\lambda \\
  \frac { {\lambda}^2(\gamma+1)} 2 &
      if |x| \geq \gamma\lambda
\end{array} \right.</script></li>
<li><p><em>Derivate of SCAD</em></p>
<script type="math/tex; mode=display">
R(x; \lambda, \gamma) = \left \{ \begin{array} {l}
  \lambda \qquad & if |x| \leq \gamma \\
  \frac {\gamma\lambda - |x|} {\gamma - 1} &
      if \lambda < |x| < \gamma\lambda \\
  0 & if |x| \geq \gamma\lambda
\end{array} \right.</script></li>
<li><p><em>Minimax Concave Penalty</em></p>
<script type="math/tex; mode=display">
R_{\gamma}(x;\lambda) = \left \{ \begin{array} {l}
  \lambda|x| - \frac {x^2} {2\gamma} \qquad &
      if |x| \leq \gamma\lambda \\
  \frac 1 2 \gamma{\lambda}^2 &
      if |x| > \gamma\lambda
\end{array} \right.</script></li>
</ul>
</li>
<li><p>分指标：对不同指标动态设置 $\mathcal{L_0}$ 系数</p>
<ul>
<li><em>Adaptive Lasso</em>：$\lambda \sum_J w_jx_j$</li>
</ul>
</li>
</ul>
<h4 id="稀疏本质"><a href="#稀疏本质" class="headerlink" title="稀疏本质"></a>稀疏本质</h4><p>稀疏本质：极值、<strong>不光滑</strong>，即导数符号突然变化</p>
<ul>
<li><p>若某约束项导数符号突然变化、其余项在该点处导数为 0，为保证仍然取得极小值，解会聚集（极小）、疏远（极大）该点（类似坡的陡峭程度）</p>
<ul>
<li>即此类不光滑点会<strong>抑制解的变化</strong>，不光滑程度即导数变化幅度越大，抑制解变化能力越强，即吸引、排斥解能力越强</li>
<li>容易构造压缩至任意点的约束项</li>
<li>特殊的，不光滑点为 0 时，即得到稀疏解</li>
</ul>
</li>
<li><p>可以设置的多个极小不光滑点，使得解都在不连续集合中</p>
<ul>
<li>可以使用三角函数、锯齿函数等构造，但此类约束项要起效果，必然会使得目标函数非凸<ul>
<li>但是多变量场合，每个变量实际解只会在某个候选解附近，其邻域内仍然是凸的</li>
<li>且锯齿函数这样的突变非凸可能和凸函数具有相当的优秀性质</li>
</ul>
</li>
<li>当这些点均为整数时，这似乎可以近似求解 <strong>整数规划</strong></li>
</ul>
</li>
</ul>
<h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a><em>Early Stopping</em></h2><p><em>Early Stopping</em>：提前终止（训练）</p>
<ul>
<li><em>Early Stopping</em> 也可以被视为是 <em>regularizing on time</em><ul>
<li>迭代式训练随着迭代次数增加，往往会有学习复杂模型的倾向</li>
<li>对时间施加正则化，可以减小模型复杂度、提高泛化能力</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-19T09:20:00.000Z" title="8/19/2019, 5:20:00 PM">2019-08-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-08-19T09:20:00.000Z" title="8/19/2019, 5:20:00 PM">2019-08-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/TensorFlow/">TensorFlow</a></span><span class="level-item">a few seconds read (About 84 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/TensorFlow/tf_flow_control.html">TensorFlow控制算符</a></h1><div class="content"><h2 id="控制OPs"><a href="#控制OPs" class="headerlink" title="控制OPs"></a>控制OPs</h2><h3 id="Neural-Network-Building-Blocks"><a href="#Neural-Network-Building-Blocks" class="headerlink" title="Neural Network Building Blocks"></a>Neural Network Building Blocks</h3><h4 id="tf-softmax"><a href="#tf-softmax" class="headerlink" title="tf.softmax"></a><code>tf.softmax</code></h4><h4 id="tf-Sigmod"><a href="#tf-Sigmod" class="headerlink" title="tf.Sigmod"></a><code>tf.Sigmod</code></h4><h4 id="tf-ReLU"><a href="#tf-ReLU" class="headerlink" title="tf.ReLU"></a><code>tf.ReLU</code></h4><h4 id="tf-Convolution2D"><a href="#tf-Convolution2D" class="headerlink" title="tf.Convolution2D"></a><code>tf.Convolution2D</code></h4><h4 id="tf-MaxPool"><a href="#tf-MaxPool" class="headerlink" title="tf.MaxPool"></a><code>tf.MaxPool</code></h4><h3 id="Checkpointing"><a href="#Checkpointing" class="headerlink" title="Checkpointing"></a>Checkpointing</h3><h4 id="tf-Save"><a href="#tf-Save" class="headerlink" title="tf.Save"></a><code>tf.Save</code></h4><h4 id="tf-Restore"><a href="#tf-Restore" class="headerlink" title="tf.Restore"></a><code>tf.Restore</code></h4><h3 id="Queue-and-Synchronization"><a href="#Queue-and-Synchronization" class="headerlink" title="Queue and Synchronization"></a>Queue and Synchronization</h3><h4 id="tf-Enqueue"><a href="#tf-Enqueue" class="headerlink" title="tf.Enqueue"></a><code>tf.Enqueue</code></h4><h4 id="tf-Dequeue"><a href="#tf-Dequeue" class="headerlink" title="tf.Dequeue"></a><code>tf.Dequeue</code></h4><h4 id="tf-MutexAcquire"><a href="#tf-MutexAcquire" class="headerlink" title="tf.MutexAcquire"></a><code>tf.MutexAcquire</code></h4><h4 id="tf-MutexRelease"><a href="#tf-MutexRelease" class="headerlink" title="tf.MutexRelease"></a><code>tf.MutexRelease</code></h4><h3 id="Control-Flow"><a href="#Control-Flow" class="headerlink" title="Control Flow"></a>Control Flow</h3><h4 id="tf-count-up-to"><a href="#tf-count-up-to" class="headerlink" title="tf.count_up_to"></a><code>tf.count_up_to</code></h4><h4 id="tf-cond"><a href="#tf-cond" class="headerlink" title="tf.cond"></a><code>tf.cond</code></h4><p><code>pred</code>为<code>True</code>，执行<code>true_fn</code>，否则执行<code>false_fn</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.cond(</span><br><span class="line">	pred,</span><br><span class="line">	true_fn=<span class="literal">None</span>,</span><br><span class="line">	false_fn =<span class="literal">None</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="tf-case"><a href="#tf-case" class="headerlink" title="tf.case"></a><code>tf.case</code></h4><h4 id="tf-while-loop"><a href="#tf-while-loop" class="headerlink" title="tf.while_loop"></a><code>tf.while_loop</code></h4><h4 id="tf-group"><a href="#tf-group" class="headerlink" title="tf.group"></a><code>tf.group</code></h4><h4 id="tf-Merge"><a href="#tf-Merge" class="headerlink" title="tf.Merge"></a><code>tf.Merge</code></h4><h4 id="tf-Switch"><a href="#tf-Switch" class="headerlink" title="tf.Switch"></a><code>tf.Switch</code></h4><h4 id="tf-Enter"><a href="#tf-Enter" class="headerlink" title="tf.Enter"></a><code>tf.Enter</code></h4><h4 id="tf-Leave"><a href="#tf-Leave" class="headerlink" title="tf.Leave"></a><code>tf.Leave</code></h4><h4 id="tf-NextIteration"><a href="#tf-NextIteration" class="headerlink" title="tf.NextIteration"></a><code>tf.NextIteration</code></h4></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-19T09:13:00.000Z" title="8/19/2019, 5:13:00 PM">2019-08-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-02T07:08:09.000Z" title="8/2/2021, 3:08:09 PM">2021-08-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/TensorFlow/">TensorFlow</a></span><span class="level-item">10 minutes read (About 1531 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/TensorFlow/tf_operators.html">TensorFlow操作符</a></h1><div class="content"><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>张量：n-dimensional array，类型化的多维数组</p>
<ul>
<li>TF使用Tensor表示所有的数据</li>
<li>Tensor包含一个静态类型rank、一个shape</li>
<li>TF会将python原生类型转换为相应的Tensor<ul>
<li>0-d tensor：scalar</li>
<li>1-d tensor：vector，1d-array</li>
<li>2-d tensor：matrix，2d-array</li>
</ul>
</li>
</ul>
<h3 id="Data-Type"><a href="#Data-Type" class="headerlink" title="Data Type"></a>Data Type</h3><ul>
<li><p>TF被设计为和numpy可以无缝结合</p>
<ul>
<li>TF的变量类型基于numpy变量类型：<code>tf.int32==np.int32</code></li>
<li>bool、numeric等大部分类型可以不加转换的使用TF、np
变量类型</li>
<li>TF、np中string类型不完全一样，但TF仍然可以从numpy
中导入string数组，但是不能在numpy中指定类型</li>
</ul>
</li>
<li><p>但尽量使用TF变量类型</p>
<ul>
<li>python原生类型：没有<strong>细分</strong>类型，TF需要推断类型</li>
<li>numpy类型：numpy不兼容GPU，也不能自动计算衍生类型</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>tf.float16</code></td>
<td>16-bit half-precision floating-point</td>
</tr>
<tr>
<td><code>tf.float32</code></td>
<td>32-bit single-presicion floating-point</td>
</tr>
<tr>
<td><code>tf.float64</code></td>
<td>64-bit double-presicion floating-point</td>
</tr>
<tr>
<td><code>tf.bfloat16</code></td>
<td>16-bit truncated floating-point</td>
</tr>
<tr>
<td><code>tf.complex64</code></td>
<td>64-bit single-presicion complex</td>
</tr>
<tr>
<td><code>tf.complex128</code></td>
<td>128-bit double-presicion complex</td>
</tr>
<tr>
<td><code>tf.int8</code></td>
<td>8-bit signed integer</td>
</tr>
<tr>
<td><code>tf.uint8</code></td>
<td>8-bit unsigned integer</td>
</tr>
<tr>
<td><code>tf.int16</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.uint16</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.int32</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.int64</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.bool</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.string</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.qint8</code></td>
<td>quantized 8-bit signed integer</td>
</tr>
<tr>
<td><code>tf.quint8</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.qint16</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.quint16</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.qint32</code></td>
<td></td>
</tr>
<tr>
<td><code>tf.resource</code></td>
<td>handle to a mutable resource</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Constant-OPs"><a href="#Constant-OPs" class="headerlink" title="Constant OPs"></a>Constant OPs</h2><h3 id="tf-constant"><a href="#tf-constant" class="headerlink" title="tf.constant"></a><code>tf.constant</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">constant</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	value,</span></span></span><br><span class="line"><span class="params"><span class="function">	dtype=none,</span></span></span><br><span class="line"><span class="params"><span class="function">	shape=none,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="string">&quot;Const&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	verify_shape=<span class="literal">False</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
<h3 id="同值常量OPs"><a href="#同值常量OPs" class="headerlink" title="同值常量OPs"></a>同值常量OPs</h3><ul>
<li><p><em>zeros</em>：类似np中相应函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `np.zeros`</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">zeros</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	shape,</span></span></span><br><span class="line"><span class="params"><span class="function">	dtype=tf.float32,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"># `<span class="title">np</span>.<span class="title">zores_like</span>`</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">zeros_like</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	input_tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">	dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	optimizeTrue</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>若没有指明<code>dtype</code>，根据<code>input_tensor</code>确定其中值<ul>
<li>对数值型为<code>0.0</code></li>
<li>对bool型为<code>False</code></li>
<li>对字符串为<code>b&#39;&#39;</code></li>
</ul>
</li>
</ul>
</li>
<li><p><em>ones</em>：类似np中相应函数</p>
</li>
</ul>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `np.ones`</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">ones</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	shape,</span></span></span><br><span class="line"><span class="params"><span class="function">	dtype=tf.float32,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"># `<span class="title">np</span>.<span class="title">ones_like</span>`</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">ones_like</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	input_tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">	dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	optimize=<span class="literal">True</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>

-    若没有指明`dtype`，根据`input_tensor`确定
    -    对数值型为`0.0`
    -    对bool型为`True`
    -    对字符串报错
</code></pre><ul>
<li><p><em>fill</em>：以<code>value</code>填充<code>dims</code>给定形状</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `np.fill`</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">fill</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	dims,</span></span></span><br><span class="line"><span class="params"><span class="function">	value,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="列表常量OPs"><a href="#列表常量OPs" class="headerlink" title="列表常量OPs"></a>列表常量OPs</h3><blockquote>
<ul>
<li>tensor列表不能直接<code>for</code>语句等迭代</li>
</ul>
</blockquote>
<ul>
<li><p><code>tf.lin_space</code>：<code>start</code>、<code>stop</code>直接均分为<code>num</code>部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `np.linspace`</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lin_space</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	start,</span></span></span><br><span class="line"><span class="params"><span class="function">	stop,</span></span></span><br><span class="line"><span class="params"><span class="function">	num,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>tf.range</code>：<code>start</code>、<code>stop</code>间等间隔<code>delta</code>取值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `np.arange`</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">range</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	start,</span></span></span><br><span class="line"><span class="params"><span class="function">	limit=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	delta=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="string">&quot;range&quot;</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="随机常量OPs"><a href="#随机常量OPs" class="headerlink" title="随机常量OPs"></a>随机常量OPs</h3><ul>
<li><p><em>seed</em>：设置随机数种子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># np.random.seed</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">set_random_seed</span>(<span class="params">seed</span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><em>random</em>：随机生成函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">random_normal</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">truncated_normal</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	?avg=<span class="number">0</span>/<span class="built_in">int</span>/(<span class="params"><span class="built_in">int</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">	stddev=<span class="number">1.0</span>/<span class="built_in">float</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	seed=<span class="literal">None</span>/<span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">random_uniform</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	shape(<span class="params">1d-arr</span>),</span></span></span><br><span class="line"><span class="params"><span class="function">	minval=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	maxval=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	dtype=tf.float32,</span></span></span><br><span class="line"><span class="params"><span class="function">	seed=<span class="literal">None</span>/<span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=<span class="literal">None</span>/<span class="built_in">str</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">random_crop</span>()</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">multinomial</span>()</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">random_gamma</span>()</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><em>shuffle</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">random_shuffle</span>()</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="运算OPs"><a href="#运算OPs" class="headerlink" title="运算OPs"></a>运算OPs</h2><h3 id="元素OPs"><a href="#元素OPs" class="headerlink" title="元素OPs"></a>元素OPs</h3><h4 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">subtract</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">sub</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">multiply</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">mul</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function">	# 加、减、乘</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">floordiv</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">floor_div</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">div</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">truncatediv</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function">	# 地板除</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">divide</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">truediv</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function">	# 浮点除</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">realdiv</span>(<span class="params">x, y, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function">	# 实数除法，只能用于实数？</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">add_n</span>(<span class="params"><span class="built_in">input</span>, name=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function">	# `<span class="title">input</span>`：<span class="title">list</span>-<span class="title">like</span>，元素<span class="title">shape</span>、<span class="title">type</span>相同</span></span><br><span class="line"><span class="function">	# 累加`<span class="title">input</span>`中元素的值</span></span><br></pre></td></tr></table></figure>
<h4 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greater</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">less</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">equal</span>()</span></span><br></pre></td></tr></table></figure>
<h4 id="数学函数"><a href="#数学函数" class="headerlink" title="数学函数"></a>数学函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exp</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">log</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">square</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">round</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">sqrt</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">rsqrt</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">pow</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">abs</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">negative</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">sign</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">reciprocal</span>()		# 倒数</span></span><br></pre></td></tr></table></figure>
<h3 id="列表运算OPs"><a href="#列表运算OPs" class="headerlink" title="列表运算OPs"></a>列表运算OPs</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">Concat</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">Slice</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">Split</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">Rank</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">Shape</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">Shuffle</span>()</span></span><br></pre></td></tr></table></figure>
<h3 id="矩阵OPs"><a href="#矩阵OPs" class="headerlink" title="矩阵OPs"></a>矩阵OPs</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">MatMul</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">MatrixInverse</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">MatrixDeterminant</span>()</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">tf</span>.<span class="title">tensordot</span>()				# 矩阵点乘</span></span><br></pre></td></tr></table></figure>
<h3 id="梯度OPs"><a href="#梯度OPs" class="headerlink" title="梯度OPs"></a>梯度OPs</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def tf.gradients(				# 求`y`对`[xs]`个元素偏导</span><br><span class="line">	ys: tf.OPs,</span><br><span class="line">	xs: tf.OPs/[tf.OPs],</span><br><span class="line">	grad_ys=None,</span><br><span class="line">	name=None</span><br><span class="line">)</span><br><span class="line">def tf.stop_gradient(</span><br><span class="line">	input,</span><br><span class="line">	name=None</span><br><span class="line">)</span><br><span class="line"><span class="function">def <span class="title">clip_by_value</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">	t,</span></span></span><br><span class="line"><span class="params"><span class="function">	clip_value_min,</span></span></span><br><span class="line"><span class="params"><span class="function">	clip_value_max,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=None</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span></span><br><span class="line"><span class="function">def tf.<span class="title">clip_by_norm</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">	t,</span></span></span><br><span class="line"><span class="params"><span class="function">	clip_norm,</span></span></span><br><span class="line"><span class="params"><span class="function">	axes=None,</span></span></span><br><span class="line"><span class="params"><span class="function">	name=None</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Variable</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">		init_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		trainable=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		collections=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		validata_shape=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		caching_device=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		variable_def=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		expected_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		import_scope=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		constraint=<span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">	</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">	# 初始化变量</span></span><br><span class="line"><span class="function">	# `<span class="title">sess</span>.<span class="title">run</span>`其即初始化变量</span></span><br><span class="line"><span class="function">	<span class="title">def</span> <span class="title">intializer</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 读取变量值</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">value</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 获取变量初始化值，其他变量调用、声明依赖该变量</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">initilized_value</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 计算、获取变量值，类似`sess.run(OP)`</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">eval</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 给变量赋值</span></span><br><span class="line">	<span class="comment"># `assgin`内部有初始化Variable，所以有时可以不用初始化</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">assign</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line">	<span class="comment">#`assgin_add`等依赖于原始值，不会初始化变量</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">assign_add</span>(<span class="params">self, ?dec</span>)</span></span><br><span class="line"><span class="function">	<span class="title">def</span> <span class="title">assign_divide</span>(<span class="params">self, ?dec</span>)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>Variable</code>是包含很多方法的类</p>
<ul>
<li>其中<strong>方法OPs</strong>和一般的OP一样，也需要在Session中执行
才能生效</li>
<li><code>Variable</code>必须在会话中<strong>初始化</strong>后，才能使用</li>
<li>会话维护自身独立<code>Variable</code>副本，不相互影响</li>
</ul>
</li>
<li><p><code>Variable</code>和图分开存储，甚至是存储在独立参数服务器上</p>
<ul>
<li>存储大量数据也不会拖慢图载入速度</li>
<li>通常用于存储训练过程中weight、bias、维护图执行过程
中状态信息</li>
</ul>
</li>
<li><p>constants是常数OPs</p>
<ul>
<li>存储在图中：每次载入图会同时被载入，过大的constants
会使得载入图非常慢</li>
<li>所以最好只对原生类型使用constants</li>
</ul>
</li>
</ul>
<h3 id="Variable创建"><a href="#Variable创建" class="headerlink" title="Variable创建"></a>Variable创建</h3><h4 id="tf-get-variable"><a href="#tf-get-variable" class="headerlink" title="tf.get_variable"></a><code>tf.get_variable</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_variable</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	name,</span></span></span><br><span class="line"><span class="params"><span class="function">	shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	dtype=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	initializer=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	regularizer=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	trainable=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	collections=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	caching_device=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	partitioner=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	validate_shape=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	use_resource=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	custom_getter=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	constraint=<span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>此封装工厂方法相较于直接通过<code>tf.Variable</code>更好<ul>
<li>若变量已设置，可通过变量名获取变量，方便变量共享</li>
<li>可以提供更多的参数定制变量值</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">	<span class="comment"># `tf.Variable`创建变量</span></span><br><span class="line">s = tf.Variable(<span class="number">2</span>, name=<span class="string">&quot;scalar&quot;</span>)</span><br><span class="line">m = tf.Variable([[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">3</span>]], name=<span class="string">&quot;matrix&quot;</span>)</span><br><span class="line">w = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">	<span class="comment"># `tf.get_variable`创建、获取变量</span></span><br><span class="line">s = tf.get_variable(<span class="string">&quot;scalar&quot;</span>, initializer=tf.constant(<span class="number">2</span>))</span><br><span class="line">m = tf.get_variable(<span class="string">&quot;matrix&quot;</span>, initializer=tf.constant([[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">W = tf.get_variable(<span class="string">&quot;big_matrix&quot;</span>,</span><br><span class="line">	shape=(<span class="number">784</span>, <span class="number">10</span>),</span><br><span class="line">	initializer=tf.zeros_initializer()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="Variable初始化"><a href="#Variable初始化" class="headerlink" title="Variable初始化"></a>Variable初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	<span class="comment"># 初始化所有Variables</span></span><br><span class="line">	sess.run(tf.global_variable_initialier())</span><br><span class="line">	<span class="comment"># 初始化变量子集</span></span><br><span class="line">	sess.run(tf.variable_initializer([s, m])</span><br><span class="line">	<span class="comment"># 初始化指定单个变量</span></span><br><span class="line">	sess.run(s.initializer)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>若某Variable依赖其他Variable，需要使用
<code>initialized_value</code>指明依赖，确保依赖线性初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W = tr.Variable(tf.truncated_normal([<span class="number">700</span>, <span class="number">100</span>])</span><br><span class="line"><span class="comment"># 指明依赖，保证依赖线性初始化</span></span><br><span class="line">U = tf.Variable(W.initialized_value() * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-19T09:13:00.000Z" title="8/19/2019, 5:13:00 PM">2019-08-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-08-19T09:13:00.000Z" title="8/19/2019, 5:13:00 PM">2019-08-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/TensorFlow/">TensorFlow</a></span><span class="level-item">5 minutes read (About 714 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/TensorFlow/tf_persistence.html">TensorFlow 持久化</a></h1><div class="content"><h2 id="Session-Checkpoint"><a href="#Session-Checkpoint" class="headerlink" title="Session Checkpoint"></a>Session Checkpoint</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">tf</span>.<span class="title">train</span>.<span class="title">Saver</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">		var_list=<span class="literal">None</span>/<span class="built_in">list</span>/<span class="built_in">dict</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		reshape=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		sharded=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		max_to_keep=<span class="number">5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		keep_checkpoint_every_n_hours=<span class="number">10000.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		name=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		restore_sequentially=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		saver_def=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		builder=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		defer_build=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		allow_empty=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		write_version=tf.train.SaverDef.V2,</span></span></span><br><span class="line"><span class="params"><span class="function">		pad_step_number=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		save_relative_paths=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		filename=<span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="function">	</span>):</span></span><br><span class="line">		self.last_checkpoints</span><br></pre></td></tr></table></figure>
<ul>
<li><p>用途：保存Session中变量（张量值），将变量名映射至张量值</p>
</li>
<li><p>参数</p>
<ul>
<li><code>var_list</code>：待保存、恢复变量，缺省所有<ul>
<li>变量需在<code>tf.train.Saver</code>实例化前创建</li>
</ul>
</li>
<li><code>reshape</code>：允许恢复并重新设定张量形状</li>
<li><code>sharded</code>：碎片化保存至多个设备</li>
<li><code>max_to_keep</code>：最多保存checkpoint数目</li>
<li><code>keep_checkpoint_every_n_hours</code>：checkpoint有效时间</li>
<li><code>restore_sequentially</code>：各设备中顺序恢复变量，可以
减少内存消耗</li>
</ul>
</li>
<li><p>成员</p>
<ul>
<li><code>last_checkpoints</code>：最近保存checkpoints</li>
</ul>
</li>
</ul>
<h3 id="保存Session"><a href="#保存Session" class="headerlink" title="保存Session"></a>保存Session</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Saver</span>.<span class="title">save</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">	sess,</span></span></span><br><span class="line"><span class="params"><span class="function">	save_path,</span></span></span><br><span class="line"><span class="params"><span class="function">	global_step=<span class="literal">None</span>/<span class="built_in">str</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	latest_filename=<span class="literal">None</span>(<span class="params"><span class="string">&quot;checkpoint&quot;</span></span>)/<span class="built_in">str</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	meta_graph_suffix=<span class="string">&quot;meta&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	write_meta_graph=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	write_state=<span class="literal">True</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="built_in">str</span>(path):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>用途：保存Session，要求变量已初始化</p>
</li>
<li><p>参数</p>
<ul>
<li><code>global_step</code>：添加至<code>save_path</code>以区别不同步骤</li>
<li><code>latest_filename</code>：checkpoint文件名</li>
<li><code>meta_graph_suffix</code>：MetaGraphDef文件名后缀</li>
</ul>
</li>
</ul>
<h3 id="恢复Session"><a href="#恢复Session" class="headerlink" title="恢复Session"></a>恢复Session</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Saver</span>.<span class="title">restore</span>(<span class="params">sess, save_path(<span class="params"><span class="built_in">str</span></span>)</span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<ul>
<li>用途：从<code>save_path</code>指明的路径中恢复模型</li>
</ul>
<blockquote>
<ul>
<li>模型路径可以通过<code>Saver.last_checkpoints</code>属性、
  <code>tf.train.get_checkpoint_state()</code>函数获得</li>
</ul>
</blockquote>
<h3 id="tf-train-get-checkpoint-state"><a href="#tf-train-get-checkpoint-state" class="headerlink" title="tf.train.get_checkpoint_state"></a><code>tf.train.get_checkpoint_state</code></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def tf.train.get_checkpoint_state(</span><br><span class="line">	checkpoint_dir(str),</span><br><span class="line">	latest_filename=None</span><br><span class="line">):</span><br><span class="line">	pass</span><br></pre></td></tr></table></figure>
<ul>
<li>用途：获取指定checkpoint目录下checkpoint状态<ul>
<li>需要图结构已经建好、Session开启</li>
<li>恢复模型得到的变量无需初始化</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ckpt = tf.train.get_checkpoint_state(checkpoint_dir)</span><br><span class="line">saver.restore(ckpt.model_checkpoint_path)</span><br><span class="line">saver.restore(ckpt.all_model_checkpoint_paths[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h2 id="Graph-Saver"><a href="#Graph-Saver" class="headerlink" title="Graph Saver"></a>Graph Saver</h2><h3 id="tf-train-write-graph"><a href="#tf-train-write-graph" class="headerlink" title="tf.train.write_graph"></a><code>tf.train.write_graph</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">train</span>.<span class="title">write_graph</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	graph_or_graph_def: tf.Graph,</span></span></span><br><span class="line"><span class="params"><span class="function">	logdir: <span class="built_in">str</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	name: <span class="built_in">str</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	as_text=<span class="literal">True</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>用途：存储图至文件中</p>
</li>
<li><p>参数</p>
<ul>
<li><code>as_text</code>：以ASCII方式写入文件</li>
</ul>
</li>
</ul>
<h2 id="Summary-Saver"><a href="#Summary-Saver" class="headerlink" title="Summary Saver"></a>Summary Saver</h2><h3 id="tf-summary-FileWriter"><a href="#tf-summary-FileWriter" class="headerlink" title="tf.summary.FileWriter"></a><code>tf.summary.FileWriter</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">tf</span>.<span class="title">summary</span>.<span class="title">FileWriter</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">		?path=<span class="built_in">str</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		graph=tf.Graph</span></span></span><br><span class="line"><span class="params"><span class="function">	</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">	# 添加<span class="title">summary</span>记录</span></span><br><span class="line"><span class="function">	<span class="title">def</span> <span class="title">add_summary</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">		summary: OP,</span></span></span><br><span class="line"><span class="params"><span class="function">		global_step</span></span></span><br><span class="line"><span class="params"><span class="function">	</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 关闭`log`记录</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>用途：创建<code>FileWriter</code>对象用于记录log</p>
<ul>
<li>存储图到<strong>文件夹</strong>中，文件名由TF自行生成</li>
<li>可通过TensorBoard组件查看生成的event log文件</li>
</ul>
</li>
<li><p>说明</p>
<ul>
<li>一般在图定义完成后、Session执行前创建<code>FileWriter</code>
对象，Session结束后关闭</li>
</ul>
</li>
</ul>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 创建自定义summary</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&quot;summaries&quot;</span>):</span><br><span class="line">	tf.summary.scalar(<span class="string">&quot;loss&quot;</span>, self.loss)</span><br><span class="line">	tf.summary.scalar(<span class="string">&quot;accuracy&quot;</span>, self.accuracy)</span><br><span class="line">	tf.summary.histogram(<span class="string">&quot;histogram loss&quot;</span>, self.loss)</span><br><span class="line">	summary_op = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">	<span class="comment"># 从checkpoint中恢复Session</span></span><br><span class="line">	ckpt = tf.train.get_check_state(os.path.dirname(<span class="string">&quot;checkpoint_dir&quot;</span>))</span><br><span class="line">	<span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_check_path:</span><br><span class="line">		saver.restore(sess, ckpt.mode_checkpoint_path)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># summary存储图</span></span><br><span class="line">	writer = tf.summary.FileWriter(<span class="string">&quot;./graphs&quot;</span>, sess.graph)</span><br><span class="line">	<span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">		loas_batch, _, summary = session.run([loss, optimizer, summary_op])</span><br><span class="line">		writer.add_summary(summary, global_step=index)</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (index + <span class="number">1</span>) % <span class="number">1000</span> = <span class="number">0</span>:</span><br><span class="line">			saver.save(sess, <span class="string">&quot;checkpoint_dir&quot;</span>, index)</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 关闭`FileWriter`，生成event log文件</span></span><br><span class="line">write.close()</span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-19T09:13:00.000Z" title="8/19/2019, 5:13:00 PM">2019-08-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-08-19T09:13:00.000Z" title="8/19/2019, 5:13:00 PM">2019-08-19</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/TensorFlow/">TensorFlow</a></span><span class="level-item">a few seconds read (About 26 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/TensorFlow/tf_train.html">TensorFlow训练</a></h1><div class="content"><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">tf</span>.<span class="title">train</span>.<span class="title">GradientDescentOptimizer</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">tf</span>.<span class="title">train</span>.<span class="title">AdagradOptimizer</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">tf</span>.<span class="title">train</span>.<span class="title">MomentumOptimizer</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">tf</span>.<span class="title">train</span>.<span class="title">AdamOptimizer</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">tf</span>.<span class="title">train</span>.<span class="title">FtrlOptimizer</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">tf</span>.<span class="title">train</span>.<span class="title">RMSPropOptmizer</span>:</span></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-19T09:09:00.000Z" title="8/19/2019, 5:09:00 PM">2019-08-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-02T07:07:32.000Z" title="8/2/2021, 3:07:32 PM">2021-08-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Python/">Python</a><span> / </span><a class="link-muted" href="/categories/Python/TensorFlow/">TensorFlow</a></span><span class="level-item">14 minutes read (About 2161 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Python/TensorFlow/tf_execution.html">TensorFlow执行</a></h1><div class="content"><h2 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h2><p><em>Session</em>：TF中OPs对象执行、Tensor对象计算的封装环境</p>
<ul>
<li><p>Session管理图、OPs</p>
<ul>
<li>所有图必须Session中执行</li>
<li>将图中OPs、其执行方法分发到CPU、GPU、TPU等设备上</li>
<li>为当前变量值分配内存</li>
</ul>
</li>
<li><p>Session只执行<em>Data/Tensor Flow</em>中OPs，忽略不相关节点</p>
<ul>
<li>即通往<code>fetches</code>的flow中的<strong>OPs构成子图</strong>才会被计算</li>
</ul>
</li>
<li><p>各Session中各值独立维护，不会相互影响</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Session</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">		target=<span class="built_in">str</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		graph=<span class="literal">None</span>/tf.Graph,</span></span></span><br><span class="line"><span class="params"><span class="function">		config=<span class="literal">None</span>/tf.ConfigProto</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">		# 获取<span class="title">Session</span>中载入图</span></span><br><span class="line"><span class="function">		<span class="title">self</span>.<span class="title">graph</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">	# 关闭会话，释放资源</span></span><br><span class="line"><span class="function">	<span class="title">def</span> <span class="title">close</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 支持上下文语法</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__enter__</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__exit__</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 执行TensorFlow中节点，获取`fetches`中节点值</span></span><br><span class="line">	<span class="comment"># 返回值若为Tensor</span></span><br><span class="line">		<span class="comment"># python中：以`np.ndarray`返回</span></span><br><span class="line">		<span class="comment"># C++/C中：以`tensorflow:Tensor`返回</span></span><br><span class="line">	<span class="comment"># 可直接调用`.eval()`获得单个OP值</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">		fetches=tf.OPs/[tf.OPs],</span></span></span><br><span class="line"><span class="params"><span class="function">		feed_dict=<span class="literal">None</span>/<span class="built_in">dict</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		options=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">		run_metadata=<span class="literal">None</span></span>)</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-InteractiveSession"><a href="#tf-InteractiveSession" class="headerlink" title="tf.InteractiveSession"></a><code>tf.InteractiveSession</code></h3><p><code>tf.InteractiveSession</code>：开启交互式会话</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 开启交互式Session</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">a = tf.constant(<span class="number">5.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">6.0</span>)</span><br><span class="line">c = a * b</span><br><span class="line">x = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line"> <span class="comment"># 无需显式在`sess.run`中执行</span></span><br><span class="line"> <span class="comment"># 直接调用`OPs.eval/run()`方法得到结果</span></span><br><span class="line">x.initializer.run()</span><br><span class="line"><span class="built_in">print</span>(c.<span class="built_in">eval</span>())</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<h3 id="Session执行"><a href="#Session执行" class="headerlink" title="Session执行"></a>Session执行</h3><ul>
<li><p>Fetch机制：<code>sess.run()</code>执行图时，传入需<strong>取回</strong>的结果，
取回操作输出内容</p>
</li>
<li><p>Feed机制：通过<code>feed_dict</code>参数，使用自定义tensor值替代图
中任意feeable OPs的输出</p>
<ul>
<li><code>tf.placeholder()</code>表示创建占位符，执行Graph时必须
使用tensor替代</li>
<li><code>feed_dict</code>只是函数参数，只在调用它的方法内有效，
方法执行完毕则消失</li>
</ul>
</li>
<li><p>可以通过<code>feed_dict</code> feed所有feedable tensor，placeholder
只是指明必须给某些提供值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.add(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">b = tf.multiply(a, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	sess.run(b, feed_dict=&#123;a: <span class="number">15</span>&#125;)			<span class="comment"># 45</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="config参数选项"><a href="#config参数选项" class="headerlink" title="config参数选项"></a><code>config</code>参数选项</h3><ul>
<li><code>log_device_placement</code>：打印每个操作所用设备</li>
<li><code>allow_soft_placement</code>：允许不在GPU上执行操作自动迁移到
CPU上执行</li>
<li><code>gpu_options</code><ul>
<li><code>allow_growth</code>：按需分配显存</li>
<li><code>per_process_gpu_memory_fraction</code>：指定每个GPU进程
使用显存比例（无法对单个GPU分别设置）</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>具体配置参见<code>tf.ConfigProto</code></li>
</ul>
</blockquote>
<h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><p><em>Graph</em>：表示TF中计算任务，</p>
<ul>
<li><p><em>operation/node</em>：Graph中节点，包括：<em>operator</em>、
<em>variable</em>、<em>constant</em></p>
<ul>
<li>获取一个、多个Tensor执行计算、产生、返回tensor</li>
<li>不能无法对其值直接进行访问、比较、操作</li>
<li>图中节点可以命名，TF会自动给未命名节点命名</li>
</ul>
</li>
<li><p><em>tensor</em>：Graph中边，n维数组</p>
<ul>
<li>TF中所有对象都是Operators</li>
<li>tensor是OPs执行结果，在图中传递/流动</li>
</ul>
</li>
<li><p>图计算模型优势</p>
<ul>
<li>优化能力强、节省计算资源<ul>
<li>缓冲自动重用</li>
<li>常量折叠，只计算取得目标值过程中必须计算的值</li>
<li>方便并行化</li>
<li>自动权衡计算、存储效率</li>
</ul>
</li>
<li>易于部署<ul>
<li>可被割成子图（即极大连通分量），便于自动区分</li>
<li>子图可以分发到不同的设备上分布式执行，即模型并行</li>
<li>许多通用ML模型是通过有向图教学、可视化的</li>
</ul>
</li>
</ul>
</li>
<li><p>图计算模型劣势</p>
<ul>
<li>难以debug<ul>
<li>图定义之后执行才会报错</li>
<li>无法通过pdb、打印状态debug</li>
</ul>
</li>
<li>语法繁复</li>
</ul>
</li>
</ul>
<h3 id="构建图"><a href="#构建图" class="headerlink" title="构建图"></a>构建图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span>:</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 将当前图作为默认图</span></span><br><span class="line">	<span class="comment"># 支持上下文语法</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">as_default</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 强制OPs依赖关系（图中未反映），即优先执行指定OPs</span></span><br><span class="line">	<span class="comment"># 支持上下文语法</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">as_default</span>(<span class="params">self</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">control_dependencies</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">		?ops=[tf.OPs]</span>)</span></span><br><span class="line"><span class="function">		<span class="title">pass</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">	# 以<span class="title">ProtoBuf</span>格式展示<span class="title">Graph</span></span></span><br><span class="line"><span class="function">	<span class="title">def</span> <span class="title">as_graph_def</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># 判断图中节点是否可以被feed</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">is_feedable</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">		?op=tf.OPs</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>可以通过<code>tf.Graph</code>创建新图，但最好在是在一张图中使用多个
不相连的子图，而不是多张图</p>
<ul>
<li>充分性：Session执行图时忽略不必要的OPs</li>
<li>必要性<ul>
<li>多张图需要多个会话，每张图执行时默认尝试使用所有
可能资源</li>
<li>不能通过python/numpy在图间传递数据（分布式系统）</li>
</ul>
</li>
</ul>
</li>
<li><p>初始化即包含默认图，OP构造器默认为其增加节点</p>
<ul>
<li>通过<code>tf.get_default_graph()</code>获取</li>
</ul>
</li>
</ul>
<h3 id="图相关方法"><a href="#图相关方法" class="headerlink" title="图相关方法"></a>图相关方法</h3><ul>
<li><p>获取TF初始化的默认图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">get_default_graph</span>():</span></span><br><span class="line">	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="命名空间"><a href="#命名空间" class="headerlink" title="命名空间"></a>命名空间</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 均支持、利用上下文语法，将OPs定义于其下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">name_scope</span>(<span class="params">name(<span class="params"><span class="built_in">str</span></span>)</span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tf</span>.<span class="title">variable_scope</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">	name(<span class="params"><span class="built_in">str</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">	reuse=tf.AUTO_REUSE</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>tf.name_scope</code>：将变量分组<ul>
<li>只是将变量打包，不影响变量的重用、可见性等</li>
<li>方便管理、查看graph</li>
</ul>
</li>
<li><code>tf.variable_scope</code>：<ul>
<li>对变量有控制能力<ul>
<li>可设置变量重用性</li>
<li>变量可见性局限于该<em>variable scope</em>内，即不同
variable scope间可以有完全同名变量
（未被TF添加顺序后缀）</li>
</ul>
</li>
<li>会隐式创建<em>name scope</em></li>
<li>大部分情况是用于实现变量共享</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fully_connected</span>(<span class="params">x, output_dim, scope</span>):</span></span><br><span class="line">	<span class="comment"># 设置variable scope中变量自动重用</span></span><br><span class="line">	<span class="comment"># 或者调用`scope.reuse_variables()`声明变量可重用</span></span><br><span class="line">	<span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE) <span class="keyword">as</span> scope:</span><br><span class="line">		<span class="comment"># 在当前variable scope中获取、创建变量</span></span><br><span class="line">		w = tf.get_variable(<span class="string">&quot;weights&quot;</span>, [x.shape[<span class="number">1</span>]], output_dim,</span><br><span class="line">							initializer=tf.random_normal_initializer())</span><br><span class="line">		b = tf.get_variable(<span class="string">&quot;biases&quot;</span>, [output_dim],</span><br><span class="line">							initializer=tf.constant_initizer(<span class="number">0.0</span>))</span><br><span class="line">		<span class="keyword">return</span> tf.matmul(x, w) + b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_hidden_layers</span>(<span class="params">x</span>):</span></span><br><span class="line">	h1 = fully_connected(x, <span class="number">50</span>, <span class="string">&quot;h1&quot;</span>)</span><br><span class="line">	h2 =-fully_connected(h1, <span class="number">10</span>, <span class="string">&quot;h2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;two_layers&quot;</span>) <span class="keyword">as</span> scope:</span><br><span class="line">	logits1 = two_hidden_layers(x1)</span><br><span class="line">	logits2 = two_hidden_layers(x2)</span><br></pre></td></tr></table></figure>
<h3 id="Lazy-Loading"><a href="#Lazy-Loading" class="headerlink" title="Lazy Loading"></a>Lazy Loading</h3><p><em>Lazy Loading</em>：推迟声明/初始化OP对象至载入图时
（不是指TF的延迟计算，是个人代码结构问题，虽然是TF延迟图计算
模型的结果）</p>
<ul>
<li><p>延迟加载容易导致向图中添加大量重复节点，影响图的载入、
传递</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">10</span>, name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">y = tf.Variable(<span class="number">20</span>, name=<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	sess.run(tf.global_variables_initializer())</span><br><span class="line">	writer = tf.summary.FileWriter(<span class="string">&#x27;graphs/lazy_loading&#x27;</span>, sess.graph)</span><br><span class="line">	<span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">		<span class="comment"># 延迟加载节点，每次执行都会添加`tf.add`OP</span></span><br><span class="line">		sess.run(tf.add(x, y))</span><br><span class="line">	<span class="built_in">print</span>(tf.get_default_graph().as_graph_def())</span><br><span class="line">	writer.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p>解决方案</p>
<ul>
<li>总是把（图的搭建）OPs的定义、执行分开</li>
<li>python利用<code>@property</code>装饰器，使用<strong>单例模式函数</strong>
封装变量控制，保证仅首次调用函数时才创建OP</li>
</ul>
</li>
</ul>
<h2 id="Eager-Execution"><a href="#Eager-Execution" class="headerlink" title="Eager Execution"></a>Eager Execution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"> <span class="comment"># 启用TF eager execution</span></span><br><span class="line">tfe.enable_eager_execution()</span><br></pre></td></tr></table></figure>
<ul>
<li><p>优势</p>
<ul>
<li>支持python debug工具</li>
<li>提供实时报错</li>
<li>支持python数据结构</li>
<li><p>支持pythonic的控制流</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i = tf.constant(<span class="number">0</span>)</span><br><span class="line">whlile i &lt; <span class="number">1000</span>:</span><br><span class="line">	i = tf.add(i, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>eager execution开启后</p>
<ul>
<li>tensors行为类似<code>np.ndarray</code></li>
<li>大部分API和未开启同样工作，倾向于使用<ul>
<li><code>tfe.Variable</code></li>
<li><code>tf.contrib.summary</code></li>
<li><code>tfe.Iterator</code></li>
<li><code>tfe.py_func</code></li>
<li>面向对象的layers</li>
<li>需要自行管理变量存储</li>
</ul>
</li>
</ul>
</li>
<li><p>eager execution和graph大部分兼容</p>
<ul>
<li>checkpoint兼容</li>
<li>代码可以同时用于python过程、构建图</li>
<li>可使用<code>@tfe.function</code>将计算编译为图</li>
</ul>
</li>
</ul>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><ul>
<li><p>placeholder、sessions</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通TF</span></span><br><span class="line">x = tf.placholder(tf.float32, shape=[<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">m = tf.matmul(x, x)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	m_out = sess.run(m, feed_dict=&#123;x: [[<span class="number">2.</span>]]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Eager Execution</span></span><br><span class="line">x = [[<span class="number">2.</span>]]</span><br><span class="line">m = tf.matmul(x, x)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Lazy loading</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">0</span>]):</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>]):</span><br><span class="line">		<span class="comment"># 不会添加多个节点</span></span><br><span class="line">		<span class="built_in">print</span>(x[i, j])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Device"><a href="#Device" class="headerlink" title="Device"></a>Device</h2><h3 id="设备标识"><a href="#设备标识" class="headerlink" title="设备标识"></a>设备标识</h3><ul>
<li><p>设备标识：设备使用字符串进行标识</p>
<ul>
<li><code>/cpu:0</code>：所有CPU都以此作为名称</li>
<li><code>/gpu:0</code>：第一个GPU，如果有</li>
<li><code>/gpu:1</code>：第二个GPU</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为计算指定硬件资源</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">&quot;/gpu:2&quot;</span>):</span><br><span class="line">	a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], name=<span class="string">&quot;a&quot;</span>)</span><br><span class="line">	b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], name=<span class="string">&quot;b&quot;</span>)</span><br><span class="line">	c = tf.multiply(a, b)</span><br><span class="line">	<span class="comment"># creates a graph</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>环境变量：python中既可以修改<code>os.environ</code>，也可以直接设置
中设置环境变量</p>
<ul>
<li><code>CUDA_VISIBLE_DEVICES</code>：可被使用的GPU id</li>
</ul>
</li>
</ul>
<h3 id="设备执行"><a href="#设备执行" class="headerlink" title="设备执行"></a>设备执行</h3><p>设备执行：TF将图形定义转换成分布式执行的操作以充分利用计算
资源</p>
<ul>
<li><p>TF默认自动检测硬件配置，尽可能使用找到首个GPU执行操作</p>
</li>
<li><p>TF会默认占用所有GPU及每个GPU的所有显存（可配置）</p>
<ul>
<li>但只有一个GPU参与计算，其他GPU默认不参与计算（显存
仍然被占用），需要明确将OPs指派给其执行</li>
<li>指派GPU数量需通过设置环境变量实现</li>
<li>控制显存占用需设置Session <code>config</code>参数</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>注意：有些操作不能再GPU上完成，手动指派计算设备需要注意</li>
</ul>
</blockquote>
<h2 id="tf-ConfigProto"><a href="#tf-ConfigProto" class="headerlink" title="tf.ConfigProto"></a><code>tf.ConfigProto</code></h2><ul>
<li><p>Session参数配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># `tf.ConfigProto`配置方法</span></span><br><span class="line">conf = tf.ConfigProto(log_device_placement=<span class="literal">True</span>)</span><br><span class="line">conf.gpu_options.allow_growth=<span class="literal">True</span></span><br><span class="line">sess = tf.Session(config=conf)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-02T15:17:39.000Z" title="8/2/2019, 11:17:39 PM">2019-08-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T08:41:29.000Z" title="8/4/2021, 4:41:29 PM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Loss/">Loss</a></span><span class="level-item">9 minutes read (About 1280 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Loss/model_evaluation.html">模型评估</a></h1><div class="content"><h2 id="评估方向"><a href="#评估方向" class="headerlink" title="评估方向"></a>评估方向</h2><h3 id="模型误差"><a href="#模型误差" class="headerlink" title="模型误差"></a>模型误差</h3><blockquote>
<ul>
<li>给定损失函数时，基于损失函数的误差显然评估学习方法的标准</li>
</ul>
</blockquote>
<ul>
<li>回归预测模型：模型误差主要使用 <em>MSE</em></li>
<li>分类预测模型：模型误差主要是分类错误率 <em>ERR=1-ACC</em></li>
</ul>
<blockquote>
<ul>
<li>模型训练时采用损失函数不一定是评估时使用的</li>
</ul>
</blockquote>
<h4 id="Training-Error"><a href="#Training-Error" class="headerlink" title="Training Error"></a><em>Training Error</em></h4><p>训练误差：模型在训练集上的误差，损失函数 $L(Y, F(X))$ 均值</p>
<script type="math/tex; mode=display">
e_{train} = R_{emp}(\hat f) = \frac 1 N \sum_{i=1}^N
    L(y_i, \hat {f(x_i)})</script><blockquote>
<ul>
<li>$\hat f$：学习到的模型</li>
<li>$N$：训练样本容量</li>
</ul>
</blockquote>
<ul>
<li>训练时采用的损失函数和评估时一致时，训练误差等于经验风险</li>
<li>训练误差对盘对给定问题是否容易学习是有意义的，但是本质上不重要<ul>
<li>模型训练本身就以最小化训练误差为标准，如：最小化 <em>MSE</em>、最大化预测准确率，一般偏低，不能作为模型预测误差的估计</li>
<li>训练误差随模型复杂度增加单调下降（不考虑模型中随机因素）</li>
</ul>
</li>
</ul>
<h4 id="Test-Error"><a href="#Test-Error" class="headerlink" title="Test Error"></a><em>Test Error</em></h4><p>测试误差：模型在测试集上的误差，损失函数 $L(Y, f(X))$ 均值</p>
<script type="math/tex; mode=display">
e_{test} = \frac 1 {N^{'}} \sum_{i=1}^{N^{'}}
    L(y_i,\hat {f(x_i)})</script><blockquote>
<ul>
<li>$\hat f$：学习到的模型</li>
<li>$N$：测试样本容量</li>
</ul>
</blockquote>
<ul>
<li><p>测试误差反映了学习方法对未知测试数据集的预测能力，是模型 <em>generalization ability</em> 的度量，可以作为模型误差估计</p>
</li>
<li><p>测试误差随模型复杂度增加呈U型</p>
<ul>
<li>偏差降低程度大于方差增加程度，测试误差降低</li>
<li>偏差降低程度小于方差增加程度，测试误差增大</li>
</ul>
</li>
<li>训练误差小但测试误差大表明模型过拟合，使测试误差最小的模型为理想模型</li>
</ul>
<h3 id="模型复杂度"><a href="#模型复杂度" class="headerlink" title="模型复杂度"></a>模型复杂度</h3><blockquote>
<ul>
<li><em>approximation error</em>：近似误差，模型偏差，代表模型对训练集的拟合程度</li>
<li><em>estimation error</em>：估计误差，模型方差，代表模型对训练集波动的稳健性</li>
</ul>
</blockquote>
<ul>
<li><p>模型复杂度越高</p>
<ul>
<li>低偏差：对训练集的拟合充分</li>
<li>高方差：模型紧跟特定数据点，受其影响较大，预测结果不稳定</li>
<li>远离真实关系，模型在来自同系统中其他尚未观测的数据集上预测误差大</li>
</ul>
</li>
<li><p>而训练集、测试集往往不完全相同</p>
<ul>
<li>复杂度较高的模型（过拟合）在测试集上往往由于其高方差效果不好，而建立模型最终目的是用于预测未知数据</li>
<li>所以要兼顾偏差和方差，通过不同建模策略，找到恰当模型，其复杂度不太大且误差在可接受的水平</li>
<li>使得模型更贴近真实关系，泛化能力较好</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>简单模型：低方差高偏差</li>
<li><p>复杂模型：低偏差高方差</p>
</li>
<li><p>模型复杂度衡量参<em>data_science/loss</em></p>
</li>
</ul>
</blockquote>
<h4 id="Over-Fitting"><a href="#Over-Fitting" class="headerlink" title="Over-Fitting"></a><em>Over-Fitting</em></h4><p>过拟合：学习时选择的所包含的模型复杂度大（参数过多），导致模型对已知数据预测很好，对未知数据预测效果很差</p>
<ul>
<li>若在假设空间中存在“真模型”，则选择的模型应该逼近真模型（参数个数相近）</li>
<li>一味追求对训练集的预测能力，复杂度往往会比“真模型”更高</li>
</ul>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><ul>
<li>减少预测变量数量<ul>
<li>最优子集回归：选择合适评价函数（带罚）选择最优模型</li>
<li>验证集挑选模型：将训练集使用 <em>抽样技术</em> 分出部分作为 <em>validation set</em>，使用额外验证集挑选使得损失最小的模型</li>
<li>正则化（罚、结构化风险最小策略）<ul>
<li>岭回归：平方损失，$L_2$ 范数</li>
<li><em>LASSO</em>：绝对值损失，$L_1$ 范数</li>
<li><em>Elastic Net</em></li>
</ul>
</li>
</ul>
</li>
<li>减弱变量特化程度：仅适合迭代求参数的方法<ul>
<li><em>EarlyStop</em>：提前终止模型训练</li>
<li><em>Dropout</em>：每次训练部分神经元</li>
</ul>
</li>
</ul>
<h3 id="模型信息来源"><a href="#模型信息来源" class="headerlink" title="模型信息来源"></a>模型信息来源</h3><ul>
<li>训练数据包含信息</li>
<li>模型形成过程中提供的先验信息<ul>
<li>模型：采用特定内在结构（如深度学习不同网络结构）、条件假设、其他约束条件（正则项）</li>
<li>数据：调整、变换、扩展训练数据，让其展现更多、更有用的信息</li>
</ul>
</li>
</ul>
<h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><ul>
<li><p><em>Classification</em> 分类问题：输出变量$Y$为有限个离散变量</p>
<ul>
<li>混淆矩阵<ul>
<li><em>F-Measure</em></li>
<li><em>TPR</em>、<em>FPR</em></li>
</ul>
</li>
<li><em>ROC</em></li>
<li><em>AUC</em></li>
</ul>
</li>
<li><p><em>Tagging</em> 标注问题：输入 $X^{(1)}, X^{(2)}, \cdots, X^{(n)}$、输出 $Y^{(1)}, Y^{(2)}, \cdots, Y^{(n)}$ <strong>均为变量序列</strong></p>
<ul>
<li>类似分类问题</li>
</ul>
</li>
<li><p><em>Regression</em> 回归问题</p>
<ul>
<li><em>Squared Error</em><ul>
<li><em>MSE</em></li>
<li>$R^2$、$R^2_{Adj}$</li>
<li><em>AIC</em></li>
<li><em>BIC</em></li>
</ul>
</li>
<li><em>Absolute Error</em><ul>
<li><em>MAE</em></li>
<li><em>MAPE</em></li>
<li><em>SMAPE</em></li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>经验损失、结构损失总是能用作评价模型，但是意义不明确</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-31T18:10:08.000Z" title="8/1/2019, 2:10:08 AM">2019-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:52:53.000Z" title="7/16/2021, 2:52:53 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">25 minutes read (About 3688 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/maximum_entropy.html">最大熵模型</a></h1><div class="content"><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><h3 id="逻辑斯蒂分布"><a href="#逻辑斯蒂分布" class="headerlink" title="逻辑斯蒂分布"></a>逻辑斯蒂分布</h3><script type="math/tex; mode=display">\begin{align*}
F(x) & = P(X \leq x) = \frac 1 {1 + e^{-(x-\mu)/\gamma}} \\
f(x) & = F^{'}(x) = \frac {e^{-(x-\mu)/\gamma}}
    {\gamma(1+e^{-(x-\mu)/\gamma})^2}
\end{align*}</script><blockquote>
<ul>
<li>$\mu$：位置参数</li>
<li>$\gamma$：形状参数</li>
</ul>
</blockquote>
<ul>
<li>分布函数属于逻辑斯蒂函数</li>
<li><p>分布函数图像为sigmoid curve</p>
<ul>
<li>关于的$(\mu, \frac 1 2)$中心对称<script type="math/tex; mode=display">
F(-x+\mu) - \frac 1 2 = -F(x+\mu) + \frac 1 2</script></li>
<li>曲线在靠近$\mu$中心附近增长速度快，两端速度增长慢</li>
<li>形状参数$\gamma$越小，曲线在中心附近增加越快</li>
</ul>
</li>
<li><p>模型优点</p>
<ul>
<li>模型输出值位于0、1之间，天然具有概率意义，方便观测
样本概率分数</li>
<li>可以结合$l-norm$正则化解决过拟合、共线性问题</li>
<li>实现简单，广泛用于工业问题</li>
<li>分类时计算量比较小、速度快、消耗资源少</li>
</ul>
</li>
<li><p>模型缺点</p>
<ul>
<li>特征空间很大时，性能不是很好，容易欠拟合，准确率一般</li>
<li>对非线性特征需要进行转换</li>
</ul>
</li>
</ul>
<h3 id="Binomial-Logistic-Regression-Model"><a href="#Binomial-Logistic-Regression-Model" class="headerlink" title="Binomial Logistic Regression Model"></a><em>Binomial Logistic Regression Model</em></h3><p>二项逻辑斯蒂回归模型：形式为参数化逻辑斯蒂分布的二分类
生成模型</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y=1|x) & = \frac {exp(wx + b)} {1 + exp (wx + b)} \\
P(Y=0|x) & = \frac 1 {1 + exp(wx + b)} \\
P(Y=1|\hat x) & = \frac {exp(\hat w \hat x)}
    {1 + exp (\hat w \hat x)} \\
P(Y=0|\hat x) & = \frac 1 {1+exp(\hat w \hat x)}
\end{align*}</script><blockquote>
<ul>
<li>$w, b$：权值向量、偏置</li>
<li>$\hat x = (x^T|1)^T$</li>
<li>$\hat w = (w^T|b)^T$</li>
</ul>
</blockquote>
<ul>
<li><p>逻辑回归比较两个条件概率值，将实例$x$归于条件概率较大类</p>
</li>
<li><p>通过逻辑回归模型，可以将线性函数$wx$转换为概率</p>
<ul>
<li>线性函数值越接近正无穷，概率值越接近1</li>
<li>线性函数值越接近负无穷，概率值越接近0</li>
</ul>
</li>
</ul>
<h4 id="Odds-Odds-Ratio"><a href="#Odds-Odds-Ratio" class="headerlink" title="Odds/Odds Ratio"></a>Odds/Odds Ratio</h4><ul>
<li><p>在逻辑回归模型中，输出$Y=1$的对数几率是输入x的线性函数</p>
<script type="math/tex; mode=display">
log \frac {P(Y=1|x)} {1-P(Y=1|x)} = \hat w \hat x</script></li>
<li><p>OR在逻辑回归中意义：$x_i$每增加一个单位，odds将变为原来
的$e^{w_i}$倍</p>
<script type="math/tex; mode=display">\begin{align*}
odd &= \frac {P(Y=1|x)} {1-P(Y=1|x)} = e^{\hat w \hat x} \\
OR_{x_i+1 / x_i} &= e^{w_i}
\end{align*}</script><ul>
<li><p>对数值型变量</p>
<ul>
<li>多元LR中，变量对应的系数可以计算相应
<em>Conditional OR</em></li>
<li>可以建立单变量LR，得到变量系数及相应
<em>Marginal OR</em></li>
</ul>
</li>
<li><p>对分类型变量</p>
<ul>
<li>可以直接计算变量各取值间对应的OR</li>
<li>变量数值化编码建立模型，得到变量对应OR</li>
</ul>
<blockquote>
<ul>
<li>根据变量编码方式不同，变量对应OR的含义不同，其中
符合数值变量变动模式的是WOE线性编码</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>极大似然：极小对数损失（交叉熵损失）</p>
<script type="math/tex; mode=display">\begin{align*}
L(w) & = log \prod_{i=1}^N [\pi(x_i)]^{y_i}
    [1-\pi(x_i)]^{1-y_i} \\
& = \sum_{i=1}^N [y_i log \pi(x_i) + (1-y_i)log(1-\pi(x_i))] \\
& = \sum_{i=1}^N [y_i log \frac {\pi(x_i)}
    {1-\pi(x_i)} log(1-\pi(x_i))] \\
& = \sum_{i=1}^N [y_i(\hat w \hat x_i) -
    log(1+exp(\hat w \hat x_i))]
\end{align*}</script><blockquote>
<ul>
<li>$\pi(x) = P(Y=1|x)$</li>
</ul>
</blockquote>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ul>
<li>通常采用梯度下降、拟牛顿法求解有以上最优化问题</li>
</ul>
<h3 id="Multi-Nominal-Logistic-Regression-Model"><a href="#Multi-Nominal-Logistic-Regression-Model" class="headerlink" title="Multi-Nominal Logistic Regression Model"></a><em>Multi-Nominal Logistic Regression Model</em></h3><p>多项逻辑斯蒂回归：二项逻辑回归模型推广</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y=j|x) & = \frac {exp(\hat w_j \hat x)} {1+\sum_{k=1}^{K-1}
    exp(\hat w_k \hat x)}, k=1,2,\cdots,K-1 \\
P(Y=K|x) & = \frac 1 {1+\sum_{k=1}^{K-1}
    exp(\hat w_k \hat x)}
\end{align*}</script><ul>
<li>策略、算法类似二项逻辑回归模型</li>
</ul>
<h2 id="Generalized-Linear-Model"><a href="#Generalized-Linear-Model" class="headerlink" title="Generalized Linear Model"></a><em>Generalized Linear Model</em></h2><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><h2 id="Maximum-Entropy-Model"><a href="#Maximum-Entropy-Model" class="headerlink" title="Maximum Entropy Model"></a><em>Maximum Entropy Model</em></h2><h3 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h3><p>最大熵原理：学习概率模型时，在所有可能的概率模型（分布）中，
<strong>熵最大的模型是最好的模型</strong></p>
<ul>
<li><p>使用约束条件确定概率模型的集合，则最大熵原理也可以表述为
<strong>在满足约束条件的模型中选取熵最大的模型</strong></p>
</li>
<li><p>直观的，最大熵原理认为</p>
<ul>
<li>概率模型要满足已有事实（约束条件）</li>
<li>没有更多信息的情况下，不确定部分是等可能的</li>
<li>等可能不容易操作，所有考虑使用<strong>可优化</strong>的熵最大化
表示等可能性</li>
</ul>
</li>
</ul>
<h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><p>最大熵模型为生成模型</p>
<ul>
<li><p>对给定数据集$T={(x_1,y_1),\cdots,(x_N,y_N)}$，联合分布
P(X,Y)、边缘分布P(X)的经验分布如下</p>
<script type="math/tex; mode=display">\begin{align*}
\tilde P(X=x, Y=y) & = \frac {v(X=x, Y=y)} N \\
\tilde P(X=x) & = \frac {v(X=x)} N
\end{align*}</script><blockquote>
<ul>
<li>$v(X=x,Y=y)$：训练集中样本$(x,y)$出频数</li>
</ul>
</blockquote>
</li>
<li><p>用如下<em>feature function</em> $f(x, y)$描述输入x、输出y之间
某个事实</p>
<script type="math/tex; mode=display">f(x, y) = \left \{ \begin{array}{l}
1, & x、y满足某一事实 \\
0, & 否则
\end{array} \right.</script><ul>
<li><p>特征函数关于经验分布$\tilde P(X, Y)$的期望</p>
<script type="math/tex; mode=display">
E_{\tilde P} = \sum_{x,y} \tilde P(x,y)f(x,y)</script></li>
<li><p>特征函数关于生成模型$P(Y|X)$、经验分布$\tilde P(X)$
期望</p>
<script type="math/tex; mode=display">
E_P(f(x)) = \sum_{x,y} \tilde P(x)P(y|x)f(x,y)</script></li>
</ul>
</li>
<li><p>期望模型$P(Y|X)$能够获取数据中信息，则两个期望值应该相等</p>
<script type="math/tex; mode=display">\begin{align*}
E_P(f) & = E_{\tilde P}(f) \\
\sum_{x,y} \tilde P(x)P(y|x)f(x,y) & =
   \sum_{x,y} \tilde P(x,y)f(x,y)
\end{align*}</script><p>此即作为模型学习的约束条件</p>
<ul>
<li><p>此约束是纯粹的关于$P(Y|X)$的约束，只是约束形式特殊，
需要通过期望关联熵</p>
</li>
<li><p>若有其他表述形式、可以直接带入的、关于$P(Y|X)$约束，
可以直接使用</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>满足所有约束条件的模型集合为<script type="math/tex; mode=display">
  \mathcal{C} = \{P | E_{P(f_i)} = E_{\tilde P (f_i)},
      i=1,2,\cdots,n \}</script>  定义在条件概率分布$P(Y|X)$上的条件熵为<script type="math/tex; mode=display">
  H(P) = -\sum_{x,y} \tilde P(x) P(y|x) logP(y|x)</script>  则模型集合$\mathcal{C}$中条件熵最大者即为最大是模型</li>
</ul>
</blockquote>
<h3 id="策略-1"><a href="#策略-1" class="headerlink" title="策略"></a>策略</h3><p>最大熵模型的策略为以下约束最优化问题</p>
<script type="math/tex; mode=display">\begin{array}{l}
\max_{P \in \mathcal{C}} & -H(P)=\sum_{x,y} \tilde P(x)
    P(y|x) logP(y|x) \\
s.t. & E_P(f_i) - E_{\tilde P}(f_i) = 0, i=1,2,\cdots,M \\
& \sum_{y} P(y|x)  = 1
\end{array}</script><ul>
<li><p>引入拉格朗日函数</p>
<script type="math/tex; mode=display">\begin{align*}
L(P, w) & = -H(P) - w_0(1-\sum_y P(y|x)) + \sum_{m=1}^M
   w_m(E_{\tilde P}(f_i) - E_P(f_i)) \\
& = \sum_{x,y} \tilde P(x) P(y|x) logP(y|x) + w_0
   (1-\sum_y P(y|x)) + \sum_{m=1}^M w_m (\sum_{x,y}
   \tilde P(x,y)f_i(x, y) - \tilde P(x)P(y|x)f_i(x,y))
\end{align*}</script><ul>
<li><p>原始问题为</p>
<script type="math/tex; mode=display">
\min_{P \in \mathcal{C}} \max_{w} L(P, w)</script></li>
<li><p>对偶问题为</p>
<script type="math/tex; mode=display">
\max_{w} \min_{P \in \mathcal{C}} L(P, w)</script></li>
<li><p>考虑拉格朗日函数$L(P, w)$是P的凸函数，则原始问题、
对偶问题解相同</p>
</li>
</ul>
</li>
<li><p>记</p>
<script type="math/tex; mode=display">\begin{align*}
\Psi(w) & = \min_{P \in \mathcal{C}} L(P, w)
   = L(P_w, w) \\
P_w & = \arg\min_{P \in \mathcal{C}} L(P, w) = P_w(Y|X)
\end{align*}</script></li>
<li><p>求$L(P, w)$对$P(Y|X)$偏导</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial L(P, w)} {\partial P(Y|X)} & =
   \sum_{x,y} \tilde P(x)(logP(y|x)+1) - \sum_y w_0 -
   \sum_{x,y}(\tilde P(x) \sum_{i=1}^N w_i f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x)(log P(y|x) + 1 - w_0 -
   \sum_{i=1}^N w_i f_i(x, y))
\end{align*}</script><p>偏导置0，考虑到$\tilde P(x) &gt; 0$，其系数必始终为0，有</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y|X) & = \exp(\sum_{i=1}^N w_i f_i(x,y) + w_0 - 1) \\
& = \frac {exp(\sum_{i=1}^N w_i f_i(x,y))} {exp(1-w_0)}
\end{align*}</script></li>
<li><p>考虑到约束$\sum_y P(y|x) = 1$，有</p>
<script type="math/tex; mode=display">\begin{align*}
P_w(y|x) & = \frac 1 {Z_w(x)} exp(\sum_{i=1}^N w_i
   f_i(x,y)) \\
Z_w(x) & = \sum_y exp(\sum_{i=1}^N w_i f_i(x,y)) \\
& = exp(1 - w_0)
\end{align*}</script><blockquote>
<ul>
<li>$Z_w(x)$：规范化因子</li>
<li>$f(x, y)$：特征</li>
<li>$w_i$：特征权值</li>
</ul>
</blockquote>
</li>
<li><p>原最优化问题等价于求解偶问题极大化问题$\max_w \Psi(w)$</p>
<script type="math/tex; mode=display">\begin{align*}
\Psi(w) & = \sum_{x,y} \tilde P(x) P_w(y|x) logP_w(y|x)
   + \sum_{i=1}^N w_i(\sum_{x,y} \tilde P(x,y) f_i(x,y)
   - \sum_{x,y} \tilde P(x) P_w(y|x) f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) +
   \sum_{x,y} \tilde P(x,y) P_w(y|x)(log P_w(y|x) -
   \sum_{i=1}^N w_i f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_{x,y} \tilde P(x,y) P_w(y|x) log Z_w(x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_x \tilde P(x) log Z_w(x)
\end{align*}</script><p>记其解为</p>
<script type="math/tex; mode=display">w^{*} = \arg\max_w \Psi(w)</script><p>带入即可得到最优（最大熵）模型$P_{w^{*}}(Y|X)$</p>
</li>
</ul>
<h4 id="策略性质"><a href="#策略性质" class="headerlink" title="策略性质"></a>策略性质</h4><ul>
<li><p>已知训练数据的经验概率分布为$\tilde P(X,Y)$，则条件概率
分布$P(Y|X)$的对数似然函数为</p>
<script type="math/tex; mode=display">\begin{align*}
L_{\tilde P}(P_w) & = N log \prod_{x,y}
   P(y|x)^{\tilde P(x,y)} \\
& = \sum_{x,y} N * \tilde P(x,y) log P(y|x)
\end{align*}</script><blockquote>
<ul>
<li>这里省略了系数样本数量$N$</li>
</ul>
</blockquote>
</li>
<li><p>将最大熵模型带入，可得</p>
<script type="math/tex; mode=display">\begin{align*}
L_{\tilde P_w} & = \sum_{x,y} \tilde P(y|x) logP(y|x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_{x,y} \tilde P(x,y)log Z_w(x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_x \tilde P(x) log Z_w(x) \\
& = \Psi(w)
\end{align*}</script><p>对偶函数$\Psi(w)$等价于对数似然函数$L_{\tilde P}(P_w)$，
即最大熵模型中，<strong>对偶函数极大等价于模型极大似然估计</strong></p>
</li>
</ul>
<h3 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h3><ul>
<li><p>思想</p>
<ul>
<li>假设最大熵模型当前参数向量$w=(w_1,w_2,\cdots,w_M)^T$</li>
<li>希望能找到新的参数向量（参数向量更新）
$w+\sigma=(w_1+\sigma_1,\cdots,w_M+\sigma_M)$
使得模型对数似然函数/对偶函数值增加</li>
<li>不断对似然函数值进行更新，直到找到对数似然函数极大值</li>
</ul>
</li>
<li><p>对给定经验分布$\tilde P(x,y)$，参数向量更新至$w+\sigma$
时，对数似然函数值变化为</p>
<script type="math/tex; mode=display">\begin{align*}
L(w+\sigma) - L(w) & = \sum_{x,y} \tilde P(x,y)
   log P_{w+\sigma}(y|x) - \sum_{x,y} \tilde P(x,y)
   log P_w(y|x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) - \sum_x \tilde P(x) log \frac
   {Z_{w+\sigma}(x)} {Z_w(x)} \\
& \geq \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) + 1 - \sum_x \tilde P(x) \frac
   {Z_{w+\sigma}(x)} {Z_w(x)} \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) + 1 - \sum_x \tilde P(x) \sum_y P_y(y|x)
   exp(\sum_{i=1}^M \sigma_i f_i(x,y))
\end{align*}</script><ul>
<li><p>不等式步利用$a - 1 \geq log a, a \geq 1$</p>
</li>
<li><p>最后一步利用</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {Z_{w+\sigma}(x)} {Z_w(x)} & = \frac 1 {Z_w(x)}
  \sum_y exp(\sum_{i=1}^M (w_i + \sigma_i)
  f_i(x, y)) \\
& = \frac 1 {Z_w(x)} \sum_y exp(\sum_{i=1}^M w_i
  f_i(x,y) + \sigma_i f_i(x,y)) \\
& = \sum_y P_w(y|x) exp(\sum_{i=1}^n \sigma_i
  f_i(x,y))
\end{align*}</script></li>
</ul>
</li>
<li><p>记上式右端为$A(\sigma|w)$，则其为对数似然函数改变量的
一个下界</p>
<script type="math/tex; mode=display">
L(w+\sigma) - L(w) \geq A(\sigma|w)</script><ul>
<li>若适当的$\sigma$能增加其值，则对数似然函数值也应该
增加</li>
<li>函数$A(\sigma|w)$中因变量$\sigma$为向量，难以同时
优化，尝试每次只优化一个变量$\sigma_i$，固定其他变量
$\sigma_j$</li>
</ul>
</li>
<li><p>记</p>
<script type="math/tex; mode=display">f^{**} (x,y) = \sum_i f_i(x,y)</script><p>考虑到$f_i(x,y)$为二值函数，则$f^{**}(x,y)$表示所有特征
在$(x,y)$出现的次数，且有</p>
<script type="math/tex; mode=display">
A(\sigma|w) = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M
   \sigma_i f_i(x,y) + 1 - \sum_x \tilde P(x)
   \sum_y P_w(y|x) exp(f^{**}(x,y) \sum_{i=1}^M
   \frac {\sigma_i f_i(x,y)} {f^{**}(x,y)})</script></li>
<li><p>考虑到$\sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)} = 1$，
由指数函数凸性、Jensen不等式有</p>
<script type="math/tex; mode=display">
exp(\sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)} \sigma_i
   f^{**}(x,y)) \leq \sum_{i=1}^M \frac {f_i(x,y)}
   {f^{**}(x,y)} exp(\sigma_i f^{**}(x,y))</script><p>则</p>
<script type="math/tex; mode=display">
A(\sigma|w) \geq \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M
   \sigma_i f_i(x,y) + 1 - \sum_x \tilde P(x) \sum_y
   P_w(y|x) \sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)}
   exp(\sigma_i f^{**}(x,y))</script></li>
<li><p>记上述不等式右端为$B(\sigma|w)$，则有</p>
<script type="math/tex; mode=display">
L(w+\sigma) - L(w) \geq B(\sigma|w)</script><p>其为对数似然函数改变量的一个新、相对不紧的下界</p>
</li>
<li><p>求$B(\sigma|w)$对$\sigma_i$的偏导</p>
<script type="math/tex; mode=display">
\frac {\partial B(\sigma|w)} {\partial \sigma_i} =
   \sum_{x,y} \tilde P(x,y) f_i(x,y) -
   \sum_x \tilde P(x) \sum_y P_w(y|x) f_i(x,y)
   exp(\sigma_i f^{**}(x,y))</script><p>置偏导为0，可得</p>
<script type="math/tex; mode=display">
\sum_x \tilde P(x) \sum_y P_w(y|x) f_i(x,y) exp(\sigma_i
   f^{**}(x,y)) = \sum_{x,y} \tilde P(x,y) f_i(x,y) =
   E_{\tilde P}(f_i)</script><p>其中仅含变量$\sigma_i$，则依次求解以上方程即可得到
$\sigma$</p>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><blockquote>
<ul>
<li>输入：特征函数$f_1, f_2, \cdots, f_M$、经验分布
  $\tilde P(x)$、最大熵模型$P_w(x)$</li>
<li>输出：最优参数值$w<em>i^{*}$、最优模型$P</em>{w^{*}}$</li>
</ul>
</blockquote>
<ol>
<li><p>对所有$i \in {1,2,\cdots,M}$，取初值$w_i = 0$</p>
</li>
<li><p>对每个$i \in {1,2,\cdots,M}$，求解以上方程得$\sigma_i$</p>
<ul>
<li><p>若$f^{**}(x,y)=C$为常数，则$\sigma_i$有解析解</p>
<script type="math/tex; mode=display">
\sigma_i = \frac 1 C log \frac {E_{\tilde P}(f_i)}
 {E_P(f_i)}</script></li>
<li><p>若$f^{**}(x,y)$不是常数，则可以通过牛顿法迭代求解</p>
<script type="math/tex; mode=display">
\sigma_i^{(k+1)} = \sigma_i^{(k)} - \frac
 {g(\sigma_i^{(k)})} {g^{'}(\sigma_i^{(k)})}</script><blockquote>
<ul>
<li>$g(\sigma_i)$：上述方程对应函数</li>
</ul>
</blockquote>
<ul>
<li>上述方程有单根，选择适当初值则牛顿法恒收敛</li>
</ul>
</li>
</ul>
</li>
<li><p>更新$w_i$，$w_i \leftarrow w_i + \sigma_i$，若不是所有
$w_i$均收敛，重复2</p>
</li>
</ol>
<h3 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h3><p>对最大熵模型</p>
<ul>
<li><p>为方便，目标函数改为求极小</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min_{w \in R^M} f(w) = \sum_x \tilde P(x) log \sum_{y}
   exp(\sum_{i=1}^M w_i f_i(x,y)) - \sum_{x,y}
   \tilde P(x,y) \sum_{i=1}^M w_i f_i(x,y)
\end{array}</script></li>
<li><p>梯度为</p>
<script type="math/tex; mode=display">\begin{align*}
g(w) & = (\frac {\partial f(w)} {\partial w_i}, \cdots,
   \frac {\partial f(w)} {\partial w_M})^T \\
\frac {\partial f(w)} {\partial w_M} & = \sum_{x,y}
   \tilde P(x) P_w(y|x) f_i(x,y) - E_{\tilde P}(f_i)
\end{align*}</script></li>
</ul>
<h4 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h4><p>将目标函数带入BFGS算法即可</p>
<blockquote>
<ul>
<li>输入：特征函数$f_1, f_2, \cdots, f_M$、经验分布
  $\tilde P(x)$、最大熵模型$P_w(x)$</li>
<li>输出：最优参数值$w<em>i^{*}$、最优模型$P</em>{w^{*}}$</li>
</ul>
</blockquote>
<ol>
<li><p>取初值$w^{(0)}$、正定对称矩阵$B^{(0)}$，置k=0</p>
</li>
<li><p>计算$g^{(k)} = g(w^{(k)})$，若$|g^{(k)}| &lt; \epsilon$，
停止计算，得到解$w^{*} = w^{(k)}$</p>
</li>
<li><p>由拟牛顿公式$B^{(k)}p^{(k)} = -g^{(k)}$求解$p^{(k)}$</p>
</li>
<li><p>一维搜索，求解</p>
<script type="math/tex; mode=display">
\lambda^{(k)} = \arg\min_{\lambda} f(w^{(k)} +
  \lambda p_k)</script></li>
<li><p>置$w^{(k+1)} = w^{(k)} + \lambda^{(k)} p_k$</p>
</li>
<li><p>计算$g^{(k+1)} = g(w^{(k+1)})$，若
$|g^{(k+1)}| &lt; \epsilon$，停止计算，得到解
$w^{*} = w^{(k+1)}$，否则求</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} - \frac {B^{(k)} s^{(k)}
  (s^{(k)})^T B^{(k)}} {(s^{(k)})^T B^{(k)} s^{(k)}}
  + \frac {y^{(k)} (y^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script><blockquote>
<ul>
<li>$s^{(k)} = w^{(k+1)} - w^{(k)}$</li>
<li>$y^{(k)} = g^{(k+1)} - g^{(k)}$</li>
</ul>
</blockquote>
</li>
<li><p>置k=k+1，转3</p>
</li>
</ol>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/tags/Machine-Learning/">Previous</a></div><div class="pagination-next"><a href="/tags/Machine-Learning/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/tags/Machine-Learning/">1</a></li><li><a class="pagination-link is-current" href="/tags/Machine-Learning/page/2/">2</a></li><li><a class="pagination-link" href="/tags/Machine-Learning/page/3/">3</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/tags/Machine-Learning/page/8/">8</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6371777973" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>