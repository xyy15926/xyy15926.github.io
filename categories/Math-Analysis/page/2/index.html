<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: Math Analysis - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"Hexo","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">Math Analysis</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">7 minutes read (About 1060 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/unconstrained_problems_thoeries.html">无约束优化</a></h1><div class="content"><h2 id="无约束局部解"><a href="#无约束局部解" class="headerlink" title="无约束局部解"></a>无约束局部解</h2><script type="math/tex; mode=display">
minf(x), x \in R^n</script><blockquote>
<ul>
<li>若存在$x^{ <em> } \in R^n, \epsilon &gt; 0, \forall x \in R^n$
  使得$|x - x^{ </em> }| &lt; \epsilon$时，恒有</li>
<li>$f(x) \geq f(x^{ <em> })$：则称$x^{ </em> }$为f(x)的
  <em>local minimum point/solution</em>（局部极小点/局部解）</li>
<li>$f(x) &gt; f(x^{ <em> })$：则称$x^{ </em> }$为f(x)的
  严格局部极小点/局部解</li>
</ul>
</blockquote>
<h2 id="最优性条件"><a href="#最优性条件" class="headerlink" title="最优性条件"></a>最优性条件</h2><h3 id="First-Order-Necessary-Condtion"><a href="#First-Order-Necessary-Condtion" class="headerlink" title="First-Order Necessary Condtion"></a><em>First-Order Necessary Condtion</em></h3><blockquote>
<ul>
<li>无约束问题局部解的一阶必要条件：设f(x)有连续的一阶偏导，
  弱$x^{ * }$是无约束问题的局部解，则<script type="math/tex; mode=display">\triangledown f(x{* }) = 0</script></li>
</ul>
</blockquote>
<h3 id="Second-Order-Necessary-Condition"><a href="#Second-Order-Necessary-Condition" class="headerlink" title="Second-Order Necessary Condition"></a><em>Second-Order Necessary Condition</em></h3><blockquote>
<ul>
<li>无约束问题局部解的二阶必要条件：设f(x)有连续二阶偏导，
  若$x^{ * }$是无约束问题的局部解，则<blockquote>
<ul>
<li><script type="math/tex; mode=display">\triangledown f(x^{ * }) = 0</script></li>
<li><script type="math/tex; mode=display">\triangledown^2 f(x^{ * })半正定</script></li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h3 id="Second-Order-Sufficient-Condition"><a href="#Second-Order-Sufficient-Condition" class="headerlink" title="Second-Order Sufficient Condition"></a><em>Second-Order Sufficient Condition</em></h3><blockquote>
<ul>
<li>无约束问题局部解的二阶充分条件：设f(x)有连续二阶偏导，
  若在$x^{ <em> }$处满足以下，则x^{ </em> }是无约束问题的
  <strong>严格局部解</strong><blockquote>
<ul>
<li><script type="math/tex; mode=display">\triangledown f(x^{ * }) = 0</script></li>
<li><script type="math/tex; mode=display">\triangledown^2 f(x^{ * })正定</script></li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h2 id="下降算法"><a href="#下降算法" class="headerlink" title="下降算法"></a>下降算法</h2><p>迭代算法：将当前迭代点<strong>向正确方向</strong>移动<strong>一定步长</strong>，然后
检验目标值是否满足一定要求</p>
<ul>
<li><strong>方向</strong>、<strong>步长</strong>就是不同优化算法主要关心的两个方面</li>
<li>还关心算法的<em>rate of convergence</em>（收敛速率）</li>
</ul>
<h3 id="一般下降算法框架"><a href="#一般下降算法框架" class="headerlink" title="一般下降算法框架"></a>一般下降算法框架</h3><ol>
<li><p>取初始点$x^{(1)}$，置精度要求$\epsilon$，置k=1</p>
</li>
<li><p>若在点$x^{(k)}$处满足某个终止准则，则停止计算，得无约束
优化问题最优解$x^{(k)}$，否则<strong>适当地选择</strong>$x^{(k)}$处
<strong>搜索方向</strong></p>
</li>
<li><p>进行<strong>适当的一维搜索</strong>，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) =
  f(x^{(k)} + \alpha d^{(k)})</script></li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
<p>要使下降算法可行，需要确定</p>
<ul>
<li>某点出搜索方向<ul>
<li>负梯度方向</li>
<li>Newton方向：求方向的时候已确定步长，也可用做步长搜索</li>
<li>拟Newton方向</li>
</ul>
</li>
<li>求步长地一维搜索方式<ul>
<li>试探法<ul>
<li>0.618法</li>
<li>Fibonacci方法（分数法）</li>
<li>二分法</li>
</ul>
</li>
<li>插值法<ul>
<li>三点二次插值法</li>
<li>二点二次插值法</li>
<li>两点三次插值法</li>
</ul>
</li>
<li>非精确一维搜索方法<ul>
<li>Glodstein方法</li>
<li>Armijo方法</li>
<li>Wolfe-Powell方法</li>
</ul>
</li>
</ul>
</li>
<li><p>算法终止准则</p>
<ul>
<li>$|\triangledown f(x^{(k)})| &lt; \epsilon$</li>
<li>$|x^{(k+1)} - x^{(k)}| &lt; \epsilon$</li>
<li>$|f(x^{(k+1)}) - f(x^{(k)})| &lt; \epsilon$</li>
</ul>
<blockquote>
<ul>
<li>实际计算中最优解可能永远无法迭代达到，应该采用较弱
 终止准则</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="算法收敛性"><a href="#算法收敛性" class="headerlink" title="算法收敛性"></a>算法收敛性</h3><blockquote>
<ul>
<li>收敛：序列${x^{(k)}}$或其一个子列（仍记${x^{(k)}}$）
  满足<script type="math/tex; mode=display">
  \lim_{k \rightarrow \infty} x^{(k)} = x^{ * }</script><blockquote>
<ul>
<li>$x^{ * }$：无约束问题局部解</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<p>但是这样强的结果难以证明</p>
<ul>
<li>往往只能证明${x^{(k)}}$的任一聚点的稳定点</li>
<li>或是更弱的<script type="math/tex; mode=display">
\lim_{k \rightarrow \infty} inf
   \|\triangledown f(x^{(k)}) \| = 0</script></li>
</ul>
<blockquote>
<ul>
<li>局部收敛算法：只有初始点充分靠近极小点时，才能保证产生
  序列收敛</li>
<li>全局收敛算法：对任意初始点，产生序列均能收敛</li>
</ul>
</blockquote>
<h4 id="收敛速率"><a href="#收敛速率" class="headerlink" title="收敛速率"></a>收敛速率</h4><p>设序列${x^{(k)}}$收敛到$x^{ * }$，若以下极限存在</p>
<script type="math/tex; mode=display">
\lim _ {k \rightarrow \infty} \frac {\|x^{(k+1)} - x^{*}\|}
    {\|x^{(k)} - x^{*}\|} = \beta</script><blockquote>
<ul>
<li>$0 &lt; \beta &lt; 1$：线性收敛</li>
<li>$\beta = 0$：超线性收敛</li>
<li>$\beta = 1$：次线性收敛（收敛速率太慢，一般不考虑）</li>
</ul>
</blockquote>
<h4 id="算法的二次终止性"><a href="#算法的二次终止性" class="headerlink" title="算法的二次终止性"></a>算法的二次终止性</h4><blockquote>
<ul>
<li>二次终止性：若某算法对任意正定二次函数，从任意初始点出发
  ，都能经过有限步迭代达到其极小点，则称该算法有二次终止性</li>
</ul>
</blockquote>
<p>具有二次终止性的算法被认为时好算法，否则计算效果较差，原因</p>
<ul>
<li><p>正定二次目标函数有某些好的性质，好的算法应该能在有限步内
达到其极小点</p>
</li>
<li><p>对于一个一般的目标函数，若其在极小点处的Hesse矩阵
$\triangledown f(x^{( * )})$，则由泰勒展开式得到</p>
<script type="math/tex; mode=display">\begin{align*}
f(x) & = f(x^{*}) + \triangledown f(x^ {*})^T(x - x^{*}) \\
   & + \frac 1 2 (x - x^{*})^T \triangledown^2 f(x^{*})
       (x - x^{*}) \\
   & + o(\|x - x^{*}\|^2)
\end{align*}</script><p>即目标函数f(x)在极小点附近与一个正定二次函数近似，所以对
正定二次函数好的算法，对一般目标函数也应该具有较好的性质</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">17 minutes read (About 2586 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/optimization_problems.html">凸优化问题</a></h1><div class="content"><h2 id="Linear-Programming"><a href="#Linear-Programming" class="headerlink" title="Linear Programming"></a><em>Linear Programming</em></h2><h3 id="数学模型"><a href="#数学模型" class="headerlink" title="数学模型"></a>数学模型</h3><h4 id="一般数学模型"><a href="#一般数学模型" class="headerlink" title="一般数学模型"></a>一般数学模型</h4><p>线性规划问题（LP）可以抽象为一般的数学模型</p>
<script type="math/tex; mode=display">\begin{array}{l}
(\min_x, \max_x) & S=c_1 x_1 + c_2 x_2 + \cdots + c_n x_n \\
s.t. & \left \{ \begin{array} {l}
        a_{1,1} x_1 + a_{1,2} x_2 + \cdots + a_{1,n} x_n (\geq = \leq) b_1 \\
        a_{2,1} x_1 + a_{2,2} x_2 + \cdots + a_{2,n} x_n (\geq = \leq) b_2 \\
        \vdots \\
        a_{m,1} x_1 + a_{m,2} x_2 + \cdots + a_{m,n} x_n (\geq = \leq) b_m
    \end{array} \right.
\end{array}</script><blockquote>
<ul>
<li>$S = c_1 x_1 + c_2 x_2 + \cdots + c_n x_n$：目标函数</li>
<li>$x_1, x_2, …, x_n$：待求解变量</li>
<li>$b<em>i、c_i、a</em>{ij}$：实常数</li>
<li>$(\geq = \leq)$：在三种符号中取一种</li>
</ul>
</blockquote>
<h4 id="标准形式"><a href="#标准形式" class="headerlink" title="标准形式"></a>标准形式</h4><script type="math/tex; mode=display">\begin{array}{l}
\min_x & S=c_1 x_1 + c_2 x_2 + \cdots + c_n x_n \\
s.t. & a_{1,1} x_1 + a_{1,2} x_2 + \cdots + a_{1,n} x_n = b_1 \\
& \vdots \\
& a_{t,1} x_1 + a_{t,2} x_2 + \cdots + a_{t,n} x_n = b_t \\
& a_{t+1,1} x_1 + a_{t+1,2} x_2 + \cdots + a_{t+1, n} x_n
    \leq b_{t+1} \\
& \vdots \\
& a_{t+l,1} x_1 + a_{t+l,2} x_2 + \cdots + a_{t+l, n} x_n
    \leq b_{t+l} \\
\end{array}</script><blockquote>
<ul>
<li>$\max_x$：目标函数取翻转换为$\min_x$</li>
<li><p>$\geq$：不等式左右取反转换为$\leq$</p>
</li>
<li><p>线性规划一般模式都可以等价转换为标准形式</p>
</li>
</ul>
</blockquote>
<h3 id="Simplex-Method"><a href="#Simplex-Method" class="headerlink" title="Simplex Method"></a>Simplex Method</h3><p>单纯型法：利用线性规划极值点必然在单纯型顶点取得，不断迭代顶点求出极值</p>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ul>
<li>初始化：标准化线性规划问题，建立初始表格<ul>
<li>最小化目标函数：目标函数系数取反，求极大</li>
<li>不等式约束：加入松弛变量（代表不等式两端差值）</li>
<li>变量非负：定义为两个非负变量之差</li>
</ul>
</li>
<li>最优测试<ul>
<li>若目标行系数都为非负，得到最优解，迭代停止</li>
<li>基变量解在右端列中，非基变量解为 0</li>
</ul>
</li>
<li>确定主元列<ul>
<li>从目标行的前 $n$ 个单元格中选择一个非负单元格，确定主元列</li>
<li>选择首个非负：解稳定，若存在最优解总是能取到</li>
<li>选择绝对值最大：目标函数下降快，但有可能陷入死循环，无法得到最优解（不满足最优条件）</li>
</ul>
</li>
<li><p>确定主元（分离变量）（行）</p>
<ul>
<li>对主元列所有正系数，计算右端项和其比值 $\Theta$ 比率</li>
<li>最小 $\Theta$ 比率确定主元（行）（类似的为避免死循环，总是选择首个最小者）</li>
</ul>
</li>
<li><p>转轴变换（建立新单纯形表）</p>
<ul>
<li>主元变 1：主元行所有变量除以主元</li>
<li>主元列变 0：其余行减去其主元列倍主元行</li>
<li>交换基变量：主元行变量标记为主元列对应变量</li>
</ul>
</li>
</ul>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>算法时间效率<ul>
<li>极点规模随着问题规模指数增长，所以最差效率是指数级</li>
<li>实际应用表明，对 $m$ 个约束、$n$ 个变量的问题，算法迭代次数在 $m$ 到 $3m$ 之间，每次迭代次数正比于 $nm$</li>
</ul>
</li>
<li>迭代改进</li>
</ul>
<h3 id="Two-Phase-Simplex-Method"><a href="#Two-Phase-Simplex-Method" class="headerlink" title="Two-Phase Simplex Method"></a>Two-Phase Simplex Method</h3><p>两阶段单纯形法：单纯型表中没有单元矩阵，无法方便找到基本可行解时使用</p>
<ul>
<li>在给定问题的约束等式中加入人工变量，使得新问题具有明显可行解</li>
<li>利用单纯形法求解最小化新的线性规划问题</li>
</ul>
<h3 id="其他一些算法"><a href="#其他一些算法" class="headerlink" title="其他一些算法"></a>其他一些算法</h3><ul>
<li>大 M 算法</li>
<li><em>Ellipsoid Method</em> 椭球算法<ul>
<li>算法时间效率<ul>
<li>可以在多项式时间内对任意线性规划问题求解</li>
<li>实际应用效果较单纯形法差，但是最差效率更好</li>
</ul>
</li>
</ul>
</li>
<li><em>Karmarkar</em> 算法<ul>
<li>内点法（迭代改进）</li>
</ul>
</li>
</ul>
<h2 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h2><script type="math/tex; mode=display">\begin{array}{l}
\min_x & f(x) \\
s.t. & g_i(x) \leq 0, i=1，2,\cdots,k \\
& h_i(x) = 0, i=1,2,\cdots,l
\end{array}</script><blockquote>
<ul>
<li>$f(x), g(x)$：$R^n$ 上连续可微的凸函数</li>
<li>$h_i(x)$：$R^n$ 上仿射函数</li>
<li>仿射函数：满足 $f(x)=ax+b, a \in R^n, b \in R, x \in R^n$</li>
</ul>
</blockquote>
<h3 id="二次规划"><a href="#二次规划" class="headerlink" title="二次规划"></a>二次规划</h3><script type="math/tex; mode=display">\begin{array}{l}
\min_x & f(x)=\frac 1 2 x^TGx + c^Tx \\
s.t. & Ax \leq b
\end{array}</script><blockquote>
<ul>
<li>$G \in R^{n * n}$：$n$ 阶实对称矩阵</li>
<li>$A \in R^{m <em> n}$：$m </em> n$ 实矩阵</li>
<li>$b \in R^m$</li>
<li>$c \in R^n$</li>
</ul>
</blockquote>
<ul>
<li><p>$G$ 正定</p>
<ul>
<li>此时目标函数 $f(x)$ 为凸函数</li>
<li>凸二次规划</li>
<li>问题有唯一全局最小值</li>
<li>问题可可由椭球法在多项式时间内求解</li>
</ul>
</li>
<li><p>$G$ 半正定</p>
<ul>
<li>此时目标函数 $f(x)$ 为凸函数</li>
<li>半定规划</li>
<li>若约束条件可行域不空，且目标函数在此可行域有下界，则问题有全局最小值</li>
</ul>
</li>
<li><p>$G$非正定</p>
<ul>
<li>目标函数有多个平稳点（局部极小），<em>NP-hard</em> 问题</li>
</ul>
</li>
</ul>
<h4 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h4><ul>
<li>椭球法</li>
<li>内点法</li>
<li>增广拉格朗日法</li>
<li>投影梯度法</li>
</ul>
<h3 id="二阶锥规划"><a href="#二阶锥规划" class="headerlink" title="二阶锥规划"></a>二阶锥规划</h3><script type="math/tex; mode=display">\begin{array}{l}
\min_x & f^Tx \\
s.t. & \|A_ix + b_i \|_2 \leq c_i^Tx + d_i, i=1,2,\cdots,m  \\
    & Bx=g
\end{array}</script><blockquote>
<ul>
<li>$f \in R^n$</li>
<li>$A_i \in R^{n_i * n}$，$b_i \in R^{n_i}$，$c_i \in R^{n_i}$，$d_i \in R$</li>
<li>$B \in R^{p * n}$，$g \in R^n$</li>
</ul>
</blockquote>
<ul>
<li><p>$A_i=0,i=1,\cdots,m$：退化为线性规划</p>
</li>
<li><p>一般的二阶规划可以转换为二阶锥规划</p>
<script type="math/tex; mode=display">
X^TAX + qTX + C \leq 0 \Rightarrow
   \|A^{1/2}x + \frac 1 2 A^{-1/2}q\|^{1/2} \leq
   -\frac 1 4 q^TA^{-1}q - c</script></li>
</ul>
<blockquote>
<ul>
<li>二阶锥规划可以使用内点法很快求解（多项式时间）</li>
</ul>
</blockquote>
<h2 id="非线性最小二乘"><a href="#非线性最小二乘" class="headerlink" title="非线性最小二乘"></a>非线性最小二乘</h2><script type="math/tex; mode=display">\begin{align*}
f(x) & = \frac 1 2 \sum_{i=1}^m r^2_i(x) \\
& = \frac 1 2 r(x) r^T(x)
\end{align*}</script><blockquote>
<ul>
<li>$r_i(x)$：通常为非线性函数</li>
<li>$r(x) = (r_1(x), \cdots, r_n(x))^T$</li>
<li>$x \in R^n, m \geq n$</li>
</ul>
</blockquote>
<ul>
<li><p>考虑目标函数梯度、<em>Hesse</em> 矩阵</p>
<script type="math/tex; mode=display">\begin{align*}
\nabla f(x) & = \sum_{i=1}^m \nabla r_i(x)
   r_i(x) \\
& = J(x)^T r(x) \\

\nabla^2 f(x) & = \sum_{i=1}^m \nabla r_i(x)
   r_i(x) + \sum_{i=1}^m r_i \nabla^2 r_i(x) \\
& = J(x)^T J(x) + \sum_{i=1}^m r_i(x) \nabla^2 r_i(x)
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li><script type="math/tex; mode=display">
  J(x) = \begin{bmatrix}
  \frac {\partial r_1} {\partial x_1} &
      \frac {\partial r_1} {\partial x_2} & \cdots &
      \frac {\partial r_1} {\partial x_n} \\
  \frac {\partial r_2} {\partial x_1} &
      \frac {\partial r_2} {\partial x_2} & \cdots &
      \frac {\partial r_2} {\partial x_n} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac {\partial r_m} {\partial x_1} &
      \frac {\partial r_m} {\partial x_2} & \cdots &
      \frac {\partial r_m} {\partial x_n}
  \end{bmatrix}
  = \begin{bmatrix}
  \nabla r_1(x)^T \\
  \nabla r_2(x)^T \\
  \vdots \\
  \nabla r_m(x)^T \\
  \end{bmatrix}</script>  为$r(x)$的 <em>Jacobi</em> 矩阵</li>
</ul>
</blockquote>
<h3 id="Gauss-Newton-法"><a href="#Gauss-Newton-法" class="headerlink" title="Gauss-Newton 法"></a><em>Gauss-Newton</em> 法</h3><ul>
<li><p>为简化计算，略去 <em>Newton</em> 法中 <em>Hesse</em> 矩阵中 $\sum_{i=1}^m r_i(x) \nabla^2 r_i(x)$ 项，即直接求解方程组</p>
<script type="math/tex; mode=display">
J(x^{(k)})^T J(x^{(k)}) d = -J(x^{(k)})^T r(x^{(k)})</script></li>
<li><p>求解同一般 <em>Newton</em> 法</p>
</li>
</ul>
<h4 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>实际问题中</p>
<ul>
<li>局部解 $x^{ <em> }$ 对应的目标函数值 $f(x^{ </em> })$ 接近 0 时，采用 <em>Gauss-Newton</em> 法效果较好，此时<ul>
<li>$|r(x^{(k)})|$ 较小</li>
<li>曲线$r_i(x)$接近直线</li>
<li>$\nabla^2 r_i(x) \approx 0$</li>
</ul>
</li>
<li>否则效果一般</li>
</ul>
</li>
<li><p>矩阵 $J(x^{(k)})^T J(x^{(k)})$ 是半正定矩阵</p>
<ul>
<li>当 <em>Jacobi</em> 矩阵列满秩时为正定矩阵，此时虽然 $d^{(k)}$ 是下降方向，但仍需类似修正牛顿法增加一维搜索策略保证目标函数值不上升</li>
</ul>
</li>
</ul>
<h3 id="Levenberg-Marquardt-方法"><a href="#Levenberg-Marquardt-方法" class="headerlink" title="Levenberg-Marquardt 方法"></a><em>Levenberg-Marquardt</em> 方法</h3><ul>
<li><p>考虑到 $J(x^{(k)})$ 中各列线性相关、接近线性相关，求解 <em>Newton-Gauss </em>方法中的方程组会出现困难，可以改为求解</p>
<script type="math/tex; mode=display">
(J(x^{(k)})^T J(x^{(k)}) + vI) d = -J(x^{(k)})^T r(x^{(k)})</script></li>
</ul>
<blockquote>
<ul>
<li>$v$：迭代过程中需要调整的参数，<em>LM</em> 方法的关键即如何调整</li>
</ul>
</blockquote>
<h4 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h4><blockquote>
<ul>
<li>若 $d(v)$ 是以上方程组的解，则 $|d(v)|^2$ 是 $v$ 的连续下降函数，且 $v \rightarrow +\infty, |d(v)| \rightarrow 0$</li>
</ul>
</blockquote>
<ul>
<li><p>$J(x^{(k)})^T J(x^{(k)})$ 是对称半正定矩阵，则存在正交阵</p>
<script type="math/tex; mode=display">
(P^{(k)})^T J(x^{(k)})^T J(x^{(k)}) P^{(k)} = \Lambda^{(k)}</script></li>
<li><p>则可以解出 $|d(v)|^2$</p>
</li>
</ul>
<blockquote>
<ul>
<li>增大 $v$ 可以限制 $|d^{(k)}|$，所以 <em>LM</em> 方法也被称为阻尼最小二乘法</li>
</ul>
</blockquote>
<h4 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h4><blockquote>
<ul>
<li>若 $d(v)$ 是以上方程的解，则 $d(v)$ 是 $f(x)$ 在 $x^{(k)}$ 处的下降方向，且 $v \rightarrow + \infty$ 时，$d(v)$ 的方向与 $-J(x^{(k)})^T r(x^{(k)})$ 方向一致</li>
</ul>
</blockquote>
<ul>
<li>下降方向：$\nabla f(x^{(k)}) d(v) &lt; 0$ 即可</li>
<li>方向一致：夹角余弦</li>
</ul>
<blockquote>
<ul>
<li>$v$充分大时，<em>LM</em> 方法产生的搜索方向 $d^{(k)}$ 和负梯度方向一致</li>
</ul>
</blockquote>
<h4 id="参数调整方法"><a href="#参数调整方法" class="headerlink" title="参数调整方法"></a>参数调整方法</h4><blockquote>
<ul>
<li>使用梯度、近似 <em>Hesse</em> 矩阵定义二次函数<script type="math/tex; mode=display">
  q(d) = f(x^{(k)}) + (J(x^{(k)})^T r(x^{(k)}))^T d + \frac 1 2 d^T (J(x^{(k)})^T J(x^{(k)})) d</script></li>
</ul>
</blockquote>
<ul>
<li><p>其增量为</p>
<script type="math/tex; mode=display">\begin{align*}
\Delta q^{(k)} & = q(d^{(k)}) - q(0) \\
& = (J(x^{(k)})^T r(x^{(k)}))^T d^{(k)} + \frac 1 2
   (d^{(k)})^T (J(x^{(k)})^T J(x^{(k)})) d^{(k)}
\end{align*}</script></li>
<li><p>目标函数增量</p>
<script type="math/tex; mode=display">\begin{align*}
\Delta f^{(k)} & = f(x^{(k)} + d^{(k)}) - f(x^{(k)}) \\
& = f(x^{(k+1)}) - f(x^{(k)})
\end{align*}</script></li>
<li><p>定义$\gamma_k = \frac {\Delta f^{(k)}} {\Delta q^{(k)}}$</p>
<ul>
<li><p>$\gamma_k$接近1说明$\Delta f^{(k)}$接近$\Delta q^{(k)}$</p>
<ul>
<li>即$f(x^{(k)} + d^{(k+1)})$接近$q(d^{(k)})$</li>
<li>即$f(x)$在$x^{(k)}$附近接近二次函数</li>
<li>即使用Gauss-Newton方法求解最小二乘问题效果较好</li>
<li>即LM方法求解时$v$参数应该较小</li>
</ul>
</li>
<li><p>$\gamma_k$接近0说明$\Delta f^{(k)}$与$\Delta q^{(k)}$
近似程度不好</p>
<ul>
<li>$d^{(k)}$不应取得过大，应减少$d^{(k)}$得模长</li>
<li>应该增加参数$v$进行限制</li>
<li>迭代方向趋近于负梯度方向</li>
</ul>
</li>
<li><p>$\gamma_k$适中时，认为参数$v$选取合适，不做调整</p>
<ul>
<li>临界值通常为0.25、0.75</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始点 $x^{(1)}$、初始参数 $v$（小值）、精度要求 $\epsilon$，置 $k=k+1$</p>
</li>
<li><p>若 $|J(x^{(k)})^T r(x^{(k)})| &lt; \epsilon$，则停止计算，得到问题解 $x^{(k)}$，否则求解线性方程组</p>
<script type="math/tex; mode=display">
(J(x^{(k)})^T J(x^{(k)}) + v_kI) d = -J(x^{(k)})^T
  r(x^{(k)})</script><p>得到 $d^{(k)}$</p>
</li>
<li><p>置 $x^{(k+1)} = x^{(k)} + d^{(k)}$，计算 $\gamma_k$</p>
</li>
<li><p>考虑 $\gamma$</p>
<ul>
<li>$\gamma &lt; 0.25$，置$v_{k+1} = 4 v_k$</li>
<li>$\gamma &gt; 0.75$，置$v_{k+1} = v_k / 2$</li>
<li>否则置$v_{k+1} = v_k$</li>
</ul>
</li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
<h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><h3 id="整形规划"><a href="#整形规划" class="headerlink" title="整形规划"></a>整形规划</h3><p>整形规划：求线性函数的最值，函数包含若干<strong>整数变量</strong>，并且满足线性等式、不等式的有限约束</p>
<h3 id="Unregularized-Least-Squares-Learning-Problem"><a href="#Unregularized-Least-Squares-Learning-Problem" class="headerlink" title="Unregularized Least Squares Learning Problem"></a><em>Unregularized Least Squares Learning Problem</em></h3><script type="math/tex; mode=display">
w_T = \frac \gamma n \sum_{i=0}^{T-1} (I - \frac \gamma n
    {\hat X}^T \hat X)^i {\hat X}^T \hat Y</script><blockquote>
<ul>
<li>$\gamma$：被引入保证 $|I - \frac \gamma n {\hat X}^T \hat X| &lt; 1$</li>
</ul>
</blockquote>
<h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><ul>
<li><p>考虑</p>
<script type="math/tex; mode=display">
\min_w I_s(w) = \frac 1 {2n} \|\hat X w - \hat Y\|^2</script></li>
<li><p>将$w_{t+1}$带入$I_s(w)$即可证明每次迭代$I_s(w)$减小</p>
<script type="math/tex; mode=display">
w_0 = 0 \\
w_{t+1} = (I - \frac \gamma n {\hat X}^T \hat X)w_t + \frac \gamma n {\hat X}^T \hat Y</script></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">7 minutes read (About 1121 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/unconstrained_specials.html">无约束优化特殊问题</a></h1><div class="content"><h2 id="正定二次目标函数"><a href="#正定二次目标函数" class="headerlink" title="正定二次目标函数"></a>正定二次目标函数</h2><script type="math/tex; mode=display">
min f(x) = \frac 1 2 x^T G x + r^T x + \sigma</script><h2 id="非线性最小二乘"><a href="#非线性最小二乘" class="headerlink" title="非线性最小二乘"></a>非线性最小二乘</h2><script type="math/tex; mode=display">\begin{align*}
f(x) & = \frac 1 2 \sum_{i=1}^m r^2_i(x) \\
& = \frac 1 2 r(x) r^T(x)
\end{align*}</script><blockquote>
<ul>
<li>$r_i(x)$：通常为非线性函数</li>
<li>$r(x) = (r_1(x), \cdots, r_n(x))^T$</li>
<li>$x \in R^n, m \geq n$</li>
</ul>
</blockquote>
<p>则</p>
<script type="math/tex; mode=display">\begin{align*}
\nabla f(x) & = \sum_{i=1}^m \nabla r_i(x)
    r_i(x) \\
& = J(x)^T r(x) \\

\nabla^2 f(x) & = \sum_{i=1}^m \nabla r_i(x)
    r_i(x) + \sum_{i=1}^m r_i \nabla^2 r_i(x) \\
& = J(x)^T J(x) + \sum_{i=1}^m r_i(x) \nabla^2 r_i(x)
\end{align*}</script><blockquote>
<ul>
<li><script type="math/tex; mode=display">
  J(x) = \begin{bmatrix}
  \frac {\partial r_1} {\partial x_1} &
      \frac {\partial r_1} {\partial x_2} & \cdots &
      \frac {\partial r_1} {\partial x_n} \\
  \frac {\partial r_2} {\partial x_1} &
      \frac {\partial r_2} {\partial x_2} & \cdots &
      \frac {\partial r_2} {\partial x_n} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac {\partial r_m} {\partial x_1} &
      \frac {\partial r_m} {\partial x_2} & \cdots &
      \frac {\partial r_m} {\partial x_n}
  \end{bmatrix}
  = \begin{bmatrix}
  \nabla r_1(x)^T \\
  \nabla r_2(x)^T \\
  \vdots \\
  \nabla r_m(x)^T \\
  \end{bmatrix}</script>  为$r(x)$的Jacobi矩阵</li>
</ul>
</blockquote>
<h3 id="Gauss-Newton法"><a href="#Gauss-Newton法" class="headerlink" title="Gauss-Newton法"></a>Gauss-Newton法</h3><p>Newton法中为简化计算，略去其Hesse矩阵中
$\sum_{i=1}^m r_i(x) \nabla^2 r_i(x)$项，即直接求解
方程组</p>
<script type="math/tex; mode=display">
J(x^{(k)})^T J(x^{(k)}) d = -J(x^{(k)})^T r(x^{(k)})</script><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>同Newton法，仅求解Newton方程改为求解以上方程组</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>实际问题中</p>
<ul>
<li>局部解$x^{ <em> }$对应的目标函数值$f(x^{ </em> })$接近0
时，$|r(x^{(k)})|$较小</li>
<li>曲线$r_i(x)$接近直线，
$\nabla^2 r_i(x) \approx 0$</li>
</ul>
<p>采用Gauss-Newton法效果较好，否则效果一般</p>
</li>
<li><p>矩阵$J(x^{(k)})^T J(x^{(k)})$是半正定矩阵，当Jacobi矩阵
列满秩时为正定矩阵，此时虽然$d^{(k)}$是下降方向，但仍需
类似修正牛顿法增加一维搜索策略保证目标函数值不上升</p>
</li>
</ul>
<h3 id="Levenberg-Marquardt方法"><a href="#Levenberg-Marquardt方法" class="headerlink" title="Levenberg-Marquardt方法"></a>Levenberg-Marquardt方法</h3><p>但$J(x^{(k)})$中各列线性相关、接近线性相关，则求解
Newton-Gauss方法中的方程组会出现困难，可以改为求解</p>
<script type="math/tex; mode=display">
(J(x^{(k)})^T J(x^{(k)}) + vI) d = -J(x^{(k)})^T r(x^{(k)})</script><blockquote>
<ul>
<li>$v$：迭代过程中需要调整的参数，LM方法的关键即如何调整</li>
</ul>
</blockquote>
<h4 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h4><blockquote>
<ul>
<li>若$d(v)$是以上方程组的解，则$|d(v)|^2$是$v$的连续下降
  函数，且$v \rightarrow +\infty, |d(v)| \rightarrow 0$</li>
</ul>
</blockquote>
<ul>
<li><p>$J(x^{(k)})^T J(x^{(k)})$是对称半正定矩阵，则存在正交阵</p>
<script type="math/tex; mode=display">
(P^{(k)})^T J(x^{(k)})^T J(x^{(k)}) P^{(k)} =
   \Lambda^{(k)}</script></li>
<li><p>则可以解出$|d(v)|^2$</p>
</li>
</ul>
<blockquote>
<ul>
<li>增大$v$可以限制$|d^{(k)}|$，所以LM方法也被称为阻尼最小
  二乘法</li>
</ul>
</blockquote>
<h4 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h4><blockquote>
<ul>
<li>若$d(v)$是以上方程的解，则$d(v)$是$f(x)$在$x^{(k)}$处的
  下降方向，且$v \rightarrow + \infty$时，$d(v)$的方向与
  $-J(x^{(k)})^T r(x^{(k)})$方向一致</li>
</ul>
</blockquote>
<ul>
<li>下降方向：$\nabla f(x^{(k)}) d(v) &lt; 0$即可</li>
<li>方向一致：夹角余弦</li>
</ul>
<blockquote>
<ul>
<li>$v$充分大时，LM方法产生的搜索方向$d^{(k)}$和负梯度方向
  一致</li>
</ul>
</blockquote>
<h4 id="参数调整方法"><a href="#参数调整方法" class="headerlink" title="参数调整方法"></a>参数调整方法</h4><p>使用梯度、近似Hesse矩阵定义二次函数</p>
<script type="math/tex; mode=display">
q(d) = f(x^{(k)}) + (J(x^{(k)})^T r(x^{(k)}))^T d +
    \frac 1 2 d^T (J(x^{(k)})^T J(x^{(k)})) d</script><blockquote>
<ul>
<li><p>其增量为</p>
<script type="math/tex; mode=display">\begin{align*}
  \Delta q^{(k)} & = q(d^{(k)}) - q(0) \\
  & = (J(x^{(k)})^T r(x^{(k)}))^T d^{(k)} + \frac 1 2
      (d^{(k)})^T (J(x^{(k)})^T J(x^{(k)})) d^{(k)}
  \end{align*}</script></li>
<li><p>目标函数增量</p>
<script type="math/tex; mode=display">\begin{align*}
  \Delta f^{(k)} & = f(x^{(k)} + d^{(k)}) - f(x^{(k)}) \\
  & = f(x^{(k+1)}) - f(x^{(k)})
  \end{align*}</script></li>
<li><p>定义$\gamma_k = \frac {\Delta f^{(k)}} {\Delta q^{(k)}}$</p>
</li>
</ul>
</blockquote>
<ul>
<li><p>$\gamma_k$接近1说明$\Delta f^{(k)}$接近$\Delta q^{(k)}$</p>
<ul>
<li>即$f(x^{(k)} + d^{(k+1)})$接近$q(d^{(k)})$</li>
<li>即$f(x)$在$x^{(k)}$附近接近二次函数</li>
<li>即使用Gauss-Newton方法求解最小二乘问题效果较好</li>
<li>即LM方法求解时$v$参数应该较小</li>
</ul>
</li>
<li><p>$\gamma_k$接近0说明$\Delta f^{(k)}$与$\Delta q^{(k)}$
近似程度不好</p>
<ul>
<li>$d^{(k)}$不应取得过大，应减少$d^{(k)}$得模长</li>
<li>应该增加参数$v$进行限制</li>
<li>迭代方向趋近于负梯度方向</li>
</ul>
</li>
<li><p>$\gamma_k$适中时，认为参数$v$选取合适，不做调整</p>
<ul>
<li>临界值通常为0.25、0.75</li>
</ul>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始点$x^{(1)}$、初始参数$v$（小值）、精度要求$\epsilon$
，置k=k+1</p>
</li>
<li><p>若$|J(x^{(k)})^T r(x^{(k)})| &lt; \epsilon$，则停止计算，
得到问题解$x^{(k)}$，否则求解线性方程组</p>
<script type="math/tex; mode=display">
(J(x^{(k)})^T J(x^{(k)}) + v_kI) d = -J(x^{(k)})^T
  r(x^{(k)})</script><p>得到$d^{(k)}$</p>
</li>
<li><p>置$x^{(k+1)} = x^{(k)} + d^{(k)}$，计算$\gamma_k$</p>
</li>
<li><p>若</p>
<ul>
<li>$\gamma &lt; 0.25$，置$v_{k+1} = 4 v_k$</li>
<li>$\gamma &gt; 0.75$，置$v_{k+1} = v_k / 2$</li>
<li>否则置$v_{k+1} = v_k$</li>
</ul>
</li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:29:45.000Z" title="8/4/2021, 11:29:45 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Real-Analysis/">Real Analysis</a></span><span class="level-item">a minute read (About 120 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Real-Analysis/cone.html">Cone</a></h1><div class="content"><h2 id="Cone"><a href="#Cone" class="headerlink" title="Cone"></a><em>Cone</em></h2><ul>
<li><p>锥：$C \subset V, \forall x \in C, a&gt;0 \Rightarrow  ax \in C$</p>
<ul>
<li>锥总是无界的</li>
</ul>
<blockquote>
<ul>
<li>$V$：向量空间</li>
</ul>
</blockquote>
</li>
<li><p><em>Convex Cone</em> 凸锥：$\forall x,y \in C, \forall a,b &gt; 0 \Rightarrow ax + by \in C$</p>
<ul>
<li>凸锥必然是凸集</li>
<li>非凸锥：凸锥的补集</li>
</ul>
</li>
<li><p><em>Norm Cone</em> $n$ 维标准锥：$C = { (x,t)| |x|_2 \leq t, x \in R^{n-1}, t \in R }$</p>
</li>
<li><p><em>Second Order Cone</em> 二阶锥：$C = { (x,t)|Ax+b|_2 \leq c^Tx + d }$</p>
<ul>
<li>二阶锥相对于对标准锥做了仿射变换（平移变换）</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Real-Analysis/">Real Analysis</a></span><span class="level-item">a minute read (About 168 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Real-Analysis/convex.html">凸分析</a></h1><div class="content"><h2 id="Notations-and-Terminology"><a href="#Notations-and-Terminology" class="headerlink" title="Notations and Terminology"></a>Notations and Terminology</h2><h3 id="Strong-Convexity"><a href="#Strong-Convexity" class="headerlink" title="Strong Convexity"></a><em>Strong Convexity</em></h3><ul>
<li><p>凸函数 $f$ 满足</p>
<script type="math/tex; mode=display">
\forall x, y \in R, \forall \lambda \in (0,1), 
   f(\lambda x + (1-\lambda) y) \leq \lambda f(x) +
   (1-\lambda)f(y)</script></li>
<li><p>强凸函数：不等式严格不等的凸函数</p>
<ul>
<li>为保证强凸性，常添加二次项保证，如：增广拉格朗日</li>
</ul>
</li>
</ul>
<h3 id="凸集相关标记"><a href="#凸集相关标记" class="headerlink" title="凸集相关标记"></a>凸集相关标记</h3><blockquote>
<ul>
<li>$C \subseteq R^N$：非空凸集</li>
<li>$x \in R^N$</li>
</ul>
</blockquote>
<ul>
<li><p>点 $x \in R^N$ 到 $C$ 的距离为</p>
<script type="math/tex; mode=display">D_C(x) = \min_{y \in C} \|x-y\|_2</script></li>
<li><p>点 $x \in R^N$ 在 $C$ 上投影为</p>
<script type="math/tex; mode=display">P_Cx \in C, D_C(x) = \|x - P_Cx\|_2</script><ul>
<li>$C \subseteq R^N$：闭凸集</li>
</ul>
</li>
<li><p><em>Indicator Function</em> 凸集 $C$ 的示性函数为</p>
<script type="math/tex; mode=display">
l_C(x) = \left \{ \begin{array}
   0 & if x \in C \\
   +\infty & if x \notin C
\end{array} \right.</script></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Real-Analysis/">Real Analysis</a></span><span class="level-item">a minute read (About 146 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Real-Analysis/functions.html">函数</a></h1><div class="content"><h3 id="齐次函数"><a href="#齐次函数" class="headerlink" title="齐次函数"></a>齐次函数</h3><p>齐次函数：有倍数性质的函数，若变量乘以系数 $\alpha$，则新函数为原函数乘上 $\alpha^k$ 倍</p>
<script type="math/tex; mode=display">
f(\alpha x) = \alpha^k f(x)</script><blockquote>
<ul>
<li>$\alpha \neq 0 \in F, x \in X$</li>
<li>$f: X \rightarrow W$：域 $F$ 内两个向量空间之间的 $k$ 次齐次函数</li>
</ul>
</blockquote>
<ul>
<li>线性函数 $f: X \rightarrow W$ 是一次齐次函数</li>
<li>多线性函数 $f: x_1 <em> x_2 </em> \cdots * x_n \rightarrow W$ 是 $n$ 次齐次函数</li>
</ul>
<h4 id="基本定理"><a href="#基本定理" class="headerlink" title="基本定理"></a>基本定理</h4><blockquote>
<ul>
<li>欧拉定理：函数 $f: R^n \rightarrow R$ 可导、$k$ 次齐次函数，则有 $x \nabla f(x) = kf(x)$</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:31:21.000Z" title="8/4/2021, 11:31:21 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Real-Analysis/">Real Analysis</a></span><span class="level-item">7 minutes read (About 1092 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Real-Analysis/subgredient.html">次梯度</a></h1><div class="content"><h2 id="次梯度"><a href="#次梯度" class="headerlink" title="次梯度"></a>次梯度</h2><ul>
<li><p>次梯度：实变量凸函数 $f$ 在点 $x_0$ 的次梯度 $c$ 满足</p>
<script type="math/tex; mode=display">
\forall x, f(x) - f(x_0) \geq c(x - x_0)</script></li>
<li><p>可证明凸函数 $f$ 在 $x_0$ 处所有次梯度的集合 $\partial f(x)$ 是非空凸紧集</p>
<ul>
<li><p>$\partial f(x) = [a, b]$，其中$a, b$为单侧极限</p>
<script type="math/tex; mode=display">\begin{align*}
a & = lim_{x \rightarrow x_0^{-}} \frac {f(x) - f_0(x)} {x - x_0} \\
b & = lim_{x \rightarrow x_0^{+}} \frac {f(x) - f_0(x)} {x - x_0}
\end{align*}</script></li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>凸函数均指下凸函数，梯度不减</li>
</ul>
</blockquote>
<h3 id="次梯度性质"><a href="#次梯度性质" class="headerlink" title="次梯度性质"></a>次梯度性质</h3><h4 id="运算性质"><a href="#运算性质" class="headerlink" title="运算性质"></a>运算性质</h4><ul>
<li><p>数乘性质</p>
<script type="math/tex; mode=display">
\partial(\alpha f)(x) = \alpha \partial f(x), \alpha > 0</script></li>
<li><p>加法性质</p>
<script type="math/tex; mode=display">\begin{align*}
f &= f_1 + f_2 + \cdots + f_m, \\
\partial f &= \partial f_1 + \cdots + \partial f_m
\end{align*}</script></li>
<li><p>仿射性质：$f$为凸函数</p>
<script type="math/tex; mode=display">\begin{align*}
h(x) &=  f(Ax + b) \\
\partial h(x) &= A^T \partial f(Ax + b)
\end{align*}</script></li>
</ul>
<h4 id="最优化性质"><a href="#最优化性质" class="headerlink" title="最优化性质"></a>最优化性质</h4><ul>
<li><p>凸函数 $f$ 在点 $x_0$ 可导，当且仅当次梯度仅包含一个点，即该点导数</p>
</li>
<li><p>点 $x_0$ 是凸函数 $f$ 最小值，当且仅当次微分中包含 0
（此性质为“可导函数极小点导数为 0 ”推广）</p>
</li>
<li><p>负次梯度方向不一定是下降方向</p>
</li>
</ul>
<h2 id="次梯度求解"><a href="#次梯度求解" class="headerlink" title="次梯度求解"></a>次梯度求解</h2><ul>
<li>逐点（分段）极值的函数求次梯度</li>
<li>求出该点相应极值函数</li>
<li>求出对应梯度即为次梯度</li>
</ul>
<h3 id="Pointwise-Maximum"><a href="#Pointwise-Maximum" class="headerlink" title="Pointwise Maximum"></a><em>Pointwise Maximum</em></h3><p>逐点最大函数：目标函数为</p>
<script type="math/tex; mode=display">\begin{align*}
f(x) & = max \{f_1(x), f_2(x), \cdots, f_m(x)\} \\
I(x) & = \{i | f_i(x) = f(x)\}
\end{align*}</script><blockquote>
<ul>
<li>$I(x)$：保存 $x$ 处取值最大的函数下标</li>
</ul>
</blockquote>
<ul>
<li><p>弱结果：$I(x)$ 中随机抽取，以 $f_i(x)$ 在该处梯度作为次梯度</p>
</li>
<li><p>强结果</p>
<script type="math/tex; mode=display">
\partial f(x) = conv \cup_{i \in I(x)} \partial f_i(x)</script><ul>
<li>先求支撑平面，再求所有支撑平面的凸包</li>
<li>可导情况实际上是不可导的特殊情况</li>
</ul>
</li>
</ul>
<h4 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h4><p><img src="/imgs/subgredient_piecewise_function.png" alt="subgredient_piecewise_function"></p>
<ul>
<li><p>折点处</p>
<script type="math/tex; mode=display">
\partial f(x) = conv\{a_i, a_{i+1}\} = [a_i, a_{i+1}]</script></li>
<li><p>非折点处</p>
<script type="math/tex; mode=display">\partial f(x) = {a_i}</script></li>
</ul>
<h4 id="L-1-范数"><a href="#L-1-范数" class="headerlink" title="$L_1$范数"></a>$L_1$范数</h4><p><img src="/imgs/subgredient_l1norm.png" alt="subgredient_l1norm"></p>
<h3 id="PointWise-Supremum"><a href="#PointWise-Supremum" class="headerlink" title="PointWise Supremum"></a><em>PointWise Supremum</em></h3><p>逐点上确界：目标函数为</p>
<script type="math/tex; mode=display">\begin{align*}
f(x) &= \sup_{\alpha \in A} f_{\alpha}(x) \\
I(x) &= \{\alpha \in A | f_{\alpha}(x) = f(x)\}
\end{align*}</script><ul>
<li><p>弱结果：可行梯度为</p>
<script type="math/tex; mode=display">
\partial (\max_{\alpha} f_{\alpha}(x)) \in partial f(x)</script></li>
<li><p>强结果</p>
<script type="math/tex; mode=display">
\partial f(x) = conv \cup_{\alpha \in I(x)} \partial f_{alpha}(x) \subseteq \partial f(x)</script></li>
</ul>
<h4 id="最大特征值"><a href="#最大特征值" class="headerlink" title="最大特征值"></a>最大特征值</h4><script type="math/tex; mode=display">\begin{align*}
f(x) & = \lambda_{max}(A(x)) = \sup_{\|y\|_2 = 1} \\
A(x) & = A_0 + x_1 A_1 + \cdots + x_n A_n
\end{align*}</script><blockquote>
<ul>
<li>$A_n$：对称矩阵</li>
</ul>
</blockquote>
<ul>
<li><p>对确定 $\hat {x}$，$A(x)$ 最大特征值 $\lambda_{max}$、对应特征向量 $y$，则该点此梯度为</p>
<script type="math/tex; mode=display">(y^T A_0 y, \cdots, y^T A_n y)</script></li>
</ul>
<h3 id="Pointwise-Inferior"><a href="#Pointwise-Inferior" class="headerlink" title="Pointwise Inferior"></a><em>Pointwise Inferior</em></h3><p>逐点下确界：目标函数为</p>
<script type="math/tex; mode=display">
f(x) = \inf_y h(x, y)</script><blockquote>
<ul>
<li>$h$：凸函数</li>
</ul>
</blockquote>
<ul>
<li><p>弱结果：给定$x = \hat x$，可行次梯度为</p>
<script type="math/tex; mode=display">
(\partial h(x, \hat y)|_{x=\hat x}, 0) \in \partial f(x)</script></li>
</ul>
<h3 id="复合函数"><a href="#复合函数" class="headerlink" title="复合函数"></a>复合函数</h3><script type="math/tex; mode=display">
f(x) = h(f_1(x), \cdots, f_n(x))</script><blockquote>
<ul>
<li>$h$：凸、不降函数</li>
<li>$f_i$：凸函数</li>
</ul>
</blockquote>
<ul>
<li><p>弱结果：给定$x = \hat x$，可行次梯度为</p>
<script type="math/tex; mode=display">
g = z_1 g_1 + \cdots + z_k g_k \in \partial f(\hat x)</script><blockquote>
<ul>
<li>$z \in \partial h(f_1(\hat x), \cdots, f_k(\hat x))$</li>
<li>$g_i \in \partial f_i(\hat x)$</li>
</ul>
</blockquote>
<ul>
<li><p>证明</p>
<script type="math/tex; mode=display">\begin{align*}
f(x) & \geq h(f_1(\hat x) + g_1^T(x - \hat x), \cdots,
  f_k(\hat x) + g_k^T(x - \hat x) \\
& \geq h(f_1(\hat x), \cdots, f_k(\hat x)) +
  z^T(g_1^T(x - \hat x), \cdots, g_k^T(x - \hat x)) \\
& = f(\hat x) + g^T(x - \hat x)
\end{align*}</script></li>
</ul>
</li>
</ul>
<h2 id="次梯度法"><a href="#次梯度法" class="headerlink" title="次梯度法"></a>次梯度法</h2><script type="math/tex; mode=display">\begin{align*}
x^{(k+1)} & = x^{(k)} + \alpha_k g^{(k)} \\
f_{best}^{(k+1)} & = min{f_{best}^{(k)}, f(x^{(k+1)})}
\end{align*}</script><blockquote>
<ul>
<li>$g^{(k)}$：函数$f$在$x^{(k)}$处次梯度</li>
</ul>
</blockquote>
<ul>
<li><p>求解凸函数最优化的问题的一种迭代方法</p>
</li>
<li><p>相较于较内点法、牛顿法慢</p>
<ul>
<li>应用范围更广泛：能够用于不可微目标函数，目标函数可微时，无约束问题次梯度与梯度下降法有同样搜索方向</li>
<li>空间复杂度更小</li>
<li>可以和分解技术结合，得到简单分配算法</li>
<li>通常不会产生稀疏解</li>
</ul>
</li>
</ul>
<h3 id="步长选择"><a href="#步长选择" class="headerlink" title="步长选择"></a>步长选择</h3><blockquote>
<ul>
<li>次梯度法选取步长方法很多，以下为保证收敛步长规则</li>
</ul>
</blockquote>
<ul>
<li>恒定步长：$\alpha_k = \alpha$</li>
<li>恒定间隔：$\alpha_k = \gamma / |g^{(k)}|_2$</li>
<li>步长平方可加、步长不可加：$\sum<em>{k=1}^{\infty} \alpha_k^2 &lt; \infty, \sum</em>{k=1}^{\infty} \alpha_k = \infty$</li>
<li>步长不可加、但递减：$lim_{k \rightarrow \infty} \alpha_k = 0$</li>
<li>间隔不可加、但递减：$lim_{k \rightarrow \gamma_k} \gamma_k = 0$</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-26T17:01:10.000Z" title="6/27/2019, 1:01:10 AM">2019-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-06-26T17:01:10.000Z" title="6/27/2019, 1:01:10 AM">2019-06-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">7 minutes read (About 999 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/lagrange_duality.html">Lagrange 对偶</a></h1><div class="content"><h2 id="Langrangian-Duality"><a href="#Langrangian-Duality" class="headerlink" title="Langrangian Duality"></a><em>Langrangian Duality</em></h2><p>拉格朗日对偶</p>
<ul>
<li><p>考虑优化问题：找到$f(x)$满足约束的最好下界</p>
<script type="math/tex; mode=display">
z^{*} = \min_{x} f(x) \\
\begin{align*}
s.t. \quad & g_i(x) \leq 0, i=1,2,\cdots,m \\
   & x \in X
\end{align*}</script></li>
<li><p>考虑方程组</p>
<script type="math/tex; mode=display">
\left \{ \begin{array}{l}
f(x) < v \\
g_i(x) \leq 0, i=1,2,\cdots,m
\end{array} \right.</script><ul>
<li><p><strong>方程组无解</strong>：$v$是优化问题的一个下界</p>
</li>
<li><p><strong>方程组有解</strong>：则可以推出</p>
<script type="math/tex; mode=display">
\forall \lambda \geq 0, \exists x, 
f(x) + \sum_{i=1}^m \lambda_ig_i(x) < v</script><blockquote>
<ul>
<li>显然，取$g_1 + g_2 = 0, g_1(x) &gt; 0$是反例，不能
推出原方程有解</li>
</ul>
</blockquote>
</li>
<li><p>由以上方程组有解逆否命题：方程组无解<strong>充分条件</strong>如下</p>
<script type="math/tex; mode=display">
\exists \lambda \geq 0,
\min_{x} f(x) + \sum _{i=1}^m \lambda_ig_i(x) \geq v</script></li>
</ul>
</li>
<li><p>由此方法推出的最好下界，即拉格朗日对偶问题</p>
<script type="math/tex; mode=display">
v^{*} = \max_{\lambda \geq 0} \min_{x} f(x) +
   \sum_{i=1}^m \lambda_ig_i(x)</script></li>
</ul>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><ul>
<li><p>拉格朗日对偶对实数域上的优化问题都存在，对目标函数、
约束函数都没有要求</p>
</li>
<li><p>强对偶定理：$v^{<em>} = z^{</em>}$，需要$f,g$满足特定条件才成立</p>
<ul>
<li>线性规划</li>
<li>半正定规划</li>
<li>凸优化</li>
</ul>
<blockquote>
<ul>
<li>即需要给约束条件加以限制，使得<script type="math/tex; mode=display">
 \forall \lambda \geq 0, \exists x, 
 f(x) + \sum_{i=1}^m \lambda_ig_i(x) < v</script> 是上述方程组有解的冲要条件</li>
</ul>
</blockquote>
</li>
<li><p>弱对偶定理：$v^{<em>} \leq z^{</em>}$，永远成立（以上即可证）</p>
<ul>
<li>通过弱对偶定理，可以得到原问题的一个下界</li>
<li>对求解原问题有帮助，比如：分支界限法中快速求下界</li>
</ul>
</li>
<li><p>对偶问题相关算法往往原问题算法在实际应用中往往更加有效</p>
<ul>
<li><em>dual-simplex</em></li>
<li><em>primal-dual interior point method</em></li>
<li><em>augmented Lagrangian Method</em></li>
</ul>
</li>
</ul>
<h2 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h2><p>约束最优化问题</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min_{x \in R^n} & f(x) \\
s.t. & c_i(x) \leq 0, i = 1,2,\cdots,k \\
& h_j(x) = 0, j = 1,2,\cdots,l
\end{array}</script><h3 id="Generalized-Lagrange-Function"><a href="#Generalized-Lagrange-Function" class="headerlink" title="Generalized Lagrange Function"></a><em>Generalized Lagrange Function</em></h3><ul>
<li><p>引入<em>Generalized Lagrange Function</em></p>
<script type="math/tex; mode=display">
L(x, \alpha, \beta) = f(x) + \sum_{i=1}^k \alpha_i
   c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><blockquote>
<ul>
<li>$x=(x_1, x_2, \cdots, x_n) \in R^n$</li>
<li>$\alpha_i \geq 0, \beta_j$：拉格朗日乘子</li>
</ul>
</blockquote>
</li>
<li><p>考虑关于x的函数</p>
<script type="math/tex; mode=display">
\theta_P(x) = \max_{\alpha, \beta: \alpha_i \geq 0}
   L(x, \alpha, \beta)</script><blockquote>
<ul>
<li>$P$：primal，原始问题</li>
</ul>
</blockquote>
<ul>
<li><p>若x满足原始问题的两组约束条件，则$\theta_P(x)=f(x)$</p>
</li>
<li><p>若x违反等式约束j，取$\beta_j \rightarrow \infty$，
则有$\theta_P(x) \rightarrow \infty$</p>
</li>
<li><p>若x违反不等式约束i，取$\alpha_i \rightarrow \infty$
，则有$\theta_P(x) \rightarrow \infty$</p>
</li>
</ul>
<p>则有</p>
<script type="math/tex; mode=display">\theta_P(x) = \left \{ \begin{array}{l}
f(x), & x 满足原始问题约束条件 \\
+\infty, & 其他
\end{array} \right.</script></li>
<li><p>则极小化问题，称为广义拉格朗日函数的极小极大问题</p>
<script type="math/tex; mode=display">
\min_x \theta_P(x) = \max_{\alpha, \beta: \alpha_i \geq 0}
   L(x, \alpha, \beta)</script><p>与原始最优化问题等价，两问题最优值相同，记为</p>
<script type="math/tex; mode=display">
p^{*} = \min_x \theta_P(x)</script></li>
</ul>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><ul>
<li><p>定义</p>
<script type="math/tex; mode=display">
\theta_D (\alpha, \beta) = \min_x L(x, \alpha, \beta)</script></li>
<li><p>再考虑极大化$\theta_D(\alpha, \beta)$，得到广义拉格朗日
函数的极大极小问题，即</p>
<script type="math/tex; mode=display">
\max_{\alpha, \beta: \alpha \geq 0}
   \theta_D(\alpha, \beta) =
   \max_{\alpha, \beta: \alpha \geq 0} \min_x
   L(x, \alpha, \beta)</script><p>表示为约束最优化问题如下</p>
<script type="math/tex; mode=display">\begin{align*}
\max_{\alpha, \beta} & \theta_D(\alpha, \beta) =
   \max_{\alpha, \beta} \min_x L(x, \alpha, \beta) \\
s.t. & \alpha_i \geq 0, i=1,2,\cdots,k
\end{align*}</script><p>称为原始问题的对偶问题，其最优值定义记为</p>
<script type="math/tex; mode=display">
d^{*} = \max_{\alpha, \beta: \alpha \geq 0}
   \theta_D(\alpha, \beta)</script></li>
</ul>
<h2 id="原始、对偶问题关系"><a href="#原始、对偶问题关系" class="headerlink" title="原始、对偶问题关系"></a>原始、对偶问题关系</h2><h3 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h3><blockquote>
<ul>
<li>若原始问题、对偶问题都有最优值，则<script type="math/tex; mode=display">
  d^{*} = \max_{\alpha, \beta: \alpha \geq 0} \min_x
      L(x, \alpha, \beta) \leq
  \min_x \max_{\alpha, \beta: \alpha \geq 0}
      L(x, \alpha, \beta) = p^{*}</script></li>
</ul>
</blockquote>
<ul>
<li><p>$\forall x, \alpha, \beta$有</p>
<script type="math/tex; mode=display">
\theta_D(\alpha, \beta) = \min_x L(x, \alpha, \beta)
   \leq L(x, \alpha, \beta) \leq
   \max_{\alpha, \beta: \alpha \geq 0} = \theta_P(x)</script><p>即</p>
<script type="math/tex; mode=display">
\theta_D(\alpha, \beta) \leq \theta_P(x)</script></li>
<li><p>而原始、对偶问题均有最优值，所以得证</p>
</li>
</ul>
<blockquote>
<ul>
<li>设$x^{<em>}$、$\alpha^{</em>}, \beta^{<em>}$分别是原始问题、对偶
  问题的可行解，且$d^{</em>} = p^{*}$，则其分别是原始问题、
  对偶问题的最优解</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:26:19.000Z" title="8/4/2021, 11:26:19 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">16 minutes read (About 2327 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/gredient_based.html">Gradient Descent Method</a></h1><div class="content"><h2 id="思想：最速下降-amp-牛顿"><a href="#思想：最速下降-amp-牛顿" class="headerlink" title="思想：最速下降&amp;牛顿"></a>思想：最速下降&amp;牛顿</h2><p>对目标函数$f(x)$在$x^{(1)}$进行展开</p>
<script type="math/tex; mode=display">
f(x) = f(x^{(1)}) + \nabla f(x^{(1)})(x - x^{(1)})+
    \frac 1 2 \nabla^2 f(x^{(1)})(x - x^{(1)})^2 +
    o((x - x^{(1)})^2)</script><blockquote>
<ul>
<li>最速下降法：只保留一阶项，即使用线性函数近似原目标函数</li>
<li>Newton法：保留一阶、二阶项，即使用二次函数近似</li>
</ul>
</blockquote>
<ul>
<li><p>利用近似函数求解元素问题极小值</p>
<ul>
<li>最速下降法：<strong>线性函数无极值，需要确定步长、迭代</strong></li>
<li>Newton法：<strong>二次函数有极值，直接求导算出极值、迭代</strong></li>
</ul>
</li>
<li><p>最速下降法</p>
<ul>
<li>只考虑一阶导：甚至说根本没有考虑拟合原目标函数</li>
</ul>
</li>
<li><p>Newton法</p>
<ul>
<li>考虑二阶导：每步迭代还考虑了二阶导，即当前更新完毕
后，下一步能够更好的更新（二阶导的意义）</li>
<li>甚至从后面部分可以看出，Newton法甚至考虑是全局特征，
不只是局部性质（前提目标函数性质足够好）</li>
<li>二次函数拟合更接近函数极值处的特征</li>
</ul>
</li>
</ul>
<h2 id="最速下降算法"><a href="#最速下降算法" class="headerlink" title="最速下降算法"></a>最速下降算法</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>设$x=x(t)$为最优点$x$从初始点、沿负梯度方向经过的曲线，
则有</p>
<script type="math/tex; mode=display">\left \{ \begin{array}{l}
& \frac {dx(t)} {dt} = -\nabla f(x(t)) \\
& x(t_1) = x^{(1)}
\end{array} \right.</script><blockquote>
<ul>
<li>$t_1, x^{(1)}$：初始时刻、初始位置</li>
</ul>
</blockquote>
</li>
<li><p>可以证明，$x(t)$解存在，且$t \rightarrow \infty$时，有
$x(t) \rightarrow x^{ * }$，即得到无约束问题最优解</p>
</li>
<li><p>但微分方程组求解可能很麻烦，可能根本无法求解</p>
<ul>
<li>考虑将以上曲线离散化，每次前进到“不应该”前进为止</li>
<li>然后更换方向，逐步迭代得到最优解</li>
</ul>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>搜索方向最速下降方向：负梯度方向</li>
<li>终止准则：$\nabla f(x^{(k)})=0$</li>
</ul>
</blockquote>
<ol>
<li><p>取初始点$x^{(1)}$，置k=1</p>
</li>
<li><p>若$\nabla f(x^{(k)})=0$，则停止计算，得到最优解，
否则置</p>
<script type="math/tex; mode=display">d^{(k)} = -\nabla f(x^{(k)})</script><p>以负梯度作为前进方向</p>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) =
  f(x^{(k)} + \alpha d^{(k)})</script><p>得$\alpha_k$前进步长，置</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}</script></li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
<blockquote>
<ul>
<li>最速下降算法不具有二次终止性</li>
</ul>
</blockquote>
<h2 id="叠加惯性"><a href="#叠加惯性" class="headerlink" title="叠加惯性"></a>叠加惯性</h2><p>模拟物体运动时惯性：指数平滑更新步长</p>
<p><img src="/imgs/momentum.png" alt="momentum"></p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a><em>Momentum</em></h3><p>冲量方法：在<strong>原始更新步</strong>上叠加上次更新步，类似指数平滑</p>
<script type="math/tex; mode=display">
v^{(t)} = \gamma v^{(t-1)} + (1 - \gamma) \eta
    \bigtriangledown_\theta L(\theta^{(t-1)}) \\
\theta^{(t)} = \theta^{(t-1)} - v^{(t)}</script><blockquote>
<ul>
<li>$v^{(t)}$：第$t$步时第k个参数更新步</li>
<li>$L(\theta)$：往往是batch损失函数</li>
</ul>
</blockquote>
<ul>
<li>更新参数时，一定程度<strong>保持</strong>上次更新方向</li>
<li>可以在一定程度上保持稳定性，学习速度更快</li>
<li>能够越过部分局部最优解</li>
</ul>
<h3 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a><em>Nesterov Momentum</em></h3><p><em>NGA</em>：在使用冲量修正最终方向基础上，使用冲量对当前
<strong>参数位置</strong>进行修正，即使用“未来”位置计算梯度</p>
<ul>
<li>先使用冲量更新一步</li>
<li>再在更新后位置计算新梯度进行第二步更新</li>
</ul>
<script type="math/tex; mode=display">
v^{(t)} = \gamma v^{(t-1)} + \eta \bigtriangledown_\theta
    L(\theta^{(t-1)} - \gamma v^{(t-1)}) \\

\theta^{(t)} = \theta^{(t-1)} - v^{(t)}</script><h2 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h2><ul>
<li>学习率太小收敛速率缓慢、过大则会造成较大波动</li>
<li>在训练过程中动态调整学习率大小较好</li>
</ul>
<blockquote>
<ul>
<li>模拟退火思想：达到一定迭代次数、损失函数小于阈值时，减小
  学习速率</li>
</ul>
</blockquote>
<p><img src="/imgs/param_estimation_comparion_1.png" alt="param_estimation_comparion_1">
<img src="/imgs/param_estimation_comparion_2.png" alt="param_estimation_comparion_2"></p>
<h3 id="Vanilla-Gradient-Descent"><a href="#Vanilla-Gradient-Descent" class="headerlink" title="Vanilla Gradient Descent"></a><em>Vanilla Gradient Descent</em></h3><p>每次迭代减小学习率$\eta$</p>
<script type="math/tex; mode=display">
\eta^{(t)} = \frac \eta {\sqrt {t+1}} \\

\theta^{(t)} = \theta^{(t-1)} - \eta^{(t)}
    \bigtriangledown_\theta L(\theta^{(t-1)})</script><ul>
<li>学习率逐渐减小，避免学习后期参数在最优解附近反复震荡</li>
</ul>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a><em>Adagrad</em></h3><p><em>adaptive gradient</em>：训练中<strong>不同参数</strong>学习率随着迭代次数、
梯度动态变化，使得参数收敛更加平稳</p>
<script type="math/tex; mode=display">
v^{(t)}_k = \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\

\theta^{(t)}_k = \theta^{(t-1)}_k - \frac \eta
    {\sqrt {\sum_{i=0}^{t-1} (v^{(i)}_k)^2 + \epsilon}}
    v^{(t)}_k</script><blockquote>
<ul>
<li>$\epsilon$：fuss factor，避免分母为0</li>
<li>$\theta^{(t)}_k$：第t轮迭代完成后待估参数第k个分量
  （之前未涉及参数间不同，统一为向量）</li>
</ul>
</blockquote>
<ul>
<li><p>特点</p>
<ul>
<li>较大梯度参数真正学习率会被拉小；较小梯度真正学习率
参数被拉小幅度较小</li>
<li>可以和异步更新参数结合使用，给不常更新参数更大学习率</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>在训练后期，分母中梯度平方累加很大，学习步长趋于0，
收敛速度慢（可能触发阈值，提前结束训练）</li>
</ul>
</li>
</ul>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a><em>RMSprop</em></h3><p><em>root mean square prop</em>：指数平滑更新学习率分母</p>
<script type="math/tex; mode=display">
v^{(t)}_k = \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\

\theta^{(t)}_k = \theta^{(t-1)}_k - \frac \eta
    {\sqrt { \gamma \sum_{i=1}^{t-1}(v^{(i)}_k)^2 +
        (1 - \gamma)((v^{(t)})^2 + \epsilon}
    } v^{(t)}</script><ul>
<li>赋予当前梯度更大权重，减小学习率分母，避免学习速率下降
太快</li>
</ul>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><em>Adam</em></h3><p><em>adptive moment estimation</em>：指数平滑更新步、学习率分母</p>
<script type="math/tex; mode=display">
\begin{align*}
v^{(t)}_k & = \gamma_1 v^{(t-1)}_k + (1 - \gamma_1)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\
s^{(t)}_k & = \gamma_2 s^{(t-1)}_k + (1 - \gamma_2)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\hat{v^{(t)}_k} & = \frac {v^{(t)}_k} {1 - \gamma_1^t} \\
\hat{s^{(t)}_k} & = \frac {s^{(t)}_k} {1 - \gamma_2^t} \\

\theta^{(t)}_k & = \theta^{(t-1)}_k - \frac \eta
    {\sqrt{\hat{s^{(t)}_k} + \epsilon}} \hat{v^{(t)}_k}
\end{align*}</script><blockquote>
<ul>
<li>$\gamma_1$：通常为0.9</li>
<li>$\gamma_2$：通常为0.99</li>
<li>$\hat{v^{(t)}_k} = \frac {v^{(t)}_k} {1 - \gamma_1^t}$
  ：权值修正，使得过去个时间步，小批量随机梯度权值之和为1</li>
</ul>
</blockquote>
<ul>
<li><p>利用梯度的一阶矩$v^{(t)}$、二阶矩$s^{(t)}$动态调整每个
参数学习率</p>
</li>
<li><p>类似于<em>mommentum</em>、<em>RMSprop</em>结合</p>
</li>
<li><p>经过偏执矫正后，每次迭代学习率都有确定范围，参数比较平稳</p>
</li>
</ul>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a><em>Adadelta</em></h3><p>指数平滑更新学习率（分子）、学习率分母</p>
<script type="math/tex; mode=display">
\begin{align*}
s^{(t)}_k & = \gamma_1 s^{(t-1)}_k + (1 - \gamma_1)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\hat{v^{(t)}_k} & = \sqrt {\frac {\Delta \theta^{(t-1)}_k + \epsilon}
    {s^{(t)}_k + \epsilon}}
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\Delta \theta^{(t)}_k & = \gamma_1 \Delta \theta^{(t-1)}_k +
    (1 - \gamma_1) \hat{v^{(t)}_k}^2 \\

\theta^{(t)}_k & = \theta^{(t)}_k - \hat{v^{(t)}_k}
\end{align*}</script><blockquote>
<ul>
<li>$s, \Delta \theta$共用超参$\gamma_1$</li>
</ul>
</blockquote>
<ul>
<li>在<em>RMSprop</em>基础上，使用$\sqrt {\Delta \theta}$作为学习率</li>
<li>$\hat v$：中超参$\gamma_1$在分子、分母“抵消”，模型对
超参不敏感</li>
</ul>
<h2 id="样本量"><a href="#样本量" class="headerlink" title="样本量"></a>样本量</h2><h3 id="Singular-Loss-Stocastic-Gradient-Descent"><a href="#Singular-Loss-Stocastic-Gradient-Descent" class="headerlink" title="Singular Loss/Stocastic Gradient Descent"></a>Singular Loss/Stocastic Gradient Descent</h3><p><em>SGD</em>：用模型在某个样本点上的损失极小化目标函数、计算梯度、
更新参数</p>
<ul>
<li><p>单点损失度量模型“一次”预测的好坏</p>
<ul>
<li>代表模型在单点上的优劣，无法代表模型在总体上性质</li>
<li>具有很强随机性</li>
</ul>
</li>
<li><p>单点损失不常用，SGD范围也不局限于单点损失</p>
</li>
</ul>
<blockquote>
<ul>
<li>损失函数具体参见<em>ml_xxxxx</em></li>
</ul>
</blockquote>
<h3 id="全局估计"><a href="#全局估计" class="headerlink" title="全局估计"></a>全局估计</h3><p>全局损失：用模型在全体样本点上损失极小化目标函数、计算梯度、
更新参数</p>
<script type="math/tex; mode=display">
\theta^{(t)} = \theta^{(t-1)} - \eta \bigtriangledown_\theta
    L_{total}(\theta_{(t-1)})</script><blockquote>
<ul>
<li>$\theta^{(t)}$：第t步迭代完成后待估参数</li>
<li>$\eta$：学习率</li>
<li>$L<em>{total}(\theta) = \sum</em>{i=1}^N L(\theta, x_i, y_i)$：
  训练样本整体损失</li>
<li>$N$：训练样本数量</li>
</ul>
</blockquote>
<ul>
<li><p>若损失函数有解析解、样本量不大，可<strong>一步更新（计算）</strong>
完成（传统参数估计场合）</p>
<ul>
<li>矩估计</li>
<li>最小二乘估计</li>
<li>极大似然估计</li>
</ul>
</li>
<li><p>否则需要迭代更新参数</p>
<ul>
<li>样本量较大场合</li>
<li>并行计算</li>
</ul>
</li>
</ul>
<h3 id="Mini-Batch-Loss"><a href="#Mini-Batch-Loss" class="headerlink" title="Mini-Batch Loss"></a>Mini-Batch Loss</h3><p><em>mini-batch loss</em>：用模型在某个batch上的损失极小化目标函数、
计算梯度、更新参数</p>
<script type="math/tex; mode=display">
\theta^{(t)} = \theta^{(t-1)} - \eta \bigtriangledown_\theta
    L_{batch}(\theta^{(t-1)})</script><blockquote>
<ul>
<li>$L<em>{batch}(\theta)=\sum</em>{i \in B} L(\theta, x_i, y_i)$：
  当前batch整体损失</li>
<li>$B$：当前更新步中，样本组成的集合batch</li>
</ul>
</blockquote>
<ul>
<li><p>batch-loss是模型在batch上的特征，对整体的代表性取决于
batch大小</p>
<ul>
<li>batch越大对整体代表性越好，越稳定；越小对整体代表
越差、不稳定、波动较大、难收敛</li>
<li>batch大小为1时，就是SGD</li>
<li>batch大小为整个训练集时，就是经验（结构）风险</li>
</ul>
</li>
<li><p>batch-loss是学习算法中最常用的loss，SGD优化常指此</p>
<ul>
<li>实际中往往是使用batch-loss替代整体损失，表示经验风险
极小化</li>
<li>batch-loss同样可以带正则化项，表示结构风险极小化</li>
<li>损失极值：SVM（几何间隔最小）</li>
</ul>
</li>
</ul>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>适合样本量较大、无法使用样本整体估计使用</li>
<li>一定程度能避免局部最优（随机batch可能越过局部极值）</li>
<li>开始阶段收敛速度快</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li><p>限于每次只使用单batch中样本更新参数，batch-size较小时，
结果可能不稳定，往往很难得到最优解</p>
</li>
<li><p>无法保证良好的收敛性，学习率小收敛速度慢，学习率过大
则损失函数可能在极小点反复震荡</p>
</li>
<li><p>对所有参数更新应用相同学习率，没有对低频特征有优化
（更的学习率）</p>
</li>
<li><p>依然容易陷入局部最优点</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">5 minutes read (About 752 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/newtons.html">Newton&#039;s Method</a></h1><div class="content"><h2 id="Newton法"><a href="#Newton法" class="headerlink" title="Newton法"></a>Newton法</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>若$x^{ * }$是无约束问题局部解，则有</p>
<script type="math/tex; mode=display">\nabla f(x^{ * }) = 0</script><p>可求解此问题，得到无约束问题最优解</p>
</li>
<li><p>原始问题是非线性，考虑求解其线性逼近，在初始点$x^{(1)}$
处泰勒展开</p>
<script type="math/tex; mode=display">
\nabla f(x) \approx \nabla f(x^{(1)})
   + \nabla^2 f(x^{(1)})(x - x^{(1)})</script><p>解得</p>
<script type="math/tex; mode=display">
x^{(2)} = x^{(1)} - (\nabla^2 f(x^{(1)}))^{-1}
   \nabla f(x^{(1)})</script><p>作为$x^{ * }$的第二次近似</p>
</li>
<li><p>不断迭代，得到如下序列</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + d^{(k)}</script><blockquote>
<ul>
<li>$d^{(k)}$：Newton方向，即以下方程解<script type="math/tex; mode=display">
 \nabla^2 f(x^{(k)}) d = -\nabla
     f(x^{(k)})</script></li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ul>
<li><p>初始点 $x^{(1)}$、精度要求 $\epsilon$，置 $k=1$</p>
</li>
<li><p>考虑 $|\nabla f(x^{(k)})| \leq \epsilon$</p>
<ul>
<li>若满足，则停止计算，得到最优解 $x^{(k)}$</li>
<li><p>否则求解如下方程，得到 $d^{(k)}$</p>
<script type="math/tex; mode=display">
\nabla^2 f(x^{(k)}) d = -\nabla f(x^{(k)})</script></li>
</ul>
</li>
<li><p>如下设置，并转2</p>
<script type="math/tex; mode=display">x^{(k+1)} = x^{(k)} + d^{(k)}, k = k+1</script></li>
</ul>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>优点</p>
<ul>
<li>产生点列 ${x^{k}}$ 若收敛，则具有二阶收敛速率</li>
<li>具有二次终止性，事实上对正定二次函数，一步即可收敛</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>可能会在某步迭代时目标函数值上升</li>
<li>当初始点 $x^{(1)}$ 距离最优解 $x^{ * }$ 时，产生的点列
可能不收敛，或者收敛到鞍点</li>
<li>需要计算 <em>Hesse</em> 矩阵<ul>
<li>计算量大</li>
<li><em>Hesse</em> 矩阵可能不可逆，算法终止</li>
<li><em>Hesse</em> 矩阵不正定，<em>Newton</em> 方向可能不是下降方向</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="阻尼-修正-Newton-法"><a href="#阻尼-修正-Newton-法" class="headerlink" title="阻尼/修正 Newton 法"></a>阻尼/修正 <em>Newton</em> 法</h2><ul>
<li>克服 <em>Newton</em> 法目标函数值上升的缺点</li>
<li>一定程度上克服点列可能不收敛缺点</li>
</ul>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul>
<li><p>初始点 $x^{(1)}$、精度要求 $\epsilon$，置 $k=1$</p>
</li>
<li><p>考虑 $|\nabla f(x^{(k)})| \leq \epsilon$</p>
<ul>
<li>若满足，停止计算，得到最优解 $x^{(k)}$</li>
<li>否则求解如下方程得到 $d^{(k)}$<script type="math/tex; mode=display">
\nabla^2 f(x^{(k)}) d = -\nabla f(x^{(k)})</script></li>
</ul>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) = f(x^{(k)} + \alpha d^{(k)})</script><p>得到 $\alpha_k$，如下设置并转2</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}, k = k+1</script></li>
</ul>
<h2 id="其他改进"><a href="#其他改进" class="headerlink" title="其他改进"></a>其他改进</h2><ul>
<li><em>Newton</em> 法、修正 <em>Newton</em> 法的改进方向<ul>
<li>结合最速下降方向修正迭代方向</li>
<li><em>Hesse</em> 矩阵不正定情形下的替代</li>
</ul>
</li>
</ul>
<h3 id="结合最速下降方向"><a href="#结合最速下降方向" class="headerlink" title="结合最速下降方向"></a>结合最速下降方向</h3><blockquote>
<ul>
<li>将 <em>Newton</em> 方向和最速下降方向结合</li>
</ul>
</blockquote>
<ul>
<li><p>设 $\theta_k$ 是 $<d^{(k)}, -\nabla f(x^{(k)})>$ 之间夹角，显然希望 $\theta &lt; \frac \pi 2$</p>
</li>
<li><p>则置限制条件 $\eta$，取迭代方向</p>
<script type="math/tex; mode=display">
d^{(k)} = \left \{ \begin{array}{l}
   d^{(k)}, & cos\theta_k \geq \eta \\
   -\nabla f(x^{(k)}), & 其他
\end{array} \right.</script></li>
</ul>
<h3 id="Negative-Curvature"><a href="#Negative-Curvature" class="headerlink" title="Negative Curvature"></a><em>Negative Curvature</em></h3><blockquote>
<ul>
<li>当 <em>Hesse</em> 矩阵非正定时，选择负曲率下降方向 $d^{(k)}$（一定存在）</li>
</ul>
</blockquote>
<ul>
<li><p><em>Hesse</em> 矩阵非正定时，一定存在负特征值、相应特征向量 $u$</p>
<ul>
<li><p>取负曲率下降方向作为迭代方向</p>
<script type="math/tex; mode=display">
d^{(k)} = -sign(u^T \nabla f(x^{(k)})) u</script></li>
<li><p>$x^{(k)}$ 处负曲率方向 $d^{(k)}$ 满足</p>
<script type="math/tex; mode=display">
{d^{(k)}}^T \nabla^2 f(x^{(k)}) d^{(k)} < 0</script></li>
</ul>
</li>
</ul>
<h3 id="修正-Hesse-矩阵"><a href="#修正-Hesse-矩阵" class="headerlink" title="修正 Hesse 矩阵"></a>修正 <em>Hesse</em> 矩阵</h3><ul>
<li><p>取迭代方向 $d^{(k)}$ 为以下方程的解</p>
<script type="math/tex; mode=display">
(\nabla^2 f(x^{(k)}) + v_k I) d = -\nabla f(x^{k})</script></li>
</ul>
<blockquote>
<ul>
<li>$v_k$：大于 $\nabla^2 f(x^{(k)})$ 最大负特征值绝对值</li>
</ul>
</blockquote>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/Math-Analysis/">Previous</a></div><div class="pagination-next"><a href="/categories/Math-Analysis/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/Math-Analysis/">1</a></li><li><a class="pagination-link is-current" href="/categories/Math-Analysis/page/2/">2</a></li><li><a class="pagination-link" href="/categories/Math-Analysis/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6371777973" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>