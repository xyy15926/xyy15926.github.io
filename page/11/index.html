<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"Hexo","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Algebra/">Math Algebra</a><span> / </span><a class="link-muted" href="/categories/Math-Algebra/Linear-Algebra/">Linear Algebra</a></span><span class="level-item">9 minutes read (About 1383 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Algebra/Linear-Algebra/matrix_decomposition.html">Matrix Decomposition</a></h1><div class="content"><h2 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h2><ul>
<li><p>矩阵加法分解：将矩阵分解为三角阵、对角阵之和</p>
<ul>
<li>常用于迭代求解线性方程组</li>
</ul>
</li>
<li><p>矩阵乘法分解：将矩阵分解为三角镇、对角阵、正交阵之积</p>
</li>
</ul>
<blockquote>
<ul>
<li>以下分解均在实数域上，扩展至复数域需同样将相应因子矩阵
  扩充至复数域上定义</li>
</ul>
</blockquote>
<h2 id="矩阵加法分解"><a href="#矩阵加法分解" class="headerlink" title="矩阵加法分解"></a>矩阵加法分解</h2><h3 id="Jacobi分解"><a href="#Jacobi分解" class="headerlink" title="Jacobi分解"></a>Jacobi分解</h3><p>Jacobi分解：将矩阵分解为对角阵、非对角阵</p>
<p><img src="/imgs/matrix_decomposition_jacobi.png" alt="matrix_decomposition_jacobi"></p>
<h3 id="Gauss-Seidel分解"><a href="#Gauss-Seidel分解" class="headerlink" title="Gauss-Seidel分解"></a>Gauss-Seidel分解</h3><p>Gauss-Seidel分解：将矩阵分解为上、下三角矩阵</p>
<p><img src="/imgs/matrix_decomposition_gauss_seidel.png" alt="matrix_decomposition_gauss_seidel"></p>
<h3 id="Successive-Over-Relaxation"><a href="#Successive-Over-Relaxation" class="headerlink" title="Successive Over Relaxation"></a>Successive Over Relaxation</h3><p>SOR：逐次超松弛迭代法，分解为对角、上三角、上三角矩阵，同时
增加权重$w$调整分解后比例</p>
<p><img src="/imgs/matrix_decomposition_sor.png" alt="matrix_decomposition_sor"></p>
<ul>
<li>利用内在等式应用的平衡性、不动点收敛理论可以快速迭代<ul>
<li>$x$拆分到等式左右两侧，可以视为$y=x$和另外函数交点</li>
<li>根据不动点收敛理论可以进行迭代求解</li>
</ul>
</li>
</ul>
<h2 id="LU系列分解"><a href="#LU系列分解" class="headerlink" title="LU系列分解"></a>LU系列分解</h2><h3 id="LU-Decomposition"><a href="#LU-Decomposition" class="headerlink" title="LU Decomposition"></a>LU Decomposition</h3><p>LU分解：将方阵分解为<em>lower triangualr matrix</em>、
<em>upper triangluar matrix</em></p>
<script type="math/tex; mode=display">\begin{align*}
A & = L U \\

\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m,1} & a_{m,2} & \cdots & a_{m,m}
\end{bmatrix} & = 

\begin{bmatrix}
l_{1,1} & 0 & \cdots & 0 \\
l_{2,1} & l_{2,2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
l_{m,1} & l_{m,2} & \cdots & l_{m,m}
\end{bmatrix}

\begin{bmatrix}
u_{1,1} & u_{1,2} & \cdots & u_{1,m} \\
0 & u_{2,2} & \cdots & u_{2,m} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & u_{m,m}
\end{bmatrix}

\end{align*}</script><blockquote>
<ul>
<li>$L$：下三角矩阵</li>
<li>$U$：上三角矩阵</li>
</ul>
</blockquote>
<ul>
<li>特别的可以要求某个矩阵对角线元素为1</li>
<li>几何意义：由单位阵出发，经过竖直、水平切变</li>
</ul>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>LU分解实际上不需要额外存储空间，矩阵L、U可以合并存储</p>
</li>
<li><p>LU分解可以快速求解线性方程组，可以视为高斯消元法的矩阵
形式</p>
<ul>
<li><p>得到矩阵LU分解后，对任意向量b，可使用<strong>已有</strong>LU分解
求解</p>
<ul>
<li>L为消元过程中的行系数和对角线全为1的下三角矩阵
（负系数在矩阵中为正值）</li>
<li>U为消元结果上三角矩阵</li>
</ul>
</li>
<li><p>则解方程组$Ax=b$等价于$LUx=b$</p>
<ul>
<li>先求解$Ly=b$</li>
<li>再求解$Ux=x$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="LDU-Decomposition"><a href="#LDU-Decomposition" class="headerlink" title="LDU Decomposition"></a>LDU Decomposition</h3><p>LDU分解：将矩阵分解为下三角、上三角、对角矩阵</p>
<script type="math/tex; mode=display">\begin{align*}
A & = L D U \\

\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m,1} & a_{m,2} & \cdots & a_{m,m}
\end{bmatrix} & = 

\begin{bmatrix}
1 & 0 & \cdots & 0 \\
l_{2,1} & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
l_{m,1} & l_{m,2} & \cdots & 1
\end{bmatrix}

\begin{bmatrix}
u_{1,1} & 0 & \cdots & 0\\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & u_{m,m}
\end{bmatrix}

\begin{bmatrix}
1 & u_{1,2}/u_{1,1} & \cdots & u_{1,m}/u_{1,1} \\
0 & 1 & \cdots & u_{2,m}/u_{2,2} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}

\end{align*}</script><ul>
<li>LU分解可以方便的得到LDU分解：提取对角阵、然后对应矩阵
元素等比缩放</li>
</ul>
<h3 id="PLU-Q-Decomposition"><a href="#PLU-Q-Decomposition" class="headerlink" title="PLU[Q] Decomposition"></a>PLU[Q] Decomposition</h3><blockquote>
<ul>
<li>PLU分解：将方阵分解为置换矩阵、下三角、上三角矩阵</li>
<li>PLUQ分解：将方阵分解为置换矩阵、下三角、上三角、置换矩阵</li>
</ul>
</blockquote>
<ul>
<li>考虑$P^{-1}A=LU$，交换$A$行即可作普通LU分解，PLUQ分解
类似</li>
<li>PLU分解数值稳定性好、实用工具</li>
</ul>
<h3 id="LL-Cholesky-Decomposition"><a href="#LL-Cholesky-Decomposition" class="headerlink" title="LL/Cholesky Decomposition"></a>LL/Cholesky Decomposition</h3><p>LL分解：将对称阵分解为下三角、转置</p>
<script type="math/tex; mode=display">\begin{align*}
A & = L L^T \\

\begin{bmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m,1} & a_{m,2} & \cdots & a_{m,m}
\end{bmatrix} & = 

\begin{bmatrix}
l_{1,1} & 0 & \cdots & 0 \\
l_{2,1} & l_{2,2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
l_{m,1} & l_{m,2} & \cdots & l_{m,m}
\end{bmatrix}

\begin{bmatrix}
l_{1,1} & l_{2,1} & \cdots & l_{m,1} \\
0 & l_{2,2} & \cdots & l_{m,2} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & l_{m,m}
\end{bmatrix}

\end{align*}</script><ul>
<li><p>Cholesky分解常用于分解$A^TA$</p>
<ul>
<li>常用于相关分析，分解相关系数阵、协方差阵</li>
</ul>
</li>
<li><p>相较于一般LU分解，Cholesky分解速度更快、数值稳定性更好</p>
</li>
</ul>
<blockquote>
<ul>
<li>类似的有LDL分解，同时提取对角线元素即可</li>
</ul>
</blockquote>
<h2 id="Singular-Value-Decomposition"><a href="#Singular-Value-Decomposition" class="headerlink" title="Singular Value Decomposition"></a>Singular Value Decomposition</h2><p><em>SVD</em>奇异值分解：将矩阵分解为正交矩阵、对角矩阵、正交矩阵</p>
<script type="math/tex; mode=display">
M_{m*n} = U_{m*r} \Sigma_{r*r} V_{n*r}^T</script><ul>
<li><strong>特征值分解</strong>在任意矩阵上推广：相应的特征值、特征向量
被称为奇异值、奇异向量</li>
<li>几何意义：由单位阵出发，经旋转、缩放、再旋转</li>
</ul>
<h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>$\Sigma$对角线元素为$M^T M$、$M M^T$的奇异值</p>
<ul>
<li>可视为在输入输出间进行标量的放缩控制</li>
<li>同$U$、$V$的列向量相对应</li>
</ul>
</li>
<li><p>$U$的列向量称为左奇异向量</p>
<ul>
<li>$M M^T$的特征向量</li>
<li>与$M$正交的“输入”或“分析”基向量</li>
</ul>
</li>
<li><p>$V$的列向量成为右奇异向量</p>
<ul>
<li>$M^T M$的特征向量</li>
<li>与$M$正交的“输出”基向量</li>
</ul>
</li>
</ul>
<h3 id="低阶近似"><a href="#低阶近似" class="headerlink" title="低阶近似"></a>低阶近似</h3><ul>
<li><p>对$m * n$阶原始矩阵$M$</p>
<ul>
<li>设其秩为$K \leq min(m, n)$，奇异值为
$d_1 \geq d_2 \geq \cdots \geq d_K &gt; 0$</li>
<li>不失一般性可以设其均值为0</li>
</ul>
</li>
<li><p>根据<em>Eckart and Young</em>的结果</p>
<script type="math/tex; mode=display">
\forall r \leq K, \sum_{k=1}^r d_k u_k v_k^T =
   \arg\min_{\bar M \in M(r)} \| M - \bar M \|_F^2</script><blockquote>
<ul>
<li>$u_k, v_k$：$U, V$的第$k$列向量</li>
<li>$|M|_F$：矩阵的Frobenius范数</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="QR-Decomposition"><a href="#QR-Decomposition" class="headerlink" title="QR Decomposition"></a>QR Decomposition</h2><p>QR分解：将矩阵分解为正交矩阵、上三角矩阵</p>
<script type="math/tex; mode=display">\begin{align*}
A = Q R
\end{align*}</script><ul>
<li>几何意义：由单位阵出发，经旋转、切变</li>
</ul>
<h3 id="特点-2"><a href="#特点-2" class="headerlink" title="特点"></a>特点</h3><ul>
<li>正交矩阵逆为其转置，同样可以方便求解线性方程组</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">13 minutes read (About 1892 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/proxmial_method.html">Proximal Gredient Method</a></h1><div class="content"><h2 id="Proximal-Operator"><a href="#Proximal-Operator" class="headerlink" title="Proximal Operator"></a><em>Proximal Operator</em></h2><script type="math/tex; mode=display">
prox_{f}(x) = \arg\min_u (f(u) + \frac 1 2 \|u - x\|^2)</script><blockquote>
<ul>
<li>$f(x)$：凸函数</li>
</ul>
</blockquote>
<ul>
<li>由于$L_2$范数的强凸性，近端算子也强凸，解总是唯一存在</li>
<li>直观理解：寻找距离点$x$不太远、$f(u)$尽可能小的$u$</li>
<li>以下算法都是近端算法的特例<ul>
<li><em>shrinkage thresholding algorithm</em></li>
<li><em>projected Landweber</em></li>
<li><em>projected gradient</em></li>
<li><em>alternating projections</em></li>
<li><em>alternating-directions method of multipliers</em></li>
<li><em>alternating split Bregman</em></li>
</ul>
</li>
</ul>
<p><img src="/imgs/proximal_operator.png" alt="proximal_operator"></p>
<blockquote>
<ul>
<li>近端算子连续可微</li>
</ul>
</blockquote>
<h3 id="Moreau-Envolop"><a href="#Moreau-Envolop" class="headerlink" title="Moreau Envolop"></a><em>Moreau Envolop</em></h3><script type="math/tex; mode=display">\begin{align*}
M_{\gamma, f}(x) & = prox_{\gamma, f}(x) \\
&= \arg\min_u (f(u) + \frac 1 {2\gamma} \|u - x\|^2) \\
\nabla prox_{\gamma, f}(x) & = \frac 1 {\gamma}(x - prox_f(x))
\end{align*}</script><ul>
<li>$\gamma &gt; 0$：平衡参数，$\gamma = 1$即为普通近端算子</li>
</ul>
<h3 id="近端算子求解"><a href="#近端算子求解" class="headerlink" title="近端算子求解"></a>近端算子求解</h3><ul>
<li><p>对一般凸$f(x)$，通常使用次梯度进行优化，其近端算子解为
（即解变动方向$p-x$为负次梯度方向）</p>
<script type="math/tex; mode=display">
p = prox_f(x) \Leftrightarrow x - p \in \partial f(p)
   \quad (\forall (x,p) \in R^N * R^N)</script></li>
<li><p>对光滑凸函数$f$，上述等式对其近端算子约简为
（即解变动方向$p-x$为负梯度方向）</p>
<script type="math/tex; mode=display">
p = prox_f(x) \Leftrightarrow x-p = \bigtriangledown f(p)</script></li>
</ul>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><h4 id="分离函数"><a href="#分离函数" class="headerlink" title="分离函数"></a>分离函数</h4><script type="math/tex; mode=display">\begin{align*}
f(x_1, \cdots, x_m) & = \sum_{i=1}^m f_i(x_i) \\
prox_f(x_1, \cdots, x_m) & = [prox_{f_1}(x_1), \cdots, prox_{fm}(x_m)]
\end{align*}</script><ul>
<li><p>取$f(x) = |x|_1$，即可得即软阈值算子</p>
<script type="math/tex; mode=display">
(prox_{\gamma, f}(x))_i = \left \{ \begin{array}{l}
   x_i - \gamma, & x_i \geq \gamma \\
   0, & |x_i| < \gamma \\
   x_i + \gamma, & x_i \leq -\gamma
\end{array} \right.</script><blockquote>
<ul>
<li>参考坐标下降：近端算子中二次项中各分量无关，所以一轮
 迭代即为最优解</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="仿射函数分解"><a href="#仿射函数分解" class="headerlink" title="仿射函数分解"></a>仿射函数分解</h4><script type="math/tex; mode=display">\begin{align*}
f(x) & = g(Ax + b) \\
prox_f(x) & = x + \frac 1 {\alpha} A^T (prox_{\alpha g}(Ax + b) - Ax - b)
\end{align*}</script><blockquote>
<ul>
<li>$A^T A = \alpha I, \alpha &gt; 0$：线性变换</li>
<li>$g$：良好闭凸函数</li>
</ul>
</blockquote>
<h4 id="第一投影定理"><a href="#第一投影定理" class="headerlink" title="第一投影定理"></a>第一投影定理</h4><blockquote>
<ul>
<li>取$f(x)$为示性函数、约束条件，即得到投影算子</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">\begin{align*}
prox_{\gamma, f}(x) & = proj_C(x) = \arg\min_{u \in C}
    \|u - x\|_2^2 \\
f(x) & = I_C(x) = \left \{ \begin{array}{l}
        0, x \in C \\
        \infty, x \notin C
    \end{array} \right.
\end{align*}</script><h4 id="第二临近定理"><a href="#第二临近定理" class="headerlink" title="第二临近定理"></a>第二临近定理</h4><blockquote>
<ul>
<li>$f$为良好闭凸函数，则以下三条等价<blockquote>
<ul>
<li>$y = prox_f(x)$</li>
<li>$x - y \in \partial f(y)$：由近端算子定义即得</li>
<li>$\forall z, <x - y, z - y> \leq f(z) - f(y)$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h4 id="Moreau-Decomposition"><a href="#Moreau-Decomposition" class="headerlink" title="Moreau Decomposition"></a><em>Moreau Decomposition</em></h4><script type="math/tex; mode=display">\begin{align*}
prox_f(x) + prox_{f^{*}}(x) & = x \\
prox_{\lambda f}(x) + \lambda prox_{f^{*}/lambda}(x/\lambda) & = x,
    \lambda > 0
\end{align*}</script><h4 id="最小值"><a href="#最小值" class="headerlink" title="最小值"></a>最小值</h4><script type="math/tex; mode=display">\begin{align*}
\min_x prox_f(x) & = \min_x f(x) \\
\arg\min_x prox_f(x) & = \arg\min_x f(x)
\end{align*}</script><p>证明</p>
<script type="math/tex; mode=display">\begin{align*}
f(x_f) & = f(x_f) + \frac 1 2 \|x_f - x_f\|_2^2 \\
& \geq \min_u {f(u) + \frac 1 2 \|u - x_f\|_2^2} \\
& = prox_f(x_f) \\
& \geq prox_f(x_p) \\
& = f(x_p) + \frac 1 2 \|x_p - x_f\|_2^2 \\
& \geq f(x_p) \geq f(x_f)
\end{align*}</script><blockquote>
<ul>
<li>$x_f = \arg\min_x f(x)$</li>
<li>$x_p = \arg\min_x prox_f(x)$</li>
</ul>
</blockquote>
<h3 id="例"><a href="#例" class="headerlink" title="例"></a>例</h3><ul>
<li>$f(x)=c$：<script type="math/tex">prox_{f}(x) = x</script></li>
</ul>
<h4 id="Projection-Operator"><a href="#Projection-Operator" class="headerlink" title="Projection Operator"></a><em>Projection Operator</em></h4><p>投影算子</p>
<script type="math/tex; mode=display">\begin{align*}
proj_C(x) & = \arg\min_{y \in C} \|y - x\|^2 \\
& = \arg\min_{y \in R^N} l_C(x) + \frac 1 2 \|y-x\|^2
\end{align*}</script><ul>
<li>点$x$在凸集$C$上的投影：$X$上距离$x$的欧式距离最近的点</li>
</ul>
<h4 id="Alternating-Projection-Method"><a href="#Alternating-Projection-Method" class="headerlink" title="Alternating Projection Method"></a><em>Alternating Projection Method</em></h4><p><em>POCS/project onto convex sets method</em>：用于解同时满足多个
凸约束的算法</p>
<ul>
<li><p>$f_i$作为非空闭凸集$C_i$示性函数，表示一个约束，则整个
问题约简为<em>convex feasibility problem</em></p>
</li>
<li><p>只需要找到位于所有$C_i$交集的解即可</p>
</li>
<li><p>每次迭代</p>
<script type="math/tex; mode=display">
x^{(k+1)} = P_{C_1}P_{C_2} \cdots P_{C_n}x_k</script></li>
</ul>
<blockquote>
<ul>
<li>在其他问题中投影算子不再适合，需要更一般的算子，在其他
  各种同样的凸投影算子中，近端算子最合适</li>
</ul>
</blockquote>
<h2 id="Proximal-Gradient-Method"><a href="#Proximal-Gradient-Method" class="headerlink" title="Proximal Gradient Method"></a><em>Proximal Gradient Method</em></h2><p>近端算法：分两步分别优化可微凸$F(x)$、凸$R(x)$，近似优化目标
函数整体，不断迭代直至收敛</p>
<script type="math/tex; mode=display">
\min_{x \in \mathcal{H}}F(x) + R(x)</script><blockquote>
<ul>
<li>$F(x)$：可微、凸函数</li>
<li>$\nabla F(x)$：<em>Lipschitz continous</em>、利普希茨常数为$L$</li>
<li>$R(x)$：下半连续凸函函数，可能不光滑</li>
<li>$\mathcal{H}$：目标函数定义域集合，如：希尔伯特空间</li>
</ul>
</blockquote>
<ul>
<li><p><em>gredient step</em>：从$x^{(k)}$处沿$F(x)$负梯度方向微小移动
达到$x^{(k.5)}$</p>
<script type="math/tex; mode=display">
x^{(k.5)} = x^{(k)} - \gamma \nabla F(x^{(k)})</script></li>
<li><p><em>proximal operator step</em>：在$x^{(k.5)}$处应用$R(x)$近端
算子，即寻找$x^{(k.5)}$附近且使得$R(x)$较小点</p>
<script type="math/tex; mode=display">
x^{(k+1)} = prox_{\gamma R}(x^{(k.5)})</script></li>
</ul>
<h3 id="目标函数推导"><a href="#目标函数推导" class="headerlink" title="目标函数推导"></a>目标函数推导</h3><script type="math/tex; mode=display">\begin{align*}
prox_{\gamma R}(x - \gamma \nabla F(x)) & = \arg\min_u
    (R(u) + \frac 1 {2\gamma} \|u - x + \gamma \nabla F(x)\|_2^2) \\
& = \arg\min_u (R(u) + \frac {\gamma} 2 \|\nabla F(x)\|_2^2 +
    \nabla F(x)^T (u-x) + \frac 1 {2\gamma} \|u-x\|_2^2) \\
& = \arg\min_u (R(u) + F(x) + \nabla F(x)^T (u-x) +
    \frac 1 {2\gamma} \|u - x\|_2^2) \\
& \approx \arg\min_u(R(u) + F(u))
\end{align*}</script><blockquote>
<ul>
<li>$\frac {\gamma} 2 |\nabla F(x)|_2^2, F(x)$：与$u$无关
  ，相互替换不影响极值</li>
<li>$0 &lt; \gamma \leq \frac 1 L$：保证最后反向泰勒展开成立</li>
</ul>
</blockquote>
<ul>
<li><p>则$prox_{\gamma R}(x-\gamma \nabla F(x))$解即为
“原问题最优解”（若泰勒展开完全拟合$F(x)$）</p>
<ul>
<li>近端算法中距离微调项部分可加法分离</li>
<li>若$R(x)$部分也可分离，则整个目标函数可以分离，可以
<strong>拆分为多个一元函数分别求极值</strong></li>
</ul>
</li>
<li><p>考虑泰勒展开是局部性质，$u$作为极小值点只能保证在$x$附近
领域成立，可将近端算子解作为下个迭代点</p>
<script type="math/tex; mode=display">
x^{(k+1)} = prox_{\gamma R}(x^{(k)} - \gamma \nabla
   F(x^{(k)}))</script></li>
<li><p>迭代终止条件即</p>
<script type="math/tex; mode=display">
\hat x = prox_{\gamma R}(\hat x - \gamma \nabla F(\hat x))</script></li>
</ul>
<h4 id="二阶近似证明"><a href="#二阶近似证明" class="headerlink" title="二阶近似证明"></a>二阶近似证明</h4><script type="math/tex; mode=display">\begin{align*}
F(u) & = F(x) + \nabla F(x)^T (u - x) + \frac 1 2
    (u - x)^T \nabla^2 F(\zeta)(u - x) \\
& \geq F(x) + \nabla F(x)^T (u - x) \\
& \leq F(x) + \nabla F(x)^T (u - x) + \frac L 2 \|u-x\|^2
\end{align*}</script><blockquote>
<ul>
<li>$\nabla^2 F(\zeta)$：凸函数二阶导正定</li>
<li>$|\nabla F(u) - \nabla F(x)|_2 \leq L |u-x|_2$：
  $\nabla F(x)$利普希茨连续性质</li>
</ul>
</blockquote>
<h3 id="参数确定"><a href="#参数确定" class="headerlink" title="参数确定"></a>参数确定</h3><ul>
<li><p>$L$已知时，可直接确定$\gamma \in (0, \frac 1 L]$，</p>
</li>
<li><p>否则可线性迭代搜索$\gamma := \beta \gamma,\beta &lt; 1$，
直至</p>
<script type="math/tex; mode=display">
F(x - PG_{\gamma R}(x)) \leq F(x) - \nabla F(x) PG_{\gamma R}(x)
   + \frac 1 2 \|PG_{\gamma R}(x)\|_2^2</script><blockquote>
<ul>
<li>$PG<em>{\gamma R}(x)=x-prox</em>{\gamma R}(x-\gamma \nabla F(x))$</li>
<li>直接根据下述利普希茨条件须求Hasse矩阵，计算量较大</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="反向推导"><a href="#反向推导" class="headerlink" title="反向推导"></a>反向推导</h3><ul>
<li><p>对$F(x)+R(x)$在$x_0$附近作泰勒展开</p>
<script type="math/tex; mode=display">
F(u)+R(u) \leq F(x) + \nabla F(x)^T (u - x) +
   \frac 1 {2\gamma} \|u - x\|_2^2 + R(x)</script><blockquote>
<ul>
<li>$\lambda \in (0, \frac 1 L]$</li>
<li>$L$：$F(x)$利普希茨常数</li>
<li>$\leq$：由Lipschitz连续可取</li>
</ul>
</blockquote>
<ul>
<li>则不等式右边就是$F(x)+R(x)$的一个上界，可以对将对其
求极小化转化对此上界求极小</li>
</ul>
</li>
<li><p>考虑对极小化目标添加常数项不影响极值，对不等式右侧添加
与$u$无关项$\frac \gamma 2 |\nabla F(x)|_2^2$、剔除
剔除$F(x)$凑出近端算子</p>
<script type="math/tex; mode=display">\begin{align*}
prox_{\gamma R} & = \arg\min_u (R(u) + \frac {\gamma} 2
   \|\nabla F(x)\|_2^2 + \nabla F(x)^T (u-x) +
   \frac 1 {2\gamma} \|u-x\|_2^2) \\
& = \arg\min_u (R(u) + \|u - x + \frac 1 {2\gamma} \nabla F(x)\|_2^2)
\end{align*}</script></li>
</ul>
<h2 id="近端算法推广"><a href="#近端算法推广" class="headerlink" title="近端算法推广"></a>近端算法推广</h2><h3 id="问题推广"><a href="#问题推广" class="headerlink" title="问题推广"></a>问题推广</h3><blockquote>
<ul>
<li>求解<em>non-differentiable</em>凸优化问题的通用投影形式</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
\min_{x \in R^N} \sum_{i=1}^N f_i(x)</script><blockquote>
<ul>
<li>$f_i(x)$：凸函数，不一定处处可微</li>
</ul>
</blockquote>
<ul>
<li><p>目标函数中包含不处处连续可微函数，整个目标函数不光滑</p>
<ul>
<li>无法使用传统的光滑优化手段，如：最速下降、共轭梯度</li>
<li>极小化条件为$0 \in \partial(F+R)(x)$</li>
</ul>
</li>
<li><p>分开考虑各个函数，对非光滑函数使用近端算子处理</p>
</li>
</ul>
<h3 id="算子推广"><a href="#算子推广" class="headerlink" title="算子推广"></a>算子推广</h3><blockquote>
<ul>
<li>考虑使用<em>Bregman Divergence</em>替代近端算子中欧式距离</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
prox_{\gamma, f}(x) = \arg\min_u (f(u) + \mu(u) - \mu(x) +
    <\nabla \mu(x), u - x>)</script><blockquote>
<ul>
<li>取$\mu(x) = \frac 1 2 |x|_2^2$时，即为普通近端算子</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:25:54.000Z" title="8/4/2021, 11:25:54 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">5 minutes read (About 697 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/coordinate_descent.html">Coordinate Descent</a></h1><div class="content"><h2 id="坐标下降"><a href="#坐标下降" class="headerlink" title="坐标下降"></a>坐标下降</h2><p>坐标下降法：在当前点处延一个坐标方向进行一维搜索以求得函数
的局部极小值</p>
<ul>
<li>非梯度优化算法，但能提供超过一阶的信息<ul>
<li>SMO算法就是两块贪心坐标下降</li>
</ul>
</li>
</ul>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><ul>
<li><p>优化方向从算法一开始就固定，如：选择线性空间中一组基作为
搜索方向</p>
</li>
<li><p>循环极小化各坐标方向上目标函数值，即：若$x^k$给定，则</p>
<script type="math/tex; mode=display">
x_i^{(k+1)} = \arg\min_{x \in R} f(x_1^{k+1}, \cdots,
   x_{i-1}^{k+1}, x, x_{i+1}^{k}, \cdots, x_n^k)</script><blockquote>
<ul>
<li>$k$：第k轮迭代</li>
<li>$i$：某轮迭代中更新第i个方向</li>
</ul>
</blockquote>
</li>
<li><p>对初始值为$X_0$得到迭代序列$X_1, \cdots, X_n$，对精确
一维搜索类似最速下降有</p>
<script type="math/tex; mode=display">
F(X_0) \geq F(X_1) \geq \cdots \geq F(x_n)</script></li>
<li><p>若某轮迭代中目标函数无法被有优化，说明已经达到驻点</p>
</li>
</ul>
<h3 id="Adaptive-Coordinate-Descent"><a href="#Adaptive-Coordinate-Descent" class="headerlink" title="Adaptive Coordinate Descent"></a>Adaptive Coordinate Descent</h3><p>自适应坐标下降：变换坐标系使得在考虑目标函数的情况下，新坐标
间尽可能不相关</p>
<ul>
<li><p>对非可拆分函数（可加？），算法可能无法在较小迭代步数中
求得最优解</p>
<ul>
<li>可采用适当坐标系加速收敛，如：主成分分析进行自适应
编码</li>
<li>性能远超过传统坐标下降算法，甚至可以达到梯度下降的
性能</li>
</ul>
<p><img src="/imgs/adaptive_coordinate_descent_illustration.png" alt="adaptive_coordinate_descent_illustration"></p>
</li>
<li><p>自适应坐标下降具有以下特性，和最先进的进化算法相当</p>
<ul>
<li>缩放不变性</li>
<li>旋转不变性</li>
</ul>
</li>
</ul>
<h3 id="Block-Coordinate-Descent"><a href="#Block-Coordinate-Descent" class="headerlink" title="Block Coordinate Descent"></a>Block Coordinate Descent</h3><p>块坐标下降：在当前点处在一个超平面内方向进行搜索以求得函数
的局部极小值</p>
<ul>
<li>即同时更新一组坐标的坐标下降</li>
</ul>
<h3 id="例"><a href="#例" class="headerlink" title="例"></a>例</h3><h4 id="Lasso求解"><a href="#Lasso求解" class="headerlink" title="Lasso求解"></a>Lasso求解</h4><ul>
<li><p>目标函数</p>
<script type="math/tex; mode=display">
L(x) = RSS(x) + \lambda \|x\|_1 = \frac 1 2 (Ax - y)^T(Ax - y)
   + \lambda \|x\|_1</script></li>
<li><p>RSS求导</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial RSS} {\partial x} & = (Ax - y)^T A \\
(\frac {\partial RSS} {\partial x})_i & = (Ax - y)^T A_{:i} \\
& = (Ax_{i0} - y)^T A_{:i} + x_i A_{:i}^T A_{:i} \\
& = z_i + \rho_i x_i
\end{align*}</script><blockquote>
<ul>
<li>$(\frac {\partial RSS} {\partial x})_i$：RSS对$x$
 导数第$i$分量，即对$x_i$偏导</li>
<li>$A_{:i}$：$A$第$i$列</li>
<li>$x_{i0}$：$x$第$i$分量置零</li>
<li>$z<em>i = (Ax</em>{i0} - y)^T A_{:i}$</li>
<li>$\rho<em>i = A</em>{:i}^T A_{:i}$</li>
</ul>
</blockquote>
</li>
<li><p>则$x_i$整体次梯度为</p>
<script type="math/tex; mode=display">
\frac {\partial L} {\partial x_i} = z_i + \rho_i x_i +
   \left \{ \begin{array}{l}
       -\lambda, & x_i < 0 \\
       [-\lambda, \lambda], & x_i = 0 \\
       \lambda, & x_i > 0
   \end{array} \right.</script></li>
<li><p>分类讨论：令整体次梯度为0求解$x_i$、回带确定参数条件</p>
<script type="math/tex; mode=display">
x_i = \left \{ \begin{array}{l}
   \frac {-z_i + \lambda} {\rho_i}, & z_i > \lambda \\
   0 , & -\lambda < z_i < \lambda \\
   \frac {-z_i - \lambda} {\rho_i}, & z_i < -\lambda
\end{array} \right.</script><blockquote>
<ul>
<li>此算子也称<em>soft threshholding</em></li>
</ul>
</blockquote>
<p><img src="/imgs/lasso_ridge_lse.svg" alt="lasso_ridge_lse"></p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">4 minutes read (About 543 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/convolutional.html">Convolutional</a></h1><div class="content"><h2 id="Convolutional"><a href="#Convolutional" class="headerlink" title="Convolutional"></a>Convolutional</h2><p>卷积：卷积区域逐点乘积、求和作为卷积中心取值</p>
<ul>
<li><p>用途：</p>
<ul>
<li>提取<strong>更高层次</strong>的特征，对图像作局部变换、但保留局部特征</li>
<li>选择和其类似信号、过滤掉其他信号、探测局部是否有相应模式，如<ul>
<li><em>sobel</em> 算子获取图像边缘</li>
</ul>
</li>
</ul>
</li>
<li><p>可变卷积核与传统卷积核区别</p>
<ul>
<li>传统卷积核参数人为确定，用于提取确定的信息</li>
<li>可变卷积核通过训练学习参数，以得到效果更好卷积核</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>卷积类似向量内积</li>
</ul>
</blockquote>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>局部感知：卷积核所覆盖的像素只是小部分、局部特征</p>
<ul>
<li>类似于生物视觉中的 <em>receptive field</em></li>
</ul>
</li>
<li><p>多核卷核：卷积核代表、提取某特征，多各卷积核获取不同特征</p>
</li>
<li><p>权值共享：给定通道、卷积核，共用滤波器参数</p>
<ul>
<li>卷积层的参数取决于：卷积核、通道数</li>
<li>参数量远小于全连接神经网络</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>receptive field</em>：感受野，视觉皮层中对视野小区域单独反应的神经元<blockquote>
<ul>
<li>相邻细胞具有相似和重叠的感受野</li>
<li>感受野大小、位置在皮层之间系统地变化，形成完整的视觉空间图</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h3 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h3><ul>
<li>1980 年 <em>neocognitron</em> 新认知机提出<ul>
<li>第一个初始卷积神经网络，是感受野感念在人工神经网络首次应用</li>
<li>将视觉模式分解成许多子模式（特征），然后进入分层递阶式的特征平面处理</li>
</ul>
</li>
</ul>
<h2 id="卷积应用"><a href="#卷积应用" class="headerlink" title="卷积应用"></a>卷积应用</h2><h3 id="Guassian-Convolutional-Kernel"><a href="#Guassian-Convolutional-Kernel" class="headerlink" title="Guassian Convolutional Kernel"></a><em>Guassian Convolutional Kernel</em></h3><p>高斯卷积核：是实现 <strong>尺度变换</strong> 的唯一线性核</p>
<script type="math/tex; mode=display">\begin{align*}
L(x, y, \sigma) & = G(x, y, \sigma) * I(x, y) \\
G(x, y, \sigma) & = \frac 1 {2\pi\sigma^2}
    exp\{\frac {-((x-x_0)^2 + (y-y_0)^2)} {2\sigma^2} \}
\end{align*}</script><blockquote>
<ul>
<li>$G(x,y,\sigma)$：尺度可变高斯函数</li>
<li>$I(x,y)$：放缩比例，保证卷积核中各点权重和为 1</li>
<li>$(x,y)$：卷积核中各点空间坐标</li>
<li>$\sigma$：尺度变化参数，越大图像的越平滑、尺度越粗糙</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:02:19.000Z" title="7/16/2021, 3:02:19 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">11 minutes read (About 1586 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/attention.html">Attention Machanism</a></h1><div class="content"><h2 id="Attention-Machanism"><a href="#Attention-Machanism" class="headerlink" title="Attention Machanism"></a>Attention Machanism</h2><p>注意力机制：将<em>query</em>、<em>key-value</em>映射至输出的权重生成机制</p>
<script type="math/tex; mode=display">
Attention(Q, K, V) = \phi(f_{Att}(Q, K), V)</script><blockquote>
<ul>
<li>$V_{L <em> d_v}$：value矩阵，<em>*信息序列矩阵</em></em></li>
<li>$K_{L * d_k}$：key矩阵，大部分情况即为$V$</li>
<li>$Q_{L * d_k}$：query矩阵，其他环境信息</li>
<li>$L, d_k, d_v$：输入序列长度、key向量维度、value向量维度</li>
<li>key、value向量为$K, V$中行向量</li>
</ul>
</blockquote>
<ul>
<li><p>合理分配注意力，优化输入信息来源</p>
<ul>
<li>给重要特征分配较大权</li>
<li>不重要、噪声分配较小权</li>
</ul>
</li>
<li><p>在不同模型间学习对齐</p>
<ul>
<li>attention机制常联合Seq2Seq结构使用，通过隐状态对齐</li>
<li>如：图像至行为、翻译</li>
</ul>
</li>
</ul>
<h3 id="Attention-Model"><a href="#Attention-Model" class="headerlink" title="Attention Model"></a>Attention Model</h3><blockquote>
<ul>
<li>Attenion机制一般可以细化如下</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">\begin{align*}
c_t & = \phi(\alpha_t, V) \\

\alpha_{t} & = softmax(e_t) \\
& = \{ \frac {exp(e_{t,j})} {\sum_{k=1}^K exp(e_{t,k})} \} \\

e_t & = f_{Att}(K, Q)
\end{align*}</script><blockquote>
<ul>
<li>$c_t$：<em>context vector</em>，注意力机制输出上下文向量</li>
<li>$e_{t,j}$：$t$时刻$i$标记向量注意力得分</li>
<li>$\alpha_{t,i}$：$t$时刻$i$标记向量注意力权重</li>
<li>softmax归一化注意力得分</li>
</ul>
</blockquote>
<ul>
<li><p>$f_{Att}$：计算各标记向量注意力得分</p>
<ul>
<li><em>additive attention</em></li>
<li><em>multiplicative/dot-product attention</em>：</li>
<li><em>local attention</em></li>
</ul>
<blockquote>
<ul>
<li>其参数需联合整个模型训练、输入取决于具体场景</li>
</ul>
</blockquote>
</li>
<li><p>$\phi_{Att}$：根据标记向量注意力权重计算输出上下文向量</p>
<ul>
<li><em>stochastic hard attention</em></li>
<li><em>deterministic soft attention</em></li>
</ul>
</li>
<li><p>$Q$可能包括很多信息</p>
<ul>
<li>Decoder结构输出、Encoder结构输入</li>
<li>$W$待训练权重矩阵</li>
<li>LSTM、RNN等结构隐状态</li>
</ul>
</li>
</ul>
<h3 id="Additive-Attention"><a href="#Additive-Attention" class="headerlink" title="Additive Attention"></a>Additive Attention</h3><ul>
<li><p>单隐层前馈网络（MLP）</p>
<script type="math/tex; mode=display">
e_{t,j} = v_a^T f_{act}(W_a [h_{t-1}; g_j])</script><blockquote>
<ul>
<li>$h_{t-1}$：输出结构隐状态</li>
<li>$g_j$：输入结构隐状态</li>
<li>$W_a, v_a$：待训练参数</li>
<li>$f_{act}$：激活函数$tanh$、$ReLU$等</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Multiplicative-Dot-product-Attention"><a href="#Multiplicative-Dot-product-Attention" class="headerlink" title="Multiplicative/Dot-product Attention"></a>Multiplicative/Dot-product Attention</h3><script type="math/tex; mode=display">
e_{t,j} = \left \{ \begin{array}{l}
    h_{t-1}^T g_j, & dot \\
    h_{t-1}^T W_a g_j, & general \\
    W_a h_{t-1}, & location
\end{array} \right.</script><ul>
<li>相较于加法attention实际应用中更快、空间效率更高
（可以利用高度优化的矩阵乘法运算）</li>
</ul>
<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03044">MLP</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">內积形式</a></li>
</ul>
</blockquote>
<h4 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h4><ul>
<li><p>将输出作为输入引入，考虑上一次输出影响</p>
<p><img src="/imgs/attention_with_output_as_input_feeding.png" alt="attention_with_output_as_input_feeding"></p>
</li>
<li><p><em>Scaled Dot-Product Attention</em></p>
<script type="math/tex; mode=display">
f_{Att} = \frac {Q K^T} {\sqrt{d_k}}</script><ul>
<li>避免內积随着key向量维度$d_k$增大而增大，导致softmax
中梯度过小</li>
</ul>
</li>
</ul>
<h3 id="Stochastic-Hard-Attention"><a href="#Stochastic-Hard-Attention" class="headerlink" title="Stochastic Hard Attention"></a>Stochastic Hard Attention</h3><p><em>hard attention</em>：随机抽取标记向量作为注意力位置</p>
<ul>
<li><p>注意力位置视为中间one-hot隐向量，每次只关注某个标记向量</p>
</li>
<li><p>模型说明</p>
<ul>
<li>$f_{Att}$为随机从标记向量$a$中抽取一个</li>
<li>$\alpha$视为多元伯努利分布参数，各分量取值表示对应
标记向量被抽中概率，此时上下文向量也为随机变量</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{align*}
p(s_{t,i}=1) & = \alpha_{t,i} \\
c_t & = V s
\end{align*}</script><blockquote>
<ul>
<li>$s$：注意力位置，中间隐one-hot向量，服从$\alpha$指定的
  多元伯努利分布</li>
<li>$h_i$：第$i$上下文向量</li>
</ul>
</blockquote>
<h4 id="参数训练"><a href="#参数训练" class="headerlink" title="参数训练"></a>参数训练</h4><ul>
<li><p>参数$\alpha$不可导、含有中间隐变量$s$，考虑使用EM算法
思想求解</p>
<script type="math/tex; mode=display">\begin{align*}
log p(y) & = log \sum_s p(s) p(y|s) \\
& \geq \sum_s p(s) log p(y|s) := L_s \\

\frac {\partial L_s} {\partial W} & = \sum_s [
   \frac {\partial p(s)} {\partial W} + \frac 1 {p(y|s)}
   \frac {\partial p(y|s)} {\partial W}] \\
& = \sum_s p(s) [\frac {\partial log p(y|s)} {\partial W} +
   log p(y|s) \frac {\partial log p(s|a)} {\partial W}]
\end{align*}</script><blockquote>
<ul>
<li>$L_s$：原对数似然的函数的下界，以其作为新优化目标</li>
<li>$W$：参数</li>
</ul>
</blockquote>
</li>
<li><p>用蒙特卡罗采样方法近似求以上偏导</p>
<ul>
<li><p>$s$按多元伯努利分布抽样$N$次，求$N$次偏导均值</p>
<script type="math/tex; mode=display">
\frac {\partial L_s} {W} \approx \frac 1 N \sum_{n=1}^N
  [\frac {\partial log p(y|\tilde s_n)} {\partial W} +
  log p(y|\tilde s_n) \frac {\partial log
  p(\tilde s_n|a)} {\partial W}]</script><blockquote>
<ul>
<li>$\tilde s_n$：第$n$次抽样结果</li>
</ul>
</blockquote>
</li>
<li><p>可对$p(y|\tilde s_n)$进行指数平滑减小估计方差</p>
</li>
</ul>
</li>
</ul>
<h3 id="Deterministic-Soft-Attention"><a href="#Deterministic-Soft-Attention" class="headerlink" title="Deterministic Soft Attention"></a>Deterministic Soft Attention</h3><p><em>soft attention</em>：从标记向量估计上下文向量期望</p>
<ul>
<li>考虑到所有上下文向量，所有标记向量加权求和上下文向量</li>
<li>模型说明<ul>
<li>$f_{Att}$计算所有标记向量注意力得分</li>
<li>$\alpha$可视为个标记向量权重</li>
</ul>
</li>
</ul>
<p><img src="/imgs/attention_global.png" alt="attention_global"></p>
<script type="math/tex; mode=display">
E_{p(s_t)} [c_t] = \sum_{i=1}^L \alpha_{t,i} a_i</script><ul>
<li>模型光滑可微：可直接用反向传播算法训练</li>
</ul>
<h3 id="Local-Attention"><a href="#Local-Attention" class="headerlink" title="Local Attention"></a>Local Attention</h3><p><em>local attention</em>：从所有标记向量中选取部分计算soft attention</p>
<ul>
<li>可以视为hard、soft attention结合<ul>
<li>hard attention选取标记向量子区间，避免噪声干扰</li>
<li>soft attention加权求和，方便训练</li>
</ul>
</li>
</ul>
<p><img src="/imgs/attention_local.png" alt="attention_local"></p>
<h4 id="子区间选取"><a href="#子区间选取" class="headerlink" title="子区间选取"></a>子区间选取</h4><blockquote>
<ul>
<li>为目标$t$选取对齐位置$p_t$，得到子区间$[p_t-D, p_t+D]$
  （$D$为经验选取）</li>
</ul>
</blockquote>
<ul>
<li><p><em>monotonic alignment</em>：直接设置$p_t=t$</p>
</li>
<li><p><em>predictive alignment</em>：</p>
<script type="math/tex; mode=display">
p_t = S sigmoid(v_p^T tanh(W_p h_t))</script><blockquote>
<ul>
<li>$W_p, v_p$：待学习参数</li>
</ul>
</blockquote>
</li>
</ul>
<blockquote>
<ul>
<li>可以使用高斯分布给注意力权重加权，强化$p_t$附近标记向量
  （根据经验可以设置$\sigma = \frac D 2$）<script type="math/tex; mode=display">
  \alpha_{t,j} = softmax(e_{t,j}) exp(-\frac {(j - p_t)^2}
      {2\sigma^2})</script></li>
</ul>
</blockquote>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h3><p><em>Self Attention</em>/<em>Intra-Attention</em>：关联同一序列内不同位置、
以学习序列表示的attenion机制</p>
<ul>
<li><p>类似卷积、循环结构</p>
<ul>
<li>将不定长的序列映射为等长的另一序列</li>
<li>从序列中提取高层特征</li>
</ul>
</li>
<li><p>特点</p>
<ul>
<li>类似卷积核，多个self attention可以完全并行</li>
<li>无需循环网络多期传递信息，输入序列同期被处理</li>
<li>可使用local attention机制限制计算复杂度</li>
</ul>
</li>
</ul>
<p><img src="/imgs/multi_head_self_attention.png" alt="multi_head_self_attention"></p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p><em>Multi-Head Attention</em>：从相同输入、输出序列学习多个
attention机制</p>
<p><img src="/imgs/multi_head_attention.png" alt="multi_head_attention"></p>
<script type="math/tex; mode=display">\begin{align*}
MultiHead(X) & = Concat(head1, ..., head_h) W^O \\
head_i & = Attention(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}</script><blockquote>
<ul>
<li>$Q, K, V$：元信息矩阵，据此训练多组query、key-value，
  一般就是原始输入序列矩阵</li>
</ul>
</blockquote>
<ul>
<li>可以并行训练，同时从序列中提取多组特征</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:07:19.000Z" title="7/16/2021, 3:07:19 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">2 minutes read (About 341 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/interaction.html">Interaction Layers</a></h1><div class="content"><h2 id="人工交互作用层"><a href="#人工交互作用层" class="headerlink" title="人工交互作用层"></a>人工交互作用层</h2><p>交互作用层：人工设置特征之间交互方式</p>
<h3 id="Flatten-Layer"><a href="#Flatten-Layer" class="headerlink" title="Flatten Layer"></a><em>Flatten Layer</em></h3><p>展平层：直接拼接特征，交互作用交由之后网络训练</p>
<script type="math/tex; mode=display">
f_{Flat}(V_x) = \begin{bmatrix} x_1 v_1 \\ \vdots\\ x_M v_M
    \end{bmatrix}</script><blockquote>
<ul>
<li>$V_x$：特征向量集合</li>
</ul>
</blockquote>
<ul>
<li>对同特征域特征处理方式<ul>
<li>平均</li>
<li>最大</li>
</ul>
</li>
</ul>
<h3 id="二阶交互作用"><a href="#二阶交互作用" class="headerlink" title="二阶交互作用"></a>二阶交互作用</h3><p>二阶交互作用层：特征向量之间两两逐元素交互</p>
<ul>
<li>交互方式<ul>
<li>逐元素<ul>
<li>乘积</li>
<li>求最大值：无</li>
</ul>
</li>
<li>按向量</li>
</ul>
</li>
<li>聚合方式<ul>
<li>求和<ul>
<li>平权</li>
<li>Attention加权</li>
</ul>
</li>
<li>求最大值：无</li>
</ul>
</li>
</ul>
<h4 id="Bi-Interaction-Layer"><a href="#Bi-Interaction-Layer" class="headerlink" title="Bi-Interaction Layer"></a><em>Bi-Interaction Layer</em></h4><p><em>Bi-Interaction Layer</em>：特征向量两两之间逐元素乘积、求和</p>
<script type="math/tex; mode=display">\begin{align*}
f_{BI}(V) & = \sum_{i=1}^M \sum_{j=i+1}^M v_i \odot v_j \\
& = \frac 1 2 (\|\sum_{i=1}^M v_i\|_2^2 -
    \sum_{i=1}^M \|v_i\|_2^2)
\end{align*}</script><blockquote>
<ul>
<li>$\odot$：逐元素乘积</li>
</ul>
</blockquote>
<ul>
<li>没有引入额外参数，可在线性时间$\in O(kM_x)$内计算</li>
<li>可在低层次捕获二阶交互影响，较拼接操作更informative<ul>
<li>方便学习更高阶特征交互</li>
<li>模型实际中更容易训练</li>
</ul>
</li>
</ul>
<h4 id="Attention-based-Pooling"><a href="#Attention-based-Pooling" class="headerlink" title="Attention-based Pooling"></a><em>Attention-based Pooling</em></h4><p><em>Attention-based Pooling</em>：特征向量两两之间逐元素乘积、加权
求和</p>
<script type="math/tex; mode=display">
f_{AP}(V) & = \sum_{i=1}^M \sum_{j=i+1}^M \alpha_{i,j}
    (v_i \odot v_j)</script><blockquote>
<ul>
<li>$\alpha_{i,j}$：交互作用注意力权重，通过注意力网络训练</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-19T10:06:05.000Z" title="7/19/2021, 6:06:05 PM">2021-07-19</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">2 minutes read (About 357 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/embedding.html">Embedding</a></h1><div class="content"><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>嵌入层：将高维空间中离散变量映射为低维稠密 <em>embedding</em> 向量表示</p>
<ul>
<li><p><em>embedding</em> 向量更能体现样本之间关联</p>
<ul>
<li>內积（內积）体现样本之间接近程度</li>
<li>可通过可视化方法体现样本差异</li>
</ul>
</li>
<li><p><em>embedding</em> 向量更适合某些模型训练</p>
<ul>
<li>模型不适合高维稀疏向量</li>
<li><em>embedding</em> 向量矩阵可以联合模型整体训练，相当于提取特征</li>
<li><em>embedding</em> 向量也可能类似迁移学习独立训练之后直接融入模型中</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>Embedding</em>：将度量空间中对象映射到另个（低维）度量空间，并尽可能<strong>保持不同对象之间拓扑关系</strong>，如 <em>Word-Embedding</em></li>
</ul>
</blockquote>
<h3 id="Embedding表示"><a href="#Embedding表示" class="headerlink" title="Embedding表示"></a>Embedding表示</h3><ul>
<li><p>特征不分组表示</p>
<script type="math/tex; mode=display">\begin{align*}
\varepsilon_x & =  E x \\
& = [x_1v_1, x_2v_2, \cdots, x_Mv_M] \\
& = [x_{M_1} v_{M_1}, \cdots, x_{M_m} v_{M_m}]
\end{align*}</script><blockquote>
<ul>
<li>$E$：embedding向量矩阵</li>
<li>$M$：特征数量</li>
<li>$v_i$：$k$维embedding向量</li>
<li>$x_i$：特征取值，对0/1特征仍等价于查表，只需考虑非0特征<blockquote>
<ul>
<li>$x_{M_i}$：第$j$个非0特征，编号为$M_i$</li>
<li>$m$：非零特征数量</li>
</ul>
</blockquote>
</li>
<li>$\varepsilon_x$：特征向量集合</li>
</ul>
</blockquote>
</li>
<li><p>特征分组表示</p>
<script type="math/tex; mode=display">\begin{align*}
\varepsilon_x & = [V_1 g_1, V_2 g_2, \cdots, V_G g_G]
\end{align*}</script><blockquote>
<ul>
<li>$G$：特征组数量</li>
<li>$V_i$：第$i$特征组特征向量矩阵</li>
<li>$g_i$：第$i$特征组特征取值向量</li>
</ul>
</blockquote>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">a minute read (About 140 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/pooling.html">Pooling Layers</a></h1><div class="content"><h2 id="池化-下采样"><a href="#池化-下采样" class="headerlink" title="池化/下采样"></a>池化/下采样</h2><p>池化：在每个区域中选择只保留一个值</p>
<ul>
<li><p>用于减小数据处理量同时保留有用的信息</p>
<ul>
<li>相邻区域特征类似，单个值能表征特征、同时减少数据量</li>
</ul>
</li>
<li><p>保留值得选择有多种</p>
<ul>
<li>极值</li>
<li>平均值</li>
<li>全局最大</li>
</ul>
</li>
<li><p>直观上</p>
<ul>
<li>模糊图像，丢掉一些不重要的细节</li>
</ul>
</li>
</ul>
<h3 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h3><p>最大值采样：使用区域中最大值作为代表</p>
<h3 id="Average-Pooling"><a href="#Average-Pooling" class="headerlink" title="Average Pooling"></a>Average Pooling</h3><p>平均值采样：使用池中平均值作为代表</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:09:47.000Z" title="7/16/2021, 3:09:47 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">9 minutes read (About 1278 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/recurrent.html">Recurrent Neural Network</a></h1><div class="content"><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><p>RNN：处理前后数据有关联的序列数据</p>
<p><img src="/imgs/rnn_unfolding.png" alt="rnn_unfolding"></p>
<blockquote>
<ul>
<li>左侧：为折叠的神经网络，右侧：按时序展开后的网络</li>
<li>$h$：循环隐层，其中神经元之间有权连接，随序列输入上一期
  隐层会影响下一期</li>
<li>$o$、$y$：输出预测值、实际值</li>
<li>$L$：损失函数，随着时间累加</li>
</ul>
</blockquote>
<ul>
<li>序列往往长短不一，难以拆分为独立样本通过普通DNN训练</li>
</ul>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p><img src="/imgs/rnn_structures.png" alt="rnn_structures"></p>
<blockquote>
<ul>
<li>普通的DNN：固定大小输入得到固定输出</li>
<li>单个输入、序列输出：输入图片，得到描述文字序列</li>
<li>序列输入、单个输出：情感分析</li>
<li>异步序列输入、输出：机器翻译</li>
<li>同步序列输入、输出：视频帧分类</li>
</ul>
</blockquote>
<h4 id="权值连接"><a href="#权值连接" class="headerlink" title="权值连接"></a>权值连接</h4><ul>
<li><p>循环隐层内神经元之间也建立权连接，即<strong>循环</strong></p>
<ul>
<li>基础神经网络只在层与层之间建立权值连接是RNN同普通DNN
最大不同之处</li>
</ul>
</li>
<li><p>循环隐层中神经元只会和其<strong>当前层中神经元</strong>建立权值连接</p>
<ul>
<li>即不受上期非同层神经元影响</li>
<li>循环隐层中神经元$t$期状态$h^{(t)}$由当期输入、
$h^{(t-1)}$共同决定</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>Gated Feedback RNN</em>：循环隐层会对下期其他隐层产生影响
  <img src="/imgs/rnn_gated_feedback.png" alt="rnn_gated_feedback"></li>
</ul>
</blockquote>
<h4 id="逻辑结构"><a href="#逻辑结构" class="headerlink" title="逻辑结构"></a>逻辑结构</h4><blockquote>
<ul>
<li>RNN网络实际结构是线性、折叠的，逻辑结构则是展开的结构，
  考虑RNN性质应该在展开的逻辑结构中考虑</li>
</ul>
</blockquote>
<ul>
<li><p>序列输入</p>
<ul>
<li>实际结构：依次输入</li>
<li>逻辑结构：里是整体作为一次输入、才是一个样本，损失、
反向传播都应该以完整序列为间隔</li>
</ul>
</li>
<li><p>权值共享</p>
<ul>
<li>实际结构：不同期的权值实际是同一组</li>
<li>逻辑结构：称为<strong>权值共享</strong></li>
</ul>
</li>
<li><p>重复模块链</p>
<ul>
<li>实际结构：同一个模块</li>
<li>逻辑结构：不同期模块之间信息流动形成链式形式</li>
</ul>
</li>
</ul>
<h4 id="信息传递"><a href="#信息传递" class="headerlink" title="信息传递"></a>信息传递</h4><blockquote>
<ul>
<li>RNN循环层中信息只能由上一期直接传递给下一期</li>
</ul>
</blockquote>
<ul>
<li><p>输入、输出相关信息间隔较近时，普通RNN可以胜任</p>
<p><img src="/imgs/rnn_short_dependencies.png" alt="rnn_short_dependencies"></p>
</li>
<li><p>当间隔很长，RNN理论上虽然能够处理，但由于梯度消失问题，
实际上长期依赖会消失，需要LSTM网络</p>
<p><img src="/imgs/rnn_long_dependencies.png" alt="rnn_long_dependencies"></p>
</li>
</ul>
<h3 id="Forward-Propogation"><a href="#Forward-Propogation" class="headerlink" title="Forward Propogation"></a><em>Forward Propogation</em></h3><ul>
<li><p>$h^{(t)} = \sigma(z^{(t)}) = \sigma(Ux^{(t)} + Wh^{(t-1)} +b )$</p>
<blockquote>
<ul>
<li>$\sigma$：RNN激活函数，一般为$tanh$</li>
<li>$b$：循环隐层偏置</li>
</ul>
</blockquote>
</li>
<li><p>$o^{(t)} = Vh^{(t)} + c$</p>
<blockquote>
<ul>
<li>$c$：输出层偏置</li>
</ul>
</blockquote>
</li>
<li><p>$\hat{y}^{(t)} = \sigma(o^{(t)})$</p>
<blockquote>
<ul>
<li>$\sigma$：RNN激活函数，分类时一般时$softmax$</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Back-Propogation-Through-Time"><a href="#Back-Propogation-Through-Time" class="headerlink" title="Back-Propogation Through Time"></a><em>Back-Propogation Through Time</em></h3><p><em>BPTT</em>：训练RNN的常用方法</p>
<blockquote>
<ul>
<li><p>本质仍然是BP算法，但是RNN处理序列数据，损失随期数累加，
  即计算梯度时使用最终损失$L = \sum_{t=1}^\tau L^{(t)}$</p>
</li>
<li><p>对循环层中参数，梯度沿着期数反向传播，第t期反向传播时，
  需要逐级求导</p>
</li>
</ul>
</blockquote>
<ul>
<li>序列整体作为一次输入，进行一次反向传播</li>
<li>理论上可以漂亮的解决序列数据的训练，但是和DNN一样有梯度
消失的问题，尤其是序列很长时，所以一般不能直接应用</li>
</ul>
<h4 id="非循环层"><a href="#非循环层" class="headerlink" title="非循环层"></a>非循环层</h4><ul>
<li><p>$\frac{\partial L}{\partial c}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial c} & = \sum_{t=1}^{\tau}
   \frac{\partial L^{(t)}}{\partial c}
& = \sum_{t=1}^{\tau}\frac{\partial L^{(t)}}
   {\partial o^{(t)}} \frac{\partial o^{(t)}}{\partial c}
& = \sum_{t=1}^{\tau}\hat{y}^{(t)} - y^{(t)}
\end{align*}</script><blockquote>
<ul>
<li>$L^{(t)} = \frac 1 2 (\hat{y}^{(t)} - y^{(t)})^2$：
 使用平方损失</li>
</ul>
</blockquote>
</li>
<li><p>$\frac{\partial L}{\partial V}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial V} & = \sum_{t=1}^{\tau}
   \frac{\partial L^{(t)}}{\partial V}
& = \sum_{t=1}^{\tau} \frac{\partial L^{(t)}}
   {\partial o^{(t)}} \frac{\partial o^{(t)}}{\partial V}
& = \sum_{t=1}^{\tau}(\hat{y}^{(t)} - y^{(t)})
   (h^{(t)})^T
\end{align*}</script></li>
</ul>
<h4 id="循环层"><a href="#循环层" class="headerlink" title="循环层"></a>循环层</h4><blockquote>
<ul>
<li>为方便定义：
  $\delta^{(t)} = \frac {\partial L} {\partial h^{(t)}}$</li>
</ul>
</blockquote>
<ul>
<li><p>$\delta^{(t)}$</p>
<script type="math/tex; mode=display">\begin{align*}
\delta^{(t)} & = \frac {\partial L} {\partial h^{(t)}} \\
   & = \frac{\partial L}{\partial o^{(t)}}
       \frac{\partial o^{(t)}}{\partial h^{(t)}} +
       \frac{\partial L}{\partial h^{(t+1)}}
       \frac{\partial h^{(t+1)}}{\partial h^{(t)}}
   & = V^T(\hat{y}^{(t)} - y^{(t)}) +
       W^T\delta^{(t+1)}diag(1-h^{(t+1)})^2)
\end{align*}</script><blockquote>
<ul>
<li>$\frac{\partial h^{(t+1)}}{\partial h^{(t)}} = diag(1-h^{(t+1)})^2)$
 ：$tanh(x)$梯度性质</li>
<li>$h^{(t)}(t&lt;\tau)$梯度：被后一期影响（反向传播），需递推</li>
</ul>
</blockquote>
</li>
<li><p>$\delta^{(\tau)}$</p>
<script type="math/tex; mode=display">\begin{align*}
\delta^{(\tau)} & = \frac{\partial L}{\partial o^{(\tau)}}
   \frac{\partial o^{(\tau)}}{\partial h^{(\tau)}}
& = V^T(\hat{y}^{(\tau)} - y^{(\tau)})
\end{align*}</script><blockquote>
<ul>
<li>$\tau$期后没有其他序列，可以直接求出</li>
</ul>
</blockquote>
</li>
<li><p>$\frac{\partial L}{\partial W}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial W} & = \sum_{t=1}^{\tau}
   \frac{\partial L}{\partial h^{(t)}}
   \frac{\partial h^{(t)}}{\partial W}
& = \sum_{t=1}^{\tau}diag(1-(h^{(t)})^2)
   \delta^{(t)}(h^{(t-1)})^T
\end{align*}</script><blockquote>
<ul>
<li>需要由$\sigma^{(t)}$累加得到</li>
</ul>
</blockquote>
</li>
<li><p>$\frac{\partial L}{\partial b}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial b} & = \sum_{t=1}^{\tau}
   \frac{\partial L}{\partial h^{(t)}}
   \frac{\partial h^{(t)}}{\partial b}
& = \sum_{t=1}^{\tau} diag(1-(h^{(t)})^2)\delta^{(t)}
\end{align*}</script></li>
<li><p>$\frac{\partial L}{\partial U}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial U} & = \sum_{t=1}^{\tau}
   \frac{\partial L}{\partial h^{(t)}}
   \frac{\partial h^{(t)}}{\partial U}
& = \sum_{t=1}^{\tau}diag(1-(h^{(t)})^2)
   \delta^{(t)}(x^{(t)})^T
\end{align*}</script></li>
</ul>
<p>}$$</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">9 minutes read (About 1389 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/long_short_term_memory.html">Long Short Term Memory</a></h1><div class="content"><h2 id="Long-Short-Term-Memory"><a href="#Long-Short-Term-Memory" class="headerlink" title="Long Short Term Memory"></a>Long Short Term Memory</h2><p><em>LSTM</em>：通过刻意设计、默认可以学习长期依赖信息的RNN网络</p>
<p><img src="/imgs/lstm_flow_along_time.png" alt="lstm_flow_along_time">
<img src="/imgs/lstm_flow_notations.png" alt="lstm_flow_notions"></p>
<ul>
<li><p>LSTM中每个重复的模块（层）称为细胞</p>
<ul>
<li>细胞结构经过特殊设计，相较于准RNN简单细胞结构能较好
保留长时信息</li>
</ul>
</li>
<li><p>多个LSTM细胞可以组成<em>block</em>，其中细胞<strong>门权值共享</strong></p>
<ul>
<li>block中各个细胞状态不同</li>
<li>这个是同时刻、不同层的真权值共享，类似CNN中的卷积核</li>
<li>减少参数个数，效率更高</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>long term memory</em>：长期记忆，参数</li>
<li><em>short term memory</em>：短期记忆，数据流</li>
<li><em>long short term memory</em>：长[的]短期记忆，细胞状态</li>
</ul>
</blockquote>
<h3 id="LSTM标准细胞结构"><a href="#LSTM标准细胞结构" class="headerlink" title="LSTM标准细胞结构"></a>LSTM标准细胞结构</h3><p><img src="/imgs/lstm_cell_structure.png" alt="lstm_cell_structure"></p>
<script type="math/tex; mode=display">\begin{align*}
i^{(t)} & = \sigma(W_i[x^{(t)}, h^{(t-1)}], b_i), & input gate \\
f^{(t)} & = \sigma(W_f[x^{(t)}, h^{(t-1)}], b_f), & forget gate \\
o^{(t)} & = \sigma(W_o[x^{(t)}, h^{(t-1)}], b_o), & output gate \\
\tilde C^{(t)} & = tanh((W_c[x^{(t)}, h^{(t)}])), & memory alternates \\
C^{(t)} & = f^{(t)} \odot c^{(t-1)} + i^{(t)} \odot c^{(t)}, & new memory \\
h^{(t)} & = o^{(t)} \odot tanh(c^{(t)}), & output
\end{align*}</script><blockquote>
<ul>
<li>$W_i, b_i, W_f, b_f, W_o, b_o$：输入门、遗忘门、输出门
  参数</li>
<li>$\odot$：逐项乘积</li>
<li>$x_t$：第$t$期输入</li>
<li>$i^{(t)}$：输出门权重，决定需要更新的信息</li>
<li>$f^{(t)}$：遗忘门权重，决定需要遗忘的信息</li>
<li>$o^{(t)}$：输出门权重，决定需要输出的信息</li>
<li>$h^{(t-1)}$：第$t-1$期细胞状态输出</li>
<li>$\tilde C_t$：第$t$期更新备选内容</li>
<li>$C^{(t)}$：第$t$期更新完成后细胞状态</li>
</ul>
</blockquote>
<ul>
<li><p>输入、遗忘、输出门特点</p>
<ul>
<li>当期输入$x^{(t)}$、上期输出$h^{(t-1)}$作为输入</li>
<li>sigmoid作为激活函数，得到$[0,1]$间<strong>控制、权重</strong>向量<ul>
<li><em>1</em>：完全保留</li>
<li><em>0</em>：完全舍弃</li>
</ul>
</li>
</ul>
</li>
<li><p>细胞状态、输出特点</p>
<ul>
<li>tanh作激活函数，得到$[-1,1]$间<strong>信息</strong>向量<ul>
<li>$h^{(t-1)}, x^t$：备选更新信息输入</li>
<li>$C^{(t-1)}$：输出信息输入</li>
</ul>
</li>
<li>与门限权重逐项乘积确定最终遗忘、输入、输出</li>
</ul>
<blockquote>
<ul>
<li>细胞状态选择（候选、输出）都是使用双曲正切激活，应该
 是为了有正由负</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="Gates"><a href="#Gates" class="headerlink" title="Gates"></a>Gates</h4><ul>
<li><p><em>Forget Gate</em>：遗忘门，决定要从细胞状态中舍弃的信息</p>
<p><img src="/imgs/lstm_forget_gate.png" alt="lstm_forget_gate"></p>
</li>
<li><p><em>Input Gate</em>：输入门，决定向细胞状态中保留的信息</p>
<p><img src="/imgs/lstm_input_gate.png" alt="lstm_input_gate"></p>
</li>
<li><p><em>Ouput Gate</em>：输出门，决定从细胞状态中输出的信息</p>
<p><img src="/imgs/lstm_output_gate.png" alt="lstm_output_gate"></p>
</li>
</ul>
<h4 id="Cell-State"><a href="#Cell-State" class="headerlink" title="Cell State"></a>Cell State</h4><p>细胞状态：LSTM中最重要的核心思想</p>
<p><img src="/imgs/lstm_cell_status.png" alt="lstm_cell_status"></p>
<ul>
<li><p>随着时间流动，承载之前所有状态信息，代表长期记忆</p>
<ul>
<li>类似于传送带，直接在整个链上运行，只有少量<strong>线性交互</strong></li>
<li>信息其上流派很容易保持不变</li>
<li>通过“三个门”保护、控制</li>
</ul>
</li>
<li><p>LSTM可以保证长短时记忆可以理解为</p>
<ul>
<li>$C_t$中历史信息比重由$f^{(t)}$确定</li>
<li>$f^{(t)}$趋近于1时历史信息能较好的保留</li>
</ul>
</li>
</ul>
<h3 id="Gated-Recurrent-Unit"><a href="#Gated-Recurrent-Unit" class="headerlink" title="Gated Recurrent Unit"></a>Gated Recurrent Unit</h3><p><img src="/imgs/lstm_gru.png" alt="lstm_gru"></p>
<script type="math/tex; mode=display">\begin{align*}
r^{(t)} & = \sigma(W_r [h^{(t-1)}, x^{(t)}] + b_r), & reset gate \\
z^{(t)} & = \sigma(W_z [h^{(t-1)}, x^{(t)}] + b_z), & update gate \\
\tilde h^{(t)} &= tanh(W_h [r^{(t)} h^{(t-1)}, x^{(t)}]),
    & memory alternates\\
h^{(t)} & = (1 - z^{(t)}) \odot h^{(t-1)} + z^{(t)} \odot \tilde h^{(t)},
    & new memory
\end{align*}</script><blockquote>
<ul>
<li>$W_r, b_r, W_z, b_z$：重置门、更新门参数</li>
<li>$h^{(t)}$：原细胞状态、隐层输出合并</li>
<li>$\tilde{h}_t$：第$t$期更新备选信息</li>
<li>$r^{(t)}$：重置门权重输出，重置上期状态$h_{t-1}$再作为更新
  门输入</li>
<li>$z^{(t)]$：更新门权重输出，当期状态$h<em>t$中$h</em>{t-1}$、
  $\tilde{h}_t$占比（遗忘、更新的结合）</li>
</ul>
</blockquote>
<ul>
<li>合并细胞状态、隐层输出</li>
<li>合并遗忘门、输出门为更新门</li>
</ul>
<h3 id="其他变体结构"><a href="#其他变体结构" class="headerlink" title="其他变体结构"></a>其他变体结构</h3><h4 id="Vanilla-LSTM"><a href="#Vanilla-LSTM" class="headerlink" title="Vanilla LSTM"></a><em>Vanilla LSTM</em></h4><p><img src="/imgs/lstm_peephole_connection.png" alt="lstm_peephole_connection"></p>
<blockquote>
<ul>
<li><em>Peephole Connection</em>：细胞状态也作为3个门中sigmoid的
  输入，影响控制向量的生成</li>
</ul>
</blockquote>
<h4 id="Coupled-Input-and-Forget-Gate"><a href="#Coupled-Input-and-Forget-Gate" class="headerlink" title="Coupled Input and Forget Gate"></a><em>Coupled Input and Forget Gate</em></h4><p><img src="/imgs/lstm_cifg.png" alt="lstm_cifg"></p>
<blockquote>
<ul>
<li>$1-f_i$代替$i_t$，结合遗忘门、输入门</li>
</ul>
</blockquote>
<h4 id="结构比较"><a href="#结构比较" class="headerlink" title="结构比较"></a>结构比较</h4><p>在Vanilla LSTM基础上的8个变体在TIMIT语音识别、手写字符识别、
复调音乐建模三个应用中比较</p>
<blockquote>
<ul>
<li><em>No Input Gate</em>：NIG，没有输入门</li>
<li><em>No Forget Gate</em>：NFG，没有遗忘门</li>
<li><em>No Output Gate</em>：NOG，没有输出门</li>
<li><em>No Input Acitivation Function</em>：NIAF，输入门没有tanh
  激活</li>
<li><em>No Output Activation Function</em>：NOAF，输出门没有tanh
  激活</li>
<li><em>No Peepholes</em>：NP，普通LSTM</li>
<li><em>Coupled Input and Forget Gate</em>：CIFG，遗忘、输出门结合</li>
<li><em>Full Gate Recurrence</em>：FGR，所有门之间有回路</li>
</ul>
</blockquote>
<ul>
<li><p>Vanilla LSTM效果均良好，其他变体没有性能提升</p>
</li>
<li><p>细胞结构</p>
<ul>
<li>遗忘门、输入门是最重要的部分<ul>
<li>遗忘门对LSTM性能影响十分关键</li>
<li>输出门对限制无约束细胞状态输出必要</li>
</ul>
</li>
<li>CIFG、NP简化结构，单对结果没有太大影响</li>
</ul>
</li>
<li><p>超参</p>
<ul>
<li>学习率、隐层数量是LSTM主要调节参数<ul>
<li>两者之间没有相互影响，可以独立调参</li>
<li>学习率可以可以使用小网络结构独立校准</li>
</ul>
</li>
<li>动量因子影响不大</li>
<li>高斯噪声的引入有损性能、增加训练时间</li>
</ul>
</li>
</ul>
<p>?时间</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/10/">Previous</a></div><div class="pagination-next"><a href="/page/12/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/10/">10</a></li><li><a class="pagination-link is-current" href="/page/11/">11</a></li><li><a class="pagination-link" href="/page/12/">12</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/40/">40</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6371777973" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>