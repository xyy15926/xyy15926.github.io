<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: ML Model - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="follow_it-verification-code" content="SVBypAPPHxjjr7Y4hHfn"><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">ML Model</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-08-10T11:37:47.000Z" title="8/10/2020, 7:37:47 PM">2020-08-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T08:26:34.000Z" title="7/16/2021, 4:26:34 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Specification/">ML Specification</a><span> / </span><a class="link-muted" href="/categories/ML-Specification/FinTech/">FinTech</a><span> / </span><a class="link-muted" href="/categories/ML-Specification/FinTech/Risk-Control/">Risk Control</a></span><span class="level-item">23 minutes read (About 3463 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Specification/FinTech/Risk-Control/rc_models.html">评分卡模型</a></h1><div class="content"><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ul>
<li><p>模型是策略的工具，策略包含模型，是模型的延伸</p>
<ul>
<li>相较于专家规则，机器学习模型<ul>
<li>允许加入更多特征维度，描述更加全面</li>
<li>上限更高、下限更低</li>
<li>涉及更多维度特征时，维护更方便</li>
</ul>
</li>
<li>机器学习模型和专家规则并非相互替代，更多的是串联</li>
</ul>
</li>
<li><p>业务问题转换为带解决数学问题</p>
<ul>
<li>尽量将业务问题转换为更容易解决分类问题而不是回归问题</li>
<li>数学问题应尽量贴近业务：评估指标好不等于业务价值高<ul>
<li>远离业务问题的训练出模型，其线下评估效果好也不意味着上线效果好，如：针对客户而不是订单评价</li>
<li>影响客户体验，如：客户等待时间预估偏低而不是偏高</li>
</ul>
</li>
</ul>
</li>
<li><p>样本构造</p>
<ul>
<li>标签定义<ul>
<li>尽量为客观事实（是否、数量），而非主观判断（等级）</li>
<li>样本粒度贴合实际、业务（订单粒度、客户粒度）</li>
</ul>
</li>
<li>样本数量<ul>
<li>二分类场景：正例样本大于 2000，占比超过 1%</li>
</ul>
</li>
<li>采样<ul>
<li>尽量不进行人工采样，保持训练数据正、负例比例和真实情况对齐</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="传统评分卡"><a href="#传统评分卡" class="headerlink" title="传统评分卡"></a>传统评分卡</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>评分卡</th>
<th>复杂学习</th>
</tr>
</thead>
<tbody>
<tr>
<td>特征筛选</td>
<td>需筛选强特征，依赖业务经验</td>
<td>支持弱特征入模</td>
</tr>
<tr>
<td>特征处理</td>
<td><em>WOE</em> 分箱，稳定性好</td>
<td></td>
</tr>
<tr>
<td>非线性</td>
<td>仅 <em>WOE</em> 分箱提供非线性，解释性好</td>
<td>非线性充分挖掘数据信息，解释性差</td>
</tr>
<tr>
<td>复杂度</td>
<td>模型简单，泛化性好，样本需求小</td>
<td>模型复杂，表达能力强，样本少时容易过拟合</td>
</tr>
<tr>
<td>调参</td>
<td>超参少</td>
<td>调参难度大</td>
</tr>
<tr>
<td>模型提升方向</td>
<td>分（样本）群建模</td>
<td><em>Stacking</em> 结合评分卡</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>信用评分卡模型：利用模型将账户的属性特征按取值分组、并赋予一定分数，对账户进行信用评分</p>
<ul>
<li>最常见的金融风控手段之一，用于决定是否给予授信以及授信的额度和利率</li>
<li>常用逻辑回归作为模型</li>
<li>应用形式为查分组得分表、得分加和<ul>
<li>变量总是被分组，同组内得分相同</li>
<li>用户属性变化不足以跨越箱边界，则得分不改变</li>
</ul>
</li>
</ul>
</li>
<li><p>评分卡更关注得分相对值，即得分变动情况，评分绝对值含义意义不大</p>
<ul>
<li>常用 <em>LR</em> 中 <em>sigmoid</em> 函数内线性函数结果作为初始得分<ul>
<li>根据 <em>LR</em> 意义，此时得分可以映射为账户的违约概率</li>
</ul>
</li>
<li>为美观，可能会对得分做线性变换<ul>
<li>常对各特征得分做放缩、对账户得分和做平移，此时放缩比例除以 $ln2$ 即为 <em>PDO</em>
（对特征得分同时做等比例放缩、平移可行但蠢）</li>
<li>线性变换后得分绝对值无意义，特征重要性可用特征各分组得分差距衡量</li>
</ul>
</li>
</ul>
</li>
<li><p>评分卡在不同业务阶段体现的方式、功能不一样，按照借贷用户借贷时间可以分为</p>
<ul>
<li>申请评分卡 <em>Application Score Card</em>：贷前申请评分卡</li>
<li>行为评分卡 <em>Behavior Score Card</em>：贷中行为评分卡</li>
<li>催收评分卡 <em>Collection Score Card</em>：贷后催收评分卡</li>
</ul>
</li>
</ul>
<h3 id="Stacking-评分卡"><a href="#Stacking-评分卡" class="headerlink" title="Stacking 评分卡"></a><em>Stacking</em> 评分卡</h3><blockquote>
<ul>
<li>考虑将评分卡、机器学习模型结合，使用机器学习模型构建特征，在此基础之上建立评分卡模型</li>
</ul>
</blockquote>
<ul>
<li><p><em>Stacking</em> 思想下的模型架构</p>
<ul>
<li>原始数据域</li>
<li>数据挖掘、特征工程</li>
<li>数据域特征子模型</li>
<li>评分卡模型</li>
</ul>
</li>
<li><p>架构优势</p>
<ul>
<li>可解释性：保留在数据域粒度上的可解释性</li>
<li>信息提取：子模型提取弱特征信息，降低特征工程门槛</li>
<li>维度多样性：特征子模型机制，降低特征筛选必要性，保证各数据域都有特征入模</li>
<li>模块化：具有良好扩展性，支持子模型替换、删除</li>
<li>并行化：各数据域特征子模型专业、独立负责，提高效率</li>
</ul>
</li>
<li><p>架构劣势</p>
<ul>
<li>牺牲部分可解释性：若策略、模型使用相同变量，策略阈值调整对模型影响难以估计<ul>
<li>控制入模变量数目，便于快速定位</li>
<li>利用 <em>SHAP</em>、<em>LIME</em> 等工具解释模型</li>
</ul>
</li>
<li>增加上线、维护成本：需要上线多个模型，且对多个架构多个层次都进行监控</li>
<li>协同建模增加对接成本</li>
<li>分数据域特征子模型建模，容易造成数据孤岛，无法捕捉不同数据域间的数据联系<ul>
<li>跨数据域构造特征，构建跨数据域子模型</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="B-卡-Behavior-Scoring"><a href="#B-卡-Behavior-Scoring" class="headerlink" title="B 卡 - Behavior Scoring"></a><em>B</em> 卡 - <em>Behavior Scoring</em></h3><p>贷中风控：根据借款人放贷后行为表现，预测未来逾期风险</p>
<ul>
<li><p><em>B</em> 卡用于动态监控放款后风险变化</p>
<ul>
<li>贷前阶段对借款人履约行为掌握少，且为静态数据</li>
<li>一般无需实时，离线T+1计算即可</li>
</ul>
</li>
<li><p><em>B</em> 卡适合的信贷场景</p>
<ul>
<li>还款周期长<ul>
<li>长周期场景用户风险变化可能性大，与 <em>A</em> 卡形成区分</li>
<li>引入贷中客户信息、还款履约行为，更准确识别客户逾期风险</li>
</ul>
</li>
<li>循环授信<ul>
<li>贷前阶段，无法很好识别客户风险，设置初始额度</li>
<li>贷中与客户更多交互之后，可根据获取的贷中行为信息进行提额、降额操作</li>
</ul>
</li>
</ul>
</li>
<li><p><em>B</em> 卡区分度一般很高</p>
<ul>
<li>除贷前数据之外，还可以使用账户的贷中表现数据</li>
<li>特别的，不考虑排序性的情况下，使用是否逾期作为划分依据也能得到较高的 <em>TPR-FPR</em>，给出 <em>KS</em> 的下限</li>
</ul>
</li>
<li><p><em>B</em> 卡建模主要基于老客</p>
<ul>
<li>老客有足够长的申贷、还款记录</li>
<li>新、老客定义口径<ul>
<li>新客：无历史结清订单</li>
<li>老客：至少有1笔结清订单</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="C-卡-Collection-Scoring"><a href="#C-卡-Collection-Scoring" class="headerlink" title="C 卡 - Collection Scoring"></a><em>C</em> 卡 - <em>Collection Scoring</em></h3><p>贷后催收评分卡：当前状态为逾期情况下，预测未来出催可能性</p>
<ul>
<li><p>现阶段业界对 <em>C</em> 卡不够重视</p>
<ul>
<li>贷前风控最重要，优秀的贷前带来更容易的贷中、贷后</li>
<li>催收效果和人员更相关，而逾期发生之后往往会委外</li>
<li>随信贷行业的发展，贷后催收会趋向于精细化、专业化的发展，模型+策略的优化愈发重要</li>
</ul>
</li>
<li><p>模型分群</p>
<ul>
<li>新老入催用户<ul>
<li>首次入催</li>
<li>再次入催</li>
</ul>
</li>
<li><em>MOB</em> 信息（数据厚薄）<ul>
<li>还款月份数</li>
<li>催记月份数</li>
</ul>
</li>
<li>订单详情<ul>
<li>利率</li>
<li>期限</li>
<li>金额</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="样本选择"><a href="#样本选择" class="headerlink" title="样本选择"></a>样本选择</h4><ul>
<li><p>建模样本窗口选择</p>
<ul>
<li>特征覆盖度：保证数据厚薄程度相同</li>
<li>催收动作变化：出催没有大幅度变动</li>
<li>客群变化：入催没有大幅变动</li>
</ul>
</li>
<li><p>同用户订单合案</p>
<ul>
<li>不合案：同用户多笔订单视为不同样本<ul>
<li>表现期内入催当期结清视为出催</li>
</ul>
</li>
<li>合案：同用户相近观察点入催订单合并<ul>
<li>表现期内入催当期所有账单还清视为出催</li>
<li>对发生过 <em>M2+</em> 逾期者，可将只要出催一期即视为出催</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="C-卡模型"><a href="#C-卡模型" class="headerlink" title="C 卡模型"></a><em>C</em> 卡模型</h4><blockquote>
<ul>
<li>根据模型作用时间段分类</li>
</ul>
</blockquote>
<ul>
<li><p><em>M1</em> 全量模型：预测 <em>M1</em> 阶段（逾期 30 天内）还款概率</p>
<ul>
<li>样本：所有入催样本整体<ul>
<li>若缓催期内催出用户较多，则模型主要学习了缓催样本信息，约等于缓催响应模型，对非缓催样本效果较差</li>
</ul>
</li>
<li>时间窗口<ul>
<li>观察点：还款日</li>
<li>表现期：<em>M1</em> 阶段</li>
</ul>
</li>
</ul>
</li>
<li><p>缓催响应模型：预测适合缓催人群</p>
<ul>
<li>样本：需要积累足够的缓催响应样本<ul>
<li>若有足够缓催响应样本，可以和M1全量模型同时构建</li>
<li>否则，在 <em>M1</em> 全量模型得分高（出催概率高）人群上进行 <em>AB Test</em>，积累缓催响应样本</li>
</ul>
</li>
<li>时间窗口<ul>
<li>观察点：还款日</li>
<li>表现期：缓催响应日(2-3 天)</li>
</ul>
</li>
</ul>
</li>
<li><p>贷后 <em>N</em> 天流转模型：预测贷后N天后的还款概率</p>
<ul>
<li>样本：缓催内未出催样本<ul>
<li>去除缓催样本影响，更多学习缓催期外出催样本信息</li>
<li>优先对催出概率高的人群进行催收，提高出催概率</li>
</ul>
</li>
<li>时间窗口<ul>
<li>观察点：还款日（逾期）后 <em>N</em> 天</li>
<li>表现期：至下个流转模型观察点、逾期阶段结束时间点</li>
</ul>
</li>
</ul>
</li>
<li><p><em>M2+</em> 模型：预测 <em>M2+</em> 阶段的还款概率（类似贷后流转模型）</p>
<ul>
<li>样本：<em>M1</em> 阶段未出催样本</li>
<li>时间窗口<ul>
<li>观察点：<em>M2</em> 阶段起始</li>
<li>表现期：至下个流转模型观察点、逾期阶段结束时间点</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="模型应用方法"><a href="#模型应用方法" class="headerlink" title="模型应用方法"></a>模型应用方法</h4><ul>
<li><p>缓催响应人群确定</p>
<ul>
<li>交叉 <em>M1</em> 模型、缓催响应模型，根据模型交叉结果设置阈值</li>
<li>根据阈值筛选缓催响应人群</li>
<li>限定缓催期（2-3 天），将缓催响应样本分为人工催收、缓催两组，观察两组在缓催期限内出催率变化<ul>
<li>若出催率相同，则认为缓催响应人群分析方法可行，对缓催响应人群可采取缓催策略</li>
<li>若出催率相差较大，则调整缓催响应人群分析方法</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>缓催模型响应时间（缓催期）可根据响应时间段内的出催率变化设置</li>
</ul>
</blockquote>
</li>
<li><p>模型搭建策略</p>
<ul>
<li><em>M1</em> 阶段出催概率较大，在M1阶段会设计多个细分模型<ul>
<li>至少：<em>M1</em> 阶段全量模型</li>
<li>缓催样本足够<ul>
<li>缓催响应模型</li>
<li>贷后 <em>N</em> 天流转模型</li>
</ul>
</li>
<li>精细化管理：多个不同时间窗口的贷后流转模型</li>
</ul>
</li>
<li><em>M2+</em> 阶段根据样本量、精细化程度设置适量模型</li>
</ul>
</li>
</ul>
<h2 id="开发流程标准化"><a href="#开发流程标准化" class="headerlink" title="开发流程标准化"></a>开发流程标准化</h2><ul>
<li>风控模型开发流程标准化意义<ul>
<li>提高建模效率：可批量快速生产模型，提高效率</li>
<li>帮助理解指标逻辑、业务含义，利于调试优化</li>
<li>流程规范约束<ul>
<li>统一建模流程，减少出错概率、便于问题回溯</li>
<li>统一命名方式，便于汇总文档</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h4><ul>
<li><p>特征离散化</p>
</li>
<li><p><em>WOE</em> 编码特征</p>
<ul>
<li><em>WOE</em> 曲线应符合业务逻辑（一般单调），并且经过跨时间
窗口验证，否则应该调整</li>
<li><em>LR</em> 模型中特征权重应该全为正值，否则<ul>
<li>同数据 <em>WOE</em> 值体现的逻辑相违背</li>
<li>负值权重特征存在较严重共线性</li>
</ul>
</li>
</ul>
</li>
<li><p><em>one-hot</em> 编码特征</p>
<ul>
<li>同特征下个分箱单独作为独立变量取值<ul>
<li>权重灵活性更大，模型效果可能较好</li>
<li>变量数量多，需要样本数量大，模型效果可能较差（随机解法）</li>
</ul>
</li>
<li>各特征分箱之间无联系，难以通过模型剔除某个变量</li>
</ul>
</li>
</ul>
<h4 id="样本赋权"><a href="#样本赋权" class="headerlink" title="样本赋权"></a>样本赋权</h4><ul>
<li>样本赋权：充分利用所有样本的信息，避免样本有偏<ul>
<li>按样本距今时间赋权，近期样本高权重</li>
<li>按业务特性赋权，不同额度、利率、期限不同权重</li>
<li>按账户类型赋权</li>
</ul>
</li>
</ul>
<h4 id="拒绝推断"><a href="#拒绝推断" class="headerlink" title="拒绝推断"></a>拒绝推断</h4><ul>
<li><em>Reject Inference</em> 拒绝推断：避免样本偏差导致模型估计过于乐观</li>
</ul>
<h3 id="Exploratory-Data-Analysis"><a href="#Exploratory-Data-Analysis" class="headerlink" title="Exploratory Data Analysis"></a><em>Exploratory Data Analysis</em></h3><ul>
<li><p>风控领域样本较少，一般按月粒度观察，即将样本按月分组为 <em>vintage</em> 进行分析，探索、评估数据</p>
<ul>
<li>稳定性</li>
<li>信息量</li>
<li>信息重复/相关性</li>
</ul>
</li>
<li><p>实操中可逐阶段设置多组阈值，分布进行变量探索、筛选</p>
<ul>
<li>多组阈值逐步剔除能尽可能保留高信息量特征</li>
<li>避免相关性、<em>RF</em> 特征重要度等 <strong>非单变量指标</strong> 剔除过多特征</li>
</ul>
</li>
</ul>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><ul>
<li><p>有效性/区分度</p>
<ul>
<li><em>GINI</em> 指数</li>
<li><em>KS</em> 值</li>
<li>坏样本率：组内、累计</li>
<li>提升度 = 召回样本坏样本率 / 全部样本坏样本率</li>
<li><em>odds</em> = 坏样本率 / 好样本率</li>
</ul>
</li>
<li><p>排序性</p>
<ul>
<li><em>AUC</em> 值/<em>ROC</em> 曲线</li>
</ul>
</li>
<li><p>稳定性</p>
<ul>
<li><em>PSI</em></li>
<li>各 <em>Vintage</em> 内坏占比、<em>Lift</em> 值、<em>odds</em> 等指标稳定性</li>
</ul>
</li>
<li><p>模型得分展示表</p>
<ul>
<li>箱内样本数</li>
<li>好、坏样本数</li>
<li>箱内坏样本、比例</li>
<li>累计好、坏样本</li>
<li>累计好、坏样本比例：<em>TPR</em>、<em>FPR</em>、<em>TPR-FPR</em></li>
<li>累计通过率、坏样本比例</li>
</ul>
</li>
</ul>
<h3 id="模型应用"><a href="#模型应用" class="headerlink" title="模型应用"></a>模型应用</h3><h4 id="Calibration-模型校准"><a href="#Calibration-模型校准" class="headerlink" title="Calibration 模型校准"></a><em>Calibration</em> 模型校准</h4><ul>
<li>一致性校准：将模型预测概率校准到真实概率</li>
<li>尺度变换：将风险概率转换为整数分数</li>
</ul>
<h4 id="导出得分"><a href="#导出得分" class="headerlink" title="导出得分"></a>导出得分</h4><ul>
<li><p>原始得分</p>
<ul>
<li><em>one-hot</em> 编码：<em>LR</em> 模型系数</li>
<li><em>WOE</em> 编码：<em>LR</em> 模型系数（权重）、<em>WOE</em> 值之积</li>
</ul>
</li>
<li><p>常对各特征得分做放缩、对账户得分和做平移</p>
<ul>
<li><em>PDO</em>：违约翻倍得分<ul>
<li>用于缩放原始得分</li>
<li>得分按 $\frac {PDO} {ln2}$ 缩放后，得分减少 $PDO$ 分，用户违约 <em>odds</em> 翻倍，缺省即 $ln2$</li>
</ul>
</li>
<li>账户得分总和平移则仅仅是为了美观</li>
</ul>
<blockquote>
<ul>
<li>对特征得分同时做等比例放缩、平移可行但蠢</li>
</ul>
</blockquote>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-08-29T10:12:36.000Z" title="8/29/2019, 6:12:36 PM">2019-08-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-08-29T10:12:43.000Z" title="8/29/2019, 6:12:43 PM">2019-08-29</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Model-Enhencement/">Model Enhencement</a></span><span class="level-item">a few seconds read (About 1 word)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Model-Enhencement/lightgbm.html">LightGBM</a></h1><div class="content"><h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-31T18:10:08.000Z" title="8/1/2019, 2:10:08 AM">2019-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T06:52:53.000Z" title="7/16/2021, 2:52:53 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Linear-Model/">Linear Model</a></span><span class="level-item">25 minutes read (About 3688 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Linear-Model/maximum_entropy.html">最大熵模型</a></h1><div class="content"><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><h3 id="逻辑斯蒂分布"><a href="#逻辑斯蒂分布" class="headerlink" title="逻辑斯蒂分布"></a>逻辑斯蒂分布</h3><script type="math/tex; mode=display">\begin{align*}
F(x) & = P(X \leq x) = \frac 1 {1 + e^{-(x-\mu)/\gamma}} \\
f(x) & = F^{'}(x) = \frac {e^{-(x-\mu)/\gamma}}
    {\gamma(1+e^{-(x-\mu)/\gamma})^2}
\end{align*}</script><blockquote>
<ul>
<li>$\mu$：位置参数</li>
<li>$\gamma$：形状参数</li>
</ul>
</blockquote>
<ul>
<li>分布函数属于逻辑斯蒂函数</li>
<li><p>分布函数图像为sigmoid curve</p>
<ul>
<li>关于的$(\mu, \frac 1 2)$中心对称<script type="math/tex; mode=display">
F(-x+\mu) - \frac 1 2 = -F(x+\mu) + \frac 1 2</script></li>
<li>曲线在靠近$\mu$中心附近增长速度快，两端速度增长慢</li>
<li>形状参数$\gamma$越小，曲线在中心附近增加越快</li>
</ul>
</li>
<li><p>模型优点</p>
<ul>
<li>模型输出值位于0、1之间，天然具有概率意义，方便观测
样本概率分数</li>
<li>可以结合$l-norm$正则化解决过拟合、共线性问题</li>
<li>实现简单，广泛用于工业问题</li>
<li>分类时计算量比较小、速度快、消耗资源少</li>
</ul>
</li>
<li><p>模型缺点</p>
<ul>
<li>特征空间很大时，性能不是很好，容易欠拟合，准确率一般</li>
<li>对非线性特征需要进行转换</li>
</ul>
</li>
</ul>
<h3 id="Binomial-Logistic-Regression-Model"><a href="#Binomial-Logistic-Regression-Model" class="headerlink" title="Binomial Logistic Regression Model"></a><em>Binomial Logistic Regression Model</em></h3><p>二项逻辑斯蒂回归模型：形式为参数化逻辑斯蒂分布的二分类
生成模型</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y=1|x) & = \frac {exp(wx + b)} {1 + exp (wx + b)} \\
P(Y=0|x) & = \frac 1 {1 + exp(wx + b)} \\
P(Y=1|\hat x) & = \frac {exp(\hat w \hat x)}
    {1 + exp (\hat w \hat x)} \\
P(Y=0|\hat x) & = \frac 1 {1+exp(\hat w \hat x)}
\end{align*}</script><blockquote>
<ul>
<li>$w, b$：权值向量、偏置</li>
<li>$\hat x = (x^T|1)^T$</li>
<li>$\hat w = (w^T|b)^T$</li>
</ul>
</blockquote>
<ul>
<li><p>逻辑回归比较两个条件概率值，将实例$x$归于条件概率较大类</p>
</li>
<li><p>通过逻辑回归模型，可以将线性函数$wx$转换为概率</p>
<ul>
<li>线性函数值越接近正无穷，概率值越接近1</li>
<li>线性函数值越接近负无穷，概率值越接近0</li>
</ul>
</li>
</ul>
<h4 id="Odds-Odds-Ratio"><a href="#Odds-Odds-Ratio" class="headerlink" title="Odds/Odds Ratio"></a>Odds/Odds Ratio</h4><ul>
<li><p>在逻辑回归模型中，输出$Y=1$的对数几率是输入x的线性函数</p>
<script type="math/tex; mode=display">
log \frac {P(Y=1|x)} {1-P(Y=1|x)} = \hat w \hat x</script></li>
<li><p>OR在逻辑回归中意义：$x_i$每增加一个单位，odds将变为原来
的$e^{w_i}$倍</p>
<script type="math/tex; mode=display">\begin{align*}
odd &= \frac {P(Y=1|x)} {1-P(Y=1|x)} = e^{\hat w \hat x} \\
OR_{x_i+1 / x_i} &= e^{w_i}
\end{align*}</script><ul>
<li><p>对数值型变量</p>
<ul>
<li>多元LR中，变量对应的系数可以计算相应
<em>Conditional OR</em></li>
<li>可以建立单变量LR，得到变量系数及相应
<em>Marginal OR</em></li>
</ul>
</li>
<li><p>对分类型变量</p>
<ul>
<li>可以直接计算变量各取值间对应的OR</li>
<li>变量数值化编码建立模型，得到变量对应OR</li>
</ul>
<blockquote>
<ul>
<li>根据变量编码方式不同，变量对应OR的含义不同，其中
符合数值变量变动模式的是WOE线性编码</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
<h4 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h4><p>极大似然：极小对数损失（交叉熵损失）</p>
<script type="math/tex; mode=display">\begin{align*}
L(w) & = log \prod_{i=1}^N [\pi(x_i)]^{y_i}
    [1-\pi(x_i)]^{1-y_i} \\
& = \sum_{i=1}^N [y_i log \pi(x_i) + (1-y_i)log(1-\pi(x_i))] \\
& = \sum_{i=1}^N [y_i log \frac {\pi(x_i)}
    {1-\pi(x_i)} log(1-\pi(x_i))] \\
& = \sum_{i=1}^N [y_i(\hat w \hat x_i) -
    log(1+exp(\hat w \hat x_i))]
\end{align*}</script><blockquote>
<ul>
<li>$\pi(x) = P(Y=1|x)$</li>
</ul>
</blockquote>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ul>
<li>通常采用梯度下降、拟牛顿法求解有以上最优化问题</li>
</ul>
<h3 id="Multi-Nominal-Logistic-Regression-Model"><a href="#Multi-Nominal-Logistic-Regression-Model" class="headerlink" title="Multi-Nominal Logistic Regression Model"></a><em>Multi-Nominal Logistic Regression Model</em></h3><p>多项逻辑斯蒂回归：二项逻辑回归模型推广</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y=j|x) & = \frac {exp(\hat w_j \hat x)} {1+\sum_{k=1}^{K-1}
    exp(\hat w_k \hat x)}, k=1,2,\cdots,K-1 \\
P(Y=K|x) & = \frac 1 {1+\sum_{k=1}^{K-1}
    exp(\hat w_k \hat x)}
\end{align*}</script><ul>
<li>策略、算法类似二项逻辑回归模型</li>
</ul>
<h2 id="Generalized-Linear-Model"><a href="#Generalized-Linear-Model" class="headerlink" title="Generalized Linear Model"></a><em>Generalized Linear Model</em></h2><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><h2 id="Maximum-Entropy-Model"><a href="#Maximum-Entropy-Model" class="headerlink" title="Maximum Entropy Model"></a><em>Maximum Entropy Model</em></h2><h3 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h3><p>最大熵原理：学习概率模型时，在所有可能的概率模型（分布）中，
<strong>熵最大的模型是最好的模型</strong></p>
<ul>
<li><p>使用约束条件确定概率模型的集合，则最大熵原理也可以表述为
<strong>在满足约束条件的模型中选取熵最大的模型</strong></p>
</li>
<li><p>直观的，最大熵原理认为</p>
<ul>
<li>概率模型要满足已有事实（约束条件）</li>
<li>没有更多信息的情况下，不确定部分是等可能的</li>
<li>等可能不容易操作，所有考虑使用<strong>可优化</strong>的熵最大化
表示等可能性</li>
</ul>
</li>
</ul>
<h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><p>最大熵模型为生成模型</p>
<ul>
<li><p>对给定数据集$T={(x_1,y_1),\cdots,(x_N,y_N)}$，联合分布
P(X,Y)、边缘分布P(X)的经验分布如下</p>
<script type="math/tex; mode=display">\begin{align*}
\tilde P(X=x, Y=y) & = \frac {v(X=x, Y=y)} N \\
\tilde P(X=x) & = \frac {v(X=x)} N
\end{align*}</script><blockquote>
<ul>
<li>$v(X=x,Y=y)$：训练集中样本$(x,y)$出频数</li>
</ul>
</blockquote>
</li>
<li><p>用如下<em>feature function</em> $f(x, y)$描述输入x、输出y之间
某个事实</p>
<script type="math/tex; mode=display">f(x, y) = \left \{ \begin{array}{l}
1, & x、y满足某一事实 \\
0, & 否则
\end{array} \right.</script><ul>
<li><p>特征函数关于经验分布$\tilde P(X, Y)$的期望</p>
<script type="math/tex; mode=display">
E_{\tilde P} = \sum_{x,y} \tilde P(x,y)f(x,y)</script></li>
<li><p>特征函数关于生成模型$P(Y|X)$、经验分布$\tilde P(X)$
期望</p>
<script type="math/tex; mode=display">
E_P(f(x)) = \sum_{x,y} \tilde P(x)P(y|x)f(x,y)</script></li>
</ul>
</li>
<li><p>期望模型$P(Y|X)$能够获取数据中信息，则两个期望值应该相等</p>
<script type="math/tex; mode=display">\begin{align*}
E_P(f) & = E_{\tilde P}(f) \\
\sum_{x,y} \tilde P(x)P(y|x)f(x,y) & =
   \sum_{x,y} \tilde P(x,y)f(x,y)
\end{align*}</script><p>此即作为模型学习的约束条件</p>
<ul>
<li><p>此约束是纯粹的关于$P(Y|X)$的约束，只是约束形式特殊，
需要通过期望关联熵</p>
</li>
<li><p>若有其他表述形式、可以直接带入的、关于$P(Y|X)$约束，
可以直接使用</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>满足所有约束条件的模型集合为<script type="math/tex; mode=display">
  \mathcal{C} = \{P | E_{P(f_i)} = E_{\tilde P (f_i)},
      i=1,2,\cdots,n \}</script>  定义在条件概率分布$P(Y|X)$上的条件熵为<script type="math/tex; mode=display">
  H(P) = -\sum_{x,y} \tilde P(x) P(y|x) logP(y|x)</script>  则模型集合$\mathcal{C}$中条件熵最大者即为最大是模型</li>
</ul>
</blockquote>
<h3 id="策略-1"><a href="#策略-1" class="headerlink" title="策略"></a>策略</h3><p>最大熵模型的策略为以下约束最优化问题</p>
<script type="math/tex; mode=display">\begin{array}{l}
\max_{P \in \mathcal{C}} & -H(P)=\sum_{x,y} \tilde P(x)
    P(y|x) logP(y|x) \\
s.t. & E_P(f_i) - E_{\tilde P}(f_i) = 0, i=1,2,\cdots,M \\
& \sum_{y} P(y|x)  = 1
\end{array}</script><ul>
<li><p>引入拉格朗日函数</p>
<script type="math/tex; mode=display">\begin{align*}
L(P, w) & = -H(P) - w_0(1-\sum_y P(y|x)) + \sum_{m=1}^M
   w_m(E_{\tilde P}(f_i) - E_P(f_i)) \\
& = \sum_{x,y} \tilde P(x) P(y|x) logP(y|x) + w_0
   (1-\sum_y P(y|x)) + \sum_{m=1}^M w_m (\sum_{x,y}
   \tilde P(x,y)f_i(x, y) - \tilde P(x)P(y|x)f_i(x,y))
\end{align*}</script><ul>
<li><p>原始问题为</p>
<script type="math/tex; mode=display">
\min_{P \in \mathcal{C}} \max_{w} L(P, w)</script></li>
<li><p>对偶问题为</p>
<script type="math/tex; mode=display">
\max_{w} \min_{P \in \mathcal{C}} L(P, w)</script></li>
<li><p>考虑拉格朗日函数$L(P, w)$是P的凸函数，则原始问题、
对偶问题解相同</p>
</li>
</ul>
</li>
<li><p>记</p>
<script type="math/tex; mode=display">\begin{align*}
\Psi(w) & = \min_{P \in \mathcal{C}} L(P, w)
   = L(P_w, w) \\
P_w & = \arg\min_{P \in \mathcal{C}} L(P, w) = P_w(Y|X)
\end{align*}</script></li>
<li><p>求$L(P, w)$对$P(Y|X)$偏导</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial L(P, w)} {\partial P(Y|X)} & =
   \sum_{x,y} \tilde P(x)(logP(y|x)+1) - \sum_y w_0 -
   \sum_{x,y}(\tilde P(x) \sum_{i=1}^N w_i f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x)(log P(y|x) + 1 - w_0 -
   \sum_{i=1}^N w_i f_i(x, y))
\end{align*}</script><p>偏导置0，考虑到$\tilde P(x) &gt; 0$，其系数必始终为0，有</p>
<script type="math/tex; mode=display">\begin{align*}
P(Y|X) & = \exp(\sum_{i=1}^N w_i f_i(x,y) + w_0 - 1) \\
& = \frac {exp(\sum_{i=1}^N w_i f_i(x,y))} {exp(1-w_0)}
\end{align*}</script></li>
<li><p>考虑到约束$\sum_y P(y|x) = 1$，有</p>
<script type="math/tex; mode=display">\begin{align*}
P_w(y|x) & = \frac 1 {Z_w(x)} exp(\sum_{i=1}^N w_i
   f_i(x,y)) \\
Z_w(x) & = \sum_y exp(\sum_{i=1}^N w_i f_i(x,y)) \\
& = exp(1 - w_0)
\end{align*}</script><blockquote>
<ul>
<li>$Z_w(x)$：规范化因子</li>
<li>$f(x, y)$：特征</li>
<li>$w_i$：特征权值</li>
</ul>
</blockquote>
</li>
<li><p>原最优化问题等价于求解偶问题极大化问题$\max_w \Psi(w)$</p>
<script type="math/tex; mode=display">\begin{align*}
\Psi(w) & = \sum_{x,y} \tilde P(x) P_w(y|x) logP_w(y|x)
   + \sum_{i=1}^N w_i(\sum_{x,y} \tilde P(x,y) f_i(x,y)
   - \sum_{x,y} \tilde P(x) P_w(y|x) f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) +
   \sum_{x,y} \tilde P(x,y) P_w(y|x)(log P_w(y|x) -
   \sum_{i=1}^N w_i f_i(x,y)) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_{x,y} \tilde P(x,y) P_w(y|x) log Z_w(x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_x \tilde P(x) log Z_w(x)
\end{align*}</script><p>记其解为</p>
<script type="math/tex; mode=display">w^{*} = \arg\max_w \Psi(w)</script><p>带入即可得到最优（最大熵）模型$P_{w^{*}}(Y|X)$</p>
</li>
</ul>
<h4 id="策略性质"><a href="#策略性质" class="headerlink" title="策略性质"></a>策略性质</h4><ul>
<li><p>已知训练数据的经验概率分布为$\tilde P(X,Y)$，则条件概率
分布$P(Y|X)$的对数似然函数为</p>
<script type="math/tex; mode=display">\begin{align*}
L_{\tilde P}(P_w) & = N log \prod_{x,y}
   P(y|x)^{\tilde P(x,y)} \\
& = \sum_{x,y} N * \tilde P(x,y) log P(y|x)
\end{align*}</script><blockquote>
<ul>
<li>这里省略了系数样本数量$N$</li>
</ul>
</blockquote>
</li>
<li><p>将最大熵模型带入，可得</p>
<script type="math/tex; mode=display">\begin{align*}
L_{\tilde P_w} & = \sum_{x,y} \tilde P(y|x) logP(y|x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_{x,y} \tilde P(x,y)log Z_w(x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^N w_i f_i(x,y) -
   \sum_x \tilde P(x) log Z_w(x) \\
& = \Psi(w)
\end{align*}</script><p>对偶函数$\Psi(w)$等价于对数似然函数$L_{\tilde P}(P_w)$，
即最大熵模型中，<strong>对偶函数极大等价于模型极大似然估计</strong></p>
</li>
</ul>
<h3 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h3><ul>
<li><p>思想</p>
<ul>
<li>假设最大熵模型当前参数向量$w=(w_1,w_2,\cdots,w_M)^T$</li>
<li>希望能找到新的参数向量（参数向量更新）
$w+\sigma=(w_1+\sigma_1,\cdots,w_M+\sigma_M)$
使得模型对数似然函数/对偶函数值增加</li>
<li>不断对似然函数值进行更新，直到找到对数似然函数极大值</li>
</ul>
</li>
<li><p>对给定经验分布$\tilde P(x,y)$，参数向量更新至$w+\sigma$
时，对数似然函数值变化为</p>
<script type="math/tex; mode=display">\begin{align*}
L(w+\sigma) - L(w) & = \sum_{x,y} \tilde P(x,y)
   log P_{w+\sigma}(y|x) - \sum_{x,y} \tilde P(x,y)
   log P_w(y|x) \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) - \sum_x \tilde P(x) log \frac
   {Z_{w+\sigma}(x)} {Z_w(x)} \\
& \geq \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) + 1 - \sum_x \tilde P(x) \frac
   {Z_{w+\sigma}(x)} {Z_w(x)} \\
& = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M \sigma_i
   f_i(x,y) + 1 - \sum_x \tilde P(x) \sum_y P_y(y|x)
   exp(\sum_{i=1}^M \sigma_i f_i(x,y))
\end{align*}</script><ul>
<li><p>不等式步利用$a - 1 \geq log a, a \geq 1$</p>
</li>
<li><p>最后一步利用</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {Z_{w+\sigma}(x)} {Z_w(x)} & = \frac 1 {Z_w(x)}
  \sum_y exp(\sum_{i=1}^M (w_i + \sigma_i)
  f_i(x, y)) \\
& = \frac 1 {Z_w(x)} \sum_y exp(\sum_{i=1}^M w_i
  f_i(x,y) + \sigma_i f_i(x,y)) \\
& = \sum_y P_w(y|x) exp(\sum_{i=1}^n \sigma_i
  f_i(x,y))
\end{align*}</script></li>
</ul>
</li>
<li><p>记上式右端为$A(\sigma|w)$，则其为对数似然函数改变量的
一个下界</p>
<script type="math/tex; mode=display">
L(w+\sigma) - L(w) \geq A(\sigma|w)</script><ul>
<li>若适当的$\sigma$能增加其值，则对数似然函数值也应该
增加</li>
<li>函数$A(\sigma|w)$中因变量$\sigma$为向量，难以同时
优化，尝试每次只优化一个变量$\sigma_i$，固定其他变量
$\sigma_j$</li>
</ul>
</li>
<li><p>记</p>
<script type="math/tex; mode=display">f^{**} (x,y) = \sum_i f_i(x,y)</script><p>考虑到$f_i(x,y)$为二值函数，则$f^{**}(x,y)$表示所有特征
在$(x,y)$出现的次数，且有</p>
<script type="math/tex; mode=display">
A(\sigma|w) = \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M
   \sigma_i f_i(x,y) + 1 - \sum_x \tilde P(x)
   \sum_y P_w(y|x) exp(f^{**}(x,y) \sum_{i=1}^M
   \frac {\sigma_i f_i(x,y)} {f^{**}(x,y)})</script></li>
<li><p>考虑到$\sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)} = 1$，
由指数函数凸性、Jensen不等式有</p>
<script type="math/tex; mode=display">
exp(\sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)} \sigma_i
   f^{**}(x,y)) \leq \sum_{i=1}^M \frac {f_i(x,y)}
   {f^{**}(x,y)} exp(\sigma_i f^{**}(x,y))</script><p>则</p>
<script type="math/tex; mode=display">
A(\sigma|w) \geq \sum_{x,y} \tilde P(x,y) \sum_{i=1}^M
   \sigma_i f_i(x,y) + 1 - \sum_x \tilde P(x) \sum_y
   P_w(y|x) \sum_{i=1}^M \frac {f_i(x,y)} {f^{**}(x,y)}
   exp(\sigma_i f^{**}(x,y))</script></li>
<li><p>记上述不等式右端为$B(\sigma|w)$，则有</p>
<script type="math/tex; mode=display">
L(w+\sigma) - L(w) \geq B(\sigma|w)</script><p>其为对数似然函数改变量的一个新、相对不紧的下界</p>
</li>
<li><p>求$B(\sigma|w)$对$\sigma_i$的偏导</p>
<script type="math/tex; mode=display">
\frac {\partial B(\sigma|w)} {\partial \sigma_i} =
   \sum_{x,y} \tilde P(x,y) f_i(x,y) -
   \sum_x \tilde P(x) \sum_y P_w(y|x) f_i(x,y)
   exp(\sigma_i f^{**}(x,y))</script><p>置偏导为0，可得</p>
<script type="math/tex; mode=display">
\sum_x \tilde P(x) \sum_y P_w(y|x) f_i(x,y) exp(\sigma_i
   f^{**}(x,y)) = \sum_{x,y} \tilde P(x,y) f_i(x,y) =
   E_{\tilde P}(f_i)</script><p>其中仅含变量$\sigma_i$，则依次求解以上方程即可得到
$\sigma$</p>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><blockquote>
<ul>
<li>输入：特征函数$f_1, f_2, \cdots, f_M$、经验分布
  $\tilde P(x)$、最大熵模型$P_w(x)$</li>
<li>输出：最优参数值$w<em>i^{*}$、最优模型$P</em>{w^{*}}$</li>
</ul>
</blockquote>
<ol>
<li><p>对所有$i \in {1,2,\cdots,M}$，取初值$w_i = 0$</p>
</li>
<li><p>对每个$i \in {1,2,\cdots,M}$，求解以上方程得$\sigma_i$</p>
<ul>
<li><p>若$f^{**}(x,y)=C$为常数，则$\sigma_i$有解析解</p>
<script type="math/tex; mode=display">
\sigma_i = \frac 1 C log \frac {E_{\tilde P}(f_i)}
 {E_P(f_i)}</script></li>
<li><p>若$f^{**}(x,y)$不是常数，则可以通过牛顿法迭代求解</p>
<script type="math/tex; mode=display">
\sigma_i^{(k+1)} = \sigma_i^{(k)} - \frac
 {g(\sigma_i^{(k)})} {g^{'}(\sigma_i^{(k)})}</script><blockquote>
<ul>
<li>$g(\sigma_i)$：上述方程对应函数</li>
</ul>
</blockquote>
<ul>
<li>上述方程有单根，选择适当初值则牛顿法恒收敛</li>
</ul>
</li>
</ul>
</li>
<li><p>更新$w_i$，$w_i \leftarrow w_i + \sigma_i$，若不是所有
$w_i$均收敛，重复2</p>
</li>
</ol>
<h3 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h3><p>对最大熵模型</p>
<ul>
<li><p>为方便，目标函数改为求极小</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min_{w \in R^M} f(w) = \sum_x \tilde P(x) log \sum_{y}
   exp(\sum_{i=1}^M w_i f_i(x,y)) - \sum_{x,y}
   \tilde P(x,y) \sum_{i=1}^M w_i f_i(x,y)
\end{array}</script></li>
<li><p>梯度为</p>
<script type="math/tex; mode=display">\begin{align*}
g(w) & = (\frac {\partial f(w)} {\partial w_i}, \cdots,
   \frac {\partial f(w)} {\partial w_M})^T \\
\frac {\partial f(w)} {\partial w_M} & = \sum_{x,y}
   \tilde P(x) P_w(y|x) f_i(x,y) - E_{\tilde P}(f_i)
\end{align*}</script></li>
</ul>
<h4 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h4><p>将目标函数带入BFGS算法即可</p>
<blockquote>
<ul>
<li>输入：特征函数$f_1, f_2, \cdots, f_M$、经验分布
  $\tilde P(x)$、最大熵模型$P_w(x)$</li>
<li>输出：最优参数值$w<em>i^{*}$、最优模型$P</em>{w^{*}}$</li>
</ul>
</blockquote>
<ol>
<li><p>取初值$w^{(0)}$、正定对称矩阵$B^{(0)}$，置k=0</p>
</li>
<li><p>计算$g^{(k)} = g(w^{(k)})$，若$|g^{(k)}| &lt; \epsilon$，
停止计算，得到解$w^{*} = w^{(k)}$</p>
</li>
<li><p>由拟牛顿公式$B^{(k)}p^{(k)} = -g^{(k)}$求解$p^{(k)}$</p>
</li>
<li><p>一维搜索，求解</p>
<script type="math/tex; mode=display">
\lambda^{(k)} = \arg\min_{\lambda} f(w^{(k)} +
  \lambda p_k)</script></li>
<li><p>置$w^{(k+1)} = w^{(k)} + \lambda^{(k)} p_k$</p>
</li>
<li><p>计算$g^{(k+1)} = g(w^{(k+1)})$，若
$|g^{(k+1)}| &lt; \epsilon$，停止计算，得到解
$w^{*} = w^{(k+1)}$，否则求</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} - \frac {B^{(k)} s^{(k)}
  (s^{(k)})^T B^{(k)}} {(s^{(k)})^T B^{(k)} s^{(k)}}
  + \frac {y^{(k)} (y^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script><blockquote>
<ul>
<li>$s^{(k)} = w^{(k+1)} - w^{(k)}$</li>
<li>$y^{(k)} = g^{(k+1)} - g^{(k)}$</li>
</ul>
</blockquote>
</li>
<li><p>置k=k+1，转3</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-31T17:48:22.000Z" title="8/1/2019, 1:48:22 AM">2019-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:06:49.000Z" title="7/16/2021, 3:06:49 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Theory/">ML Theory</a><span> / </span><a class="link-muted" href="/categories/ML-Theory/Loss/">Loss</a></span><span class="level-item">9 minutes read (About 1379 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Theory/Loss/func_loss.html">Loss Function</a></h1><div class="content"><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul>
<li>损失函数可以视为<strong>模型与真实的距离</strong>的度量<ul>
<li>因此损失函数设计关键即，寻找可以代表模型与真实的距离的统计量</li>
<li>同时为求解方便，应该损失函数最好应满足导数存在</li>
</ul>
</li>
</ul>
<h3 id="Surrogate-Loss"><a href="#Surrogate-Loss" class="headerlink" title="Surrogate Loss"></a>Surrogate Loss</h3><p>代理损失函数：用优化方便的损失函数代替难以优化的损失函数，间接达到优化原损失函数的目标</p>
<ul>
<li>如 0-1 损失难以优化，考虑使用二次损失、交叉熵损失替代</li>
</ul>
<h3 id="损失函数设计"><a href="#损失函数设计" class="headerlink" title="损失函数设计"></a>损失函数设计</h3><ul>
<li><p>对有监督学习：<strong>真实</strong> 已知，可以直接设计损失函数</p>
</li>
<li><p>对无监督学习：<strong>真实</strong> 未知，需要给定 <strong>真实标准</strong></p>
<ul>
<li><em>NLP</em>：需要给出语言模型</li>
<li><em>EM</em> 算法：熵最大原理</li>
</ul>
</li>
</ul>
<h2 id="常用损失函数"><a href="#常用损失函数" class="headerlink" title="常用损失函数"></a>常用损失函数</h2><p><img src="/imgs/01_se_ce_hinge_loss.png" alt="01_se_ce_hinge_loss"></p>
<h3 id="0-1-Loss"><a href="#0-1-Loss" class="headerlink" title="0-1 Loss"></a>0-1 Loss</h3><script type="math/tex; mode=display">
L(y, f(x)) = \left \{ \begin{array}{l}
    1, & y \neq f(x) \\
    0, & y = f(x)
\end{array} \right.</script><ul>
<li><p>0-1 损失函数梯度要么为 0、要么不存在，无法通过梯度下降方法优化 0-1 损失</p>
</li>
<li><p>适用场合</p>
<ul>
<li>二分类：<em>Adaboost</em></li>
<li>多分类：<em>Adaboost.M1</em></li>
</ul>
</li>
</ul>
<h3 id="Quadratic-Squared-Error-Loss"><a href="#Quadratic-Squared-Error-Loss" class="headerlink" title="Quadratic / Squared Error Loss"></a><em>Quadratic</em> / <em>Squared Error Loss</em></h3><script type="math/tex; mode=display">
L(y, f(x)) = \frac 1 2 (y - f(x))^2</script><ul>
<li><p>平方错误损失函数可导，可以基于梯度下降算法优化损失函数</p>
</li>
<li><p>适用场合</p>
<ul>
<li>回归预测：线性回归</li>
<li>分类预测：0-1 二分类（根据预测得分、阈值划分）</li>
</ul>
</li>
</ul>
<h3 id="Logistic-SE"><a href="#Logistic-SE" class="headerlink" title="Logistic SE"></a><em>Logistic SE</em></h3><ul>
<li><p>平方损失用于二分类时存在如下问题（模型输出无限制）</p>
<ul>
<li>若模型对某样本非常确信为正例，给出大于1预测值</li>
<li>此时模型会进行不必要、开销较大的优化</li>
</ul>
</li>
<li><p>考虑对模型输出进行 <em>sigmoid</em> 变换后作为预测值，再应用平方错误损失函数</p>
<script type="math/tex; mode=display">
L(y, f(x)) = \frac 1 2 (y - \sigma(f(x)))^2</script><ul>
<li><em>Logistic SE</em> 损失函数曲线对 0-1 损失拟合优于平方损失</li>
<li>但负区间存在饱和问题，损失最大只有 0.5</li>
</ul>
</li>
</ul>
<h3 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a><em>Cross Entropy</em></h3><p>交叉熵损失</p>
<script type="math/tex; mode=display">\begin{align*}
L(y, f(x)) & = -ylog(f(x)) \\
& = - \sum_{k=1}^K y_k log f(x)_k
\end{align*}</script><blockquote>
<ul>
<li>$y$：样本实际值</li>
<li>$f(x)$：各类别预测概率</li>
<li>$K$：分类数目</li>
</ul>
</blockquote>
<ul>
<li><p>交叉熵损失综合二次损失、<em>logistic SE</em> 优势，以正样本为例</p>
<ul>
<li>预测值较大时：损失接近 0，避免无效优化</li>
<li>预测值较小时：损失偏导趋近于 -1，不会出现饱和现象</li>
</ul>
</li>
<li><p>$y$ 为 <em>one-hot</em> 编码时实际值时</p>
<ul>
<li>分类问题仅某分量为 1：此时交叉熵损失同对数损失（负对数极大似然函数）</li>
<li>标签问题则可有分量为 1</li>
</ul>
</li>
<li><p>适合场合</p>
<ul>
<li>多分类问题</li>
<li>标签问题</li>
</ul>
</li>
</ul>
<h3 id="Hinge-Loss"><a href="#Hinge-Loss" class="headerlink" title="Hinge Loss"></a><em>Hinge Loss</em></h3><script type="math/tex; mode=display">\begin{align*}
L(y, f(x)) & = [1 - yf(x)]_{+} \\
[z]_{+} & = \left \{ \begin{array}{l}
    z, & z > 0 \\
    0, & z \leq 0
\end{array} \right.
\end{align*}</script><blockquote>
<ul>
<li>$y \in {-1, +1}$</li>
</ul>
</blockquote>
<ul>
<li><p>合页损失函数：0-1 损失函数的上界，效果类似交叉熵损失函数</p>
<ul>
<li>要求分类不仅正确，还要求确信度足够高损失才为 0</li>
<li>即对学习有更高的要求</li>
</ul>
</li>
<li><p>适用场合</p>
<ul>
<li>二分类：线性支持向量机</li>
</ul>
</li>
</ul>
<h3 id="收敛速度对比"><a href="#收敛速度对比" class="headerlink" title="收敛速度对比"></a>收敛速度对比</h3><ul>
<li><p>指数激活函数时：相较于二次损失，收敛速度更快</p>
</li>
<li><p>二次损失对 $w$ 偏导</p>
<script type="math/tex; mode=display">
\frac {\partial L} {\partial w} = (\sigma(z) - y) \sigma^{'}(z) x</script><blockquote>
<ul>
<li>$\sigma$：<em>sigmoid</em>、<em>softmax</em> 激活函数</li>
<li>$z = wx + b$</li>
</ul>
</blockquote>
<ul>
<li>考虑到 <em>sigmoid</em> 函数输入值绝对值较大时，其导数较小</li>
<li>激活函数输入 $z=wx+b$ 较大时，$\sigma^{‘}(z)$ 较小，更新速率较慢</li>
</ul>
</li>
<li><p><em>Softmax</em> 激活函数时，交叉熵对 $w$ 偏导</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial L} {\partial w} & = -y\frac 1 {\sigma(z)}
   \sigma^{'}(z) x \\
& = y(\sigma(z) - 1)x
\end{align*}</script></li>
<li><p>特别的，对 <em>sigmoid</em> 二分类</p>
<script type="math/tex; mode=display">\begin{align*}
\frac {\partial L} {\partial w_j} & = -(\frac y {\sigma(z)}
   - \frac {(1-y)} {1-\sigma(z)}) \sigma^{'}(z) x \\
& = -\frac {\sigma^{'}(z) x} {\sigma(z)(1-\sigma(z))}
   (\sigma(z) - y) \\
& = x(\sigma(z) - y)
\end{align*}</script><ul>
<li>考虑 $y \in {(0,1), (1,0)}$、$w$ 有两组</li>
<li>带入一般形式多分类也可以得到二分类结果</li>
</ul>
</li>
</ul>
<h2 id="不常用损失函数"><a href="#不常用损失函数" class="headerlink" title="不常用损失函数"></a>不常用损失函数</h2><h3 id="Absolute-Loss"><a href="#Absolute-Loss" class="headerlink" title="Absolute Loss"></a><em>Absolute Loss</em></h3><p>绝对损失函数</p>
<script type="math/tex; mode=display">
L(y, f(x)) = |y-f(x)|</script><ul>
<li>适用场合<ul>
<li>回归预测</li>
</ul>
</li>
</ul>
<h3 id="Logarithmic-Loss"><a href="#Logarithmic-Loss" class="headerlink" title="Logarithmic Loss"></a><em>Logarithmic Loss</em></h3><p>对数损失函数（负对数极大似然损失函数）</p>
<script type="math/tex; mode=display">
L(y, P(y|x)) = -logP(y|x)</script><ul>
<li>适用场合<ul>
<li>多分类：贝叶斯生成模型、逻辑回归</li>
</ul>
</li>
</ul>
<h3 id="Exponential-Loss"><a href="#Exponential-Loss" class="headerlink" title="Exponential Loss"></a><em>Exponential Loss</em></h3><p>指数函数函数</p>
<script type="math/tex; mode=display">
L(y, f(x)) = exp\{-yf(x)\}</script><ul>
<li>适用场合<ul>
<li>二分类：前向分步算法</li>
</ul>
</li>
</ul>
<h3 id="Pseudo-Loss"><a href="#Pseudo-Loss" class="headerlink" title="Pseudo Loss"></a><em>Pseudo Loss</em></h3><p>伪损失：考虑个体损失 $(x_i, y_i)$ 如下，据此构造伪损失</p>
<ul>
<li>$h(x_i, y_i)=1, \sum h(x_i, y)=0$：完全正确预测</li>
<li>$h(x_i, y_i)=0, \sum h(x_i, y)=1$：完全错误预测</li>
<li>$h(x_i, y_i)=1/M$：随机预测（M为分类数目）</li>
</ul>
<script type="math/tex; mode=display">
L(y, f(x)) = \frac 1 2 \sum_{y^{(j)} \neq f(x)} w_j (1 - f(x, y) + f(x, y^{(j)}))</script><blockquote>
<ul>
<li>$w_j$：样本个体错误标签权重，对不同个体分布可不同</li>
<li>$f(x, y^{(j)})$：分类器将输入 $x$ 预测为第 $j$ 类 $y^{(j)}$ 的置信度</li>
</ul>
</blockquote>
<ul>
<li><p>伪损失函数考虑了预测 <strong>标签</strong> 的权重分布</p>
<ul>
<li>通过改变此分布，能够更明确的关注难以预测的个体标签，而不仅仅个体</li>
</ul>
</li>
<li><p>伪损失随着分类器预测准确率增加而减小</p>
<ul>
<li>分类器 $f$ 对所有可能类别输出置信度相同时，伪损失最大达到 0.5，此时就是随机预测</li>
<li>伪损失大于 0.5 时，应该将使用 $1-f$</li>
</ul>
</li>
<li><p>适用场景</p>
<ul>
<li>多分类：<em>Adaboost.M2</em></li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">4 minutes read (About 543 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/convolutional.html">Convolutional</a></h1><div class="content"><h2 id="Convolutional"><a href="#Convolutional" class="headerlink" title="Convolutional"></a>Convolutional</h2><p>卷积：卷积区域逐点乘积、求和作为卷积中心取值</p>
<ul>
<li><p>用途：</p>
<ul>
<li>提取<strong>更高层次</strong>的特征，对图像作局部变换、但保留局部特征</li>
<li>选择和其类似信号、过滤掉其他信号、探测局部是否有相应模式，如<ul>
<li><em>sobel</em> 算子获取图像边缘</li>
</ul>
</li>
</ul>
</li>
<li><p>可变卷积核与传统卷积核区别</p>
<ul>
<li>传统卷积核参数人为确定，用于提取确定的信息</li>
<li>可变卷积核通过训练学习参数，以得到效果更好卷积核</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>卷积类似向量内积</li>
</ul>
</blockquote>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>局部感知：卷积核所覆盖的像素只是小部分、局部特征</p>
<ul>
<li>类似于生物视觉中的 <em>receptive field</em></li>
</ul>
</li>
<li><p>多核卷核：卷积核代表、提取某特征，多各卷积核获取不同特征</p>
</li>
<li><p>权值共享：给定通道、卷积核，共用滤波器参数</p>
<ul>
<li>卷积层的参数取决于：卷积核、通道数</li>
<li>参数量远小于全连接神经网络</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>receptive field</em>：感受野，视觉皮层中对视野小区域单独反应的神经元<blockquote>
<ul>
<li>相邻细胞具有相似和重叠的感受野</li>
<li>感受野大小、位置在皮层之间系统地变化，形成完整的视觉空间图</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<h3 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h3><ul>
<li>1980 年 <em>neocognitron</em> 新认知机提出<ul>
<li>第一个初始卷积神经网络，是感受野感念在人工神经网络首次应用</li>
<li>将视觉模式分解成许多子模式（特征），然后进入分层递阶式的特征平面处理</li>
</ul>
</li>
</ul>
<h2 id="卷积应用"><a href="#卷积应用" class="headerlink" title="卷积应用"></a>卷积应用</h2><h3 id="Guassian-Convolutional-Kernel"><a href="#Guassian-Convolutional-Kernel" class="headerlink" title="Guassian Convolutional Kernel"></a><em>Guassian Convolutional Kernel</em></h3><p>高斯卷积核：是实现 <strong>尺度变换</strong> 的唯一线性核</p>
<script type="math/tex; mode=display">\begin{align*}
L(x, y, \sigma) & = G(x, y, \sigma) * I(x, y) \\
G(x, y, \sigma) & = \frac 1 {2\pi\sigma^2}
    exp\{\frac {-((x-x_0)^2 + (y-y_0)^2)} {2\sigma^2} \}
\end{align*}</script><blockquote>
<ul>
<li>$G(x,y,\sigma)$：尺度可变高斯函数</li>
<li>$I(x,y)$：放缩比例，保证卷积核中各点权重和为 1</li>
<li>$(x,y)$：卷积核中各点空间坐标</li>
<li>$\sigma$：尺度变化参数，越大图像的越平滑、尺度越粗糙</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:02:19.000Z" title="7/16/2021, 3:02:19 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">11 minutes read (About 1586 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/attention.html">Attention Machanism</a></h1><div class="content"><h2 id="Attention-Machanism"><a href="#Attention-Machanism" class="headerlink" title="Attention Machanism"></a>Attention Machanism</h2><p>注意力机制：将<em>query</em>、<em>key-value</em>映射至输出的权重生成机制</p>
<script type="math/tex; mode=display">
Attention(Q, K, V) = \phi(f_{Att}(Q, K), V)</script><blockquote>
<ul>
<li>$V_{L <em> d_v}$：value矩阵，<em>*信息序列矩阵</em></em></li>
<li>$K_{L * d_k}$：key矩阵，大部分情况即为$V$</li>
<li>$Q_{L * d_k}$：query矩阵，其他环境信息</li>
<li>$L, d_k, d_v$：输入序列长度、key向量维度、value向量维度</li>
<li>key、value向量为$K, V$中行向量</li>
</ul>
</blockquote>
<ul>
<li><p>合理分配注意力，优化输入信息来源</p>
<ul>
<li>给重要特征分配较大权</li>
<li>不重要、噪声分配较小权</li>
</ul>
</li>
<li><p>在不同模型间学习对齐</p>
<ul>
<li>attention机制常联合Seq2Seq结构使用，通过隐状态对齐</li>
<li>如：图像至行为、翻译</li>
</ul>
</li>
</ul>
<h3 id="Attention-Model"><a href="#Attention-Model" class="headerlink" title="Attention Model"></a>Attention Model</h3><blockquote>
<ul>
<li>Attenion机制一般可以细化如下</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">\begin{align*}
c_t & = \phi(\alpha_t, V) \\

\alpha_{t} & = softmax(e_t) \\
& = \{ \frac {exp(e_{t,j})} {\sum_{k=1}^K exp(e_{t,k})} \} \\

e_t & = f_{Att}(K, Q)
\end{align*}</script><blockquote>
<ul>
<li>$c_t$：<em>context vector</em>，注意力机制输出上下文向量</li>
<li>$e_{t,j}$：$t$时刻$i$标记向量注意力得分</li>
<li>$\alpha_{t,i}$：$t$时刻$i$标记向量注意力权重</li>
<li>softmax归一化注意力得分</li>
</ul>
</blockquote>
<ul>
<li><p>$f_{Att}$：计算各标记向量注意力得分</p>
<ul>
<li><em>additive attention</em></li>
<li><em>multiplicative/dot-product attention</em>：</li>
<li><em>local attention</em></li>
</ul>
<blockquote>
<ul>
<li>其参数需联合整个模型训练、输入取决于具体场景</li>
</ul>
</blockquote>
</li>
<li><p>$\phi_{Att}$：根据标记向量注意力权重计算输出上下文向量</p>
<ul>
<li><em>stochastic hard attention</em></li>
<li><em>deterministic soft attention</em></li>
</ul>
</li>
<li><p>$Q$可能包括很多信息</p>
<ul>
<li>Decoder结构输出、Encoder结构输入</li>
<li>$W$待训练权重矩阵</li>
<li>LSTM、RNN等结构隐状态</li>
</ul>
</li>
</ul>
<h3 id="Additive-Attention"><a href="#Additive-Attention" class="headerlink" title="Additive Attention"></a>Additive Attention</h3><ul>
<li><p>单隐层前馈网络（MLP）</p>
<script type="math/tex; mode=display">
e_{t,j} = v_a^T f_{act}(W_a [h_{t-1}; g_j])</script><blockquote>
<ul>
<li>$h_{t-1}$：输出结构隐状态</li>
<li>$g_j$：输入结构隐状态</li>
<li>$W_a, v_a$：待训练参数</li>
<li>$f_{act}$：激活函数$tanh$、$ReLU$等</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Multiplicative-Dot-product-Attention"><a href="#Multiplicative-Dot-product-Attention" class="headerlink" title="Multiplicative/Dot-product Attention"></a>Multiplicative/Dot-product Attention</h3><script type="math/tex; mode=display">
e_{t,j} = \left \{ \begin{array}{l}
    h_{t-1}^T g_j, & dot \\
    h_{t-1}^T W_a g_j, & general \\
    W_a h_{t-1}, & location
\end{array} \right.</script><ul>
<li>相较于加法attention实际应用中更快、空间效率更高
（可以利用高度优化的矩阵乘法运算）</li>
</ul>
<blockquote>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03044">MLP</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025">內积形式</a></li>
</ul>
</blockquote>
<h4 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h4><ul>
<li><p>将输出作为输入引入，考虑上一次输出影响</p>
<p><img src="/imgs/attention_with_output_as_input_feeding.png" alt="attention_with_output_as_input_feeding"></p>
</li>
<li><p><em>Scaled Dot-Product Attention</em></p>
<script type="math/tex; mode=display">
f_{Att} = \frac {Q K^T} {\sqrt{d_k}}</script><ul>
<li>避免內积随着key向量维度$d_k$增大而增大，导致softmax
中梯度过小</li>
</ul>
</li>
</ul>
<h3 id="Stochastic-Hard-Attention"><a href="#Stochastic-Hard-Attention" class="headerlink" title="Stochastic Hard Attention"></a>Stochastic Hard Attention</h3><p><em>hard attention</em>：随机抽取标记向量作为注意力位置</p>
<ul>
<li><p>注意力位置视为中间one-hot隐向量，每次只关注某个标记向量</p>
</li>
<li><p>模型说明</p>
<ul>
<li>$f_{Att}$为随机从标记向量$a$中抽取一个</li>
<li>$\alpha$视为多元伯努利分布参数，各分量取值表示对应
标记向量被抽中概率，此时上下文向量也为随机变量</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{align*}
p(s_{t,i}=1) & = \alpha_{t,i} \\
c_t & = V s
\end{align*}</script><blockquote>
<ul>
<li>$s$：注意力位置，中间隐one-hot向量，服从$\alpha$指定的
  多元伯努利分布</li>
<li>$h_i$：第$i$上下文向量</li>
</ul>
</blockquote>
<h4 id="参数训练"><a href="#参数训练" class="headerlink" title="参数训练"></a>参数训练</h4><ul>
<li><p>参数$\alpha$不可导、含有中间隐变量$s$，考虑使用EM算法
思想求解</p>
<script type="math/tex; mode=display">\begin{align*}
log p(y) & = log \sum_s p(s) p(y|s) \\
& \geq \sum_s p(s) log p(y|s) := L_s \\

\frac {\partial L_s} {\partial W} & = \sum_s [
   \frac {\partial p(s)} {\partial W} + \frac 1 {p(y|s)}
   \frac {\partial p(y|s)} {\partial W}] \\
& = \sum_s p(s) [\frac {\partial log p(y|s)} {\partial W} +
   log p(y|s) \frac {\partial log p(s|a)} {\partial W}]
\end{align*}</script><blockquote>
<ul>
<li>$L_s$：原对数似然的函数的下界，以其作为新优化目标</li>
<li>$W$：参数</li>
</ul>
</blockquote>
</li>
<li><p>用蒙特卡罗采样方法近似求以上偏导</p>
<ul>
<li><p>$s$按多元伯努利分布抽样$N$次，求$N$次偏导均值</p>
<script type="math/tex; mode=display">
\frac {\partial L_s} {W} \approx \frac 1 N \sum_{n=1}^N
  [\frac {\partial log p(y|\tilde s_n)} {\partial W} +
  log p(y|\tilde s_n) \frac {\partial log
  p(\tilde s_n|a)} {\partial W}]</script><blockquote>
<ul>
<li>$\tilde s_n$：第$n$次抽样结果</li>
</ul>
</blockquote>
</li>
<li><p>可对$p(y|\tilde s_n)$进行指数平滑减小估计方差</p>
</li>
</ul>
</li>
</ul>
<h3 id="Deterministic-Soft-Attention"><a href="#Deterministic-Soft-Attention" class="headerlink" title="Deterministic Soft Attention"></a>Deterministic Soft Attention</h3><p><em>soft attention</em>：从标记向量估计上下文向量期望</p>
<ul>
<li>考虑到所有上下文向量，所有标记向量加权求和上下文向量</li>
<li>模型说明<ul>
<li>$f_{Att}$计算所有标记向量注意力得分</li>
<li>$\alpha$可视为个标记向量权重</li>
</ul>
</li>
</ul>
<p><img src="/imgs/attention_global.png" alt="attention_global"></p>
<script type="math/tex; mode=display">
E_{p(s_t)} [c_t] = \sum_{i=1}^L \alpha_{t,i} a_i</script><ul>
<li>模型光滑可微：可直接用反向传播算法训练</li>
</ul>
<h3 id="Local-Attention"><a href="#Local-Attention" class="headerlink" title="Local Attention"></a>Local Attention</h3><p><em>local attention</em>：从所有标记向量中选取部分计算soft attention</p>
<ul>
<li>可以视为hard、soft attention结合<ul>
<li>hard attention选取标记向量子区间，避免噪声干扰</li>
<li>soft attention加权求和，方便训练</li>
</ul>
</li>
</ul>
<p><img src="/imgs/attention_local.png" alt="attention_local"></p>
<h4 id="子区间选取"><a href="#子区间选取" class="headerlink" title="子区间选取"></a>子区间选取</h4><blockquote>
<ul>
<li>为目标$t$选取对齐位置$p_t$，得到子区间$[p_t-D, p_t+D]$
  （$D$为经验选取）</li>
</ul>
</blockquote>
<ul>
<li><p><em>monotonic alignment</em>：直接设置$p_t=t$</p>
</li>
<li><p><em>predictive alignment</em>：</p>
<script type="math/tex; mode=display">
p_t = S sigmoid(v_p^T tanh(W_p h_t))</script><blockquote>
<ul>
<li>$W_p, v_p$：待学习参数</li>
</ul>
</blockquote>
</li>
</ul>
<blockquote>
<ul>
<li>可以使用高斯分布给注意力权重加权，强化$p_t$附近标记向量
  （根据经验可以设置$\sigma = \frac D 2$）<script type="math/tex; mode=display">
  \alpha_{t,j} = softmax(e_{t,j}) exp(-\frac {(j - p_t)^2}
      {2\sigma^2})</script></li>
</ul>
</blockquote>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h3><p><em>Self Attention</em>/<em>Intra-Attention</em>：关联同一序列内不同位置、
以学习序列表示的attenion机制</p>
<ul>
<li><p>类似卷积、循环结构</p>
<ul>
<li>将不定长的序列映射为等长的另一序列</li>
<li>从序列中提取高层特征</li>
</ul>
</li>
<li><p>特点</p>
<ul>
<li>类似卷积核，多个self attention可以完全并行</li>
<li>无需循环网络多期传递信息，输入序列同期被处理</li>
<li>可使用local attention机制限制计算复杂度</li>
</ul>
</li>
</ul>
<p><img src="/imgs/multi_head_self_attention.png" alt="multi_head_self_attention"></p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p><em>Multi-Head Attention</em>：从相同输入、输出序列学习多个
attention机制</p>
<p><img src="/imgs/multi_head_attention.png" alt="multi_head_attention"></p>
<script type="math/tex; mode=display">\begin{align*}
MultiHead(X) & = Concat(head1, ..., head_h) W^O \\
head_i & = Attention(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}</script><blockquote>
<ul>
<li>$Q, K, V$：元信息矩阵，据此训练多组query、key-value，
  一般就是原始输入序列矩阵</li>
</ul>
</blockquote>
<ul>
<li>可以并行训练，同时从序列中提取多组特征</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:07:19.000Z" title="7/16/2021, 3:07:19 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">2 minutes read (About 341 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/interaction.html">Interaction Layers</a></h1><div class="content"><h2 id="人工交互作用层"><a href="#人工交互作用层" class="headerlink" title="人工交互作用层"></a>人工交互作用层</h2><p>交互作用层：人工设置特征之间交互方式</p>
<h3 id="Flatten-Layer"><a href="#Flatten-Layer" class="headerlink" title="Flatten Layer"></a><em>Flatten Layer</em></h3><p>展平层：直接拼接特征，交互作用交由之后网络训练</p>
<script type="math/tex; mode=display">
f_{Flat}(V_x) = \begin{bmatrix} x_1 v_1 \\ \vdots\\ x_M v_M
    \end{bmatrix}</script><blockquote>
<ul>
<li>$V_x$：特征向量集合</li>
</ul>
</blockquote>
<ul>
<li>对同特征域特征处理方式<ul>
<li>平均</li>
<li>最大</li>
</ul>
</li>
</ul>
<h3 id="二阶交互作用"><a href="#二阶交互作用" class="headerlink" title="二阶交互作用"></a>二阶交互作用</h3><p>二阶交互作用层：特征向量之间两两逐元素交互</p>
<ul>
<li>交互方式<ul>
<li>逐元素<ul>
<li>乘积</li>
<li>求最大值：无</li>
</ul>
</li>
<li>按向量</li>
</ul>
</li>
<li>聚合方式<ul>
<li>求和<ul>
<li>平权</li>
<li>Attention加权</li>
</ul>
</li>
<li>求最大值：无</li>
</ul>
</li>
</ul>
<h4 id="Bi-Interaction-Layer"><a href="#Bi-Interaction-Layer" class="headerlink" title="Bi-Interaction Layer"></a><em>Bi-Interaction Layer</em></h4><p><em>Bi-Interaction Layer</em>：特征向量两两之间逐元素乘积、求和</p>
<script type="math/tex; mode=display">\begin{align*}
f_{BI}(V) & = \sum_{i=1}^M \sum_{j=i+1}^M v_i \odot v_j \\
& = \frac 1 2 (\|\sum_{i=1}^M v_i\|_2^2 -
    \sum_{i=1}^M \|v_i\|_2^2)
\end{align*}</script><blockquote>
<ul>
<li>$\odot$：逐元素乘积</li>
</ul>
</blockquote>
<ul>
<li>没有引入额外参数，可在线性时间$\in O(kM_x)$内计算</li>
<li>可在低层次捕获二阶交互影响，较拼接操作更informative<ul>
<li>方便学习更高阶特征交互</li>
<li>模型实际中更容易训练</li>
</ul>
</li>
</ul>
<h4 id="Attention-based-Pooling"><a href="#Attention-based-Pooling" class="headerlink" title="Attention-based Pooling"></a><em>Attention-based Pooling</em></h4><p><em>Attention-based Pooling</em>：特征向量两两之间逐元素乘积、加权
求和</p>
<script type="math/tex; mode=display">
f_{AP}(V) & = \sum_{i=1}^M \sum_{j=i+1}^M \alpha_{i,j}
    (v_i \odot v_j)</script><blockquote>
<ul>
<li>$\alpha_{i,j}$：交互作用注意力权重，通过注意力网络训练</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-19T10:06:05.000Z" title="7/19/2021, 6:06:05 PM">2021-07-19</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">2 minutes read (About 357 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/embedding.html">Embedding</a></h1><div class="content"><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>嵌入层：将高维空间中离散变量映射为低维稠密 <em>embedding</em> 向量表示</p>
<ul>
<li><p><em>embedding</em> 向量更能体现样本之间关联</p>
<ul>
<li>內积（內积）体现样本之间接近程度</li>
<li>可通过可视化方法体现样本差异</li>
</ul>
</li>
<li><p><em>embedding</em> 向量更适合某些模型训练</p>
<ul>
<li>模型不适合高维稀疏向量</li>
<li><em>embedding</em> 向量矩阵可以联合模型整体训练，相当于提取特征</li>
<li><em>embedding</em> 向量也可能类似迁移学习独立训练之后直接融入模型中</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>Embedding</em>：将度量空间中对象映射到另个（低维）度量空间，并尽可能<strong>保持不同对象之间拓扑关系</strong>，如 <em>Word-Embedding</em></li>
</ul>
</blockquote>
<h3 id="Embedding表示"><a href="#Embedding表示" class="headerlink" title="Embedding表示"></a>Embedding表示</h3><ul>
<li><p>特征不分组表示</p>
<script type="math/tex; mode=display">\begin{align*}
\varepsilon_x & =  E x \\
& = [x_1v_1, x_2v_2, \cdots, x_Mv_M] \\
& = [x_{M_1} v_{M_1}, \cdots, x_{M_m} v_{M_m}]
\end{align*}</script><blockquote>
<ul>
<li>$E$：embedding向量矩阵</li>
<li>$M$：特征数量</li>
<li>$v_i$：$k$维embedding向量</li>
<li>$x_i$：特征取值，对0/1特征仍等价于查表，只需考虑非0特征<blockquote>
<ul>
<li>$x_{M_i}$：第$j$个非0特征，编号为$M_i$</li>
<li>$m$：非零特征数量</li>
</ul>
</blockquote>
</li>
<li>$\varepsilon_x$：特征向量集合</li>
</ul>
</blockquote>
</li>
<li><p>特征分组表示</p>
<script type="math/tex; mode=display">\begin{align*}
\varepsilon_x & = [V_1 g_1, V_2 g_2, \cdots, V_G g_G]
\end{align*}</script><blockquote>
<ul>
<li>$G$：特征组数量</li>
<li>$V_i$：第$i$特征组特征向量矩阵</li>
<li>$g_i$：第$i$特征组特征取值向量</li>
</ul>
</blockquote>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">a minute read (About 140 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/pooling.html">Pooling Layers</a></h1><div class="content"><h2 id="池化-下采样"><a href="#池化-下采样" class="headerlink" title="池化/下采样"></a>池化/下采样</h2><p>池化：在每个区域中选择只保留一个值</p>
<ul>
<li><p>用于减小数据处理量同时保留有用的信息</p>
<ul>
<li>相邻区域特征类似，单个值能表征特征、同时减少数据量</li>
</ul>
</li>
<li><p>保留值得选择有多种</p>
<ul>
<li>极值</li>
<li>平均值</li>
<li>全局最大</li>
</ul>
</li>
<li><p>直观上</p>
<ul>
<li>模糊图像，丢掉一些不重要的细节</li>
</ul>
</li>
</ul>
<h3 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h3><p>最大值采样：使用区域中最大值作为代表</p>
<h3 id="Average-Pooling"><a href="#Average-Pooling" class="headerlink" title="Average Pooling"></a>Average Pooling</h3><p>平均值采样：使用池中平均值作为代表</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-29T13:16:01.000Z" title="7/29/2019, 9:16:01 PM">2019-07-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-16T07:09:47.000Z" title="7/16/2021, 3:09:47 PM">2021-07-16</time></span><span class="level-item"><a class="link-muted" href="/categories/ML-Model/">ML Model</a><span> / </span><a class="link-muted" href="/categories/ML-Model/Model-Component/">Model Component</a></span><span class="level-item">9 minutes read (About 1278 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/ML-Model/Model-Component/recurrent.html">Recurrent Neural Network</a></h1><div class="content"><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><p>RNN：处理前后数据有关联的序列数据</p>
<p><img src="/imgs/rnn_unfolding.png" alt="rnn_unfolding"></p>
<blockquote>
<ul>
<li>左侧：为折叠的神经网络，右侧：按时序展开后的网络</li>
<li>$h$：循环隐层，其中神经元之间有权连接，随序列输入上一期
  隐层会影响下一期</li>
<li>$o$、$y$：输出预测值、实际值</li>
<li>$L$：损失函数，随着时间累加</li>
</ul>
</blockquote>
<ul>
<li>序列往往长短不一，难以拆分为独立样本通过普通DNN训练</li>
</ul>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p><img src="/imgs/rnn_structures.png" alt="rnn_structures"></p>
<blockquote>
<ul>
<li>普通的DNN：固定大小输入得到固定输出</li>
<li>单个输入、序列输出：输入图片，得到描述文字序列</li>
<li>序列输入、单个输出：情感分析</li>
<li>异步序列输入、输出：机器翻译</li>
<li>同步序列输入、输出：视频帧分类</li>
</ul>
</blockquote>
<h4 id="权值连接"><a href="#权值连接" class="headerlink" title="权值连接"></a>权值连接</h4><ul>
<li><p>循环隐层内神经元之间也建立权连接，即<strong>循环</strong></p>
<ul>
<li>基础神经网络只在层与层之间建立权值连接是RNN同普通DNN
最大不同之处</li>
</ul>
</li>
<li><p>循环隐层中神经元只会和其<strong>当前层中神经元</strong>建立权值连接</p>
<ul>
<li>即不受上期非同层神经元影响</li>
<li>循环隐层中神经元$t$期状态$h^{(t)}$由当期输入、
$h^{(t-1)}$共同决定</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>Gated Feedback RNN</em>：循环隐层会对下期其他隐层产生影响
  <img src="/imgs/rnn_gated_feedback.png" alt="rnn_gated_feedback"></li>
</ul>
</blockquote>
<h4 id="逻辑结构"><a href="#逻辑结构" class="headerlink" title="逻辑结构"></a>逻辑结构</h4><blockquote>
<ul>
<li>RNN网络实际结构是线性、折叠的，逻辑结构则是展开的结构，
  考虑RNN性质应该在展开的逻辑结构中考虑</li>
</ul>
</blockquote>
<ul>
<li><p>序列输入</p>
<ul>
<li>实际结构：依次输入</li>
<li>逻辑结构：里是整体作为一次输入、才是一个样本，损失、
反向传播都应该以完整序列为间隔</li>
</ul>
</li>
<li><p>权值共享</p>
<ul>
<li>实际结构：不同期的权值实际是同一组</li>
<li>逻辑结构：称为<strong>权值共享</strong></li>
</ul>
</li>
<li><p>重复模块链</p>
<ul>
<li>实际结构：同一个模块</li>
<li>逻辑结构：不同期模块之间信息流动形成链式形式</li>
</ul>
</li>
</ul>
<h4 id="信息传递"><a href="#信息传递" class="headerlink" title="信息传递"></a>信息传递</h4><blockquote>
<ul>
<li>RNN循环层中信息只能由上一期直接传递给下一期</li>
</ul>
</blockquote>
<ul>
<li><p>输入、输出相关信息间隔较近时，普通RNN可以胜任</p>
<p><img src="/imgs/rnn_short_dependencies.png" alt="rnn_short_dependencies"></p>
</li>
<li><p>当间隔很长，RNN理论上虽然能够处理，但由于梯度消失问题，
实际上长期依赖会消失，需要LSTM网络</p>
<p><img src="/imgs/rnn_long_dependencies.png" alt="rnn_long_dependencies"></p>
</li>
</ul>
<h3 id="Forward-Propogation"><a href="#Forward-Propogation" class="headerlink" title="Forward Propogation"></a><em>Forward Propogation</em></h3><ul>
<li><p>$h^{(t)} = \sigma(z^{(t)}) = \sigma(Ux^{(t)} + Wh^{(t-1)} +b )$</p>
<blockquote>
<ul>
<li>$\sigma$：RNN激活函数，一般为$tanh$</li>
<li>$b$：循环隐层偏置</li>
</ul>
</blockquote>
</li>
<li><p>$o^{(t)} = Vh^{(t)} + c$</p>
<blockquote>
<ul>
<li>$c$：输出层偏置</li>
</ul>
</blockquote>
</li>
<li><p>$\hat{y}^{(t)} = \sigma(o^{(t)})$</p>
<blockquote>
<ul>
<li>$\sigma$：RNN激活函数，分类时一般时$softmax$</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="Back-Propogation-Through-Time"><a href="#Back-Propogation-Through-Time" class="headerlink" title="Back-Propogation Through Time"></a><em>Back-Propogation Through Time</em></h3><p><em>BPTT</em>：训练RNN的常用方法</p>
<blockquote>
<ul>
<li><p>本质仍然是BP算法，但是RNN处理序列数据，损失随期数累加，
  即计算梯度时使用最终损失$L = \sum_{t=1}^\tau L^{(t)}$</p>
</li>
<li><p>对循环层中参数，梯度沿着期数反向传播，第t期反向传播时，
  需要逐级求导</p>
</li>
</ul>
</blockquote>
<ul>
<li>序列整体作为一次输入，进行一次反向传播</li>
<li>理论上可以漂亮的解决序列数据的训练，但是和DNN一样有梯度
消失的问题，尤其是序列很长时，所以一般不能直接应用</li>
</ul>
<h4 id="非循环层"><a href="#非循环层" class="headerlink" title="非循环层"></a>非循环层</h4><ul>
<li><p>$\frac{\partial L}{\partial c}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial c} & = \sum_{t=1}^{\tau}
   \frac{\partial L^{(t)}}{\partial c}
& = \sum_{t=1}^{\tau}\frac{\partial L^{(t)}}
   {\partial o^{(t)}} \frac{\partial o^{(t)}}{\partial c}
& = \sum_{t=1}^{\tau}\hat{y}^{(t)} - y^{(t)}
\end{align*}</script><blockquote>
<ul>
<li>$L^{(t)} = \frac 1 2 (\hat{y}^{(t)} - y^{(t)})^2$：
 使用平方损失</li>
</ul>
</blockquote>
</li>
<li><p>$\frac{\partial L}{\partial V}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial V} & = \sum_{t=1}^{\tau}
   \frac{\partial L^{(t)}}{\partial V}
& = \sum_{t=1}^{\tau} \frac{\partial L^{(t)}}
   {\partial o^{(t)}} \frac{\partial o^{(t)}}{\partial V}
& = \sum_{t=1}^{\tau}(\hat{y}^{(t)} - y^{(t)})
   (h^{(t)})^T
\end{align*}</script></li>
</ul>
<h4 id="循环层"><a href="#循环层" class="headerlink" title="循环层"></a>循环层</h4><blockquote>
<ul>
<li>为方便定义：
  $\delta^{(t)} = \frac {\partial L} {\partial h^{(t)}}$</li>
</ul>
</blockquote>
<ul>
<li><p>$\delta^{(t)}$</p>
<script type="math/tex; mode=display">\begin{align*}
\delta^{(t)} & = \frac {\partial L} {\partial h^{(t)}} \\
   & = \frac{\partial L}{\partial o^{(t)}}
       \frac{\partial o^{(t)}}{\partial h^{(t)}} +
       \frac{\partial L}{\partial h^{(t+1)}}
       \frac{\partial h^{(t+1)}}{\partial h^{(t)}}
   & = V^T(\hat{y}^{(t)} - y^{(t)}) +
       W^T\delta^{(t+1)}diag(1-h^{(t+1)})^2)
\end{align*}</script><blockquote>
<ul>
<li>$\frac{\partial h^{(t+1)}}{\partial h^{(t)}} = diag(1-h^{(t+1)})^2)$
 ：$tanh(x)$梯度性质</li>
<li>$h^{(t)}(t&lt;\tau)$梯度：被后一期影响（反向传播），需递推</li>
</ul>
</blockquote>
</li>
<li><p>$\delta^{(\tau)}$</p>
<script type="math/tex; mode=display">\begin{align*}
\delta^{(\tau)} & = \frac{\partial L}{\partial o^{(\tau)}}
   \frac{\partial o^{(\tau)}}{\partial h^{(\tau)}}
& = V^T(\hat{y}^{(\tau)} - y^{(\tau)})
\end{align*}</script><blockquote>
<ul>
<li>$\tau$期后没有其他序列，可以直接求出</li>
</ul>
</blockquote>
</li>
<li><p>$\frac{\partial L}{\partial W}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial W} & = \sum_{t=1}^{\tau}
   \frac{\partial L}{\partial h^{(t)}}
   \frac{\partial h^{(t)}}{\partial W}
& = \sum_{t=1}^{\tau}diag(1-(h^{(t)})^2)
   \delta^{(t)}(h^{(t-1)})^T
\end{align*}</script><blockquote>
<ul>
<li>需要由$\sigma^{(t)}$累加得到</li>
</ul>
</blockquote>
</li>
<li><p>$\frac{\partial L}{\partial b}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial b} & = \sum_{t=1}^{\tau}
   \frac{\partial L}{\partial h^{(t)}}
   \frac{\partial h^{(t)}}{\partial b}
& = \sum_{t=1}^{\tau} diag(1-(h^{(t)})^2)\delta^{(t)}
\end{align*}</script></li>
<li><p>$\frac{\partial L}{\partial U}$</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial L}{\partial U} & = \sum_{t=1}^{\tau}
   \frac{\partial L}{\partial h^{(t)}}
   \frac{\partial h^{(t)}}{\partial U}
& = \sum_{t=1}^{\tau}diag(1-(h^{(t)})^2)
   \delta^{(t)}(x^{(t)})^T
\end{align*}</script></li>
</ul>
<p>}$$</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/tags/ML-Model/page/0/">Previous</a></div><div class="pagination-next"><a href="/tags/ML-Model/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/tags/ML-Model/">1</a></li><li><a class="pagination-link" href="/tags/ML-Model/page/2/">2</a></li><li><a class="pagination-link" href="/tags/ML-Model/page/3/">3</a></li><li><a class="pagination-link" href="/tags/ML-Model/page/4/">4</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="https://api.follow.it/subscription-form/WWxwMVBsOUtoNTdMSlJ4Z1lWVnRISERsd2t6ek9MeVpEUWs0YldlZGxUdXlKdDNmMEZVV1hWaFZFYWFSNmFKL25penZodWx3UzRiaVkxcnREWCtOYUJhZWhNbWpzaUdyc1hPangycUh5RTVjRXFnZnFGdVdSTzZvVzJBcTJHKzl8aXpDK1ROWWl4N080YkFEK3QvbEVWNEJuQjFqdWdxODZQcGNoM1NqbERXST0=/8" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>