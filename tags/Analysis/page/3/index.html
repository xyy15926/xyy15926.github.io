<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: Analysis - UBeaRLy</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="UBeaRLy&#039;s Proxy"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="UBeaRLy&#039;s Proxy"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="UBeaRLy"><meta property="og:url" content="https://xyy15926.github.io/"><meta property="og:site_name" content="UBeaRLy"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://xyy15926.github.io/img/og_image.png"><meta property="article:author" content="UBeaRLy"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://xyy15926.github.io"},"headline":"UBeaRLy","image":["https://xyy15926.github.io/img/og_image.png"],"author":{"@type":"Person","name":"UBeaRLy"},"publisher":{"@type":"Organization","name":"UBeaRLy","logo":{"@type":"ImageObject","url":"https://xyy15926.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="/atom.xml" title="UBeaRLy" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/darcula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-5385776267343559" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Visit on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Analysis</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:31:21.000Z" title="8/4/2021, 11:31:21 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Real-Analysis/">Real Analysis</a></span><span class="level-item">7 minutes read (About 1092 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Real-Analysis/subgredient.html">次梯度</a></h1><div class="content"><h2 id="次梯度"><a href="#次梯度" class="headerlink" title="次梯度"></a>次梯度</h2><ul>
<li><p>次梯度：实变量凸函数 $f$ 在点 $x_0$ 的次梯度 $c$ 满足</p>
<script type="math/tex; mode=display">
\forall x, f(x) - f(x_0) \geq c(x - x_0)</script></li>
<li><p>可证明凸函数 $f$ 在 $x_0$ 处所有次梯度的集合 $\partial f(x)$ 是非空凸紧集</p>
<ul>
<li><p>$\partial f(x) = [a, b]$，其中$a, b$为单侧极限</p>
<script type="math/tex; mode=display">\begin{align*}
a & = lim_{x \rightarrow x_0^{-}} \frac {f(x) - f_0(x)} {x - x_0} \\
b & = lim_{x \rightarrow x_0^{+}} \frac {f(x) - f_0(x)} {x - x_0}
\end{align*}</script></li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>凸函数均指下凸函数，梯度不减</li>
</ul>
</blockquote>
<h3 id="次梯度性质"><a href="#次梯度性质" class="headerlink" title="次梯度性质"></a>次梯度性质</h3><h4 id="运算性质"><a href="#运算性质" class="headerlink" title="运算性质"></a>运算性质</h4><ul>
<li><p>数乘性质</p>
<script type="math/tex; mode=display">
\partial(\alpha f)(x) = \alpha \partial f(x), \alpha > 0</script></li>
<li><p>加法性质</p>
<script type="math/tex; mode=display">\begin{align*}
f &= f_1 + f_2 + \cdots + f_m, \\
\partial f &= \partial f_1 + \cdots + \partial f_m
\end{align*}</script></li>
<li><p>仿射性质：$f$为凸函数</p>
<script type="math/tex; mode=display">\begin{align*}
h(x) &=  f(Ax + b) \\
\partial h(x) &= A^T \partial f(Ax + b)
\end{align*}</script></li>
</ul>
<h4 id="最优化性质"><a href="#最优化性质" class="headerlink" title="最优化性质"></a>最优化性质</h4><ul>
<li><p>凸函数 $f$ 在点 $x_0$ 可导，当且仅当次梯度仅包含一个点，即该点导数</p>
</li>
<li><p>点 $x_0$ 是凸函数 $f$ 最小值，当且仅当次微分中包含 0
（此性质为“可导函数极小点导数为 0 ”推广）</p>
</li>
<li><p>负次梯度方向不一定是下降方向</p>
</li>
</ul>
<h2 id="次梯度求解"><a href="#次梯度求解" class="headerlink" title="次梯度求解"></a>次梯度求解</h2><ul>
<li>逐点（分段）极值的函数求次梯度</li>
<li>求出该点相应极值函数</li>
<li>求出对应梯度即为次梯度</li>
</ul>
<h3 id="Pointwise-Maximum"><a href="#Pointwise-Maximum" class="headerlink" title="Pointwise Maximum"></a><em>Pointwise Maximum</em></h3><p>逐点最大函数：目标函数为</p>
<script type="math/tex; mode=display">\begin{align*}
f(x) & = max \{f_1(x), f_2(x), \cdots, f_m(x)\} \\
I(x) & = \{i | f_i(x) = f(x)\}
\end{align*}</script><blockquote>
<ul>
<li>$I(x)$：保存 $x$ 处取值最大的函数下标</li>
</ul>
</blockquote>
<ul>
<li><p>弱结果：$I(x)$ 中随机抽取，以 $f_i(x)$ 在该处梯度作为次梯度</p>
</li>
<li><p>强结果</p>
<script type="math/tex; mode=display">
\partial f(x) = conv \cup_{i \in I(x)} \partial f_i(x)</script><ul>
<li>先求支撑平面，再求所有支撑平面的凸包</li>
<li>可导情况实际上是不可导的特殊情况</li>
</ul>
</li>
</ul>
<h4 id="分段函数"><a href="#分段函数" class="headerlink" title="分段函数"></a>分段函数</h4><p><img src="/imgs/subgredient_piecewise_function.png" alt="subgredient_piecewise_function"></p>
<ul>
<li><p>折点处</p>
<script type="math/tex; mode=display">
\partial f(x) = conv\{a_i, a_{i+1}\} = [a_i, a_{i+1}]</script></li>
<li><p>非折点处</p>
<script type="math/tex; mode=display">\partial f(x) = {a_i}</script></li>
</ul>
<h4 id="L-1-范数"><a href="#L-1-范数" class="headerlink" title="$L_1$范数"></a>$L_1$范数</h4><p><img src="/imgs/subgredient_l1norm.png" alt="subgredient_l1norm"></p>
<h3 id="PointWise-Supremum"><a href="#PointWise-Supremum" class="headerlink" title="PointWise Supremum"></a><em>PointWise Supremum</em></h3><p>逐点上确界：目标函数为</p>
<script type="math/tex; mode=display">\begin{align*}
f(x) &= \sup_{\alpha \in A} f_{\alpha}(x) \\
I(x) &= \{\alpha \in A | f_{\alpha}(x) = f(x)\}
\end{align*}</script><ul>
<li><p>弱结果：可行梯度为</p>
<script type="math/tex; mode=display">
\partial (\max_{\alpha} f_{\alpha}(x)) \in partial f(x)</script></li>
<li><p>强结果</p>
<script type="math/tex; mode=display">
\partial f(x) = conv \cup_{\alpha \in I(x)} \partial f_{alpha}(x) \subseteq \partial f(x)</script></li>
</ul>
<h4 id="最大特征值"><a href="#最大特征值" class="headerlink" title="最大特征值"></a>最大特征值</h4><script type="math/tex; mode=display">\begin{align*}
f(x) & = \lambda_{max}(A(x)) = \sup_{\|y\|_2 = 1} \\
A(x) & = A_0 + x_1 A_1 + \cdots + x_n A_n
\end{align*}</script><blockquote>
<ul>
<li>$A_n$：对称矩阵</li>
</ul>
</blockquote>
<ul>
<li><p>对确定 $\hat {x}$，$A(x)$ 最大特征值 $\lambda_{max}$、对应特征向量 $y$，则该点此梯度为</p>
<script type="math/tex; mode=display">(y^T A_0 y, \cdots, y^T A_n y)</script></li>
</ul>
<h3 id="Pointwise-Inferior"><a href="#Pointwise-Inferior" class="headerlink" title="Pointwise Inferior"></a><em>Pointwise Inferior</em></h3><p>逐点下确界：目标函数为</p>
<script type="math/tex; mode=display">
f(x) = \inf_y h(x, y)</script><blockquote>
<ul>
<li>$h$：凸函数</li>
</ul>
</blockquote>
<ul>
<li><p>弱结果：给定$x = \hat x$，可行次梯度为</p>
<script type="math/tex; mode=display">
(\partial h(x, \hat y)|_{x=\hat x}, 0) \in \partial f(x)</script></li>
</ul>
<h3 id="复合函数"><a href="#复合函数" class="headerlink" title="复合函数"></a>复合函数</h3><script type="math/tex; mode=display">
f(x) = h(f_1(x), \cdots, f_n(x))</script><blockquote>
<ul>
<li>$h$：凸、不降函数</li>
<li>$f_i$：凸函数</li>
</ul>
</blockquote>
<ul>
<li><p>弱结果：给定$x = \hat x$，可行次梯度为</p>
<script type="math/tex; mode=display">
g = z_1 g_1 + \cdots + z_k g_k \in \partial f(\hat x)</script><blockquote>
<ul>
<li>$z \in \partial h(f_1(\hat x), \cdots, f_k(\hat x))$</li>
<li>$g_i \in \partial f_i(\hat x)$</li>
</ul>
</blockquote>
<ul>
<li><p>证明</p>
<script type="math/tex; mode=display">\begin{align*}
f(x) & \geq h(f_1(\hat x) + g_1^T(x - \hat x), \cdots,
  f_k(\hat x) + g_k^T(x - \hat x) \\
& \geq h(f_1(\hat x), \cdots, f_k(\hat x)) +
  z^T(g_1^T(x - \hat x), \cdots, g_k^T(x - \hat x)) \\
& = f(\hat x) + g^T(x - \hat x)
\end{align*}</script></li>
</ul>
</li>
</ul>
<h2 id="次梯度法"><a href="#次梯度法" class="headerlink" title="次梯度法"></a>次梯度法</h2><script type="math/tex; mode=display">\begin{align*}
x^{(k+1)} & = x^{(k)} + \alpha_k g^{(k)} \\
f_{best}^{(k+1)} & = min{f_{best}^{(k)}, f(x^{(k+1)})}
\end{align*}</script><blockquote>
<ul>
<li>$g^{(k)}$：函数$f$在$x^{(k)}$处次梯度</li>
</ul>
</blockquote>
<ul>
<li><p>求解凸函数最优化的问题的一种迭代方法</p>
</li>
<li><p>相较于较内点法、牛顿法慢</p>
<ul>
<li>应用范围更广泛：能够用于不可微目标函数，目标函数可微时，无约束问题次梯度与梯度下降法有同样搜索方向</li>
<li>空间复杂度更小</li>
<li>可以和分解技术结合，得到简单分配算法</li>
<li>通常不会产生稀疏解</li>
</ul>
</li>
</ul>
<h3 id="步长选择"><a href="#步长选择" class="headerlink" title="步长选择"></a>步长选择</h3><blockquote>
<ul>
<li>次梯度法选取步长方法很多，以下为保证收敛步长规则</li>
</ul>
</blockquote>
<ul>
<li>恒定步长：$\alpha_k = \alpha$</li>
<li>恒定间隔：$\alpha_k = \gamma / |g^{(k)}|_2$</li>
<li>步长平方可加、步长不可加：$\sum<em>{k=1}^{\infty} \alpha_k^2 &lt; \infty, \sum</em>{k=1}^{\infty} \alpha_k = \infty$</li>
<li>步长不可加、但递减：$lim_{k \rightarrow \infty} \alpha_k = 0$</li>
<li>间隔不可加、但递减：$lim_{k \rightarrow \gamma_k} \gamma_k = 0$</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-20T16:46:35.000Z" title="7/21/2019, 12:46:35 AM">2019-07-21</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a></span><span class="level-item">27 minutes read (About 4010 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/func_hash.html">Hashing</a></h1><div class="content"><h2 id="Hash-Function"><a href="#Hash-Function" class="headerlink" title="Hash Function"></a><em>Hash Function</em></h2><blockquote>
<ul>
<li><em>hash</em>：散列/哈希，将任意类型值转换为关键码值</li>
<li><em>hash function</em>：哈希/散列函数，从任何数据中创建小的数字“指纹”的方法</li>
<li><em>hash value</em>：哈希值，哈希函数产生关键码值</li>
<li><em>collision</em>：冲突，不同两个数据得到相同哈希值</li>
</ul>
</blockquote>
<ul>
<li>哈希函数应该尽可能使得哈希值均匀分布在目标空间中<ul>
<li>降维：将高维数据映射到低维空间</li>
<li>数据应该低维空间中尽量均匀分布</li>
</ul>
</li>
</ul>
<h3 id="数据相关性"><a href="#数据相关性" class="headerlink" title="数据相关性"></a>数据相关性</h3><ul>
<li><p><em>Data Independent Hashing</em>：数据无关哈希，无监督，哈希函数基于某种概率理论</p>
<ul>
<li>对原始的特征空间作均匀划分</li>
<li>对分布不均、有趋向性的数据集时，可能会导致高密度区域哈希桶臃肿，降低索引效率</li>
</ul>
</li>
<li><p><em>Data Dependent Hashing</em>：数据依赖哈希，有监督，通过学习数据集的分布从而给出较好划分的哈希函数</p>
<ul>
<li>得到针对数据密度动态划分的哈希索引</li>
<li>破坏了传统哈希函数的数据无关性，索引不具备普适性</li>
</ul>
</li>
</ul>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><ul>
<li>查找数据结构：<em>cs_algorithm/data_structure/hash_table</em><ul>
<li>哈希表</li>
</ul>
</li>
<li>信息安全方向：<em>cs_algorithm/specification/info_security</em><ul>
<li>文件检验</li>
<li>数字签名</li>
<li>鉴权协议</li>
</ul>
</li>
</ul>
<h2 id="哈希函数"><a href="#哈希函数" class="headerlink" title="哈希函数"></a>哈希函数</h2><ul>
<li><p>简单哈希函数主要用于提升查找效率（构建哈希表）</p>
<ul>
<li>要求哈希函数的降维、缩小查找空间性质</li>
<li>计算简单、效率高</li>
</ul>
</li>
<li><p>复杂哈希函数主要用于信息提取</p>
<ul>
<li>要求哈希函数的信息提取不可逆、非单调映射</li>
<li>查表哈希<ul>
<li><em>CRC</em> 系列算法：本身不是查表，但查表是其最快实现</li>
<li><em>Zobrist Hashing</em></li>
</ul>
</li>
<li>混合哈希：利用以上各种方式<ul>
<li><em>MD5</em></li>
<li><em>Tiger</em></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="单值输入"><a href="#单值输入" class="headerlink" title="单值输入"></a>单值输入</h3><ul>
<li><p>直接寻址法：取关键字、或其某个线性函数值 $hash(key) = (a * key + b) \% prime$</p>
<ul>
<li>$prime$：一般为质数，以使哈希值尽量均匀分布，常用的如：$2^{32}-5$</li>
</ul>
</li>
<li><p>数字分析法：寻找、利用数据规律构造冲突几率较小者</p>
<ul>
<li>如：生日信息前 2、3 位大体相同，冲突概率较大，优先舍去</li>
</ul>
</li>
<li><p>平方取中法：取关键字平方后中间几位</p>
</li>
<li><p>折叠法：将关键字分割为位数相同部分，取其叠加和</p>
</li>
<li><p>随机数法：以关键字作为随机数种子生成随机值</p>
<ul>
<li>适合关键字长度不同场合</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>常用于之前哈希结果再次映射为更小范围的最终哈希值</li>
</ul>
</blockquote>
<h3 id="序列输入"><a href="#序列输入" class="headerlink" title="序列输入"></a>序列输入</h3><h4 id="加法哈希"><a href="#加法哈希" class="headerlink" title="加法哈希"></a>加法哈希</h4><p>加法哈希：将输入元素相加得到哈希值</p>
<ul>
<li><p>标准加法哈希</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">AddingHash(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> += ele</span><br><span class="line">	<span class="comment"># prime 为任意质数，常用 2^32 - 5</span></span><br><span class="line">	<span class="built_in">hash</span> = <span class="built_in">hash</span>  % prime</span><br></pre></td></tr></table></figure>
<ul>
<li>最终哈希结果 $\in [0, prime-1]$</li>
</ul>
</li>
</ul>
<h4 id="位运算哈希"><a href="#位运算哈希" class="headerlink" title="位运算哈希"></a>位运算哈希</h4><p>位运算哈希：利用位运算（移位、异或等）充分混合输入元素</p>
<ul>
<li><p>标准旋转哈希</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">RotationHash(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = (<span class="built_in">hash</span> &lt;&lt; <span class="number">4</span>) ^ (<span class="built_in">hash</span> &gt;&gt; <span class="number">28</span>) ^ ele</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">hash</span> % prime</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形 1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> = (<span class="built_in">hash</span>&lt;&lt; <span class="number">5</span>) ^ (<span class="built_in">hash</span> &gt;&gt; <span class="number">27</span>) ^ ele</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> += ele</span><br><span class="line"><span class="built_in">hash</span> ^= (<span class="built_in">hash</span> &lt;&lt; <span class="number">10</span>)</span><br><span class="line"><span class="built_in">hash</span> ^= (<span class="built_in">hash</span> &gt;&gt; <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形3</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (ele &amp; <span class="number">1</span>) == <span class="number">0</span>:</span><br><span class="line">	<span class="built_in">hash</span> ^= (<span class="built_in">hash</span> &lt;&lt; <span class="number">7</span>) ^ ele ^ (<span class="built_in">hash</span> &gt;&gt; <span class="number">3</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	<span class="built_in">hash</span> ^= ~((<span class="built_in">hash</span> &lt;&lt; <span class="number">11</span>) ^ ele ^ (<span class="built_in">hash</span> &gt;&gt; <span class="number">5</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形4</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> += (<span class="built_in">hash</span> &lt;&lt; <span class="number">5</span>) + ele</span><br></pre></td></tr></table></figure>
</li>
<li><p>变形5</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> = ele + (<span class="built_in">hash</span> &lt;&lt; <span class="number">6</span>) + (<span class="built_in">hash</span> &gt;&gt; <span class="number">16</span>) - <span class="built_in">hash</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>变形6</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">hash</span> ^= (<span class="built_in">hash</span> &lt;&lt; <span class="number">5</span>) + ele + (<span class="built_in">hash</span> &gt;&gt; <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="乘法哈希"><a href="#乘法哈希" class="headerlink" title="乘法哈希"></a>乘法哈希</h4><p>乘法哈希：利用乘法的不相关性</p>
<ul>
<li><p>平方取头尾随机数生成法：效果不好</p>
</li>
<li><p><em>Bernstein</em> 算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Bernstein(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = <span class="number">33</span> * <span class="built_in">hash</span> + ele</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">hash</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>其他常用乘数：31、131、1313、13131、131313</li>
</ul>
</blockquote>
</li>
<li><p>32位 <em>FNV</em> 算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">M_SHIFT =</span><br><span class="line">M_MASK =</span><br><span class="line">FNVHash(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">2166136261</span>;</span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = (<span class="built_in">hash</span> * <span class="number">16777619</span>) ^ ele</span><br><span class="line">	<span class="keyword">return</span> (<span class="built_in">hash</span> ^ (<span class="built_in">hash</span> &gt;&gt; M_SHIFT)) &amp; M_MASK</span><br></pre></td></tr></table></figure>
</li>
<li><p>改进的 <em>FNV</em> 算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FNVHash_2(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">2166136261</span>;</span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = (<span class="built_in">hash</span> ^ ele) * <span class="number">16777619</span></span><br><span class="line">	<span class="built_in">hash</span> += <span class="built_in">hash</span> &lt;&lt; <span class="number">13</span></span><br><span class="line">	<span class="built_in">hash</span> ^= <span class="built_in">hash</span> &gt;&gt; <span class="number">7</span></span><br><span class="line">	<span class="built_in">hash</span> += <span class="built_in">hash</span> &lt;&lt; <span class="number">3</span></span><br><span class="line">	<span class="built_in">hash</span> ^= <span class="built_in">hash</span> &gt;&gt; <span class="number">17</span></span><br><span class="line">	<span class="built_in">hash</span> += <span class="built_in">hash</span> &lt;&lt; <span class="number">5</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">hash</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>乘数不固定</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RSHash(<span class="built_in">input</span>):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	a, b = <span class="number">378551</span>, <span class="number">63689</span></span><br><span class="line">	<span class="keyword">for</span> ele <span class="keyword">in</span> <span class="built_in">input</span>:</span><br><span class="line">		<span class="built_in">hash</span> = <span class="built_in">hash</span> * a + ele</span><br><span class="line">		a *= b</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">hash</span> &amp; <span class="number">0x7FFFFFFF</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<ul>
<li>除法也类似乘法具有不相关性，但太慢</li>
</ul>
</blockquote>
<h3 id="定长序列"><a href="#定长序列" class="headerlink" title="定长序列"></a>定长序列</h3><ul>
<li><p>两步随机数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">main_rand_seq = randint(k)</span><br><span class="line">TwoHashing(<span class="built_in">input</span>[<span class="number">0</span>,...,k]):</span><br><span class="line">	<span class="built_in">hash</span> = <span class="number">0</span></span><br><span class="line">	<span class="keyword">from</span> i=<span class="number">0</span> to k:</span><br><span class="line">		<span class="built_in">hash</span> += <span class="built_in">input</span>[i] * main_rand_seq[i]</span><br><span class="line">	<span class="built_in">hash</span> = <span class="built_in">hash</span> mod prime</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Universal-Hashing"><a href="#Universal-Hashing" class="headerlink" title="Universal Hashing"></a><em>Universal Hashing</em></h2><blockquote>
<ul>
<li>全域哈希：键集合 $U$ 包含 $n$ 个键、哈希函数族 $H$ 中哈希函数 $h_i: U \rightarrow 0..m$，若 $H$ 满足以下则为全域哈希 $$<pre><code>  \forall x \neq y \in U, | \&#123;h|h \in H, h(x) = h(y) \&#125; | = \frac &#123;|H|&#125; m
</code></pre>  $$<blockquote>
<ul>
<li>$|H|$：哈希函数集合 $H$ 中函数数量</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li>独立与键值随机从中选择哈希函数，避免发生最差情况</li>
<li>可利用全域哈希构建完美哈希</li>
</ul>
<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><ul>
<li><p>全域哈希 $H$ 中任选哈希函数 $h_i$，对任意键 $x \neq y \in U$ 冲突概率小于 $\frac 1 m$</p>
<ul>
<li>由全域哈希函数定义，显然</li>
</ul>
</li>
<li><p>全域哈希 $H$ 中任选哈希函数 $h<em>i$，对任意键 $x \in U$，与其冲突键数目期望为 $\frac n m$，即 $E</em>{[collision_x]}=\frac n m$</p>
<script type="math/tex; mode=display">\begin{align*}
E(C_x) &= E[\sum_{y \in U - \{x\}} C_{xy}] \\
   &= \sum_{y \in U - \{x\}} E[C_{xy}] \\
   &= \sum_{y \in U - \{x\}} \frac 1 m \\
   &= \frac {n-1} m
\end{align*}</script><blockquote>
<ul>
<li>$C_x$：任选哈希函数，与 $x$ 冲突的键数量</li>
<li>$C_{xy} = \left { \begin{matrix} 1, &amp; h_i(x) = h_i(y) \ 0, &amp; otherwise \end{matrix} \right.$：指示 $x,y$ 是否冲突的指示变量</li>
</ul>
</blockquote>
<ul>
<li>$m = n^2$ 时，冲突期望小于 0.5<ul>
<li>$n$ 个键两两组合数目为 $C_n^2$</li>
<li>则 $E_{total} &lt; C_n^2 \frac 1 n &lt; 0.5$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="例"><a href="#例" class="headerlink" title="例"></a>例</h3><blockquote>
<ul>
<li>以下构造 $[0,p-1] \rightarrow [0,m-1]$ 全域哈希</li>
</ul>
</blockquote>
<ul>
<li><p>$p$ 为足够大素数使得所有键值 $\in [0,p-1]$</p>
<ul>
<li>记 $Z_p = { 0,1,\cdots,p-1 }$</li>
<li>记 $Z_p^{*}={ 1,2,\cdots,p-1 }$</li>
<li>且哈希函数映射上限（哈希表长度） $m &lt; max(U) &lt; p$</li>
</ul>
</li>
<li><p>记哈希函数</p>
<script type="math/tex; mode=display">
\forall a \in Z_p^{*}, b \in Z_p, h_{a, b}(k) = ((a k + b) \% p) \% m</script></li>
<li><p>则以下哈希函数族即为全域哈希</p>
<script type="math/tex; mode=display">
H_{p,m} = {h_{a,b}|a \in Z_p^{*}, b \in Z_p}</script></li>
</ul>
<h2 id="Locality-Sensitive-Hashing"><a href="#Locality-Sensitive-Hashing" class="headerlink" title="Locality Sensitive Hashing"></a><em>Locality Sensitive Hashing</em></h2><p><em>LSH</em>：局部敏感哈希</p>
<blockquote>
<ul>
<li>$(r_1,r_2,P_1,P_2)-sensitive$ 哈希函数族 $H$ 需满足如下条件 $$
  \begin{align*}<pre><code>  Pr_&#123;H&#125;[h(v) = h(q)] \geq P_1, &amp; \forall q \in B(v, r_1) \\
  Pr_&#123;H&#125;[h(v) = h(q)] \geq P_2, &amp; \forall q \notin B(v, r_2) \\
</code></pre>  \end{align*}$$<blockquote>
<ul>
<li>$h \in H$</li>
<li>$r_1 &lt; r_2, P_1 &gt; P_2$：函数族有效的条件</li>
<li>$B(v, r)$：点 $v$ 的 $r$ 邻域</li>
<li>$r_1, r_2$：距离，强调比例时会表示为 $r_1 = R, r_2 = cR$</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li>此时 <strong>相似目标（距离小）有更大概率发生冲突</strong></li>
</ul>
<h3 id="LSH查找"><a href="#LSH查找" class="headerlink" title="LSH查找"></a>LSH查找</h3><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p><img src="/imgs/general_lsh_comparsion.png" alt="general_lsh_comparsion"></p>
<ul>
<li><p>相似目标更有可能映射到相同哈希桶中</p>
<ul>
<li>则只需要在目标所属的哈希桶中进行比较、查找即可</li>
<li>无需和全集数据比较，大大缩小查找空间</li>
</ul>
</li>
<li><p>可视为降维查找方法</p>
<ul>
<li>将高维空间数据映射到 1 维空间，寻找可能近邻的数据点</li>
<li>缩小范围后再进行精确比较</li>
</ul>
</li>
</ul>
<h4 id="概率放大"><a href="#概率放大" class="headerlink" title="概率放大"></a>概率放大</h4><blockquote>
<ul>
<li>期望放大局部敏感哈希函数族 $Pr_1, Pr_2$ 之间差距</li>
</ul>
</blockquote>
<ul>
<li><p>增加哈希值长度（级联哈希函数中基本哈希函数数量） $k$</p>
<ul>
<li>每个哈希函数独立选择，则对每个级联哈希函数 $g_i$ 有 $Pr[g_i(v) = g_i(q)] \geq P_1^k$</li>
<li>虽然增加哈希键位长会减小目标和近邻碰撞的概率，但同时也更大程度上减少了和非近邻碰撞的概率、减少搜索空间</li>
</ul>
<blockquote>
<ul>
<li>级联哈希函数返回向量，需要对其再做哈希映射为标量，方便查找</li>
</ul>
</blockquote>
</li>
<li><p>增加级联哈希函数数量（哈希表数量） $L$</p>
<ul>
<li>$L$个哈希表中候选项包含真实近邻概率 <strong>至少</strong> 为 $1 - (1 - P_1^k)^L$</li>
<li>增加哈希表数量能有效增加候选集包含近邻可能性</li>
<li>但同时也会增大搜索空间</li>
</ul>
</li>
</ul>
<h4 id="搜索近似最近邻"><a href="#搜索近似最近邻" class="headerlink" title="搜索近似最近邻"></a>搜索近似最近邻</h4><ul>
<li>使用 $L$ 个级联哈希函数分别处理待搜索目标</li>
<li>在 $L$ 个哈希表分别寻找落入相同哈希桶个体作为候选项</li>
<li>在所有候选项中线性搜索近邻</li>
</ul>
<h3 id="基于汉明距离的-LSH"><a href="#基于汉明距离的-LSH" class="headerlink" title="基于汉明距离的 LSH"></a>基于汉明距离的 <em>LSH</em></h3><ul>
<li>在汉明距离空间中搜索近邻<ul>
<li>要求数据为二进制表示</li>
<li>其他距离需要嵌入汉明距离空间才能使用<ul>
<li>欧几里得距离没有直接嵌入汉明空间的方法<ul>
<li>一般假设欧几里得距离和曼哈顿距离差别不大</li>
<li>直接使用对曼哈顿距离保距嵌入方式</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="设计哈希函数族"><a href="#设计哈希函数族" class="headerlink" title="设计哈希函数族"></a>设计哈希函数族</h4><ul>
<li><p>考虑哈希函数族 $H = { h_1, h_2, \cdots, h_m }$</p>
<ul>
<li>其中函数 $h_i$ 为 ${0, 1}^d$ 到 ${0, 1}$ 的映射：随机返回特定比特位上的值</li>
</ul>
</li>
<li><p>从 $H$ 中随机的选择哈希函数 $h_i$</p>
<ul>
<li>则 $Pr[h_i(v) = h_i(q)]$ 等于 $v, q$ 相同比特数比例，则<ul>
<li>$Pr_1 = 1 - \frac R d$</li>
<li>$Pr_2 = 1 - \frac {cR} d$</li>
</ul>
</li>
<li>考虑到 $Pr_1 &gt; Pr_2$，即此哈希函数族是局部敏感的</li>
</ul>
</li>
</ul>
<h3 id="基于-Jaccard-系数的-LSH"><a href="#基于-Jaccard-系数的-LSH" class="headerlink" title="基于 Jaccard 系数的 LSH"></a>基于 <em>Jaccard</em> 系数的 <em>LSH</em></h3><ul>
<li><p>考虑 $M * N$ 矩阵 $A$，元素为 0、1</p>
<ul>
<li>其中<ul>
<li>$M$：集合元素数量</li>
<li>$N$：需要比较的集合数量</li>
</ul>
</li>
<li>目标：寻找相似集合，即矩阵中相似列</li>
</ul>
</li>
<li><p>用 <em>Jaccard</em> 系数代表集合间相似距离，用于搜索近邻</p>
<ul>
<li>要求各数据向量元素仅包含 0、1：表示集合是否包含该元素</li>
</ul>
</li>
</ul>
<h4 id="定义-Min-hashing-函数族"><a href="#定义-Min-hashing-函数族" class="headerlink" title="定义 Min-hashing 函数族"></a>定义 <em>Min-hashing</em> 函数族</h4><ul>
<li><p>对矩阵 $A$ 进行 <strong>行随机重排</strong> $\pi$，定义 <em>Min-hashing</em> 如下</p>
<script type="math/tex; mode=display">h_{\pi}(C) = \min \pi(C)</script><blockquote>
<ul>
<li>$C$：列，表示带比较集合</li>
<li>$\min \pi(C)$：$\pi$ 重排矩阵中 $C$ 列中首个 1 所在行数</li>
</ul>
</blockquote>
</li>
<li><p>则不同列（集合） <em>Min-hashing</em> 相等概率等于二者 <em>Jaccard</em> 系数</p>
<script type="math/tex; mode=display">\begin{align*}
Pr(h_{\pi}(C_1)  = h_{\pi}(C_2)) & = \frac a {a + b} \\
& = Jaccard_d(C_1, C_2)
\end{align*}</script><blockquote>
<ul>
<li>$a$：列 $C_1, C_2$ 取值均为 1 的行数</li>
<li>$b$：列 $C_1, C_2$ 中仅有一者取值为 1 的行数</li>
<li>根据 <em>Min-hashing</em> 定义，不同列均取 0 行被忽略</li>
</ul>
</blockquote>
</li>
</ul>
<h4 id="Min-hashing-实现"><a href="#Min-hashing-实现" class="headerlink" title="Min-hashing 实现"></a><em>Min-hashing</em> 实现</h4><ul>
<li><p>数据量过大时，对行随机重排仍然非常耗时，考虑使用哈希函数模拟行随机重排</p>
<ul>
<li>每个哈希函数对应一次随机重排<ul>
<li>哈希函数视为线性变换</li>
<li>然后用哈希函数结果对总行数取模</li>
</ul>
</li>
<li>原行号经过哈希函数映射即为新行号</li>
</ul>
</li>
<li><p>为减少遍历数据次数，考虑使用迭代方法求解</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i from <span class="number">0</span> to N<span class="number">-1</span>:</span><br><span class="line">	<span class="keyword">for</span> j from <span class="number">0</span> to M<span class="number">-1</span>:</span><br><span class="line">		<span class="keyword">if</span> D[i][j] == <span class="number">1</span>:</span><br><span class="line">			<span class="keyword">for</span> k from <span class="number">1</span> to K:</span><br><span class="line">				# 更新随机重拍后，第 `j` 列首个 <span class="number">1</span> 位置</span><br><span class="line">				DD[k][j] = min(h_k(i), DD[k][j])</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>$D$：原始数据特征矩阵</li>
<li>$DD$：$Min-hashing* 签名矩阵</li>
<li>$N$：特征数量，原始特征矩阵行数</li>
<li>$M$：集合数量，原始特征矩阵列数</li>
<li>$K$：模拟的随机重排次数，<em>Min-hashing</em> 签名矩阵行数</li>
<li>$h_k,k=1,…,K$：$K$ 个模拟随机重排的哈希函数，如 $h(x) = (2x + 7) mod N$</li>
</ul>
</blockquote>
<ul>
<li>初始化 <em>Min-hashing</em> 签名矩阵所有值为 $\infty$</li>
<li>遍历 $N$ 个特征、$M$ 个集合<ul>
<li>查看每个对应元素是否为 1</li>
<li>若元素为 1，则分别使用 $K$ 个哈希函数计算模拟重排后对应的行数</li>
<li>若计算出行数小于当前 *Min-hash$ 签名矩阵相应哈希函数、集合对应行数，更新</li>
</ul>
</li>
<li>遍历一遍原始数据之后即得到所有模拟重排的签名矩阵</li>
</ul>
</li>
</ul>
<h3 id="Exact-Euclidean-LSH"><a href="#Exact-Euclidean-LSH" class="headerlink" title="Exact Euclidean LSH"></a><em>Exact Euclidean LSH</em></h3><ul>
<li><p>$E^2LSH$：欧式局部LSH，<em>LSH Based-on P-stable Distribution</em></p>
<ul>
<li>使用内积将向量随机映射到哈希值</li>
<li><em>p-stable</em> 分布性质将欧式距离同哈希值相联系，实现局部敏感</li>
</ul>
</li>
<li><p>$E^2LSH$ 特点</p>
<ul>
<li>基于概率模型生成索引编码结果不稳定</li>
<li>随编码位数 $k$ 增加的，准确率提升缓慢</li>
<li>级联哈希函数数量 $L$ 较多时，需要大量存储空间，不适合大规模数据索引</li>
</ul>
</li>
</ul>
<h4 id="p-stable-哈希函数族"><a href="#p-stable-哈希函数族" class="headerlink" title="p-stable 哈希函数族"></a><em>p-stable</em> 哈希函数族</h4><script type="math/tex; mode=display">
h_{a, b}(v) = \lfloor \frac {av + b} r \rfloor</script><blockquote>
<ul>
<li>$v$：$n$ 维特征向量</li>
<li>$a = (X_1,X_2,\cdots,X_n)$：其中分量为独立同 <em>p-stable</em> 分布的随机变量</li>
<li>$b \in [0, r]$：均匀分布随机变量</li>
</ul>
</blockquote>
<h4 id="p-stable-哈希函数碰撞概率"><a href="#p-stable-哈希函数碰撞概率" class="headerlink" title="p-stable 哈希函数碰撞概率"></a><em>p-stable</em> 哈希函数碰撞概率</h4><blockquote>
<ul>
<li>考虑$|v_1 - v_2|_p = c$的两个样本碰撞概率</li>
</ul>
</blockquote>
<ul>
<li><p>显然，仅在 $|av<em>1 - av_2| \leq r$ 时，才存在合适的 $b$ 使得 $h</em>{a,b}(v<em>1) = h</em>{a,b}(v_2)$</p>
<ul>
<li>即两个样本碰撞，不失一般性可设 $av_1 \leq av_2$</li>
<li>此 $r$ 即代表局部敏感的 <strong>局部范围</strong></li>
</ul>
</li>
<li><p>若 $(k-1)r \leq av_1 \leq av_2 &lt; kr$，即两个样本与 $a$ 内积在同一分段内</p>
<ul>
<li>易得满足条件的 $b \in [0,kr-av_2) \cup [kr-av_1, r]$</li>
<li>即随机变量 $b$ 取值合适的概率为 $1 - \frac {av_2 - av_1} r$</li>
</ul>
</li>
<li><p>若 $(k-1)r \leq av_1 \leq kr \leq av_2$，即两个样本 $a$ 在相邻分段内</p>
<ul>
<li>易得满足条件的 $b \in [kr-av_1, (k+1)r-av_2)$</li>
<li>即随机变量 $b$ 取值合适的概率同样为 $1 - \frac {av_2 - av_1} r$</li>
</ul>
</li>
<li><p>考虑 $av_2 - av_1$ 分布为 $cX$，则两样本碰撞概率为</p>
<script type="math/tex; mode=display">\begin{align*}
p(c)  & = Pr_{a,b}(h_{a,b}(v_1) = h_{a,b}(v_2)) \\
& = \int_0^r \frac 1 c f_p(\frac t c)(1 - \frac t r)dt
\end{align*}</script><blockquote>
<ul>
<li>$c = |v_1 - v_2|_p$：特征向量之间$L_p$范数距离</li>
<li>$t = a(v_1 - v_2)$</li>
<li>$f$：p稳定分布的概率密度函数</li>
</ul>
</blockquote>
<ul>
<li><p>$p=1$ 柯西分布</p>
<script type="math/tex; mode=display">
p(c) = 2 \frac {tan^{-1}(r/c)} \pi - \frac 1 {\pi(r/c)} ln(1 + (r/c)^2)</script></li>
<li><p>$p=2$ 正态分布</p>
<script type="math/tex; mode=display">
p(c) = 1 - 2norm(-r/c) - \frac 2 {\sqrt{2\pi} r/c} (1 - e^{-(r^2/2c^2)})</script></li>
</ul>
</li>
</ul>
<h4 id="性质、实现"><a href="#性质、实现" class="headerlink" title="性质、实现"></a>性质、实现</h4><h5 id="限制近邻碰撞概率"><a href="#限制近邻碰撞概率" class="headerlink" title="限制近邻碰撞概率"></a>限制近邻碰撞概率</h5><ul>
<li><p>$r$ 最优值取决于数据集、查询点</p>
<ul>
<li>根据文献，建议$r = 4$</li>
</ul>
</li>
<li><p>若要求近邻 $v \in B(q,R)$以不小于$1-\sigma$ 概率碰撞，则有</p>
<script type="math/tex; mode=display">\begin{align*}
1 - (1 - p(R)^k)^L & \geq 1 - \sigma \\
\Rightarrow L & \geq \frac {log \sigma} {log(1 - p(R)^k)}
\end{align*}</script><p>则可取</p>
<script type="math/tex; mode=display">
L = \lceil \frac {log \sigma} {log(1-p(R)^k)} \rceil</script></li>
<li><p>$k$ 最优值是使得 $T_g + T_c$ 最小者</p>
<ul>
<li>$T_g = O(dkL)$：建表时间复杂度</li>
<li>$T_c = O(d |collisions|)$：精确搜索时间复杂度</li>
<li>$T_g$、$T_c$ 随着 $k$ 增大而增大、减小</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>具体实现参考<a target="_blank" rel="noopener" href="https://www.mit.edu/~andoni/LSH/manual.pdf">https://www.mit.edu/~andoni/LSH/manual.pdf</a></li>
</ul>
</blockquote>
<h5 id="限制搜索空间"><a href="#限制搜索空间" class="headerlink" title="限制搜索空间"></a>限制搜索空间</h5><ul>
<li><p>哈希表数量 $L$ 较多时，所有碰撞样本数量可能非常大，考虑只选择 $3L$ 个样本点</p>
</li>
<li><p>此时每个哈希键位长 $k$、哈希表数量 $L$ 保证以下条件，则算法正确</p>
<ul>
<li>若存在 $v^{ <em> }$ 距离待检索点 $q$ 距离小于 $r_1$，则存在 $g_j(v^{ </em> }) = g_j(q)$</li>
<li><p>与 $q$ 距离大于 $r_2$、可能和 $q$ 碰撞的点的数量小于 $3L$</p>
<script type="math/tex; mode=display">
\sum_{j=1}^L |(P-B(q,r_2)) \cap g_j^{-1}(g_j(q))|
  < 3L</script></li>
</ul>
</li>
<li><p>可以证明，$k, L$ 取以下值时，以上两个条件以常数概率成立
（此性质是局部敏感函数性质，不要求是 $E^2LSH$）</p>
<script type="math/tex; mode=display">\begin{align*}
k & = log_{1/p_2} n\\
L & = n^{\rho} \\
\rho & = \frac {ln 1/p_1} {ln 1/p_2}
\end{align*}</script></li>
<li><p>$\rho$ 对算法效率起决定性作用，且有以下定理</p>
<blockquote>
<ul>
<li>距离尺度 $D$ 下，若 $H$ 为 $(R,cR,p<em>1,p_2)$-敏感哈希函数族，则存在适合 <em>(R,c)-NN</em> 的算法，其空间复杂度为 $O(dn + n^{1+\rho})$、查询时间为 $O(n^{\rho})$ 倍距离计算、哈希函数计算为 $O(n^{\rho} log</em>{1/p_2}n)$， 其中 $\rho = \frac {ln 1/p_1} {ln 1/p_2}$</li>
</ul>
</blockquote>
<ul>
<li>$r$ 足够大、充分远离 0 时，$\rho$ 对其不是很敏感</li>
<li>$p<em>1, p_2$ 随 $r$ 增大而增大，而 $k = log</em>{1/p_2} n$ 也随之增大，所以 $r$ 不能取过大值</li>
</ul>
</li>
</ul>
<h4 id="Scalable-LSH"><a href="#Scalable-LSH" class="headerlink" title="Scalable LSH"></a><em>Scalable LSH</em></h4><p><em>Scalable LSH</em>：可扩展的 <em>LSH</em></p>
<ul>
<li><p>对动态变化的数据集，固定哈希编码的局部敏感哈希方法对数据 <strong>动态支持性有限</strong>，无法很好的适应数据集动态变化</p>
<ul>
<li>受限于初始数据集分布特性，无法持续保证有效性</li>
<li>虽然在原理上支持数据集动态变化，但若数据集大小发生较大变化，则其相应哈希参数（如哈希编码长度）等需要随之调整，需要从新索引整个数据库</li>
</ul>
</li>
<li><p>在 $E^2LSH$ 基础上通过 <strong>动态增强哈希键长</strong>，增强哈希函数区分能力，实现可扩展 <em>LSH</em></p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-07-13T15:45:15.000Z" title="7/13/2019, 11:45:15 PM">2019-07-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-07-13T04:03:11.000Z" title="7/13/2019, 12:03:11 PM">2019-07-13</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Mixin/">Math Mixin</a></span><span class="level-item">16 minutes read (About 2380 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Mixin/func_kernels.html">Kernel Function</a></h1><div class="content"><h2 id="Kernel-Function"><a href="#Kernel-Function" class="headerlink" title="Kernel Function"></a>Kernel Function</h2><blockquote>
<ul>
<li>对输入空间 $X$ （欧式空间 $R^n$ 的子集或离散集合）、特征空间 $H$ ，若存在从映射 $$<pre><code>  \phi(x): X \rightarrow H
</code></pre><script type="math/tex; mode=display">使得对所有 $x, z \in X$ ，函数 $K(x,z)$ 满足</script><pre><code>  K(x,z) = \phi(x) \phi(z)
</code></pre>  $$ 则称 $K(x,z)$ 为核函数、 $\phi(x)$ 为映射函数，其中 $\phi(x) \phi(z)$ 表示内积</li>
</ul>
</blockquote>
<ul>
<li>特征空间 $H$ 一般为无穷维<ul>
<li>特征空间必须为希尔伯特空间（内积完备空间）</li>
</ul>
</li>
</ul>
<h3 id="映射函数-phi"><a href="#映射函数-phi" class="headerlink" title="映射函数 $\phi$"></a>映射函数 $\phi$</h3><ul>
<li><p>映射函数 $\phi$：输入空间 $R^n$ 到特征空间的映射 $H$ 的映射</p>
</li>
<li><p>对于给定的核 $K(x,z)$ ，映射函数取法不唯一，映射目标的特征空间可以不同，同一特征空间也可以取不同映射，如：</p>
<ul>
<li><p>对核函数 $K(x, y) = (x y)^2$ ，输入空间为 $R^2$ ，有</p>
<script type="math/tex; mode=display">\begin{align*}
(xy)^2 & = (x_1y_1 + x_2y_2)^2 \\
& = (x_1y_1)^2 + 2x_1y_1x_2y_2 + (x_2y_2)^2
\end{align*}</script></li>
<li><p>若特征空间为$R^3$，取映射</p>
<script type="math/tex; mode=display">\phi(x) = (x_1^2, \sqrt 2 x_1x_2, x_2^2)^T</script><p>或取映射</p>
<script type="math/tex; mode=display">
\phi(x) = \frac 1 {\sqrt 2} (x_1^2 - x_2^2, 2x_1x_2, x_1^2 + x_2^2)^T</script></li>
<li><p>若特征空间为$R^4$，取映射</p>
<script type="math/tex; mode=display">
\phi(x) = (x_1^2, x_1x_2, x_1x_2, x_2^2)^T</script></li>
</ul>
</li>
</ul>
<h3 id="核函数-K-x-z"><a href="#核函数-K-x-z" class="headerlink" title="核函数 $K(x,z)$"></a>核函数 $K(x,z)$</h3><ul>
<li><p><em>Kernel Trick</em> 核技巧：利用核函数简化映射函数 $\phi(x)$ 映射、内积的计算技巧</p>
<ul>
<li>避免实际计算映射函数</li>
<li>避免高维向量空间向量的存储</li>
</ul>
</li>
<li><p>核函数即在核技巧中应用的函数</p>
<ul>
<li>实务中往往寻找到的合适的核函数即可，不关心对应的映射函数</li>
<li>单个核函数可以对应多个映射、特征空间</li>
</ul>
</li>
<li><p>核技巧常被用于分类器中</p>
<ul>
<li>根据 <em>Cover’s</em> 定理，核技巧可用于非线性分类问题，如在 <em>SVM</em> 中常用</li>
<li>核函数的作用范围：梯度变化较大的区域<ul>
<li>梯度变化小的区域，核函数值变化不大，所以没有区分能力</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><em>Cover’s</em> 定理可以简单表述为：非线性分类问题映射到高维空间后更有可能线性可分</li>
</ul>
</blockquote>
<h4 id="正定核函数"><a href="#正定核函数" class="headerlink" title="正定核函数"></a>正定核函数</h4><blockquote>
<ul>
<li>设 $X \subset R^n$，$K(x,z)$ 是定义在 $X <em> X$的对称函数，若 $\forall x_i \in \mathcal{X}, i=1,2,…,m$，$K(x,z)$ 对应的 </em>Gram* 矩阵 $$<pre><code>  G = [K(x_i, x_j)]_&#123;m*m&#125;
</code></pre>  $$ 是半正定矩阵，则称 $K(x,z)$ 为正定核</li>
</ul>
</blockquote>
<ul>
<li>可用于指导构造核函数<ul>
<li>检验具体函数是否为正定核函数不容易</li>
<li>正定核具有优秀性质<ul>
<li><em>SVM</em> 中正定核能保证优化问题为凸二次规划，即二次规划中矩阵 $G$ 为正定矩阵</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="欧式空间核函数"><a href="#欧式空间核函数" class="headerlink" title="欧式空间核函数"></a>欧式空间核函数</h2><h3 id="Linear-Kernel"><a href="#Linear-Kernel" class="headerlink" title="Linear Kernel"></a><em>Linear Kernel</em></h3><p>线性核：最简单的核函数</p>
<script type="math/tex; mode=display">
k(x, y) = x^T y</script><ul>
<li>特点<ul>
<li>适用线性核的核算法通常同普通算法结果相同<ul>
<li><em>KPCA</em> 使用线性核等同于普通 <em>PCA</em></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Polynomial-Kernel"><a href="#Polynomial-Kernel" class="headerlink" title="Polynomial Kernel"></a><em>Polynomial Kernel</em></h3><p>多项式核：<em>non-stational kernel</em></p>
<script type="math/tex; mode=display">
K(x, y) = (\alpha x^T y + c)^p</script><ul>
<li><p>特点</p>
<ul>
<li>适合正交归一化后的数据</li>
<li>参数较多，稳定<h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1></li>
</ul>
</li>
<li><p>应用场合</p>
<ul>
<li><p>SVM：<em>p</em> 次多项式分类器</p>
<script type="math/tex; mode=display">
f(x) = sgn(\sum_{i=1}^{N_s} \alpha_i^{*} y_i
  (x_i x + 1)^p + b^{*})</script></li>
</ul>
</li>
</ul>
<h3 id="Gaussian-Kernel"><a href="#Gaussian-Kernel" class="headerlink" title="Gaussian Kernel"></a><em>Gaussian Kernel</em></h3><p>高斯核：<em>radial basis kernel</em>，经典的稳健径向基核</p>
<script type="math/tex; mode=display">
K(x, y) = exp(-\frac {\|x - y\|^2} {2\sigma^2})</script><blockquote>
<ul>
<li>$\sigma$：带通，取值关于核函数效果，影响高斯分布形状<blockquote>
<ul>
<li>高估：分布过于集中，靠近边缘非常平缓，表现类似像线性一样，非线性能力失效</li>
<li>低估：分布过于平缓，失去正则化能力，决策边界对噪声高度敏感</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<ul>
<li><p>特点</p>
<ul>
<li>对数据中噪声有较好的抗干扰能力</li>
</ul>
</li>
<li><p>对应映射：省略分母</p>
<script type="math/tex; mode=display">\begin{align*}
K(x, y) & = exp(-(x - y)^2)  \\
& = exp(-(x^2 - 2 x y - y^2)) \\
& = exp(-x^2) exp(-y^2) exp(2xy) \\
& = exp(-x^2) exp(-y^2) \sum_{i=0}^\infty \frac {(2xy)^i} {i!} \\
& = \phi(x) \phi(y) \\
\phi(x) & = exp(-x^2)\sum_{i=0}^\infty \sqrt {\frac {2^i} {i!}} x^i
\end{align*}</script><p>即高斯核能够把数据映射至无穷维</p>
</li>
<li><p>应用场合</p>
<ul>
<li><p>SVM：高斯<em>radial basis function</em>分类器</p>
<script type="math/tex; mode=display">
f(x) = sgn(\sum_{i=1}^{N_s} \alpha_i^{*} y_i
  exp(-\frac {\|x - y\|^2} {2\sigma^2}) + b^{*})</script></li>
</ul>
</li>
</ul>
<h4 id="Exponential-Kernel"><a href="#Exponential-Kernel" class="headerlink" title="Exponential Kernel"></a><em>Exponential Kernel</em></h4><p>指数核：高斯核变种，仅去掉范数的平方，也是径向基核</p>
<script type="math/tex; mode=display">
K(x, y) = exp(-\frac {\|x - y\|} {2\sigma^2})</script><ul>
<li>降低了对参数的依赖性</li>
<li>适用范围相对狭窄</li>
</ul>
<h4 id="Laplacian-Kernel"><a href="#Laplacian-Kernel" class="headerlink" title="Laplacian Kernel"></a><em>Laplacian Kernel</em></h4><p>拉普拉斯核：完全等同于的指数核，只是对参数$\sigma$改变敏感
性稍低，也是径向基核</p>
<script type="math/tex; mode=display">
K(x, y) = exp(-\frac {\|x - y\|} {\sigma^2})</script><h4 id="ANOVA-Kernel"><a href="#ANOVA-Kernel" class="headerlink" title="ANOVA Kernel"></a><em>ANOVA Kernel</em></h4><p>方差核：径向基核</p>
<script type="math/tex; mode=display">
k(x,y) = \sum_{k=1}^n exp(-\sigma(x^k - y^k)^2)^d</script><ul>
<li>在多维回归问题中效果很好</li>
</ul>
<h4 id="Hyperbolic-Tangent-Sigmoid-Multilayer-Perceptron-Kernel"><a href="#Hyperbolic-Tangent-Sigmoid-Multilayer-Perceptron-Kernel" class="headerlink" title="Hyperbolic Tangent/Sigmoid/Multilayer Perceptron Kernel"></a><em>Hyperbolic Tangent/Sigmoid/Multilayer Perceptron Kernel</em></h4><p>Sigmoid核：来自神经网络领域，被用作人工神经元的激活函数</p>
<script type="math/tex; mode=display">
k(x, y) = tanh(\alpha x^T y + c)</script><ul>
<li><p>条件正定，但是实际应用中效果不错</p>
</li>
<li><p>参数</p>
<ul>
<li>$\alpha$：通常设置为$1/N$，N是数据维度</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>使用Sigmoid核的SVM等同于两层感知机神经网络</li>
</ul>
</blockquote>
<h4 id="Ration-Quadratic-Kernel"><a href="#Ration-Quadratic-Kernel" class="headerlink" title="Ration Quadratic Kernel"></a><em>Ration Quadratic Kernel</em></h4><p>二次有理核：替代高斯核，计算耗时较小</p>
<script type="math/tex; mode=display">
k(x, y) = 1 - \frac {\|x - y\|^2} {\|x - y\|^2 + c}</script><h4 id="Multiquadric-Kernel"><a href="#Multiquadric-Kernel" class="headerlink" title="Multiquadric Kernel"></a><em>Multiquadric Kernel</em></h4><p>多元二次核：适用范围同二次有理核，是非正定核</p>
<script type="math/tex; mode=display">
k(x, y) = \sqrt {\|x - y\|^2 + c^2}</script><h4 id="Inverse-Multiquadric-Kernel"><a href="#Inverse-Multiquadric-Kernel" class="headerlink" title="Inverse Multiquadric Kernel"></a><em>Inverse Multiquadric Kernel</em></h4><p>逆多元二次核：和高斯核一样，产生满秩核矩阵，产生无穷维的
特征空间</p>
<script type="math/tex; mode=display">
k(x, y) = \frac 1 {\sqrt {\|x - y\|^2 + c^2}}</script><h4 id="Circular-Kernel"><a href="#Circular-Kernel" class="headerlink" title="Circular Kernel"></a><em>Circular Kernel</em></h4><p>环形核：从统计角度考虑的核，各向同性稳定核，在$R^2$上正定</p>
<script type="math/tex; mode=display">
k(x, y) = \frac 2 \pi arccos(-\frac {\|x - y\|} \sigma) -
    \frac 2 \pi \frac {\|x - y\|} \sigma
    \sqrt{1- \frac {\|x - y\|^2} \sigma}</script><h4 id="Spherical-Kernel"><a href="#Spherical-Kernel" class="headerlink" title="Spherical Kernel"></a><em>Spherical Kernel</em></h4><p>类似环形核，在$R^3$上正定</p>
<script type="math/tex; mode=display">
k(x, y) = 1 - \frac 3 2 \frac {\|x - y\|} \sigma +
    \frac 1 2 (\frac {\|x - y\|} \sigma)^3</script><h4 id="Wave-Kernel"><a href="#Wave-Kernel" class="headerlink" title="Wave Kernel"></a><em>Wave Kernel</em></h4><p>波动核</p>
<script type="math/tex; mode=display">
k(x, y) = \frac \theta {\|x - y\|} sin(\frac {\|x - y\|}
    \theta)</script><ul>
<li>适用于语音处理场景</li>
</ul>
<h4 id="Triangular-Power-Kernel"><a href="#Triangular-Power-Kernel" class="headerlink" title="Triangular/Power Kernel"></a><em>Triangular/Power Kernel</em></h4><p>三角核/幂核：量纲不变核，条件正定</p>
<script type="math/tex; mode=display">
k(x, y) = - \|x - y\|^d</script><h4 id="Log-Kernel"><a href="#Log-Kernel" class="headerlink" title="Log Kernel"></a><em>Log Kernel</em></h4><p>对数核：在图像分隔上经常被使用，条件正定</p>
<script type="math/tex; mode=display">
k(x, y) = -log(1 + \|x - y\|^d)</script><h4 id="Spline-Kernel"><a href="#Spline-Kernel" class="headerlink" title="Spline Kernel"></a><em>Spline Kernel</em></h4><p>样条核：以分段三次多项式形式给出</p>
<script type="math/tex; mode=display">
k(x, y) = 1 + x^t y + x^t y min(x, y) - \frac {x + y} 2
    min(x, y)^2 + \frac 1 3 min(x, y)^2</script><h4 id="B-Spline-Kernel"><a href="#B-Spline-Kernel" class="headerlink" title="B-Spline Kernel"></a><em>B-Spline Kernel</em></h4><p>B-样条核：径向基核，通过递归形式给出</p>
<script type="math/tex; mode=display">\begin{align*}
k(x, y) & = \prod_{p=1}^d B_{2n+1}(x_p - y_p) \\
B_n(x) & = B_{n-1} \otimes B_0 \\
& = \frac 1 {n!} \sum_{k=0}^{n+1} \binom {n+1} {r}
    (-1)^k (x + \frac {n+1} 2 - k)_{+}^n
\end{align*}</script><blockquote>
<ul>
<li>$x_{+}^d$：截断幂函数<script type="math/tex; mode=display">x_{+}^d = \left \{ \begin{array}{l}
      x^d, & if x > 0 \\
      0, & otherwise \\
  \end{array} \right.</script></li>
</ul>
</blockquote>
<h4 id="Bessel-Kernel"><a href="#Bessel-Kernel" class="headerlink" title="Bessel Kernel"></a><em>Bessel Kernel</em></h4><p>Bessel核：在theory of function spaces of fractional smoothness
中非常有名</p>
<script type="math/tex; mode=display">
k(x, y) = \frac {J_{v+1}(\sigma\|x - y\|)}
    {\|x - y\|^{-n(v + 1)}}</script><ul>
<li>$J$：第一类Bessel函数</li>
</ul>
<h4 id="Cauchy-Kernel"><a href="#Cauchy-Kernel" class="headerlink" title="Cauchy Kernel"></a><em>Cauchy Kernel</em></h4><p>柯西核：源自柯西分布，是长尾核，定义域广泛，可以用于原始维度
很高的数据</p>
<script type="math/tex; mode=display">
k(x, y) = \frac 1 {1 + \frac {\|x - y\|^2} {\sigma}}</script><h4 id="Chi-Square-Kernel"><a href="#Chi-Square-Kernel" class="headerlink" title="Chi-Square Kernel"></a><em>Chi-Square Kernel</em></h4><p>卡方核：源自卡方分布</p>
<script type="math/tex; mode=display">\begin{align*}
k(x, y) & = 1 - \sum_{i=1}^d \frac {(x_i - y_i)^2}
    {\frac 1 2 (x_i + y_i)} \\
& \frac {x^t y} {\|x + y\|}
\end{align*}</script><h4 id="Histogram-Intersection-Min-Kernel"><a href="#Histogram-Intersection-Min-Kernel" class="headerlink" title="Histogram Intersection/Min Kernel"></a><em>Histogram Intersection/Min Kernel</em></h4><p>直方图交叉核：在图像分类中经常用到，适用于图像的直方图特征</p>
<script type="math/tex; mode=display">
k(x, y) = \sum_{i=1}^d min(x_i, y_i)</script><h4 id="Generalized-Histogram-Intersection"><a href="#Generalized-Histogram-Intersection" class="headerlink" title="Generalized Histogram Intersection"></a><em>Generalized Histogram Intersection</em></h4><p>广义直方图交叉核：直方图交叉核的扩展，可以应用于更多领域</p>
<script type="math/tex; mode=display">
k(x, y) = \sum_{i=1}^m min(|x_i|^\alpha, |y_i|^\beta)</script><h4 id="Bayesian-Kernel"><a href="#Bayesian-Kernel" class="headerlink" title="Bayesian Kernel"></a><em>Bayesian Kernel</em></h4><p>贝叶斯核：取决于建模的问题</p>
<script type="math/tex; mode=display">\begin{align*}
k(x, y) & = \prod_{i=1}^d k_i (x_i, y_i) \\
k_i(a, b) & = \sum_{c \in \{0, 1\}} P(Y=c | X_i = a)
    P(Y=c | x_k = b)
\end{align*}</script><h4 id="Wavelet-Kernel"><a href="#Wavelet-Kernel" class="headerlink" title="Wavelet Kernel"></a><em>Wavelet Kernel</em></h4><p>波核：源自波理论</p>
<script type="math/tex; mode=display">
k(x, y) = \prod_{i=1}^d h(\frac {x_i - c} a)
    h(\frac {y_i - c} a)</script><ul>
<li><p>参数</p>
<ul>
<li>$c$：波的膨胀速率</li>
<li>$a$：波的转化速率</li>
<li>$h$：母波函数，可能的一个函数为<script type="math/tex; mode=display">
h(x) = cos(1.75 x) exp(-\frac {x^2} 2)</script></li>
</ul>
</li>
<li><p>转化不变版本如下</p>
<script type="math/tex; mode=display">
k(x, y) = \prod_{i=1}^d h(\frac {x_i - y_i} a)</script></li>
</ul>
<h2 id="离散数据核函数"><a href="#离散数据核函数" class="headerlink" title="离散数据核函数"></a>离散数据核函数</h2><h3 id="String-Kernel"><a href="#String-Kernel" class="headerlink" title="String Kernel"></a><em>String Kernel</em></h3><p>字符串核函数：定义在字符串集合（离散数据集合）上的核函数</p>
<script type="math/tex; mode=display">\begin{align*}
k_n(s, t) & = \sum_{u \in \sum^n} [\phi_n(s)]_u
    [\phi_n(t)]_u \\
& = \sum_{u \in \sum^n} \sum_{(i,j): s(i) = t(j) = u}
    \lambda^{l(i)} \lambda^{l(j)}
\end{align*}</script><blockquote>
<ul>
<li><p>$[\phi<em>n(s)]_n = \sum</em>{i:s(i)=u} \lambda^{l(i)}$：长度
  大于等于n的字符串集合$S$到特征空间
  $\mathcal{H} = R^{\sum^n}$的映射，目标特征空间每维对应
  一个字符串$u \in \sum^n$</p>
</li>
<li><p>$\sum$：有限字符表</p>
</li>
<li><p>$\sum^n$：$\sum$中元素构成，长度为n的字符串集合</p>
</li>
<li><p>$u = s(i) = s(i<em>1)s(i_2)\cdots s(i</em>{|u|})$：字符串s的
  子串u（其自身也可以用此方式表示）</p>
</li>
<li><p>$i =(i<em>1, i_2, \cdots, i</em>{|u|}), 1 \leq i<em>1 &lt; i_2 &lt; … &lt; i</em>{|u|} \leq |s|$：序列指标</p>
</li>
<li><p>$l(i) = i_{|u|} - i_1 + 1 \geq |u|$：字符串长度，仅在
  序列指标$i$连续时取等号（$j$同）</p>
</li>
<li><p>$0 &lt; \lambda \leq 1$：衰减参数</p>
</li>
</ul>
</blockquote>
<ul>
<li><p>两个字符串s、t上的字符串核函数，是基于映射$\phi_n$的
特征空间中的内积</p>
<ul>
<li>给出了字符串中长度为n的所有子串组成的特征向量的余弦
相似度</li>
<li>直观上，两字符串相同子串越多，其越相似，核函数值越大</li>
<li>核函数值可由动态规划快速计算（只需要计算两字符串公共
子序列即可）</li>
</ul>
</li>
<li><p>应用场合</p>
<ul>
<li>文本分类</li>
<li>信息检索</li>
<li>信物信息学</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-26T17:01:10.000Z" title="6/27/2019, 1:01:10 AM">2019-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-06-26T17:01:10.000Z" title="6/27/2019, 1:01:10 AM">2019-06-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">7 minutes read (About 999 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/lagrange_duality.html">Lagrange 对偶</a></h1><div class="content"><h2 id="Langrangian-Duality"><a href="#Langrangian-Duality" class="headerlink" title="Langrangian Duality"></a><em>Langrangian Duality</em></h2><p>拉格朗日对偶</p>
<ul>
<li><p>考虑优化问题：找到$f(x)$满足约束的最好下界</p>
<script type="math/tex; mode=display">
z^{*} = \min_{x} f(x) \\
\begin{align*}
s.t. \quad & g_i(x) \leq 0, i=1,2,\cdots,m \\
   & x \in X
\end{align*}</script></li>
<li><p>考虑方程组</p>
<script type="math/tex; mode=display">
\left \{ \begin{array}{l}
f(x) < v \\
g_i(x) \leq 0, i=1,2,\cdots,m
\end{array} \right.</script><ul>
<li><p><strong>方程组无解</strong>：$v$是优化问题的一个下界</p>
</li>
<li><p><strong>方程组有解</strong>：则可以推出</p>
<script type="math/tex; mode=display">
\forall \lambda \geq 0, \exists x, 
f(x) + \sum_{i=1}^m \lambda_ig_i(x) < v</script><blockquote>
<ul>
<li>显然，取$g_1 + g_2 = 0, g_1(x) &gt; 0$是反例，不能
推出原方程有解</li>
</ul>
</blockquote>
</li>
<li><p>由以上方程组有解逆否命题：方程组无解<strong>充分条件</strong>如下</p>
<script type="math/tex; mode=display">
\exists \lambda \geq 0,
\min_{x} f(x) + \sum _{i=1}^m \lambda_ig_i(x) \geq v</script></li>
</ul>
</li>
<li><p>由此方法推出的最好下界，即拉格朗日对偶问题</p>
<script type="math/tex; mode=display">
v^{*} = \max_{\lambda \geq 0} \min_{x} f(x) +
   \sum_{i=1}^m \lambda_ig_i(x)</script></li>
</ul>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><ul>
<li><p>拉格朗日对偶对实数域上的优化问题都存在，对目标函数、
约束函数都没有要求</p>
</li>
<li><p>强对偶定理：$v^{<em>} = z^{</em>}$，需要$f,g$满足特定条件才成立</p>
<ul>
<li>线性规划</li>
<li>半正定规划</li>
<li>凸优化</li>
</ul>
<blockquote>
<ul>
<li>即需要给约束条件加以限制，使得<script type="math/tex; mode=display">
 \forall \lambda \geq 0, \exists x, 
 f(x) + \sum_{i=1}^m \lambda_ig_i(x) < v</script> 是上述方程组有解的冲要条件</li>
</ul>
</blockquote>
</li>
<li><p>弱对偶定理：$v^{<em>} \leq z^{</em>}$，永远成立（以上即可证）</p>
<ul>
<li>通过弱对偶定理，可以得到原问题的一个下界</li>
<li>对求解原问题有帮助，比如：分支界限法中快速求下界</li>
</ul>
</li>
<li><p>对偶问题相关算法往往原问题算法在实际应用中往往更加有效</p>
<ul>
<li><em>dual-simplex</em></li>
<li><em>primal-dual interior point method</em></li>
<li><em>augmented Lagrangian Method</em></li>
</ul>
</li>
</ul>
<h2 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h2><p>约束最优化问题</p>
<script type="math/tex; mode=display">\begin{array}{l}
\min_{x \in R^n} & f(x) \\
s.t. & c_i(x) \leq 0, i = 1,2,\cdots,k \\
& h_j(x) = 0, j = 1,2,\cdots,l
\end{array}</script><h3 id="Generalized-Lagrange-Function"><a href="#Generalized-Lagrange-Function" class="headerlink" title="Generalized Lagrange Function"></a><em>Generalized Lagrange Function</em></h3><ul>
<li><p>引入<em>Generalized Lagrange Function</em></p>
<script type="math/tex; mode=display">
L(x, \alpha, \beta) = f(x) + \sum_{i=1}^k \alpha_i
   c_i(x) + \sum_{j=1}^l \beta_j h_j(x)</script><blockquote>
<ul>
<li>$x=(x_1, x_2, \cdots, x_n) \in R^n$</li>
<li>$\alpha_i \geq 0, \beta_j$：拉格朗日乘子</li>
</ul>
</blockquote>
</li>
<li><p>考虑关于x的函数</p>
<script type="math/tex; mode=display">
\theta_P(x) = \max_{\alpha, \beta: \alpha_i \geq 0}
   L(x, \alpha, \beta)</script><blockquote>
<ul>
<li>$P$：primal，原始问题</li>
</ul>
</blockquote>
<ul>
<li><p>若x满足原始问题的两组约束条件，则$\theta_P(x)=f(x)$</p>
</li>
<li><p>若x违反等式约束j，取$\beta_j \rightarrow \infty$，
则有$\theta_P(x) \rightarrow \infty$</p>
</li>
<li><p>若x违反不等式约束i，取$\alpha_i \rightarrow \infty$
，则有$\theta_P(x) \rightarrow \infty$</p>
</li>
</ul>
<p>则有</p>
<script type="math/tex; mode=display">\theta_P(x) = \left \{ \begin{array}{l}
f(x), & x 满足原始问题约束条件 \\
+\infty, & 其他
\end{array} \right.</script></li>
<li><p>则极小化问题，称为广义拉格朗日函数的极小极大问题</p>
<script type="math/tex; mode=display">
\min_x \theta_P(x) = \max_{\alpha, \beta: \alpha_i \geq 0}
   L(x, \alpha, \beta)</script><p>与原始最优化问题等价，两问题最优值相同，记为</p>
<script type="math/tex; mode=display">
p^{*} = \min_x \theta_P(x)</script></li>
</ul>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><ul>
<li><p>定义</p>
<script type="math/tex; mode=display">
\theta_D (\alpha, \beta) = \min_x L(x, \alpha, \beta)</script></li>
<li><p>再考虑极大化$\theta_D(\alpha, \beta)$，得到广义拉格朗日
函数的极大极小问题，即</p>
<script type="math/tex; mode=display">
\max_{\alpha, \beta: \alpha \geq 0}
   \theta_D(\alpha, \beta) =
   \max_{\alpha, \beta: \alpha \geq 0} \min_x
   L(x, \alpha, \beta)</script><p>表示为约束最优化问题如下</p>
<script type="math/tex; mode=display">\begin{align*}
\max_{\alpha, \beta} & \theta_D(\alpha, \beta) =
   \max_{\alpha, \beta} \min_x L(x, \alpha, \beta) \\
s.t. & \alpha_i \geq 0, i=1,2,\cdots,k
\end{align*}</script><p>称为原始问题的对偶问题，其最优值定义记为</p>
<script type="math/tex; mode=display">
d^{*} = \max_{\alpha, \beta: \alpha \geq 0}
   \theta_D(\alpha, \beta)</script></li>
</ul>
<h2 id="原始、对偶问题关系"><a href="#原始、对偶问题关系" class="headerlink" title="原始、对偶问题关系"></a>原始、对偶问题关系</h2><h3 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h3><blockquote>
<ul>
<li>若原始问题、对偶问题都有最优值，则<script type="math/tex; mode=display">
  d^{*} = \max_{\alpha, \beta: \alpha \geq 0} \min_x
      L(x, \alpha, \beta) \leq
  \min_x \max_{\alpha, \beta: \alpha \geq 0}
      L(x, \alpha, \beta) = p^{*}</script></li>
</ul>
</blockquote>
<ul>
<li><p>$\forall x, \alpha, \beta$有</p>
<script type="math/tex; mode=display">
\theta_D(\alpha, \beta) = \min_x L(x, \alpha, \beta)
   \leq L(x, \alpha, \beta) \leq
   \max_{\alpha, \beta: \alpha \geq 0} = \theta_P(x)</script><p>即</p>
<script type="math/tex; mode=display">
\theta_D(\alpha, \beta) \leq \theta_P(x)</script></li>
<li><p>而原始、对偶问题均有最优值，所以得证</p>
</li>
</ul>
<blockquote>
<ul>
<li>设$x^{<em>}$、$\alpha^{</em>}, \beta^{<em>}$分别是原始问题、对偶
  问题的可行解，且$d^{</em>} = p^{*}$，则其分别是原始问题、
  对偶问题的最优解</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:26:19.000Z" title="8/4/2021, 11:26:19 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">16 minutes read (About 2327 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/gredient_based.html">Gradient Descent Method</a></h1><div class="content"><h2 id="思想：最速下降-amp-牛顿"><a href="#思想：最速下降-amp-牛顿" class="headerlink" title="思想：最速下降&amp;牛顿"></a>思想：最速下降&amp;牛顿</h2><p>对目标函数$f(x)$在$x^{(1)}$进行展开</p>
<script type="math/tex; mode=display">
f(x) = f(x^{(1)}) + \nabla f(x^{(1)})(x - x^{(1)})+
    \frac 1 2 \nabla^2 f(x^{(1)})(x - x^{(1)})^2 +
    o((x - x^{(1)})^2)</script><blockquote>
<ul>
<li>最速下降法：只保留一阶项，即使用线性函数近似原目标函数</li>
<li>Newton法：保留一阶、二阶项，即使用二次函数近似</li>
</ul>
</blockquote>
<ul>
<li><p>利用近似函数求解元素问题极小值</p>
<ul>
<li>最速下降法：<strong>线性函数无极值，需要确定步长、迭代</strong></li>
<li>Newton法：<strong>二次函数有极值，直接求导算出极值、迭代</strong></li>
</ul>
</li>
<li><p>最速下降法</p>
<ul>
<li>只考虑一阶导：甚至说根本没有考虑拟合原目标函数</li>
</ul>
</li>
<li><p>Newton法</p>
<ul>
<li>考虑二阶导：每步迭代还考虑了二阶导，即当前更新完毕
后，下一步能够更好的更新（二阶导的意义）</li>
<li>甚至从后面部分可以看出，Newton法甚至考虑是全局特征，
不只是局部性质（前提目标函数性质足够好）</li>
<li>二次函数拟合更接近函数极值处的特征</li>
</ul>
</li>
</ul>
<h2 id="最速下降算法"><a href="#最速下降算法" class="headerlink" title="最速下降算法"></a>最速下降算法</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>设$x=x(t)$为最优点$x$从初始点、沿负梯度方向经过的曲线，
则有</p>
<script type="math/tex; mode=display">\left \{ \begin{array}{l}
& \frac {dx(t)} {dt} = -\nabla f(x(t)) \\
& x(t_1) = x^{(1)}
\end{array} \right.</script><blockquote>
<ul>
<li>$t_1, x^{(1)}$：初始时刻、初始位置</li>
</ul>
</blockquote>
</li>
<li><p>可以证明，$x(t)$解存在，且$t \rightarrow \infty$时，有
$x(t) \rightarrow x^{ * }$，即得到无约束问题最优解</p>
</li>
<li><p>但微分方程组求解可能很麻烦，可能根本无法求解</p>
<ul>
<li>考虑将以上曲线离散化，每次前进到“不应该”前进为止</li>
<li>然后更换方向，逐步迭代得到最优解</li>
</ul>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><blockquote>
<ul>
<li>搜索方向最速下降方向：负梯度方向</li>
<li>终止准则：$\nabla f(x^{(k)})=0$</li>
</ul>
</blockquote>
<ol>
<li><p>取初始点$x^{(1)}$，置k=1</p>
</li>
<li><p>若$\nabla f(x^{(k)})=0$，则停止计算，得到最优解，
否则置</p>
<script type="math/tex; mode=display">d^{(k)} = -\nabla f(x^{(k)})</script><p>以负梯度作为前进方向</p>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) =
  f(x^{(k)} + \alpha d^{(k)})</script><p>得$\alpha_k$前进步长，置</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}</script></li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
<blockquote>
<ul>
<li>最速下降算法不具有二次终止性</li>
</ul>
</blockquote>
<h2 id="叠加惯性"><a href="#叠加惯性" class="headerlink" title="叠加惯性"></a>叠加惯性</h2><p>模拟物体运动时惯性：指数平滑更新步长</p>
<p><img src="/imgs/momentum.png" alt="momentum"></p>
<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a><em>Momentum</em></h3><p>冲量方法：在<strong>原始更新步</strong>上叠加上次更新步，类似指数平滑</p>
<script type="math/tex; mode=display">
v^{(t)} = \gamma v^{(t-1)} + (1 - \gamma) \eta
    \bigtriangledown_\theta L(\theta^{(t-1)}) \\
\theta^{(t)} = \theta^{(t-1)} - v^{(t)}</script><blockquote>
<ul>
<li>$v^{(t)}$：第$t$步时第k个参数更新步</li>
<li>$L(\theta)$：往往是batch损失函数</li>
</ul>
</blockquote>
<ul>
<li>更新参数时，一定程度<strong>保持</strong>上次更新方向</li>
<li>可以在一定程度上保持稳定性，学习速度更快</li>
<li>能够越过部分局部最优解</li>
</ul>
<h3 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a><em>Nesterov Momentum</em></h3><p><em>NGA</em>：在使用冲量修正最终方向基础上，使用冲量对当前
<strong>参数位置</strong>进行修正，即使用“未来”位置计算梯度</p>
<ul>
<li>先使用冲量更新一步</li>
<li>再在更新后位置计算新梯度进行第二步更新</li>
</ul>
<script type="math/tex; mode=display">
v^{(t)} = \gamma v^{(t-1)} + \eta \bigtriangledown_\theta
    L(\theta^{(t-1)} - \gamma v^{(t-1)}) \\

\theta^{(t)} = \theta^{(t-1)} - v^{(t)}</script><h2 id="动态学习率"><a href="#动态学习率" class="headerlink" title="动态学习率"></a>动态学习率</h2><ul>
<li>学习率太小收敛速率缓慢、过大则会造成较大波动</li>
<li>在训练过程中动态调整学习率大小较好</li>
</ul>
<blockquote>
<ul>
<li>模拟退火思想：达到一定迭代次数、损失函数小于阈值时，减小
  学习速率</li>
</ul>
</blockquote>
<p><img src="/imgs/param_estimation_comparion_1.png" alt="param_estimation_comparion_1">
<img src="/imgs/param_estimation_comparion_2.png" alt="param_estimation_comparion_2"></p>
<h3 id="Vanilla-Gradient-Descent"><a href="#Vanilla-Gradient-Descent" class="headerlink" title="Vanilla Gradient Descent"></a><em>Vanilla Gradient Descent</em></h3><p>每次迭代减小学习率$\eta$</p>
<script type="math/tex; mode=display">
\eta^{(t)} = \frac \eta {\sqrt {t+1}} \\

\theta^{(t)} = \theta^{(t-1)} - \eta^{(t)}
    \bigtriangledown_\theta L(\theta^{(t-1)})</script><ul>
<li>学习率逐渐减小，避免学习后期参数在最优解附近反复震荡</li>
</ul>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a><em>Adagrad</em></h3><p><em>adaptive gradient</em>：训练中<strong>不同参数</strong>学习率随着迭代次数、
梯度动态变化，使得参数收敛更加平稳</p>
<script type="math/tex; mode=display">
v^{(t)}_k = \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\

\theta^{(t)}_k = \theta^{(t-1)}_k - \frac \eta
    {\sqrt {\sum_{i=0}^{t-1} (v^{(i)}_k)^2 + \epsilon}}
    v^{(t)}_k</script><blockquote>
<ul>
<li>$\epsilon$：fuss factor，避免分母为0</li>
<li>$\theta^{(t)}_k$：第t轮迭代完成后待估参数第k个分量
  （之前未涉及参数间不同，统一为向量）</li>
</ul>
</blockquote>
<ul>
<li><p>特点</p>
<ul>
<li>较大梯度参数真正学习率会被拉小；较小梯度真正学习率
参数被拉小幅度较小</li>
<li>可以和异步更新参数结合使用，给不常更新参数更大学习率</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>在训练后期，分母中梯度平方累加很大，学习步长趋于0，
收敛速度慢（可能触发阈值，提前结束训练）</li>
</ul>
</li>
</ul>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a><em>RMSprop</em></h3><p><em>root mean square prop</em>：指数平滑更新学习率分母</p>
<script type="math/tex; mode=display">
v^{(t)}_k = \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\

\theta^{(t)}_k = \theta^{(t-1)}_k - \frac \eta
    {\sqrt { \gamma \sum_{i=1}^{t-1}(v^{(i)}_k)^2 +
        (1 - \gamma)((v^{(t)})^2 + \epsilon}
    } v^{(t)}</script><ul>
<li>赋予当前梯度更大权重，减小学习率分母，避免学习速率下降
太快</li>
</ul>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a><em>Adam</em></h3><p><em>adptive moment estimation</em>：指数平滑更新步、学习率分母</p>
<script type="math/tex; mode=display">
\begin{align*}
v^{(t)}_k & = \gamma_1 v^{(t-1)}_k + (1 - \gamma_1)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)}) \\
s^{(t)}_k & = \gamma_2 s^{(t-1)}_k + (1 - \gamma_2)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\hat{v^{(t)}_k} & = \frac {v^{(t)}_k} {1 - \gamma_1^t} \\
\hat{s^{(t)}_k} & = \frac {s^{(t)}_k} {1 - \gamma_2^t} \\

\theta^{(t)}_k & = \theta^{(t-1)}_k - \frac \eta
    {\sqrt{\hat{s^{(t)}_k} + \epsilon}} \hat{v^{(t)}_k}
\end{align*}</script><blockquote>
<ul>
<li>$\gamma_1$：通常为0.9</li>
<li>$\gamma_2$：通常为0.99</li>
<li>$\hat{v^{(t)}_k} = \frac {v^{(t)}_k} {1 - \gamma_1^t}$
  ：权值修正，使得过去个时间步，小批量随机梯度权值之和为1</li>
</ul>
</blockquote>
<ul>
<li><p>利用梯度的一阶矩$v^{(t)}$、二阶矩$s^{(t)}$动态调整每个
参数学习率</p>
</li>
<li><p>类似于<em>mommentum</em>、<em>RMSprop</em>结合</p>
</li>
<li><p>经过偏执矫正后，每次迭代学习率都有确定范围，参数比较平稳</p>
</li>
</ul>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a><em>Adadelta</em></h3><p>指数平滑更新学习率（分子）、学习率分母</p>
<script type="math/tex; mode=display">
\begin{align*}
s^{(t)}_k & = \gamma_1 s^{(t-1)}_k + (1 - \gamma_1)
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\hat{v^{(t)}_k} & = \sqrt {\frac {\Delta \theta^{(t-1)}_k + \epsilon}
    {s^{(t)}_k + \epsilon}}
    \bigtriangledown_{\theta_k} L(\theta^{(t-1)})^2 \\

\Delta \theta^{(t)}_k & = \gamma_1 \Delta \theta^{(t-1)}_k +
    (1 - \gamma_1) \hat{v^{(t)}_k}^2 \\

\theta^{(t)}_k & = \theta^{(t)}_k - \hat{v^{(t)}_k}
\end{align*}</script><blockquote>
<ul>
<li>$s, \Delta \theta$共用超参$\gamma_1$</li>
</ul>
</blockquote>
<ul>
<li>在<em>RMSprop</em>基础上，使用$\sqrt {\Delta \theta}$作为学习率</li>
<li>$\hat v$：中超参$\gamma_1$在分子、分母“抵消”，模型对
超参不敏感</li>
</ul>
<h2 id="样本量"><a href="#样本量" class="headerlink" title="样本量"></a>样本量</h2><h3 id="Singular-Loss-Stocastic-Gradient-Descent"><a href="#Singular-Loss-Stocastic-Gradient-Descent" class="headerlink" title="Singular Loss/Stocastic Gradient Descent"></a>Singular Loss/Stocastic Gradient Descent</h3><p><em>SGD</em>：用模型在某个样本点上的损失极小化目标函数、计算梯度、
更新参数</p>
<ul>
<li><p>单点损失度量模型“一次”预测的好坏</p>
<ul>
<li>代表模型在单点上的优劣，无法代表模型在总体上性质</li>
<li>具有很强随机性</li>
</ul>
</li>
<li><p>单点损失不常用，SGD范围也不局限于单点损失</p>
</li>
</ul>
<blockquote>
<ul>
<li>损失函数具体参见<em>ml_xxxxx</em></li>
</ul>
</blockquote>
<h3 id="全局估计"><a href="#全局估计" class="headerlink" title="全局估计"></a>全局估计</h3><p>全局损失：用模型在全体样本点上损失极小化目标函数、计算梯度、
更新参数</p>
<script type="math/tex; mode=display">
\theta^{(t)} = \theta^{(t-1)} - \eta \bigtriangledown_\theta
    L_{total}(\theta_{(t-1)})</script><blockquote>
<ul>
<li>$\theta^{(t)}$：第t步迭代完成后待估参数</li>
<li>$\eta$：学习率</li>
<li>$L<em>{total}(\theta) = \sum</em>{i=1}^N L(\theta, x_i, y_i)$：
  训练样本整体损失</li>
<li>$N$：训练样本数量</li>
</ul>
</blockquote>
<ul>
<li><p>若损失函数有解析解、样本量不大，可<strong>一步更新（计算）</strong>
完成（传统参数估计场合）</p>
<ul>
<li>矩估计</li>
<li>最小二乘估计</li>
<li>极大似然估计</li>
</ul>
</li>
<li><p>否则需要迭代更新参数</p>
<ul>
<li>样本量较大场合</li>
<li>并行计算</li>
</ul>
</li>
</ul>
<h3 id="Mini-Batch-Loss"><a href="#Mini-Batch-Loss" class="headerlink" title="Mini-Batch Loss"></a>Mini-Batch Loss</h3><p><em>mini-batch loss</em>：用模型在某个batch上的损失极小化目标函数、
计算梯度、更新参数</p>
<script type="math/tex; mode=display">
\theta^{(t)} = \theta^{(t-1)} - \eta \bigtriangledown_\theta
    L_{batch}(\theta^{(t-1)})</script><blockquote>
<ul>
<li>$L<em>{batch}(\theta)=\sum</em>{i \in B} L(\theta, x_i, y_i)$：
  当前batch整体损失</li>
<li>$B$：当前更新步中，样本组成的集合batch</li>
</ul>
</blockquote>
<ul>
<li><p>batch-loss是模型在batch上的特征，对整体的代表性取决于
batch大小</p>
<ul>
<li>batch越大对整体代表性越好，越稳定；越小对整体代表
越差、不稳定、波动较大、难收敛</li>
<li>batch大小为1时，就是SGD</li>
<li>batch大小为整个训练集时，就是经验（结构）风险</li>
</ul>
</li>
<li><p>batch-loss是学习算法中最常用的loss，SGD优化常指此</p>
<ul>
<li>实际中往往是使用batch-loss替代整体损失，表示经验风险
极小化</li>
<li>batch-loss同样可以带正则化项，表示结构风险极小化</li>
<li>损失极值：SVM（几何间隔最小）</li>
</ul>
</li>
</ul>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>适合样本量较大、无法使用样本整体估计使用</li>
<li>一定程度能避免局部最优（随机batch可能越过局部极值）</li>
<li>开始阶段收敛速度快</li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li><p>限于每次只使用单batch中样本更新参数，batch-size较小时，
结果可能不稳定，往往很难得到最优解</p>
</li>
<li><p>无法保证良好的收敛性，学习率小收敛速度慢，学习率过大
则损失函数可能在极小点反复震荡</p>
</li>
<li><p>对所有参数更新应用相同学习率，没有对低频特征有优化
（更的学习率）</p>
</li>
<li><p>依然容易陷入局部最优点</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-06-03T17:18:34.000Z" title="6/4/2019, 1:18:34 AM">2019-06-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">5 minutes read (About 752 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/newtons.html">Newton&#039;s Method</a></h1><div class="content"><h2 id="Newton法"><a href="#Newton法" class="headerlink" title="Newton法"></a>Newton法</h2><h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>若$x^{ * }$是无约束问题局部解，则有</p>
<script type="math/tex; mode=display">\nabla f(x^{ * }) = 0</script><p>可求解此问题，得到无约束问题最优解</p>
</li>
<li><p>原始问题是非线性，考虑求解其线性逼近，在初始点$x^{(1)}$
处泰勒展开</p>
<script type="math/tex; mode=display">
\nabla f(x) \approx \nabla f(x^{(1)})
   + \nabla^2 f(x^{(1)})(x - x^{(1)})</script><p>解得</p>
<script type="math/tex; mode=display">
x^{(2)} = x^{(1)} - (\nabla^2 f(x^{(1)}))^{-1}
   \nabla f(x^{(1)})</script><p>作为$x^{ * }$的第二次近似</p>
</li>
<li><p>不断迭代，得到如下序列</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + d^{(k)}</script><blockquote>
<ul>
<li>$d^{(k)}$：Newton方向，即以下方程解<script type="math/tex; mode=display">
 \nabla^2 f(x^{(k)}) d = -\nabla
     f(x^{(k)})</script></li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ul>
<li><p>初始点 $x^{(1)}$、精度要求 $\epsilon$，置 $k=1$</p>
</li>
<li><p>考虑 $|\nabla f(x^{(k)})| \leq \epsilon$</p>
<ul>
<li>若满足，则停止计算，得到最优解 $x^{(k)}$</li>
<li><p>否则求解如下方程，得到 $d^{(k)}$</p>
<script type="math/tex; mode=display">
\nabla^2 f(x^{(k)}) d = -\nabla f(x^{(k)})</script></li>
</ul>
</li>
<li><p>如下设置，并转2</p>
<script type="math/tex; mode=display">x^{(k+1)} = x^{(k)} + d^{(k)}, k = k+1</script></li>
</ul>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li><p>优点</p>
<ul>
<li>产生点列 ${x^{k}}$ 若收敛，则具有二阶收敛速率</li>
<li>具有二次终止性，事实上对正定二次函数，一步即可收敛</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>可能会在某步迭代时目标函数值上升</li>
<li>当初始点 $x^{(1)}$ 距离最优解 $x^{ * }$ 时，产生的点列
可能不收敛，或者收敛到鞍点</li>
<li>需要计算 <em>Hesse</em> 矩阵<ul>
<li>计算量大</li>
<li><em>Hesse</em> 矩阵可能不可逆，算法终止</li>
<li><em>Hesse</em> 矩阵不正定，<em>Newton</em> 方向可能不是下降方向</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="阻尼-修正-Newton-法"><a href="#阻尼-修正-Newton-法" class="headerlink" title="阻尼/修正 Newton 法"></a>阻尼/修正 <em>Newton</em> 法</h2><ul>
<li>克服 <em>Newton</em> 法目标函数值上升的缺点</li>
<li>一定程度上克服点列可能不收敛缺点</li>
</ul>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul>
<li><p>初始点 $x^{(1)}$、精度要求 $\epsilon$，置 $k=1$</p>
</li>
<li><p>考虑 $|\nabla f(x^{(k)})| \leq \epsilon$</p>
<ul>
<li>若满足，停止计算，得到最优解 $x^{(k)}$</li>
<li>否则求解如下方程得到 $d^{(k)}$<script type="math/tex; mode=display">
\nabla^2 f(x^{(k)}) d = -\nabla f(x^{(k)})</script></li>
</ul>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) = f(x^{(k)} + \alpha d^{(k)})</script><p>得到 $\alpha_k$，如下设置并转2</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}, k = k+1</script></li>
</ul>
<h2 id="其他改进"><a href="#其他改进" class="headerlink" title="其他改进"></a>其他改进</h2><ul>
<li><em>Newton</em> 法、修正 <em>Newton</em> 法的改进方向<ul>
<li>结合最速下降方向修正迭代方向</li>
<li><em>Hesse</em> 矩阵不正定情形下的替代</li>
</ul>
</li>
</ul>
<h3 id="结合最速下降方向"><a href="#结合最速下降方向" class="headerlink" title="结合最速下降方向"></a>结合最速下降方向</h3><blockquote>
<ul>
<li>将 <em>Newton</em> 方向和最速下降方向结合</li>
</ul>
</blockquote>
<ul>
<li><p>设 $\theta_k$ 是 $<d^{(k)}, -\nabla f(x^{(k)})>$ 之间夹角，显然希望 $\theta &lt; \frac \pi 2$</p>
</li>
<li><p>则置限制条件 $\eta$，取迭代方向</p>
<script type="math/tex; mode=display">
d^{(k)} = \left \{ \begin{array}{l}
   d^{(k)}, & cos\theta_k \geq \eta \\
   -\nabla f(x^{(k)}), & 其他
\end{array} \right.</script></li>
</ul>
<h3 id="Negative-Curvature"><a href="#Negative-Curvature" class="headerlink" title="Negative Curvature"></a><em>Negative Curvature</em></h3><blockquote>
<ul>
<li>当 <em>Hesse</em> 矩阵非正定时，选择负曲率下降方向 $d^{(k)}$（一定存在）</li>
</ul>
</blockquote>
<ul>
<li><p><em>Hesse</em> 矩阵非正定时，一定存在负特征值、相应特征向量 $u$</p>
<ul>
<li><p>取负曲率下降方向作为迭代方向</p>
<script type="math/tex; mode=display">
d^{(k)} = -sign(u^T \nabla f(x^{(k)})) u</script></li>
<li><p>$x^{(k)}$ 处负曲率方向 $d^{(k)}$ 满足</p>
<script type="math/tex; mode=display">
{d^{(k)}}^T \nabla^2 f(x^{(k)}) d^{(k)} < 0</script></li>
</ul>
</li>
</ul>
<h3 id="修正-Hesse-矩阵"><a href="#修正-Hesse-矩阵" class="headerlink" title="修正 Hesse 矩阵"></a>修正 <em>Hesse</em> 矩阵</h3><ul>
<li><p>取迭代方向 $d^{(k)}$ 为以下方程的解</p>
<script type="math/tex; mode=display">
(\nabla^2 f(x^{(k)}) + v_k I) d = -\nabla f(x^{k})</script></li>
</ul>
<blockquote>
<ul>
<li>$v_k$：大于 $\nabla^2 f(x^{(k)})$ 最大负特征值绝对值</li>
</ul>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-04-22T17:32:09.000Z" title="4/23/2019, 1:32:09 AM">2019-04-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-04-22T17:32:09.000Z" title="4/23/2019, 1:32:09 AM">2019-04-23</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">12 minutes read (About 1864 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/quasi_newtons.html">Quasi-Newton Method/Variable Metric Method</a></h1><div class="content"><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>拟Newton法/变度量法：不需要求解Hesse矩阵，使用一阶导构造
二阶信息的近似矩阵</p>
<ul>
<li><p>使用迭代过程中信息，创建近似矩阵$B^{(k)}$代替Hesse矩阵</p>
</li>
<li><p>用以下方程组替代Newton方程，其解$d^{(k)}$作为搜索方向</p>
<script type="math/tex; mode=display">
B^{(k)} d = - \triangledown f(x^{(k)})</script></li>
</ul>
<h3 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h3><ul>
<li><p>考虑$\triangledown f(x)$在$x^{(k+1)}$处泰勒展开</p>
<script type="math/tex; mode=display">
\triangledown f(x) \approx \triangledown f(x^{(k+1)})
   + \triangledown^2 f(x^{(k+1)})(x - x^{(k+1)})</script></li>
<li><p>取$x = x^{(k)}$，有</p>
<script type="math/tex; mode=display">\begin{align*}
\triangledown f(x^{(k+1)}) - \triangledown f(x^{(k)})
   & \approx \triangledown^2 f(x^{(x+1)})
   (x^{(k+1) } - x^{(k)}) \\
\triangledown^2 f(x^{k+1}) s^{(k)} & \approx y^{(k)}
\end{align*}</script><blockquote>
<ul>
<li>$s^{(k)} = x^{(k+1)} - x^{(k)}$</li>
<li>$y^{(k)} = \triangledown f(x^{(k+1)}) - \triangledown f(x^{(k)})$</li>
</ul>
</blockquote>
</li>
<li><p>要求$B^{(k)}$近似$\triangledown^2 f(x^{(k)})$，带入并将
$\approx$改为$=$，得到拟Newton方程</p>
<script type="math/tex; mode=display">
B^{(k+1)} s^{(k)} = y^{(k)}</script><p>并假设$B^{(k)}$对称</p>
</li>
<li><p>拟Newton方程不能唯一确定$B^{(k+1)}$，需要附加条件，自然
的想法就是$B^{(k+1)}$可由$B^{(k)}$修正得到，即</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} + \Delta B^{(k)}</script><p>且修正项$\Delta B^{(k)}$具有“简单形式”</p>
</li>
</ul>
<h2 id="Hesse矩阵修正"><a href="#Hesse矩阵修正" class="headerlink" title="Hesse矩阵修正"></a>Hesse矩阵修正</h2><h3 id="对称秩1修正"><a href="#对称秩1修正" class="headerlink" title="对称秩1修正"></a>对称秩1修正</h3><p>认为简单指矩阵秩小：即认为$\Delta B^{(k)}$秩为最小值1</p>
<ul>
<li><p>设$\Delta B^{(k)} = u v^T$，带入有</p>
<script type="math/tex; mode=display">\begin{align*}
y^{(k)} & = B^{(k+1)} s^{(k)} \\
& = B^{(k)} s^{(k)} + (v^T s^{(k)}) u \\
y^{(k)} - B^{(k)} s^{(k)} & = (v^T s^{(k)}) u
\end{align*}</script><ul>
<li>这里有的书会设$\Delta B^{(k)} = \alpha u v^T$，
其实对向量没有必要</li>
<li>$v^T s^{(k)}$是数，所以$u$必然与共线，同理也没有必要
考虑系数，直接取相等即可</li>
<li>而且系数不会影响最终结果</li>
</ul>
</li>
<li><p><strong>可取</strong>$u = y^{(k)} - B^{(k)} s{(k)}$，取$v$满足
$v^T s^{(k)}  = 1$</p>
</li>
<li><p>由$B^{(k)}$的对称性、并希望$B^{(k+1)}$保持对称，需要
$u, v$共线，则有</p>
<script type="math/tex; mode=display">\begin{align*}
v & = \lambda u = \lambda (y^{(k)} - B^{(k)} s^{(k)}) \\
1 & = \lambda (y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)}
\end{align*}</script></li>
<li><p>得到$B^{(k)}$的对称秩1修正公式</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} + \frac {(y^{(k) - B^{(k)} s^{(k)}})
   (y^{(k)} - B^{(k)} s^{(k)})^T}
   {(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)}}</script></li>
</ul>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始点$x^{(1)}$、初始矩阵$B^{(1)} = I$、精度要求
$\epsilon$、置$k=1$</p>
</li>
<li><p>若$|\triangledown f(x^{(k)})| \leq \epsilon$，停止计算
，得到解$x^{(k)}$，否则求解以下方程得到$d^{(k)}$</p>
<script type="math/tex; mode=display">
B^{(k)} d = -\triangle f(x^{(k)})</script></li>
<li><p>一维搜索，求解</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha)=f(x^{(k)} + \alpha d^{(k)})</script><p>得到$\alpha_k$，置$x^{(k+1)}=x^{(k)} + \alpha_k d^{(k)}$</p>
</li>
<li><p>修正$B^{(k)}$</p>
<script type="math/tex; mode=display">\begin{align*}
s^{(k)} & = x^{(k+1)} - x^{(k)} \\
y^{(k)} & = \triangledown f(x^{(k+1)}) -
  \triangledown f(x^{(k)}) \\
B^{(k+1)} & = B^{(k)} + \frac {(y^{(k) - B^{(k)} s^{(k)}})
  (y^{(k)} - B^{(k)} s^{(k)})^T}
  {(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)}}
\end{align*}</script></li>
<li><p>置$k = k+1$，转2</p>
</li>
</ol>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><p>缺点</p>
<ul>
<li><p>要求$(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)} \neq 0$，
否则无法继续计算</p>
</li>
<li><p>不能保证正定性传递，只有
$(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)} &gt; 0$才能保证
$B^{(k+1)}$也正定</p>
</li>
<li><p>即使$(y^{(k)} - B^{(k)} s^{(k)})^T s^{(k)} &gt; 0$，
也可能很小，容易产生较大的舍入误差</p>
</li>
</ul>
</li>
</ul>
<h3 id="对称秩2修正"><a href="#对称秩2修正" class="headerlink" title="对称秩2修正"></a>对称秩2修正</h3><ul>
<li><p>为克服秩1修正公式缺点，考虑$\Delta B^{(k)}$秩为2，设</p>
<script type="math/tex; mode=display">
\Delta B^{(k)} = u^{(1)} (v^{(1)})^T
   + u^{(2)} (v^{(2)})^T</script></li>
<li><p>带入拟Newton方程有</p>
<script type="math/tex; mode=display">
B^{(k)} s^{(k)} + ((v^{(1)})^T s^{(k)}) u^{(1)} +
   ((v^{(2)})^T s^{(k)}) u^{(2)} = y^{(k)}</script></li>
<li><p>类似的取</p>
<script type="math/tex; mode=display">\left \{ \begin{array}{l}
u^{(1)} = y^{(k)} \\
(v^{(1)})^T s^{(k)} = 1
\end{array} \right.</script><script type="math/tex; mode=display">\left \{ \begin{array}{l}
u^{(2)} = -B^{(k)} s^{(k)} \\
(v^{(2)})^T s^{(k)} = 1
\end{array} \right.</script></li>
<li><p>同秩1公式保持对称性推导，得到对称秩2修正公式/BFGS公式</p>
<script type="math/tex; mode=display">
B^{(k+1)} = B^{(k)} - \frac {B^{(k)} s^{(k)}
   (s^{(k)})^T B^{(k)}} {(s^{(k)})^T B^{(k)} s^{(k)}}
   + \frac {y^{(k)} (y^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script></li>
</ul>
<h3 id="BFGS算法"><a href="#BFGS算法" class="headerlink" title="BFGS算法"></a>BFGS算法</h3><p>类似同秩1修正算法，仅第4步使用对称秩2修正公式</p>
<h2 id="Hesse逆修正"><a href="#Hesse逆修正" class="headerlink" title="Hesse逆修正"></a>Hesse逆修正</h2><h3 id="对称秩2修正-1"><a href="#对称秩2修正-1" class="headerlink" title="对称秩2修正"></a>对称秩2修正</h3><ul>
<li><p>考虑直接构造近似于$(\triangledown^2 f(x^{(k)}))^{-1}$的
矩阵$H^{(k)}$</p>
</li>
<li><p>这样无需求解线性方程组，直接计算</p>
<script type="math/tex; mode=display">
d^{(k)} = -H^{(k)} \triangledown f(x^{(k)})</script></li>
<li><p>相应拟Newton方程为</p>
<script type="math/tex; mode=display">
H^{(k+1)} y^{(k)} = s^{(k)}</script></li>
<li><p>可得$H^{(k)}$的对称秩1修正公式</p>
<script type="math/tex; mode=display">
H^{(k+1)} = H^{(k)} + \frac {(s^{(k)} - H^{(k)} y^{(k)})
   (s^{(k)} - H^{(k)} y^{(k)})T}
   {(s^{(k)} - H^{(k)} y^{(k)})^T y^{(k)}}</script></li>
<li><p>可得$H^{(k)}$的对称秩2修正公式/DFP公式</p>
<script type="math/tex; mode=display">
H^{(k+1)} = H^{(k)} - \frac {H^{(k)} y^{(k)} (y^{(k)})^T
   H^{(k)}} {(y^{(k)})^T H^{(k)} y^{(k)}} +
   \frac {s^{(k)} (s^{(k)})^T} {(s^{(k)})^T y^{(k)}}</script></li>
</ul>
<h4 id="DFP算法"><a href="#DFP算法" class="headerlink" title="DFP算法"></a>DFP算法</h4><p>类似BFGS算法，只是</p>
<ul>
<li>使用$H^{(k)}$计算更新方向</li>
<li>使用$H^{(k)}$的对称秩2修正公式修正</li>
</ul>
<blockquote>
<ul>
<li>对正定二次函数，BFGS算法和DFP算法效果相同</li>
<li>对一般可微（非正定二次函数），一般认为BFGS算法在收敛性质
  、数值计算方面均由于DFP算法</li>
</ul>
</blockquote>
<h3 id="Hesse逆的BFGS算法"><a href="#Hesse逆的BFGS算法" class="headerlink" title="Hesse逆的BFGS算法"></a>Hesse逆的BFGS算法</h3><ul>
<li><p>考虑</p>
<script type="math/tex; mode=display">\begin{align*}
B^{(k+1)} & = B^{(k)} + u^{(1)} (v^{(1)})^T +
   u^{(2)} (v^{(2)})^T \\
H^{(k+1)} & = (B^{(k+1)})^{-1} \\
& = (B^{(k)} + u^{(1)} (v^{(1)})^T + u^{(2)}
   (v^{(2)})^T)^{-1} \\
\end{align*}</script></li>
<li><p>两次利用<em>Sherman-Morrison</em>公式，可得</p>
<script type="math/tex; mode=display">
H^{(k+1)} = (I - \frac {s^{(k)} (y^{(k)})^T} 
   {(y^{(k)})^T s^{(k)}})
   H^{(k)}
   (I - \frac {s^{(k)} (y^{(k)})^T}
       {(y^{(k)})^T s^{(k)}})^T
   + \frac {s^{(k)} (s^{(k)})^T} {(y^{(k)})^T s^{(k)}}</script></li>
</ul>
<h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><ul>
<li><p>还可以进一步展开</p>
<script type="math/tex; mode=display">
H^{(k+1)} = H^{(k)} + (\frac 1 {(s^{(k)})^T y^{(k)}} +
   \frac {(y^{(k)})^T H^{(k)} y^{(k)}}
   {((s^{(k)})^T y^{(k)})^2}) s^{(k)} (s^{(k)})^T
   - \frac 1 {(s^{(k)})^T y^{(k)}}
   (H^{(k)} y^{(k)} (s^{(k)})^T +
   s^{(k)} (y^{(k)})^T H^{(k)})</script></li>
</ul>
<h2 id="变度量法的基本性质"><a href="#变度量法的基本性质" class="headerlink" title="变度量法的基本性质"></a>变度量法的基本性质</h2><h3 id="算法的下降性"><a href="#算法的下降性" class="headerlink" title="算法的下降性"></a>算法的下降性</h3><h4 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h4><blockquote>
<ul>
<li>设$B^{(k)}$（$H^{(k)}$）是正定对称矩阵，且有
  $(s^{(k)})^T y^{(k)} &gt; 0$，则由BFGS（DFS）公式构造的
  $B^{(k+1)}$（$H^{(k+1)}$）是正定对称的</li>
</ul>
</blockquote>
<ul>
<li><p>考虑$B^{(k)}$对称正定，有
$B^{(k)} = (B^{(k)})^{1/2} (B^{(k)})^{1/2}$</p>
</li>
<li><p>带入利用柯西不等式即可证</p>
</li>
</ul>
<blockquote>
<ul>
<li>中间插入正定矩阵的向量内积不等式也称为广义柯西不等式</li>
</ul>
</blockquote>
<h4 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h4><blockquote>
<ul>
<li>若$d^{(k)}$v是下降方向，且<strong>一维搜索是精确的</strong>，设
  $B^{(k)}$（$H^{(k)}$）是正定对称矩阵，则有BFGS（DFP）
  公式构造的$B^{(k+1)}$（$H^{(k+1)}$）是正定对称的</li>
</ul>
</blockquote>
<ul>
<li>精确一维搜索$(d^{(k)})^T \triangledown f(x^{(k+1)}) = 0$</li>
<li>则有$(s^{(k)})^T y^{(k)} &gt; 0$</li>
</ul>
<h4 id="定理3"><a href="#定理3" class="headerlink" title="定理3"></a>定理3</h4><blockquote>
<ul>
<li>若用BFGS算法（DFP算法）求解无约束问题，设初始矩阵
  $B^{(1)}$（$H^{(1)}$）是正定对称矩阵，且一维搜索是精确的
  ，若$\triangledown f(x^{(k)}) \neq 0$，则产生搜索方向
  $d^{(k)}$是下降方向</li>
</ul>
</blockquote>
<ul>
<li>结合上2个结论，数学归纳法即可</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li><p>若每步迭代一维搜索精确，或满足$(s^{(k)})^T y^{(k)} &gt; 0$</p>
<ul>
<li>停止在某一稳定点</li>
<li>或产生严格递减的序列${f(x^{(k)})}$</li>
</ul>
</li>
<li><p>若目标函数满足一定条件我，可以证明变度量法产生的点列
${x^{(k)}}$收敛到极小点，且收敛速率超线性</p>
</li>
</ul>
<h3 id="搜索方向共轭性"><a href="#搜索方向共轭性" class="headerlink" title="搜索方向共轭性"></a>搜索方向共轭性</h3><blockquote>
<ul>
<li>用变度量法BFGS（DFP）算法求解正定二次函数</li>
</ul>
</blockquote>
<pre><code>$$
min f(x) = \frac 1 2 x^T G x + r^T x + \sigma
$$

若一维搜索是精确的，假设已经进行了m次迭代，则
</code></pre><blockquote>
<ul>
<li><p>搜索方向$d^{(1)}, \cdots, d^{(m)}$是m个非零的G共轭方向</p>
</li>
<li><p>对于$j = 1, 2, \cdots, m$，有</p>
</li>
</ul>
</blockquote>
<pre><code>$$
B^&#123;(m+1)&#125; s^&#123;(j)&#125; = y^&#123;(j)&#125;
(H^&#123;(m+1)&#125; y^&#123;(j)&#125; = s^&#123;(j)&#125;)
$$

且$m = n$时有吧

$$
B^&#123;(n+1)&#125; = G(H^&#123;(n+1)&#125; = G^&#123;-1&#125;)
$$
</code></pre><h3 id="变度量法二次终止"><a href="#变度量法二次终止" class="headerlink" title="变度量法二次终止"></a>变度量法二次终止</h3><blockquote>
<ul>
<li>若一维搜索是精确的，则变度量法（BFGS、DFP）具有二次终止</li>
</ul>
</blockquote>
<ul>
<li><p>若$\triangle f(x^{(k)}) = 0, k \leq n$，则得到最优解
$x^{(k)}$</p>
</li>
<li><p>否则得到的搜索方向是共轭的，由扩展空间子定理，
$x^{(n+1)}$是最优解</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-08-04T03:25:02.000Z" title="8/4/2021, 11:25:02 AM">2021-08-04</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">11 minutes read (About 1640 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/conjugate_gradient.html">Conjugate Gradient Method</a></h1><div class="content"><h2 id="共轭方向"><a href="#共轭方向" class="headerlink" title="共轭方向"></a>共轭方向</h2><blockquote>
<ul>
<li><p>设G为$n * n$阶正定对称矩阵，若$d^{(1)}, d^{(2)}$满足</p>
<script type="math/tex; mode=display">(d^{(1)})^T G d^{(2)} = 0</script><p>  则称$d^{(1)}, d^{(2)}$关于G共轭</p>
</li>
<li><p>类似正交方向，若$d^{(1)},\cdots,d^{(k)}(k \leq n)$关于
  G两两共轭，则称其为G的k个共轭方向</p>
</li>
</ul>
</blockquote>
<ul>
<li>特别的，$G=I$时，共轭方向就是正交方向</li>
</ul>
<h3 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h3><blockquote>
<ul>
<li>设目标函数为<script type="math/tex; mode=display">
  f(w) = \frac 1 2 w^T w + r^T w + \sigma</script>  $q^{(1)}, \cdots, q^{(k)}$是$k, k \leq n$个非零正交方向
  ，从任意初始点$w^{(1)}$出发，依次沿着以上正交方向做
  <strong>精确一维搜索</strong>，得到$w^{(1)}, \cdots, w^{(k+1)}$，
  则$w^{(k+1)}$是$f(w)$在线性流形<script type="math/tex; mode=display">
  \bar W_k = \{w = w^{(1)} + \sum_{i=1}^k \alpha_i q^{(i)}
      | -\infty < \alpha_i < +\infty \}</script>  上的唯一极小点，特别的k=n时，$w^{(n+1)}$是$f(w)$在整个
  空间上的唯一极小点</li>
</ul>
</blockquote>
<ul>
<li><p>$\bar W_k$上的存在唯一极小点$\hat w^{(k)}$，在所有方向
都是极小点，所以有</p>
<script type="math/tex; mode=display">
<\triangledown f(\hat w^{(k)}), q^{(i)}> = 0, i=1,2,..</script></li>
<li><p>将$\hat w^{(k)}$由正交方向表示带入梯度，求出系数表达式</p>
</li>
<li><p>解精确搜索步长，得到$w^{(k+1)}$系数表达式</p>
</li>
</ul>
<h3 id="扩展子空间定理"><a href="#扩展子空间定理" class="headerlink" title="扩展子空间定理"></a>扩展子空间定理</h3><blockquote>
<ul>
<li>设目标函数为<script type="math/tex; mode=display">
  f(w) = \frac 1 2 x^T G x + r^T x + \sigma</script>  $d^{(1)}, \cdots, d^{(k)}$是$k, k \leq n$个非零正交方向
  ，从任意初始点$x^{(1)}$出发，依次沿着以上正交方向做
  <strong>精确一维搜索</strong>，得到$x^{(1)}, \cdots, x^{(k+1)}$，
  则$x^{(k+1)}$是$f(x)$在线性流形<script type="math/tex; mode=display">
  \bar x_k = \{x = x^{(1)} + \sum_{i=1}^k \alpha_i d^{(i)}
      | -\infty < \alpha_i < +\infty \}</script>  上的唯一极小点，特别的k=n时，$x^{(n+1)}$是$f(x)$在整个
  空间上的唯一极小点</li>
</ul>
</blockquote>
<ul>
<li>引进变换$w = \sqrt G x$即可证</li>
</ul>
<blockquote>
<ul>
<li>在以上假设下，有<script type="math/tex; mode=display">
  <\triangledown f(x^{(k+1)}), d^{(i)}> = 0, i=1,2...</script></li>
</ul>
</blockquote>
<h2 id="Conjugate-Gradient-Method"><a href="#Conjugate-Gradient-Method" class="headerlink" title="Conjugate Gradient Method"></a><em>Conjugate Gradient Method</em></h2><p>共轭梯度法</p>
<h3 id="对正定二次函数函数"><a href="#对正定二次函数函数" class="headerlink" title="对正定二次函数函数"></a>对正定二次函数函数</h3><script type="math/tex; mode=display">
f(x) = \frac 1 2 x^T G x + r^T x + \sigma</script><ul>
<li><p>任取初始点$x^{(1)}$，若$\triangledown f(x^{(1)}) = 0$，
停止计算，得到极小点$x^{(1)}$，否则取</p>
<script type="math/tex; mode=display">
d^{(1)} = -\triangledown f(x^{(1)})</script></li>
<li><p>沿着$d^{(1)}$方向进行精确一维搜索得到$x^{(2)}$，若
$\triangledown f(x^{(2)}) \neq 0$，令</p>
<script type="math/tex; mode=display">
d^{(2)} = -\triangledown f(x^{(2)}) + \beta_1^{(2)}
   d^{(1)}</script><p>且满足$(d^{(1)})^T G d^{(2)} = 0$，即二者共轭，可得</p>
<script type="math/tex; mode=display">
\beta_1^{(2)} = \frac {(d^{(1)})^T G \triangledown
   f(x^{(2)})} {((d^{(1)})^T G d^{(1)})}</script><ul>
<li>这里$d^{(2)}$方向的构造方式是为类似构造后面$d^{(k)}$
，得到能方便表示的系数</li>
<li>类似于将向量组$\triangledown f(x^{(i)})$正交化</li>
</ul>
</li>
<li><p>如此重复搜索，若$\triangledown f^(x^{i)}) \neq 0$，构造
$x^{(k)}$处搜索方向$d^{(k)}$如下</p>
<script type="math/tex; mode=display">\begin{align*}
0 & = (d^{(i)})^T G d^{(k)} \\
& = -(d^{(i)})T G \triangledown f(x^{(k)}) +
   \sum_{j=1}^{k-1} \beta_j^{(k)} (d^{(i)})^T G d^{(j)} \\
& = -(d^{(i)})^T G \triangledown f(x^{(k)}) +
   \beta_i^{(k)} (d^{(i)})^T G d^{(i)}
\end{align*}</script><p>可得</p>
<script type="math/tex; mode=display">
\beta_i^{(k)} = \frac {(d^{(i)})^T G \triangledown
   f(x^{(k)})} {(d^{(i)})^T G d^{(i)}}</script><p>此时$d^{(k)}$与前k-1个方向均关于G共轭，此k个方向是G的k个
共轭方向，由扩展空间子定理，$x^{(k+1)}$是整个空间上极小</p>
</li>
</ul>
<h4 id="计算公式简化"><a href="#计算公式简化" class="headerlink" title="计算公式简化"></a>计算公式简化</h4><p>期望简化$d^{(k)}$的计算公式</p>
<ul>
<li><p>由扩展子空间定理推论有
$\triangledown f(x^{(k)})^T d^{(i)} = 0, i=1,2…,k-1$
结合以上$d^{(k)}$的构造公式，有</p>
<script type="math/tex; mode=display">\begin{align*}
& \triangledown f(x^{(k)})^T \triangledown f(x^{(i)}) \\
= & \triangledown f(x^{(k)})^T ( -d^{(i)} +
   \beta_1^{(i)} d^{(1)} + \cdots +
   \beta_{i-1}^{(i)} d^{(i-1)} ) \\
= & 0, i=1,2,...,k-1
\end{align*}</script></li>
<li><p>则有</p>
<script type="math/tex; mode=display">\begin{align*}
(d^{(i)})^T G \triangledown f(x^{(k)}) & =
   \triangledown f(x^{(k)})^T G d^{(i)} \\
& = \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   G (x^{(i+1)} - x^{(i)}) \\
& = \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   (\triangledown f(x^{(i+1)}) -
   \triangledown f(x^{(i)})) \\
& = 0, i=1,2,\cdots,k-2
\end{align*}</script><blockquote>
<ul>
<li>$d^{(k)} = \frac 1 {\alpha_i} x^{(i+1)} - x^{(i)}$</li>
</ul>
</blockquote>
</li>
<li><p>所以上述$d^{(k)}$构造公式可以简化为</p>
<script type="math/tex; mode=display">
d^{(k)} = -\triangledown f(x^{(k)}) + \beta_{k-1}
   d^{(k-1)}</script></li>
<li><p>类似以上推导有</p>
<script type="math/tex; mode=display">\begin{align*}
(d^{(k-1)})^T G \triangledown f(x^{(k)}) & =
   \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)})) \\
& = \frac 1 {\alpha_i} \triangledown f(x^{(k)})^T
   \triangledown f(x^{(k)}) \\
\end{align*}</script><script type="math/tex; mode=display">\begin{align*}
(d^{(k-1)})^T G d^{(k-1)} & = \frac 1 {\alpha_i}
   (d^{(k-1)})^T (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)})) \\
& = -\frac 1 {\alpha_i} (d^{(k-1)})^T
   \triangledown f(x^{(x-1)}) \\
& = -\frac 1 {\alpha_i} (\triangledown f(x^{(k-1)}) -
   \beta_{k-2}d^{(k-2)})^T \triangledown f(x^{(x-1)}) \\
& = -\frac 1 {\alpha_i} \triangledown f(x^{(k-1)})^T
   \triangledown f(x^{(k-1)})
\end{align*}</script><p>最终的得到简化后系数$\beta_{k-1}, k&gt;1$的PRP公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\triangledown f(x^{(k)})^T
   (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)}))}
   {\triangledown f(x^{(k-1)})^T
       \triangledown f(x^{(k-1)})}</script><p>或FR公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\|\triangledown f(x^{(k)})\|^2}
   {\|\triangledown f(x^{(k-1)}) \|^2}</script></li>
</ul>
<blockquote>
<ul>
<li><p>以上推导虽然是根据正定二次函数得出的推导，但是仍适用于
  一般可微函数</p>
</li>
<li><p>$\beta _ {k-1}$给出两种计算方式，应该是考虑到目标函数
  可能不是标准正定二次函数、一维搜索数值计算不精确性</p>
</li>
<li><p>将$\beta _ {k-1}$分子、分母推导到不同程度可以得到其他
  公式</p>
</li>
</ul>
</blockquote>
<ul>
<li><p>Growder-Wolfe公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\triangledown f(x^{(k)})^T
   (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)}))}
   {(d^{(k-1)})^T (\triangledown f(x^{(k)}) -
   \triangledown f(x^{(k-1)}))}</script></li>
<li><p>Dixon公式</p>
<script type="math/tex; mode=display">
\beta_{k-1} = \frac {\triangledown f(x^{(k)})^T
   \triangledown f(x^{(k)})}
   {(d^{(k-1)})^T \triangledown f(x^{(k-1)})}</script></li>
</ul>
<h3 id="FR-PRP算法"><a href="#FR-PRP算法" class="headerlink" title="FR/PRP算法"></a>FR/PRP算法</h3><ol>
<li><p>初始点$x^{(1)}$、精度要求$\epsilon$，置k=1</p>
</li>
<li><p>若$|\triangledown f(x^{(k)}) | \leq \epsilon$，停止
计算，得到解$x^{(k)}$，否则置</p>
<script type="math/tex; mode=display">
d^{(k)} = -\triangledown f(x^{(k)}) + \beta_{k-1}d^{(k-1)}</script><p>其中$\beta_{k-1}=0, k=1$，或由上述公式计算</p>
</li>
<li><p>一维搜索，求解一维问题</p>
<script type="math/tex; mode=display">
\arg\min_{\alpha} \phi(\alpha) = f(x^{(k)} -
  \alpha d^{(k)})</script><p>得$\alpha_k$，置$x^{(k+1)} = x^{(k)} + \alpha_k d^{(k)}$</p>
</li>
<li><p>置k=k+1，转2</p>
</li>
</ol>
<blockquote>
<ul>
<li>实际计算中，n步重新开始的FR算法优于原始FR算法</li>
<li>PRP算法中
  $\triangledown f(x^{(k)}) \approx \triangledown f(x^{(k-1)})$
  时，有$\beta_{k-1} \approx 0$，即
  $d^{(k)} \approx -\triangledown f(x^{(k)})$，自动重新开始</li>
<li>试验表明，对大型问题，PRP算法优于FR算法</li>
</ul>
</blockquote>
<h3 id="共轭方向下降性"><a href="#共轭方向下降性" class="headerlink" title="共轭方向下降性"></a>共轭方向下降性</h3><blockquote>
<ul>
<li>设$f(x)$具有连续一阶偏导，假设一维搜索是精确的，使用共轭
  梯度法求解无约束问题，若$\triangledown f(x^{(k)}) \neq 0$
  则搜索方向$d^{(k)}$是$x^{(k)}$处的下降方向</li>
</ul>
</blockquote>
<ul>
<li>将$d^{(k)}$导入即可</li>
</ul>
<h3 id="算法二次终止性"><a href="#算法二次终止性" class="headerlink" title="算法二次终止性"></a>算法二次终止性</h3><blockquote>
<ul>
<li>若一维搜索是精确的，则共轭梯度法具有二次终止性</li>
</ul>
</blockquote>
<ul>
<li><p>对正定二次函数，共轭梯度法至多n步终止，否则</p>
<ul>
<li>目标函数不是正定二次函数</li>
<li>或目标函数没有进入正定二次函数区域，</li>
</ul>
</li>
<li><p>此时共轭没有意义，搜索方向应该重新开始，即令</p>
<script type="math/tex; mode=display">
d^{(k)} = -\triangledown f(x^{(k)})</script><p>即算法每n次重新开始一次，称为n步重新开始策略</p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2019-03-21T09:27:37.000Z" title="3/21/2019, 5:27:37 PM">2019-03-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2019-03-16T02:42:12.000Z" title="3/16/2019, 10:42:12 AM">2019-03-16</time></span><span class="level-item"><a class="link-muted" href="/categories/Math-Analysis/">Math Analysis</a><span> / </span><a class="link-muted" href="/categories/Math-Analysis/Optimization/">Optimization</a></span><span class="level-item">14 minutes read (About 2091 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Math-Analysis/Optimization/line_search.html">Line Search</a></h1><div class="content"><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p>一维搜索/线搜索：单变量函数最优化，即求一维问题</p>
<script type="math/tex; mode=display">
\arg\min _ {\alpha} \phi(\alpha) = f(x^{(k)} +
    \alpha d^{(k)})</script><p>最优解的$\alpha_k$的数值方法</p>
<ul>
<li><p><em>exact line search</em>：精确一维搜索，求得<strong>最优步长</strong>
$\alpha_k$使得目标函数沿着$d^{(k)}$方向达到极小，即</p>
</li>
<li><p><em>inexact line search</em>：非精确一维搜索，求得$\alpha_k$
使得</p>
<script type="math/tex; mode=display">\begin{align*}
f(x^{(k)} + \alpha_k d^{(k)}) & < f(x^{(k)}) \\
\phi(\alpha_k) & < \phi(0)
\end{align*}</script></li>
</ul>
<h3 id="一维搜索基本结构"><a href="#一维搜索基本结构" class="headerlink" title="一维搜索基本结构"></a>一维搜索基本结构</h3><ul>
<li>确定搜索区间</li>
<li>用某种方法缩小搜索区间</li>
<li>得到所需解</li>
</ul>
<h3 id="搜索区间"><a href="#搜索区间" class="headerlink" title="搜索区间"></a>搜索区间</h3><blockquote>
<ul>
<li>搜索区间：设$\alpha^{ <em> }$是$\phi(\alpha)$极小点，若存在
  <strong>闭区间</strong>$[a, b]$使得$\alpha^{ </em> } \in [a, b]$，则称
  $[a, b]$是$phi(\alpha)$的搜索区间</li>
</ul>
</blockquote>
<h4 id="确定搜索区间的进退法"><a href="#确定搜索区间的进退法" class="headerlink" title="确定搜索区间的进退法"></a>确定搜索区间的进退法</h4><ol>
<li><p>取初始步长$\alpha$，置初始值</p>
<script type="math/tex; mode=display">
\mu_3 = 0, \phi_3 = \phi(\mu_3), k = 0</script></li>
<li><p>置</p>
<script type="math/tex; mode=display">
\mu = \mu_3 + \alpha, \phi = \phi(\mu), k = k+1</script></li>
<li><p>若$\phi &lt; \phi_3$，置</p>
<script type="math/tex; mode=display">
\mu_2 = \mu_3, \phi_2 = \phi_3 \\
\mu_3 = \mu, \phi_3 = \phi \\
\alpha = 2\alpha, k = k+1</script></li>
<li><p>若k =1，置</p>
<script type="math/tex; mode=display">
\mu_2 = mu, \phi_2 = \phi, \alpha = -\alpha</script><p>转2，否则置</p>
<script type="math/tex; mode=display">
\mu_1 = \mu_2, \phi_1 = \phi_2 \\
\mu_2 = \mu_3, \phi_2 = \phi_3 \\
\mu_3 = \mu, \phi_3 = \phi</script><p>并令$a=min{\mu_1,\mu_3}, b=max{\mu_1,\mu_3}$，停止搜索</p>
</li>
</ol>
<blockquote>
<ul>
<li>通常认为目标函数此算法得到搜索区间就是单峰函数</li>
</ul>
</blockquote>
<h2 id="试探法"><a href="#试探法" class="headerlink" title="试探法"></a>试探法</h2><ul>
<li>在搜索区间内选择<strong>两个点</strong>，计算目标函数值<ul>
<li>需要获得两个点取值才能判断极值点的所属区间</li>
</ul>
</li>
<li>去掉<strong>函数值较大者至离其较近端点</strong>段</li>
</ul>
<h3 id="0-618法"><a href="#0-618法" class="headerlink" title="0.618法"></a>0.618法</h3><ol>
<li><p>置初始搜索区间$[a, b]$，置精度要求$\epsilon$，计算左右
试探点</p>
<script type="math/tex; mode=display">\begin{align*}
a_l & = a + (1 - \tau)(b - a) \\
a_r & = a + \tau(b - a)
\end{align*}</script><p>其中$\tau = \frac {\sqrt 5 - 1} 2$，及相应函数值</p>
<script type="math/tex; mode=display">\begin{align*}
\phi_l & = \phi(a_l) \\
\phi_r & = \phi(a_r)
\end{align*}</script></li>
<li><p>若$\phi_l&lt;\phi_r$，置</p>
<script type="math/tex; mode=display">
b= a_r, a_r = a_l, \phi_l = \phi_l</script><p>并计算</p>
<script type="math/tex; mode=display">
a_l = a + (1 - \tau)(b - a), \phi_l = \phi(a_l)</script><p>否则置</p>
<script type="math/tex; mode=display">
a = a_l, a_l = a_r, \phi_l = \phi_r</script><p>并计算</p>
<script type="math/tex; mode=display">
a_r = a + \tau(b - a), \phi_r = \phi(a_r)</script></li>
<li><p>若$|b - a| \geq \epsilon$</p>
<ul>
<li>若$\phi_l &lt; \phi_r$，置$\mu = a_l$</li>
<li>否则置$\mu = \alpha_r$
得到问题解$\mu$，否则转2</li>
</ul>
</li>
</ol>
<ul>
<li>0.618法除第一次外，每次只需要计算一个新试探点、一个新
函数值，大大提高了算法效率</li>
<li>收敛速率线性，收敛比为$\tau = \frac {\sqrt 5 - 1} 2$常数</li>
</ul>
<h3 id="Fibonacci方法"><a href="#Fibonacci方法" class="headerlink" title="Fibonacci方法"></a>Fibonacci方法</h3><ol>
<li><p>置初始搜索区间$[a, b]$，置精度要求$\epsilon$，选取分离
间隔$\sigma &lt; \epsilon$，求最小正整数n，使得
$F_n &gt; \frac {b - a} \epsilon$，计算左右试探点</p>
<p>$\begin{align<em>}
a<em>l &amp; = a + \frac {F</em>{n-2}} {F<em>n} (b - a)\
a_r &amp; = a + \frac {F</em>{n-1}} {F_n} (b - a)
\end{align</em>}</p>
</li>
<li><p>置n=n-1</p>
</li>
<li><p>若$\phi_l &lt; \phi_r$，置</p>
<script type="math/tex; mode=display">
b = a_r, a_r = a_l , \phi_r = \phi_l</script><ul>
<li><p>若n&gt;2，计算</p>
<script type="math/tex; mode=display">\begin{align*}
a_l & = a + \frac {F_{n-2}} {F_n} (b - a) \\
\phi_r & = \phi(a_l)
\end{align*}</script></li>
<li><p>否则计算</p>
<script type="math/tex; mode=display">\begin{align*}
a_l & = a_r - \sigma \\
\phi_l & = \phi(a_l)
\end{align*}</script></li>
</ul>
</li>
<li><p>若$\phi_l \geq \phi_r$，置</p>
<script type="math/tex; mode=display">
a = a_l, a_l = a_r , \phi_l = \phi_r</script><ul>
<li><p>若n&gt;2，计算</p>
<script type="math/tex; mode=display">\begin{align*}
a_l & = a + \frac {F_{n-1}} {F_n} (b - a) \\
\phi_r & = \phi(a_r)
\end{align*}</script></li>
<li><p>否则计算</p>
<script type="math/tex; mode=display">\begin{align*}
a_r & = a_l + \sigma \\
\phi_r & = \phi(a_r)
\end{align*}</script></li>
</ul>
</li>
<li><p>若n=1</p>
<ul>
<li>若$\phi_l &lt; \phi_r$，置$\mu = a_r$</li>
<li>否则置$\mu = a_r$</li>
</ul>
<p>得到极小点$\mu$，停止计算，否则转2</p>
</li>
</ol>
<ul>
<li>Finonacci方法是选取实验点的最佳策略，即在实验点个数相同
情况下，最终的极小区间最小的策略</li>
</ul>
<blockquote>
<ul>
<li>Finonacci法最优性质可通过设最终区间长度为1，递推使得原始
  估计区间最大的取实验点方式，得出</li>
</ul>
</blockquote>
<h2 id="插值法"><a href="#插值法" class="headerlink" title="插值法"></a>插值法</h2><ul>
<li>利用搜索区间上某点的信息构造插值多项式（通常不超过3次）
$\hat \phi(\alpha)$</li>
<li>逐步用$\hat \phi(\alpha)$的极小点逼近$\phi(\alpha)$
极小点$\alpha^{*}$</li>
</ul>
<blockquote>
<ul>
<li>$\phi^{ * }$解析性质比较好时，插值法较试探法效果好</li>
</ul>
</blockquote>
<h3 id="三点二次插值法"><a href="#三点二次插值法" class="headerlink" title="三点二次插值法"></a>三点二次插值法</h3><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>以过三个点$(\mu_1,\phi_1), (\mu_2,\phi_2), (\mu_3,\phi_3)$
的二次插值函数逼近目标函数</p>
<script type="math/tex; mode=display">\begin{align*}
\hat \phi(\alpha) & = \phi_1 \frac {(\alpha - \mu_2)
    (\alpha - \mu_3)} {(\mu_1 - \mu_2)(\mu_1 - \mu_3)} \\
& + \phi_2 \frac {(\alpha - \mu_1) (\alpha - \mu_3)}
    {(\mu_2 - \mu_1)(\mu_2 - \mu_3)} \\
& + \phi_3 \frac {(\alpha - \mu_1) (\alpha - \mu_2)}
    {(\mu_3 - \mu_1)(\mu_3 - \mu_2)}
\end{align*}</script><ul>
<li><p>求导，得到$\hat \phi(\alpha)$的极小点</p>
<script type="math/tex; mode=display">
\mu = \frac {2[\phi_1(\mu_2-\mu_3) + \phi_2(\mu_3-\mu_1)
   + \phi_3(\mu_1 - \mu_2)]}
   {[\phi_1 (\mu_2^2-\mu_3^2) + \phi_2(\mu_3^2-\mu_1^2)
   + \phi_3(\mu_1^2 - \mu_2^2)]}</script></li>
<li><p>若插值结果不理想，继续构造插值函数求极小点近似值</p>
</li>
</ul>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>取初始点$\mu_1&lt;\mu_2&lt;\mu_3$，计算$\phi_i=\phi(\mu_i)$，
且满足$\phi_1 &gt; \phi_2, \phi_3 &gt; \phi_2$，置精度要求
$\epsilon$</p>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">
A = 2[\phi_1(\mu_2 - \mu_3) + \phi_2(\mu_3 - \mu_1) +
  \phi_3(\mu_1 - \mu_2)]</script><ul>
<li>若A=0，置$\mu = \mu_2, \phi = \phi_2$，停止计算，
输出$\mu, \phi$</li>
</ul>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">
\mu = [\phi_1 (\mu_2^2 - \mu_3^2) + \phi_2(\mu_3^2 -
  \mu_1^2) + \phi_3(\mu_1^2 - \mu_2^2)] / A</script><ul>
<li>若$\mu&lt;\mu_1 或 \mu&gt;\mu_3,\mu \notin (\mu_1,\mu_3)$
，停止计算，输出$\mu, \phi$</li>
</ul>
</li>
<li><p>计算$\phi = \phi(\mu)$，若$|\mu - \mu_2| &lt; \epsilon$，
停止计算，得到极小点$\mu$</p>
</li>
<li><p>若$\mu \in (\mu_2, \mu_3)$</p>
<ul>
<li>若$\phi &lt; \phi_2$，置<script type="math/tex; mode=display">
\mu_1=\mu_2, \phi_1=\phi_2, \mu_2=\mu, \phi_2=\phi</script></li>
<li>否则置<script type="math/tex; mode=display">\mu_3 = \mu, \phi_3 = \phi</script></li>
</ul>
<p>否则</p>
<ul>
<li><p>若$\phi &lt; \phi_2$，置</p>
<script type="math/tex; mode=display">
\mu_3=\mu_2, \phi_3=\phi_2, \mu_2=\mu, \phi_2=\phi</script></li>
<li><p>否则置</p>
<script type="math/tex; mode=display">
\mu_1 = \mu, \phi_1 = \phi</script></li>
</ul>
</li>
<li><p>转2</p>
</li>
</ol>
<h3 id="两点二次插值法"><a href="#两点二次插值法" class="headerlink" title="两点二次插值法"></a>两点二次插值法</h3><h4 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h4><p>以$\phi(\alpha)$在两点处$\mu_1, \mu_2$函数值
$\phi_1=\phi(\mu_1)$、一点处导数值
$\phi_1^{‘}=\phi^{‘}(\mu_1) &lt; 0$构造二次函数逼近原函数</p>
<script type="math/tex; mode=display">\begin{align*}
\hat \phi(\alpha) & = A(\alpha - \mu_1)^2 + B(\alpha - \mu_1)
    + C \\
A & = \frac {\phi_2 - \phi_1 - \phi_1^{'}(\mu_2 - \mu_1)}
    {(\mu_2 - \mu_1)^2} \\
B & = \phi_1^{'} \\
C & = \phi_1
\end{align*}</script><ul>
<li><p>为保证$[\mu_1, \mu_2]$中极小点，须有
$\phi_2 &gt; \phi_1 + \phi_1^{‘}(\mu_2 - \mu_1)$</p>
</li>
<li><p>求解，得到$\hat \phi (\mu)$极小值为</p>
<script type="math/tex; mode=display">
\mu = \mu_1 - \frac {\phi_1^{'}(\mu_2 - \mu_1)^2}
   {2[\phi_2 - \phi_1 - \phi_1^{'}(\mu_2 - \mu_1)]}</script></li>
<li><p>若插值不理想，继续构造插值函数求极小点的近似值</p>
</li>
</ul>
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始点$\mu_1$、初始步长$d$、步长缩减因子$\rho$、精度要求
$\epsilon$，计算</p>
<script type="math/tex; mode=display">\phi_1 = \phi(\mu_1), \phi_2 = \phi_(\mu_2)</script></li>
<li><p>若$\phi_1^{‘} &lt; 0$，置$d = |d|$，否则置$d = -|d|$</p>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">\mu_2 = \mu_1 + d, \phi_2 = \phi(\mu_2)</script></li>
<li><p>若$\phi_2 \leq \phi_1 + \phi_1^{‘}(\mu_2 - \mu_1)$，置
$d = 2d$，转3</p>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">
\mu = \mu_1 - \frac {\phi_1^{'}(\mu_2 - \mu_1)^2}
  {2[\phi_2 - \phi_1 - \phi_1^{'}(\mu_2 - \mu_1)]} \\
\phi = \phi(\mu), \phi^{'} = \phi^{'}(\mu)</script></li>
<li><p>若$|phi^{‘}| \leq \epsilon$，停止计算，得到极小点$\mu$，
否则置</p>
<script type="math/tex; mode=display">\mu_1 = \mu, \phi_1 = \phi, \phi_1^{'} = \phi^{'},
  \alpha = \rho \alpha</script></li>
</ol>
<blockquote>
<ul>
<li>其中通常取$d = 1, \rho = 0.1$</li>
</ul>
</blockquote>
<h3 id="两点三次插值法"><a href="#两点三次插值法" class="headerlink" title="两点三次插值法"></a>两点三次插值法</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>以两点$\mu_1, \mu_2$处函数值$\phi_i = \phi(\mu_i)$和其导数值
$\phi_i^{‘} = \phi^{‘}(\mu_i)$，由<em>Himiter</em>插值公式可以构造
三次插值多项式$\hat \phi(\alpha)$</p>
<ul>
<li><p>求导置0，得到$\hat \phi(\alpha)$极小点</p>
<script type="math/tex; mode=display">\begin{align*}
\mu & = \mu_1 + (\mu_2 - \mu_1)(1 - \frac {\phi_2^{'}
   + w + z} {\phi_2^{'} - \phi_1^{'} + 2w}) \\
z & = \frac {3(\phi_2 - \phi_1)} {\mu_2 - \mu_1} -
   \phi_1^{'} - \phi_2^{'} \\
w & = sign(\mu_2 - \mu_1) \sqrt {z^2 -
   \phi_1^{'} \phi_2^{'}}
\end{align*}</script></li>
</ul>
<h4 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始值$\mu_1$、初始步长$d$、步长缩减因子$\rho$、精度要求
$\epsilon$，计算</p>
<script type="math/tex; mode=display">\phi_1 = \phi(\mu_1), \phi_1^{'} = \phi^{'}(\mu_1)</script></li>
<li><p>若$\phi_1^{‘} &gt; 0$，置$d = -|d|$，否则置$d = |d|$</p>
</li>
<li><p>置$\mu_2 = \mu_1 + \alpha$，计算</p>
<script type="math/tex; mode=display">\phi_2 = \phi(\mu_2), \phi_2^{'} = \phi^{'}(\mu_2)</script></li>
<li><p>若$\phi_1^{‘} \phi_2{‘} &gt; 0$，置</p>
<script type="math/tex; mode=display">
d = 2d, \mu_1 = \mu_2, \phi_1 = \phi_2,
  \phi_1^{'} = \phi_2^{'}</script><p>转3</p>
</li>
<li><p>计算</p>
<script type="math/tex; mode=display">\begin{align*}
\mu & = \mu_1 + (\mu_2 - \mu_1)(1 - \frac {\phi_2^{'}
  + w + z} {\phi_2^{'} - \phi_1^{'} + 2w}) \\
z & = \frac {3(\phi_2 - \phi_1)} {\mu_2 - \mu_1} -
  \phi_1^{'} - \phi_2^{'} \\
w & = sign(\mu_2 - \mu_1) \sqrt {z^2 -
  \phi_1^{'} \phi_2^{'}} \\
\phi & = \phi(\mu) \\
\phi^{'} = \phi^{'}(\mu)
\end{align*}</script></li>
<li><p>若$|\phi^{‘}| &lt; \epsilon$，停止计算，得到极小点$\mu$，
否则置</p>
<script type="math/tex; mode=display">
d = \rho d, \mu_1 = \mu, \phi_1 = \phi,
  \phi_1^{'} = \phi^{'}</script><p>转2</p>
</li>
</ol>
<blockquote>
<ul>
<li>通常取$d = 1, \rho = 0.1$</li>
</ul>
</blockquote>
<h2 id="非精确一维搜索"><a href="#非精确一维搜索" class="headerlink" title="非精确一维搜索"></a>非精确一维搜索</h2><ul>
<li><p>对无约束问题整体而言，又是不要求得到极小点，只需要一定
下降量，缩短一维搜索时间，使整体效果最好</p>
</li>
<li><p>求满足$\phi(\mu) &lt; \phi(0)$、大小合适的$\mu$</p>
<ul>
<li>$\mu$过大容易不稳定</li>
<li>$\mu$过小速度慢</li>
</ul>
</li>
</ul>
<h3 id="GoldStein方法"><a href="#GoldStein方法" class="headerlink" title="GoldStein方法"></a>GoldStein方法</h3><h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><ul>
<li><p>预先指定精度要求$0&lt; \beta_1 &lt; \beta_2 &lt; 1$</p>
</li>
<li><p>以以下不等式限定步长</p>
<script type="math/tex; mode=display">\begin{align*}
\phi(\mu) & \leq \phi(0) + \mu\beta_1 \phi^{'}(0) \\
\phi(\mu) & \geq \phi(0) + \mu\beta_2 \phi^{'}(0)
\end{align*}</script></li>
</ul>
<p><img src="/imgs/line_search_goldstein.png" alt="line_search_goldstein"></p>
<h4 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h4><ol>
<li><p>初始试探点$\mu$，置$\mu<em>{min} = 0, \mu</em>{max} = \infty$，
置精度要求$0 &lt; \beta_1 &lt; \beta_2 &lt; 1$</p>
</li>
<li><p>对$\phi(mu)$</p>
<ul>
<li><p>若$\phi(\mu) &gt; \phi(0) + \beta<em>1 \phi^{‘}(0) \mu$，
置$\mu</em>{max} = \mu$</p>
</li>
<li><p>否则若$\phi(\mu) &gt; \phi(0) + \beta_2 \phi^{‘}(0)\mu$
，则停止计算，得到非精确最优解$\mu$</p>
</li>
<li><p>否则置$\mu_{min} = \mu$</p>
</li>
</ul>
</li>
<li><p>若$\mu<em>{max} &lt; \infty$，置
$\mu = \frac 1 2 (\mu</em>{min} + \mu<em>{max})$，否则置
$\mu = 2 \mu</em>{min}$</p>
</li>
<li><p>转2</p>
</li>
</ol>
<h3 id="Armijo方法"><a href="#Armijo方法" class="headerlink" title="Armijo方法"></a>Armijo方法</h3><p>Armijo方法是Goldstein方法的变形</p>
<ul>
<li><p>预先取$M &gt; 1, 0 &lt; \beta_1 &lt; 1$</p>
</li>
<li><p>选取$\mu$使得其满足以下，而$M\mu$不满足</p>
<script type="math/tex; mode=display">\phi(\mu) \leq \phi(0) + \mu \beta_1 \phi^{'}(0)</script></li>
</ul>
<blockquote>
<ul>
<li>M通常取2至10</li>
</ul>
</blockquote>
<p><img src="/imgs/line_search_armijo.png" alt="line_search_armijo"></p>
<h3 id="Wolfe-Powell方法"><a href="#Wolfe-Powell方法" class="headerlink" title="Wolfe-Powell方法"></a>Wolfe-Powell方法</h3><ul>
<li><p>预先指定参数$0 &lt; \beta_1 &lt; \beta_2 &lt;1$</p>
</li>
<li><p>选取$\mu$满足</p>
<script type="math/tex; mode=display">\begin{align*}
\phi(\mu) & \leq \phi(0) + \mu \beta_1 \phi^{'}(0) \\
\phi^{'}(\mu) & \geq \beta_2 \phi^{'}(0)
\end{align*}</script></li>
</ul>
<blockquote>
<ul>
<li>能保证可接受解中包含最优解，而Goldstein方法不能保证</li>
</ul>
</blockquote>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/tags/Analysis/page/2/">Previous</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/tags/Analysis/page/4/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/tags/Analysis/">1</a></li><li><a class="pagination-link" href="/tags/Analysis/page/2/">2</a></li><li><a class="pagination-link is-current" href="/tags/Analysis/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">36</span></span></a><ul><li><a class="level is-mobile" href="/categories/Algorithm/Data-Structure/"><span class="level-start"><span class="level-item">Data Structure</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Heuristic/"><span class="level-start"><span class="level-item">Heuristic</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Issue/"><span class="level-start"><span class="level-item">Issue</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Problem/"><span class="level-start"><span class="level-item">Problem</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithm/Specification/"><span class="level-start"><span class="level-item">Specification</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/C-C/"><span class="level-start"><span class="level-item">C/C++</span></span><span class="level-end"><span class="level-item tag">34</span></span></a><ul><li><a class="level is-mobile" href="/categories/C-C/Cppref/"><span class="level-start"><span class="level-item">Cppref</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/Cstd/"><span class="level-start"><span class="level-item">Cstd</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/MPI/"><span class="level-start"><span class="level-item">MPI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/C-C/STL/"><span class="level-start"><span class="level-item">STL</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/CS/"><span class="level-start"><span class="level-item">CS</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/CS/Character/"><span class="level-start"><span class="level-item">Character</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Parallel/"><span class="level-start"><span class="level-item">Parallel</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Program-Design/"><span class="level-start"><span class="level-item">Program Design</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/CS/Storage/"><span class="level-start"><span class="level-item">Storage</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Daily-Life/"><span class="level-start"><span class="level-item">Daily Life</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Daily-Life/Maxism/"><span class="level-start"><span class="level-item">Maxism</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">27</span></span></a><ul><li><a class="level is-mobile" href="/categories/Database/Hadoop/"><span class="level-start"><span class="level-item">Hadoop</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/SQL-DB/"><span class="level-start"><span class="level-item">SQL DB</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Database/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Java/Scala/"><span class="level-start"><span class="level-item">Scala</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">42</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Bash-Programming/"><span class="level-start"><span class="level-item">Bash Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Configuration/"><span class="level-start"><span class="level-item">Configuration</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/File-System/"><span class="level-start"><span class="level-item">File System</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/IPC/"><span class="level-start"><span class="level-item">IPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Process-Schedual/"><span class="level-start"><span class="level-item">Process Schedual</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Shell/"><span class="level-start"><span class="level-item">Shell</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Linux/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">14</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/Tool/Vi/"><span class="level-start"><span class="level-item">Vi</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Model/"><span class="level-start"><span class="level-item">ML Model</span></span><span class="level-end"><span class="level-item tag">21</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Model/Linear-Model/"><span class="level-start"><span class="level-item">Linear Model</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Model-Component/"><span class="level-start"><span class="level-item">Model Component</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Nolinear-Model/"><span class="level-start"><span class="level-item">Nolinear Model</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Model/Unsupervised-Model/"><span class="level-start"><span class="level-item">Unsupervised Model</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/"><span class="level-start"><span class="level-item">ML Specification</span></span><span class="level-end"><span class="level-item tag">17</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/"><span class="level-start"><span class="level-item">Click Through Rate</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/Click-Through-Rate/Recommandation-System/"><span class="level-start"><span class="level-item">Recommandation System</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Computer-Vision/"><span class="level-start"><span class="level-item">Computer Vision</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/"><span class="level-start"><span class="level-item">FinTech</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Specification/FinTech/Risk-Control/"><span class="level-start"><span class="level-item">Risk Control</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Specification/Graph-Analysis/"><span class="level-start"><span class="level-item">Graph Analysis</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Specification/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Technique/"><span class="level-start"><span class="level-item">ML Technique</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Technique/Feature-Engineering/"><span class="level-start"><span class="level-item">Feature Engineering</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Technique/Neural-Network/"><span class="level-start"><span class="level-item">Neural Network</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/ML-Theory/"><span class="level-start"><span class="level-item">ML Theory</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul><li><a class="level is-mobile" href="/categories/ML-Theory/Loss/"><span class="level-start"><span class="level-item">Loss</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Model-Enhencement/"><span class="level-start"><span class="level-item">Model Enhencement</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/ML-Theory/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Algebra/"><span class="level-start"><span class="level-item">Math Algebra</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Algebra/Linear-Algebra/"><span class="level-start"><span class="level-item">Linear Algebra</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Algebra/Universal-Algebra/"><span class="level-start"><span class="level-item">Universal Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Analysis/"><span class="level-start"><span class="level-item">Math Analysis</span></span><span class="level-end"><span class="level-item tag">23</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Analysis/Fourier-Analysis/"><span class="level-start"><span class="level-item">Fourier Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Functional-Analysis/"><span class="level-start"><span class="level-item">Functional Analysis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Optimization/"><span class="level-start"><span class="level-item">Optimization</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Analysis/Real-Analysis/"><span class="level-start"><span class="level-item">Real Analysis</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Math-Mixin/"><span class="level-start"><span class="level-item">Math Mixin</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Math-Mixin/Statistics/"><span class="level-start"><span class="level-item">Statistics</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Math-Mixin/Time-Series/"><span class="level-start"><span class="level-item">Time Series</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Probability/"><span class="level-start"><span class="level-item">Probability</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">89</span></span></a><ul><li><a class="level is-mobile" href="/categories/Python/Cookbook/"><span class="level-start"><span class="level-item">Cookbook</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Jupyter/"><span class="level-start"><span class="level-item">Jupyter</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Keras/"><span class="level-start"><span class="level-item">Keras</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Matplotlib/"><span class="level-start"><span class="level-item">Matplotlib</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Numpy/"><span class="level-start"><span class="level-item">Numpy</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pandas/"><span class="level-start"><span class="level-item">Pandas</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3Ref/"><span class="level-start"><span class="level-item">Py3Ref</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Py3std/"><span class="level-start"><span class="level-item">Py3std</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Pywin32/"><span class="level-start"><span class="level-item">Pywin32</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Readme/"><span class="level-start"><span class="level-item">Readme</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/TensorFlow/"><span class="level-start"><span class="level-item">TensorFlow</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Python/Twists/"><span class="level-start"><span class="level-item">Twists</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/RLang/"><span class="level-start"><span class="level-item">RLang</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Rust/"><span class="level-start"><span class="level-item">Rust</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Set/"><span class="level-start"><span class="level-item">Set</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/"><span class="level-start"><span class="level-item">Tool</span></span><span class="level-end"><span class="level-item tag">13</span></span></a><ul><li><a class="level is-mobile" href="/categories/Tool/Editor/"><span class="level-start"><span class="level-item">Editor</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Markup-Language/"><span class="level-start"><span class="level-item">Markup Language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Web-Browser/"><span class="level-start"><span class="level-item">Web Browser</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tool/Windows/"><span class="level-start"><span class="level-item">Windows</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Web/"><span class="level-start"><span class="level-item">Web</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Web/CSS/"><span class="level-start"><span class="level-item">CSS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/NPM/"><span class="level-start"><span class="level-item">NPM</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Proxy/"><span class="level-start"><span class="level-item">Proxy</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Web/Thrift/"><span class="level-start"><span class="level-item">Thrift</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="UBeaRLy"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">UBeaRLy</p><p class="is-size-6 is-block">Protector of Proxy</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">392</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">93</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">522</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/xyy15926" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/xyy15926"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-04T15:07:54.896Z">2021-08-04</time></p><p class="title"><a href="/uncategorized/README.html"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T07:46:51.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/hexo_config.html">Hexo 建站</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-03T02:32:45.000Z">2021-08-03</time></p><p class="title"><a href="/Web/NPM/config.html">NPM 总述</a></p><p class="categories"><a href="/categories/Web/">Web</a> / <a href="/categories/Web/NPM/">NPM</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-02T08:11:11.000Z">2021-08-02</time></p><p class="title"><a href="/Python/Py3std/internet_data.html">互联网数据</a></p><p class="categories"><a href="/categories/Python/">Python</a> / <a href="/categories/Python/Py3std/">Py3std</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-29T13:55:00.000Z">2021-07-29</time></p><p class="title"><a href="/Linux/Shell/sh_apps.html">Shell 应用程序</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/Shell/">Shell</a></p></div></article></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5385776267343559" data-ad-slot="6995841235" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="UBeaRLy" height="28"></a><p class="is-size-7"><span>&copy; 2021 UBeaRLy</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/xyy15926/proxy"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>